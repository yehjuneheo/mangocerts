[
    {
        "Question Number": "1",
        "Situation": "데이터 엔지니어가 고가용성과 확장성이 필요한 머신러닝 프로젝트를 위해 구조화된 데이터와 반구조화된 데이터를 저장하는 임무를 맡았습니다. 데이터는 자주 접근되며 복잡한 쿼리를 지원해야 합니다. 엔지니어는 프로젝트 요구 사항에 가장 적합한 다양한 저장 옵션을 고려하고 있습니다.",
        "Question": "이 시나리오에 가장 적합한 저장 매체는 무엇입니까?",
        "Options": {
            "1": "Amazon S3",
            "2": "Amazon EBS",
            "3": "Amazon EFS",
            "4": "Amazon RDS"
        },
        "Correct Answer": "Amazon RDS",
        "Explanation": "Amazon RDS (관계형 데이터베이스 서비스)는 복잡한 쿼리 기능과 고가용성이 필요한 구조화된 데이터에 가장 적합한 옵션입니다. 다양한 관계형 데이터베이스 엔진을 지원하며 자동 백업, 확장 및 복제를 제공하여 자주 접근하고 복잡한 쿼리가 필요한 시나리오에 이상적입니다.",
        "Other Options": [
            "Amazon S3는 주로 객체 저장을 위해 설계되었으며 복잡한 쿼리에 최적화되어 있지 않아 자주 접근하고 복잡한 쿼리가 필요한 구조화된 데이터에는 덜 적합합니다.",
            "Amazon EBS (탄력적 블록 스토어)는 EC2 인스턴스의 파일 시스템으로 일반적으로 사용되는 블록 저장소로, 높은 성능을 제공하지만 구조화된 데이터와 반구조화된 데이터에 필요한 쿼리 기능을 제공하지 않습니다.",
            "Amazon EFS (탄력적 파일 시스템)는 여러 EC2 인스턴스 간에 공유 저장소로 사용할 수 있는 파일 저장 서비스이지만, 구조화된 데이터와 복잡한 쿼리를 효과적으로 처리하는 데 필요한 데이터베이스 기능이 부족합니다."
        ]
    },
    {
        "Question Number": "2",
        "Situation": "소매 회사가 고객 이탈을 예측하여 적시에 개입하기를 원합니다. 그들은 고객 인구 통계, 거래 기록 및 고객 서비스 상호작용을 포함하는 데이터 세트를 보유하고 있습니다. 머신러닝 전문가가 이 이진 분류 작업에 적합한 알고리즘을 선택해야 합니다.",
        "Question": "이 시나리오에서 고객 이탈을 예측하는 데 가장 적합한 알고리즘은 무엇입니까?",
        "Options": {
            "1": "Random Forest",
            "2": "Linear Regression",
            "3": "K-Means Clustering",
            "4": "Support Vector Machine (SVM)"
        },
        "Correct Answer": "Random Forest",
        "Explanation": "Random Forest는 분류 작업에 효과적인 앙상블 학습 방법으로, 복잡한 데이터 세트를 처리할 때 특히 유용합니다. 숫자형 및 범주형 데이터를 모두 처리할 수 있으며 과적합에 강해 고객 이탈 예측에 적합한 선택입니다.",
        "Other Options": [
            "Support Vector Machine (SVM)은 강력한 분류 알고리즘이지만, 매개변수 조정이 필요할 수 있으며 Random Forest와 같은 앙상블 방법에 비해 대규모 데이터 세트에서 성능이 떨어질 수 있습니다.",
            "Linear Regression은 연속 출력을 예측하므로 이진 분류 작업에 적합하지 않으며, 고객 이탈 예측에 부적합한 선택입니다.",
            "K-Means Clustering은 클러스터링을 위한 비지도 학습 알고리즘으로, 분류 작업을 위해 설계되지 않았으므로 고객 이탈 예측에 직접 사용할 수 없습니다."
        ]
    },
    {
        "Question Number": "3",
        "Situation": "데이터 과학자가 고객 구매 행동을 예측하는 머신러닝 모델을 위한 데이터 세트를 준비하고 있습니다. 데이터 세트에는 연속형 특성인 나이가 포함되어 있으며, 데이터 과학자는 모델 성능을 향상시키기 위해 나이를 구간으로 나누고자 합니다. 그들은 이 목표를 달성하기 위해 다양한 구간 나누기 기법을 고려하고 있습니다.",
        "Question": "데이터 과학자가 각 구간에 동일한 수의 레코드를 포함시키고 싶다면 어떤 구간 나누기 기법을 사용해야 합니까?",
        "Options": {
            "1": "구간의 값 범위가 동일한 Equal-width binning.",
            "2": "레코드의 빈도가 구간마다 균일한 Equal-frequency binning.",
            "3": "도메인 지식에 기반하여 구간을 정의하는 Custom binning.",
            "4": "각 구간에 동일한 수의 레코드가 있는 Quantile binning."
        },
        "Correct Answer": "각 구간에 동일한 수의 레코드가 있는 Quantile binning.",
        "Explanation": "Quantile binning은 각 구간에 동일한 수의 관측치를 포함하도록 설계되어 있어 모델링 목적을 위한 데이터 분포 균형에 유용합니다.",
        "Other Options": [
            "Equal-width binning은 데이터의 범위를 동일한 크기의 구간으로 나누지만, 각 구간에 동일한 수의 레코드가 포함되지 않을 수 있어 데이터 분포가 불균형해질 수 있습니다.",
            "Custom binning은 특정 도메인 기준에 따라 구간을 설정할 수 있지만, 구간 간의 균등한 표현을 보장하지 않으므로 일부 구간에 레코드가 너무 많거나 너무 적을 수 있습니다.",
            "Equal-frequency binning은 구간 나누기 기법에서 표준 용어가 아니며 혼란을 초래할 수 있습니다; quantile binning이 구간 내에서 동일한 수의 분포를 보장하는 올바른 용어입니다."
        ]
    },
    {
        "Question Number": "4",
        "Situation": "머신 러닝 전문가가 소매 회사의 판매를 예측하기 위해 트리 기반 알고리즘을 사용하여 예측 모델을 구축하고 있습니다. 전문가는 모델의 적절한 구성, 특히 트리의 수와 각 트리의 최대 깊이에 대해 결정해야 합니다.",
        "Question": "어떤 구성이 모델의 성능을 개선하면서 과적합을 피할 가능성이 가장 높습니까?",
        "Options": {
            "1": "모든 데이터 패턴을 포착하기 위해 다양한 깊이를 가진 매우 많은 수의 트리를 사용합니다.",
            "2": "편향과 분산의 균형을 맞추기 위해 적절한 수의 트리와 중간 깊이를 사용합니다.",
            "3": "단순성을 보장하기 위해 적은 수의 트리와 얕은 수준을 사용합니다.",
            "4": "최대 정확도를 위해 많은 수의 트리와 깊은 수준을 사용합니다."
        },
        "Correct Answer": "편향과 분산의 균형을 맞추기 위해 적절한 수의 트리와 중간 깊이를 사용합니다.",
        "Explanation": "적절한 수의 트리와 중간 깊이를 사용하면 편향과 분산을 효과적으로 균형 있게 유지할 수 있어 과적합의 위험을 줄이면서 데이터의 필수 패턴을 포착할 수 있습니다.",
        "Other Options": [
            "많은 수의 트리와 깊은 수준은 과적합으로 이어질 수 있으며, 모델이 훈련 데이터에서 너무 많은 노이즈를 학습하여 새로운 데이터에 잘 일반화되지 못하게 됩니다.",
            "적은 수의 트리와 얕은 수준은 모델이 데이터 세트 내의 복잡한 패턴을 포착하지 못해 성능이 저하되는 언더피팅을 초래할 수 있습니다.",
            "다양한 깊이를 가진 매우 많은 수의 트리를 사용하는 것은 모델을 복잡하게 만들고 계산 시간을 증가시킬 수 있으며, 성능을 크게 개선하지 못하고 종종 수익 감소로 이어집니다."
        ]
    },
    {
        "Question Number": "5",
        "Situation": "금융 서비스 회사가 과거 거래 데이터를 분석하여 미래 고객의 지출 행동을 예측하고 있습니다. 그들은 과거 거래 및 인구 통계 데이터를 기반으로 각 고객이 다음 분기에 얼마나 지출할 가능성이 있는지를 이해하는 데 도움이 되는 모델을 만들고자 합니다.",
        "Question": "과거 데이터를 기반으로 미래 지출 금액을 예측하는 데 가장 적합한 머신 러닝 모델 유형은 무엇입니까?",
        "Options": {
            "1": "분류",
            "2": "군집화",
            "3": "추천",
            "4": "회귀"
        },
        "Correct Answer": "회귀",
        "Explanation": "회귀 모델은 입력 특성을 기반으로 연속적인 수치 값을 예측하도록 특별히 설계되었습니다. 이 시나리오에서 회사는 지출 금액을 예측하는 데 관심이 있으며, 이는 연속 변수이므로 회귀가 가장 적합한 선택입니다.",
        "Other Options": [
            "군집화는 유사한 항목을 특성 유사성에 따라 그룹화하는 데 사용됩니다. 값 예측이 아니라 데이터를 클러스터로 분류하므로 지출 금액 예측에는 적용되지 않습니다.",
            "분류 모델은 범주형 결과를 예측하는 데 사용됩니다. 이 경우 목표는 연속적인 수치 값(지출 금액)을 예측하는 것이므로 분류는 적합하지 않습니다.",
            "추천 시스템은 사용자의 선호도나 과거 행동에 따라 항목을 제안하도록 설계되었습니다. 지출 습관과 간접적으로 관련이 있을 수 있지만, 지출 금액과 같은 특정 수치 값을 예측하는 데 집중하지 않습니다."
        ]
    },
    {
        "Question Number": "6",
        "Situation": "데이터 엔지니어가 머신 러닝 프로젝트를 위해 대량의 데이터를 처리하는 업무를 맡고 있습니다. 팀은 분산 데이터 처리를 위한 다양한 도구를 고려하고 있으며, 대규모 데이터 세트를 효율적으로 처리할 수 있는 솔루션이 필요합니다.",
        "Question": "데이터 엔지니어가 머신 러닝 파이프라인에서 데이터 처리를 용이하게 하기 위해 사용할 수 있는 도구는 무엇입니까? (두 개 선택)",
        "Options": {
            "1": "Apache Airflow",
            "2": "Apache Hive",
            "3": "Apache Cassandra",
            "4": "Apache Spark",
            "5": "Apache Flink"
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Apache Spark",
            "Apache Hive"
        ],
        "Explanation": "Apache Spark와 Apache Hive는 모두 대규모 데이터 세트를 처리하도록 설계되어 머신 러닝 파이프라인에서 데이터 처리를 위한 훌륭한 선택입니다. Spark는 빠른 데이터 처리 기능을 제공하며 다양한 머신 러닝 라이브러리를 지원하고, Hive는 분산 저장 시스템에 저장된 대규모 데이터 세트를 쿼리하기 위한 SQL 유사 인터페이스를 제공합니다.",
        "Other Options": [
            "Apache Airflow는 복잡한 워크플로우를 관리하고 작업을 예약하기 위한 오케스트레이션 도구로, 데이터 처리 엔진이 아닙니다.",
            "Apache Cassandra는 고가용성과 확장성을 위해 설계된 NoSQL 데이터베이스이지만, 머신 러닝을 위한 데이터 처리 도구는 아닙니다.",
            "Apache Flink는 스트림 처리 프레임워크이지만, 머신 러닝 파이프라인에서 배치 처리와 관련하여 Spark와 Hive에 비해 덜 일반적으로 사용됩니다."
        ]
    },
    {
        "Question Number": "7",
        "Situation": "기계 학습 엔지니어가 다중 클래스 분류 문제를 위한 신경망을 설계하고 있습니다. 엔지니어는 모델의 출력층과 은닉층에 대한 다양한 활성화 함수를 평가하고 있습니다.",
        "Question": "엔지니어가 은닉층과 출력층에 대해 고려해야 할 활성화 함수는 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "은닉층에 대한 ReLU",
            "2": "은닉층에 대한 이진 단계 함수",
            "3": "출력층에 대한 시그모이드",
            "4": "은닉층에 대한 Tanh",
            "5": "출력층에 대한 Softmax"
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "은닉층에 대한 ReLU",
            "출력층에 대한 Softmax"
        ],
        "Explanation": "ReLU (Rectified Linear Unit)는 비선형성을 처리하고 효율적인 계산을 제공하는 능력 덕분에 은닉층에서 널리 사용됩니다. Softmax는 다중 클래스 분류 작업에서 출력층에 이상적이며, 원시 출력 로짓을 각 클래스에 대한 확률로 변환하여 출력이 1로 합산되도록 보장합니다.",
        "Other Options": [
            "이진 단계 함수는 미분이 없기 때문에 역전파를 지원하지 않아 은닉층에 적합하지 않으며, 깊은 네트워크 훈련에 비효율적입니다.",
            "다중 클래스 분류를 위한 출력층에서 시그모이드를 사용하면 독립적인 확률을 출력하므로 여러 클래스에 걸쳐 정규화된 확률 분포를 제공하지 않아 잘못된 해석을 초래할 수 있습니다.",
            "Tanh는 유효한 활성화 함수이지만, 일반적으로 깊은 네트워크의 은닉층에서는 ReLU보다 덜 선호됩니다. 이는 특히 더 깊은 아키텍처에서 기울기 소실 문제와 같은 이슈 때문입니다."
        ]
    },
    {
        "Question Number": "8",
        "Situation": "데이터 과학자가 AWS Rekognition을 활용하여 이미지와 비디오 스트림을 분석하고, 실시간으로 보안 목적으로 객체, 장면 및 얼굴을 식별하는 애플리케이션을 구축하는 임무를 맡고 있습니다. 이 애플리케이션은 또한 감지된 개인의 감정 표현을 평가하고 그들의 나이와 성별을 결정해야 합니다.",
        "Question": "데이터 과학자가 애플리케이션 요구 사항을 충족하기 위해 어떤 기능 조합을 활용해야 합니까? (두 가지 선택)",
        "Options": {
            "1": "비디오 파일을 S3에 저장하고 얼굴 분석을 위해 Lambda 함수를 트리거합니다.",
            "2": "얼굴 분석을 활용하여 나이, 성별 및 감정을 평가합니다.",
            "3": "Kinesis 비디오 스트림에서 Rekognition 서비스로 비디오를 스트리밍하여 분석합니다.",
            "4": "텍스트 감지를 구현하여 표지판을 읽고 이미지에서 텍스트 정보를 추출합니다.",
            "5": "객체 및 장면 감지를 사용하여 비디오의 다양한 요소를 식별합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "얼굴 분석을 활용하여 나이, 성별 및 감정을 평가합니다.",
            "Kinesis 비디오 스트림에서 Rekognition 서비스로 비디오를 스트리밍하여 분석합니다."
        ],
        "Explanation": "얼굴 분석을 사용하면 애플리케이션이 개인의 나이, 성별 및 감정 표현과 같은 주요 속성을 결정할 수 있어 보안 애플리케이션에 매우 중요합니다. Kinesis에서 비디오를 스트리밍하면 실시간 처리 및 분석이 가능하여 감지된 사건에 즉각적으로 대응할 수 있습니다.",
        "Other Options": [
            "객체 및 장면 감지는 유용하지만 얼굴 속성을 평가하는 요구 사항을 직접 충족하지 않으므로 지정된 애플리케이션에 대해서는 불충분합니다.",
            "텍스트 감지는 정보를 추출하는 데 유용하지만 보안 맥락에서 얼굴 및 감정 속성을 분석하는 핵심 요구 사항을 해결하지 않습니다.",
            "비디오 파일을 S3에 저장하고 분석을 위해 Lambda 함수를 트리거하는 것은 가능하지만, Kinesis 스트림이 제공하는 실시간 기능에 비해 처리 지연을 초래합니다."
        ]
    },
    {
        "Question Number": "9",
        "Situation": "한 회사가 TensorFlow를 사용하여 새로운 추천 엔진을 배포하고 있습니다. 데이터 과학 팀은 훈련 시간을 줄이기 위해 여러 머신에 작업 부하를 분산시켜 훈련 프로세스를 최적화하고자 합니다. 그들은 분산 훈련을 위한 다양한 접근 방식을 고려하고 있으며, 모델이 최소한의 다운타임으로 A/B 테스트를 통해 프로덕션에서 쉽게 테스트될 수 있도록 보장하고자 합니다.",
        "Question": "팀이 프로덕션에서 TensorFlow 모델의 효과적인 분산 훈련 및 A/B 테스트를 위해 어떤 접근 방식을 채택해야 합니까?",
        "Options": {
            "1": "오케스트레이션 없이 여러 GPU에서 모델을 훈련하고 코드 업데이트를 통해 수동 A/B 테스트에 의존합니다.",
            "2": "훈련을 위해 단일 머신을 사용하고 A/B 테스트를 위해 여러 엔드포인트에 배포하여 트래픽을 분할합니다.",
            "3": "TensorFlow의 내장 파라미터 서버 지원을 활용하고 Amazon CloudWatch를 사용하여 A/B 테스트를 구성합니다.",
            "4": "분산 훈련을 위해 Horovod를 구현하고 Amazon SageMaker의 내장 A/B 테스트 기능을 모델 배포에 사용합니다."
        },
        "Correct Answer": "분산 훈련을 위해 Horovod를 구현하고 Amazon SageMaker의 내장 A/B 테스트 기능을 모델 배포에 사용합니다.",
        "Explanation": "Horovod를 사용하면 여러 GPU 또는 머신에서 TensorFlow 모델의 효율적인 분산 훈련이 가능합니다. 이를 Amazon SageMaker의 내장 A/B 테스트 기능과 결합하면 모델 간의 트래픽 분할이 용이해져 최소한의 운영 오버헤드로 강력한 테스트를 보장합니다.",
        "Other Options": [
            "파라미터 서버는 분산 훈련에 사용할 수 있지만 Horovod에 비해 더 많은 복잡성을 초래할 수 있습니다. 또한 A/B 테스트는 Amazon CloudWatch를 사용하는 것 이상의 체계적인 접근이 필요합니다.",
            "단일 머신에서 훈련하면 모델 훈련의 확장성과 효율성이 제한됩니다. 적절한 오케스트레이션 없이 A/B 테스트를 위해 여러 엔드포인트에 배포하면 일관되지 않은 결과와 높은 지연을 초래할 수 있습니다.",
            "오케스트레이션 없이 여러 GPU에서 훈련하면 자원의 비효율적인 활용으로 이어질 수 있습니다. 코드 업데이트를 통한 수동 A/B 테스트는 인적 오류에 취약하며, 이는 프로덕션 환경에서 이상적이지 않은 상당한 다운타임을 초래할 수 있습니다."
        ]
    },
    {
        "Question Number": "10",
        "Situation": "소매 회사는 고객 문의를 분석하고 기계 학습을 사용하여 응답을 자동화함으로써 고객 서비스를 개선하고자 합니다. 이 회사는 과거 고객 문의와 해당 응답의 대규모 데이터 세트를 보유하고 있습니다.",
        "Question": "고객 문의를 분석하고 적절한 응답을 생성할 수 있는 모델을 구축하기 위해 기계 학습 전문가가 어떤 AWS 서비스를 사용해야 합니까?",
        "Options": {
            "1": "Amazon Rekognition을 활용하여 문의의 시각적 콘텐츠를 분석하고 텍스트 응답을 생성합니다.",
            "2": "Amazon Comprehend를 사용하여 문의의 텍스트를 분석한 후, Amazon Lex를 사용하여 고객에게 응답할 수 있는 챗봇을 만듭니다.",
            "3": "AWS Glue를 사용하여 데이터를 정리한 다음, Amazon SageMaker를 사용하여 응답 생성을 위한 맞춤형 모델을 구축합니다.",
            "4": "Amazon Transcribe를 사용하여 음성 문의를 텍스트로 변환한 후, Amazon Polly를 적용하여 텍스트-음성 응답을 생성합니다."
        },
        "Correct Answer": "Amazon Comprehend를 사용하여 문의의 텍스트를 분석한 후, Amazon Lex를 사용하여 고객에게 응답할 수 있는 챗봇을 만듭니다.",
        "Explanation": "Amazon Comprehend는 감정 분석 및 개체 인식과 같은 자연어 처리 작업을 위해 설계되어 고객 문의의 맥락을 이해하는 데 도움을 줍니다. Amazon Lex는 대화형 인터페이스를 생성할 수 있게 하여 분석된 문의에 기반하여 응답을 자동화할 수 있습니다.",
        "Other Options": [
            "Amazon Rekognition은 주로 이미지 및 비디오 분석에 사용되므로 텍스트 기반 문의를 분석하는 데 적합하지 않습니다.",
            "AWS Glue는 데이터 준비 서비스이며 텍스트 분석이나 응답 생성을 위한 기계 학습 모델을 직접 구축하지 않습니다.",
            "Amazon Transcribe는 음성을 텍스트로 변환하는 데 사용되며 응답을 생성하지 않습니다; Amazon Polly는 텍스트-음성 변환을 위한 것이며 문의 응답 자동화에 적합하지 않습니다."
        ]
    },
    {
        "Question Number": "11",
        "Situation": "데이터 엔지니어는 다양한 출처에서 실시간 스트리밍 데이터를 수집하고 처리하여 향후 분석을 위해 저장하는 솔루션을 설계하는 임무를 맡고 있습니다. 이 솔루션은 높은 처리량을 처리하고 데이터가 몇 초 내에 분석 가능하도록 보장해야 합니다. 엔지니어는 이 아키텍처를 구현하기 위해 AWS 서비스를 고려하고 있습니다.",
        "Question": "실시간 데이터 수집 및 저장을 달성하기 위해 가장 적합한 AWS 서비스 조합은 무엇입니까?",
        "Options": {
            "1": "Kinesis Data Streams와 Amazon S3",
            "2": "Kinesis Data Firehose와 Amazon S3",
            "3": "Kinesis Data Firehose와 Amazon Redshift",
            "4": "Kinesis Data Analytics와 DynamoDB"
        },
        "Correct Answer": "Kinesis Data Firehose와 Amazon S3",
        "Explanation": "Kinesis Data Firehose는 거의 실시간 데이터 수집을 위해 설계되었으며, 스트리밍 데이터를 Amazon S3에 직접 전달할 수 있어 나중에 분석을 위해 대량의 데이터를 빠르고 효율적으로 저장하는 데 이상적인 선택입니다.",
        "Other Options": [
            "Kinesis Data Streams와 Amazon S3는 Kinesis Data Firehose와 같은 수준의 거의 실시간 수집 기능을 제공하지 않으며, Firehose는 저장 서비스에 대한 데이터 전달을 더 쉽게 하기 위해 특별히 설계되었습니다.",
            "Kinesis Data Analytics와 DynamoDB는 주로 데이터 수집에 중점을 두지 않습니다; Kinesis Data Analytics는 스트리밍 데이터를 처리하고 분석하는 데 사용되며, DynamoDB는 스트림에서 실시간 수집을 직접 처리하지 않는 NoSQL 데이터베이스입니다.",
            "Kinesis Data Firehose와 Amazon Redshift는 최적의 선택이 아닙니다. Redshift는 실시간 수집보다는 분석 쿼리에 최적화되어 있습니다. Firehose는 수집에 적합하지만 이 시나리오에서는 S3와 함께 사용하는 것이 가장 좋습니다."
        ]
    },
    {
        "Question Number": "12",
        "Situation": "소매 회사는 고객 이탈을 줄이기 위해 서비스를 중단할 가능성이 있는 고객을 식별하고자 합니다. 이 회사는 고객 행동, 구매 패턴 및 인구 통계에 대한 역사적 데이터를 보유하고 있습니다. 기계 학습 전문가는 이 비즈니스 문제를 기계 학습 문제로 정의하는 임무를 맡고 있습니다.",
        "Question": "기계 학습 모델을 위한 이 비즈니스 문제를 정의하는 가장 적절한 방법은 무엇입니까?",
        "Options": {
            "1": "고객 세그먼트당 평균 지출을 알아냅니다.",
            "2": "구매 빈도에 따라 고객을 세그먼트로 분류합니다.",
            "3": "지난 5년간의 전체 수익 추세를 분석합니다.",
            "4": "고객 행동에 따라 어떤 고객이 이탈할지를 예측합니다."
        },
        "Correct Answer": "고객 행동에 따라 어떤 고객이 이탈할지를 예측합니다.",
        "Explanation": "올바른 접근 방식은 문제를 예측 작업으로 정의하여 특정 고객이 이탈할 가능성이 있는지를 식별하는 것입니다. 이를 통해 회사는 해당 고객을 유지하기 위한 목표 조치를 취할 수 있습니다.",
        "Other Options": [
            "구매 빈도에 따라 고객을 세그먼트로 분류하는 것은 고객 이탈 문제를 직접적으로 해결하지 않습니다. 이 접근 방식은 고객 행동을 이해하는 데 유용하지만 이탈을 구체적으로 예측하지는 않습니다.",
            "고객 세그먼트당 평균 지출을 알아내는 것은 고객 가치를 이해하는 데 도움을 주지만, 어떤 고객이 이탈할 가능성이 있는지를 식별하는 데는 도움이 되지 않아 이탈 문제와는 관련이 없습니다.",
            "지난 5년간의 전체 수익 추세를 분석하는 것은 더 넓은 비즈니스 성과를 이해하는 데 도움이 될 수 있지만, 개별 고객 행동에 초점을 맞추거나 이탈 위험을 예측하지 않습니다."
        ]
    },
    {
        "Question Number": "13",
        "Situation": "데이터 과학자가 결측치와 이상치가 상당히 많은 머신러닝 프로젝트를 진행하고 있습니다. 과학자는 모델 훈련 전에 데이터를 정리하고 준비하여 높은 정확성을 보장해야 합니다.",
        "Question": "모델링 전에 데이터셋의 결측치를 처리하기 위한 가장 효과적인 방법은 무엇인가요?",
        "Options": {
            "1": "열의 평균 또는 중앙값을 사용하여 결측치를 대체합니다.",
            "2": "결측치가 있는 모든 행을 삭제합니다.",
            "3": "결측치를 그대로 두고 모델링을 진행합니다.",
            "4": "결측치를 0과 같은 상수 값으로 대체합니다."
        },
        "Correct Answer": "열의 평균 또는 중앙값을 사용하여 결측치를 대체합니다.",
        "Explanation": "결측치를 평균 또는 중앙값으로 대체하는 것은 데이터셋의 크기를 유지하고 머신러닝 모델의 성능을 향상시킬 수 있는 일반적이고 효과적인 방법입니다. 이 접근 방식은 모델이 모든 사용 가능한 데이터를 활용하면서 결측 항목을 적절히 처리할 수 있게 합니다.",
        "Other Options": [
            "결측치가 있는 모든 행을 삭제하면 데이터의 상당한 손실이 발생할 수 있으며, 이는 많은 행에 결측치가 포함된 경우 모델의 성능에 부정적인 영향을 미칠 수 있습니다.",
            "결측치를 그대로 두면 머신러닝 알고리즘에서 예측할 수 없는 동작이 발생할 수 있습니다. 대부분의 모델은 추가 전처리 없이 결측치를 처리할 수 없습니다.",
            "결측치를 0과 같은 상수 값으로 대체하면 모델에 편향이 생기고 데이터를 잘못 표현하여 부정확한 예측을 초래할 수 있습니다."
        ]
    },
    {
        "Question Number": "14",
        "Situation": "데이터 과학자가 자연어 처리(NLP) 모델의 성능을 향상시키는 임무를 맡고 있습니다. 이를 위해 데이터 과학자는 텍스트 데이터를 분석하여 모델의 정확성을 높일 수 있는 주요 특징을 식별해야 합니다. 데이터셋에는 다양한 출처의 고객 리뷰가 대량으로 포함되어 있습니다. 데이터 과학자는 모델 훈련에 사용할 수 있는 의미 있는 특징을 추출해야 합니다.",
        "Question": "데이터 과학자가 텍스트 데이터에서 관련 특징을 식별하고 추출하기 위한 가장 좋은 접근 방식은 무엇인가요?",
        "Options": {
            "1": "키워드와 구문 목록을 생성하여 수동으로 특징 엔지니어링을 수행하고, 이를 사용하여 텍스트 데이터에서 이진 특징 벡터를 생성합니다.",
            "2": "사전 훈련된 언어 모델을 사용하여 텍스트 데이터의 임베딩을 생성한 다음, 차원 축소 기법을 적용하여 주요 특징을 식별합니다.",
            "3": "단순 카운트 벡터라이저를 사용하여 단어 수를 기반으로 특징 벡터를 생성한 다음, 클러스터링을 적용하여 잠재적인 특징 그룹을 식별합니다.",
            "4": "단어 가방 모델을 구현하여 텍스트를 숫자 벡터로 변환한 다음, TF-IDF를 적용하여 문맥에서 단어의 중요성을 평가합니다."
        },
        "Correct Answer": "사전 훈련된 언어 모델을 사용하여 텍스트 데이터의 임베딩을 생성한 다음, 차원 축소 기법을 적용하여 주요 특징을 식별합니다.",
        "Explanation": "사전 훈련된 언어 모델을 사용하여 임베딩을 생성하면 데이터 과학자는 텍스트에서 풍부한 의미 정보를 활용할 수 있습니다. 이 접근 방식은 단어의 문맥적 의미를 포착하는 데 중요하며, 이는 NLP 작업에 필수적입니다. 차원 축소는 모델에 가장 관련성이 높은 특징을 식별하는 데 도움을 줄 수 있습니다.",
        "Other Options": [
            "단어 가방 모델을 구현한 후 TF-IDF를 사용하는 것은 효과적일 수 있지만, 임베딩만큼 단어 간의 문맥적 관계를 잘 포착하지 못할 수 있어 모델 성능을 제한할 수 있습니다.",
            "수동 특징 엔지니어링은 편향을 초래할 수 있으며, 대규모 데이터셋에서 훈련된 모델이 자동으로 학습할 수 있는 중요한 특징을 놓칠 수 있습니다. 일반적으로 자동화된 방법보다 효율성과 확장성이 떨어집니다.",
            "단순 카운트 벡터라이저는 단어의 문맥과 관계의 중요성을 간과할 수 있으며, 이는 고객 리뷰의 감정과 의미를 이해하는 데 필수적입니다. 클러스터링은 또한 명확성이 부족한 모호한 특징을 초래할 수 있습니다."
        ]
    },
    {
        "Question Number": "15",
        "Situation": "ML 엔지니어가 Amazon SageMaker를 사용하여 훈련된 모델을 배포하는 임무를 맡고 있습니다. 엔지니어는 내부 서비스에서 실시간 예측을 제공해야 하며, Amazon S3에 저장된 대규모 데이터셋에 대해 배치 예측도 수행하고자 합니다. 엔지니어는 각 시나리오에 어떤 방법을 사용해야 할지 확신이 없습니다.",
        "Question": "ML 엔지니어가 실시간 추론과 배치 추론을 효과적으로 달성하기 위해 어떤 방법을 사용해야 하나요?",
        "Options": {
            "1": "실시간 추론을 위해 SageMaker Training Job을 사용하고, 배치 추론을 위해 Amazon EC2를 사용합니다.",
            "2": "실시간 추론을 위해 SageMaker 노트북 인스턴스를 사용하고, 배치 추론을 위해 Amazon Lambda를 사용합니다.",
            "3": "실시간 추론을 위해 Amazon Comprehend를 사용하고, 배치 추론을 위해 Amazon S3 Select를 사용합니다.",
            "4": "실시간 추론을 위해 InvokeEndpoint를 사용하고, 배치 추론을 위해 배치 변환 작업을 생성합니다."
        },
        "Correct Answer": "실시간 추론을 위해 InvokeEndpoint를 사용하고, 배치 추론을 위해 배치 변환 작업을 생성합니다.",
        "Explanation": "가장 좋은 접근 방식은 실시간 추론을 위해 InvokeEndpoint API를 사용하는 것입니다. 이를 통해 모델을 직접 호출하여 즉각적인 예측을 받을 수 있습니다. 배치 추론의 경우, 배치 변환 작업을 사용하는 것이 적절하며, 이는 S3에서 대량의 입력을 효율적으로 처리하고 결과를 S3로 다시 출력할 수 있습니다.",
        "Other Options": [
            "실시간 추론을 위해 SageMaker 노트북 인스턴스를 사용하는 것은 잘못된 접근입니다. 이는 예측을 제공하기 위해 설계된 것이 아니라 주로 개발 및 탐색을 위한 것입니다. Amazon Lambda는 이벤트 기반 프로세스와 제한된 실행 시간을 위해 설계되었기 때문에 배치 추론에 적합하지 않습니다.",
            "실시간 추론을 위해 SageMaker Training Job을 사용하는 것은 잘못된 접근입니다. 훈련 작업은 추론을 위한 것이 아니라 모델 훈련을 위한 것입니다. Amazon EC2는 추론에 사용할 수 있지만 SageMaker의 InvokeEndpoint와 같은 수준의 통합 및 편리함을 제공하지 않습니다.",
            "실시간 추론을 위해 Amazon Comprehend를 사용하는 것은 잘못된 접근입니다. 이는 자연어 처리 작업을 위한 서비스이지 일반 모델 배포를 위한 것이 아닙니다. Amazon S3 Select는 배치 추론을 수행하기 위해 설계된 것이 아니라 S3에서 데이터를 쿼리하는 방법입니다."
        ]
    },
    {
        "Question Number": "16",
        "Situation": "머신 러닝 엔지니어가 복잡한 회귀 작업을 위한 신경망을 설계하고 있습니다. 엔지니어는 모델이 데이터의 비선형 관계를 효과적으로 포착하면서도 계산 효율성을 유지하도록 하고 싶어합니다.",
        "Question": "원하는 성능을 달성하기 위해 신경망의 숨겨진 층에 가장 적합한 아키텍처 선택은 무엇입니까?",
        "Options": {
            "1": "숨겨진 층에 ReLU 활성화 함수를 구현하여 비선형성을 제공하고 소실 기울기 문제를 완화합니다.",
            "2": "숨겨진 층에 선형 활성화 함수를 적용하여 단순성과 해석 가능성을 유지합니다.",
            "3": "출력이 -1에서 1 사이가 되도록 숨겨진 층에 Tanh 활성화 함수를 선택합니다.",
            "4": "모든 숨겨진 층에 시그모이드 활성화 함수를 사용하여 출력을 0과 1 사이로 유지합니다."
        },
        "Correct Answer": "숨겨진 층에 ReLU 활성화 함수를 구현하여 비선형성을 제공하고 소실 기울기 문제를 완화합니다.",
        "Explanation": "ReLU (Rectified Linear Unit) 활성화 함수는 비선형성을 도입하면서도 계산 효율성이 뛰어나기 때문에 신경망의 숨겨진 층에서 널리 사용됩니다. 이 함수는 소실 기울기 문제를 방지하여 더 깊은 네트워크에서 더 빠른 훈련과 더 나은 성능을 가능하게 합니다.",
        "Other Options": [
            "모든 숨겨진 층에 시그모이드 활성화 함수를 사용하면 소실 기울기 문제를 초래할 수 있으며, 이는 기울기가 매우 작아져 더 깊은 네트워크의 훈련을 방해할 수 있습니다.",
            "Tanh 활성화 함수는 유용할 수 있지만, 특히 더 깊은 네트워크에서 소실 기울기 문제로 인해 ReLU가 복잡한 작업에 더 적합한 선택입니다.",
            "숨겨진 층에 선형 활성화 함수를 적용하면 신경망이 본질적으로 선형 모델로 축소되어 데이터의 복잡한 비선형 관계를 포착하는 데 불충분합니다."
        ]
    },
    {
        "Question Number": "17",
        "Situation": "소매 회사가 고객의 구매 행동을 분석하여 패턴을 식별하고 다양한 고객 속성과 구매 이력을 포함하는 데이터 세트의 차원을 줄이기를 원합니다. 이를 달성하기 위해 비지도 학습 알고리즘을 구현하기로 결정했습니다. 데이터는 여러 특성으로 구성되어 있으며, 그 중 일부는 데이터 세트의 분산을 설명하는 데 덜 중요합니다. 목표는 데이터 포인트의 중심 경향을 찾고 이를 저차원 공간에서 시각화하는 것입니다.",
        "Question": "회사가 데이터 내의 관계를 식별하면서 효과적으로 차원을 줄이기 위해 어떤 접근 방식을 취해야 합니까?",
        "Options": {
            "1": "주성분 분석(Principal Component Analysis, PCA)을 사용하여 데이터 세트의 주성분을 찾아 가장 많은 분산을 유지하면서 데이터를 저차원 공간으로 변환합니다.",
            "2": "K-평균과 같은 클러스터링 알고리즘을 구현하여 유사성을 기반으로 데이터를 클러스터로 분할하고 가장 관련성이 높은 특성에 집중합니다.",
            "3": "선형 판별 분석(Linear Discriminant Analysis, LDA)을 활용하여 데이터의 클래스 간 분리를 극대화하여 특성을 저차원 공간으로 투영합니다.",
            "4": "t-분포 확률적 이웃 임베딩(t-Distributed Stochastic Neighbor Embedding, t-SNE)을 적용하여 2D에서 데이터 세트를 시각화하고 원래 특성 간의 관계에만 집중합니다."
        },
        "Correct Answer": "주성분 분석(Principal Component Analysis, PCA)을 사용하여 데이터 세트의 주성분을 찾아 가장 많은 분산을 유지하면서 데이터를 저차원 공간으로 변환합니다.",
        "Explanation": "주성분 분석(PCA)을 사용하는 것은 데이터 내의 관계를 유지하면서 차원을 줄이는 데 가장 적합한 접근 방식입니다. PCA는 분산을 극대화하는 방향(주성분)을 식별하여 데이터 세트를 가장 중요한 특성을 포착하는 저차원 공간으로 효과적으로 변환합니다.",
        "Other Options": [
            "K-평균 클러스터링은 차원 축소 기법이 아니라 유사성을 기반으로 데이터 포인트를 그룹화하는 클러스터링 알고리즘입니다. 데이터 세트를 저차원 공간으로 변환하는 데 집중하지 않습니다.",
            "t-SNE는 추가 분석을 위한 차원 축소 방법이 아니라 주로 시각화 기법입니다. 데이터는 저차원으로 투영할 수 있지만 PCA처럼 차원 간의 분산을 반드시 유지하지는 않습니다.",
            "선형 판별 분석(LDA)은 클래스 분리를 극대화하는 데 집중하는 감독 방법으로, 비지도 차원 축소에 적합하지 않습니다. 클래스 식별을 위해 레이블이 있는 데이터가 필요합니다."
        ]
    },
    {
        "Question Number": "18",
        "Situation": "데이터 과학자가 다양한 특성을 기반으로 주택 가격을 예측하는 선형 회귀 모델의 성능을 개선하기 위해 작업하고 있습니다. 현재 모델은 수렴 속도가 느려 훈련 시간이 길어지고 있습니다. 데이터 과학자는 모델의 정확성을 저하시키지 않으면서 수렴 속도를 향상시키기 위해 학습률을 조정하고 싶어합니다.",
        "Question": "데이터 과학자가 선형 회귀 모델의 학습률을 조정할 때 고려해야 할 사항은 무엇입니까?",
        "Options": {
            "1": "더 작은 학습률은 더 빠른 수렴으로 이어질 수 있습니다.",
            "2": "과적합을 방지하기 위해 학습률을 0으로 설정해야 합니다.",
            "3": "학습률 조정은 훈련 시간에 영향을 미치지 않습니다.",
            "4": "너무 높은 학습률은 모델이 발산하게 만들 수 있습니다."
        },
        "Correct Answer": "너무 높은 학습률은 모델이 발산하게 만들 수 있습니다.",
        "Explanation": "너무 높은 학습률은 모델 매개변수가 진동하거나 발산하게 만들어 훈련 중 성능 저하와 불안정을 초래할 수 있습니다. 이는 선형 모델의 학습률을 조정할 때 중요한 요소입니다.",
        "Other Options": [
            "더 작은 학습률은 최소값으로 가는 작은 단계로 인해 수렴 속도가 느려질 수 있으며, 이는 훈련 시간을 증가시킬 수 있고 빠른 수렴을 목표로 할 경우 비효율적일 수 있습니다.",
            "학습률을 0으로 설정하면 모델 매개변수가 동결되어 학습이 발생하지 않게 되며, 이는 모델 훈련에 도움이 되지 않고 과소적합을 초래할 수 있습니다.",
            "학습률 조정은 훈련 시간에 상당한 영향을 미칩니다. 적절한 학습률은 수렴 속도를 높일 수 있지만, 부적절한 학습률은 속도를 늦추거나 발산을 초래할 수 있습니다."
        ]
    },
    {
        "Question Number": "19",
        "Situation": "머신 러닝 엔지니어가 Amazon SageMaker의 기능을 탐색하여 머신 러닝 모델을 구축, 훈련 및 배포하는 프로세스를 간소화하고 있습니다. 엔지니어는 SageMaker의 기능을 활용하여 노트북 인스턴스와 그 생애 주기 구성 관리를 이해하고자 합니다.",
        "Question": "Amazon SageMaker 노트북 인스턴스 및 생애 주기 구성에 대한 다음 설명 중 어떤 것이 사실입니까? (두 개 선택)",
        "Options": {
            "1": "SageMaker 노트북 인스턴스는 자동으로 단일 S3 버킷으로 제한됩니다.",
            "2": "노트북 인스턴스는 ml.t2.medium 인스턴스 유형만 사용할 수 있습니다.",
            "3": "사전 서명된 URL을 통해 SageMaker 노트북 인스턴스에 접근할 수 있습니다.",
            "4": "생애 주기 구성은 노트북 인스턴스가 시작되기 전에 bash 명령을 실행할 수 있게 해줍니다.",
            "5": "노트북 인스턴스에 대해 모든 EC2 인스턴스 유형을 선택할 수 있습니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "사전 서명된 URL을 통해 SageMaker 노트북 인스턴스에 접근할 수 있습니다.",
            "생애 주기 구성은 노트북 인스턴스가 시작되기 전에 bash 명령을 실행할 수 있게 해줍니다."
        ],
        "Explanation": "Amazon SageMaker는 사전 서명된 URL을 통해 노트북 인스턴스에 접근할 수 있도록 하여 안전한 접근을 보장합니다. 또한, 생애 주기 구성은 노트북 인스턴스가 시작되기 전에 bash 명령을 실행하는 데 사용되어 사용자 정의 및 설정 작업을 가능하게 합니다.",
        "Other Options": [
            "이 옵션은 SageMaker가 노트북 인스턴스에 대해 다양한 인스턴스 유형을 허용하므로 잘못되었습니다. ml.t2.medium으로 제한되지 않습니다.",
            "이 옵션은 SageMaker 노트북 인스턴스가 단일 S3 버킷에만 제한되지 않고 여러 S3 버킷에 접근할 수 있으므로 잘못되었습니다.",
            "이 옵션은 다양한 EC2 인스턴스 유형을 선택할 수 있지만 SageMaker 노트북 인스턴스는 ml. 접두사가 있는 인스턴스 유형을 사용하도록 특별히 설계되었기 때문에 잘못되었습니다."
        ]
    },
    {
        "Question Number": "20",
        "Situation": "데이터 엔지니어가 데이터 전처리, 모델 훈련 및 평가를 포함하는 일련의 머신 러닝 작업을 조정하는 임무를 맡고 있습니다. 엔지니어는 이러한 작업이 순차적으로 실행되고 특정 간격으로 자동으로 트리거되어 최신 데이터로 모델을 업데이트하도록 해야 합니다.",
        "Question": "데이터 엔지니어가 이러한 머신 러닝 작업을 효과적으로 예약하고 관리하기 위해 어떤 AWS 서비스를 사용해야 합니까?",
        "Options": {
            "1": "AWS Glue",
            "2": "Amazon EC2",
            "3": "Amazon SageMaker Pipelines",
            "4": "AWS Lambda"
        },
        "Correct Answer": "Amazon SageMaker Pipelines",
        "Explanation": "Amazon SageMaker Pipelines는 엔드 투 엔드 머신 러닝 워크플로우를 구축하고 관리하기 위해 설계되었습니다. 사용자가 파이프라인의 단계를 정의하고 작업을 예약하며 워크플로우를 자동화할 수 있도록 하여 데이터 전처리, 모델 훈련 및 평가를 조정하는 데 이상적입니다.",
        "Other Options": [
            "AWS Glue는 주로 데이터 준비 및 ETL(추출, 변환, 로드) 작업에 사용됩니다. 데이터 처리에 도움을 줄 수 있지만 SageMaker Pipelines와 같이 머신 러닝 워크플로우에 맞춰진 조정 및 예약 수준을 제공하지 않습니다.",
            "Amazon EC2는 클라우드의 가상 서버입니다. 이론적으로 EC2 인스턴스에서 작업을 실행할 수 있지만 머신 러닝 작업을 위한 내장된 예약 및 조정 기능이 부족하여 더 많은 수동 설정 및 관리가 필요합니다.",
            "AWS Lambda는 이벤트에 응답하여 코드를 실행하는 서버리스 컴퓨팅 서비스입니다. 짧은 작업을 위해 설계되었기 때문에 장기 실행 머신 러닝 훈련 작업이나 복잡한 워크플로우 관리에 적합하지 않습니다."
        ]
    },
    {
        "Question Number": "21",
        "Situation": "머신 러닝 엔지니어가 Amazon Transcribe를 사용하여 음성을 텍스트로 변환하는 프로젝트를 진행하고 있습니다. 이 프로젝트는 실시간 전사와 사전 녹음된 오디오 파일 분석을 포함합니다. 엔지니어는 또한 화자 식별을 구현하고 특정 용어에 대한 어휘를 사용자 정의해야 합니다.",
        "Question": "엔지니어가 음성-텍스트 기능을 효과적으로 구현하기 위해 어떤 조치를 취할 수 있습니까? (두 개 선택)",
        "Options": {
            "1": "특정 단어를 텍스트 파일에 넣고 언어를 지정한 후 S3 버킷에 업로드하여 사용자 정의 어휘를 생성합니다.",
            "2": "Amazon Transcribe의 내장 기능만 사용하고 사용자 정의 구성을 피합니다.",
            "3": "설정 중에 전사 작업 설정을 구성하여 오디오에서 서로 다른 화자를 인식하도록 화자 식별을 활성화합니다.",
            "4": "전사 작업을 생성하지 않고 Amazon Transcribe 콘솔에 오디오 파일을 직접 업로드합니다.",
            "5": "적절한 오디오 파일 입력으로 전사 작업을 생성하여 Amazon Transcribe를 사용하여 사전 녹음된 파일을 분석합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "특정 단어를 텍스트 파일에 넣고 언어를 지정한 후 S3 버킷에 업로드하여 사용자 정의 어휘를 생성합니다.",
            "설정 중에 전사 작업 설정을 구성하여 오디오에서 서로 다른 화자를 인식하도록 화자 식별을 활성화합니다."
        ],
        "Explanation": "사용자 정의 어휘를 생성함으로써 엔지니어는 전사 중에 특정 용어가 인식되도록 보장할 수 있으며, 이는 특정 산업이나 프로젝트에 매우 중요합니다. 화자 식별을 활성화하면 시스템이 화자를 구별할 수 있어 전사의 정확성과 유용성이 향상됩니다.",
        "Other Options": [
            "이 옵션은 Amazon Transcribe에 내장된 기능이 있지만 사용자 정의 어휘와 화자 식별을 활용하는 것이 특정 프로젝트 요구에 따라 성능을 최적화하는 데 필수적이므로 잘못되었습니다.",
            "이 옵션은 사용자 정의 구성이 필요하지 않다는 것을 암시하므로 잘못되었습니다. 실제로는 어휘를 사용자 정의하고 화자 식별과 같은 기능을 활성화하는 것이 원하는 전사 정확도를 달성하는 데 중요합니다.",
            "이 옵션은 오디오 파일을 콘솔에 직접 업로드하는 것만으로는 충분하지 않으며, 엔지니어는 파일이 적절하게 처리되고 전사되도록 전사 작업을 생성해야 하므로 잘못되었습니다."
        ]
    },
    {
        "Question Number": "22",
        "Situation": "머신 러닝 엔지니어가 이미지 분류를 위한 딥 러닝 모델을 구축하기 위한 프레임워크를 선택하는 임무를 맡았습니다. 엔지니어는 여러 프레임워크에 익숙하지만, 사용하기 쉬운 인터페이스를 제공하면서도 성능을 위한 강력한 백엔드를 활용할 수 있는 프레임워크를 선택해야 합니다.",
        "Question": "사용자 친화성과 강력한 백엔드 접근성을 조합한 프레임워크 중 엔지니어가 선택할 것은 무엇입니까?",
        "Options": {
            "1": "Pytorch",
            "2": "Gluon",
            "3": "MXNet",
            "4": "Scikit-learn"
        },
        "Correct Answer": "Gluon",
        "Explanation": "Gluon은 딥 러닝 모델을 구축하기 위한 고수준 API를 제공하여 개발자가 복잡한 신경망을 쉽게 만들 수 있도록 하며, MXNet의 성능을 백엔드로 활용합니다. 이러한 단순성과 강력함의 균형은 엔지니어의 요구에 이상적인 선택이 됩니다.",
        "Other Options": [
            "Scikit-learn은 전통적인 머신 러닝 알고리즘을 위해 주로 설계되었으며, 이미지 분류와 같은 모델을 구축하는 데 필요한 딥 러닝 기능을 제공하지 않습니다.",
            "Pytorch는 사용자 친화적이고 강력하지만, Gluon에 비해 더 많은 보일러플레이트 코드가 필요할 수 있으며, 모델 개발을 단순화하기 위해 특별히 설계된 Gluon보다 학습 곡선이 더 가파를 수 있습니다.",
            "MXNet은 기본 프레임워크로 성능 이점을 제공하지만, Gluon이 제공하는 고수준 추상화가 부족하여 빠른 모델 프로토타입 제작에 덜 사용자 친화적입니다."
        ]
    },
    {
        "Question Number": "23",
        "Situation": "데이터 과학자가 데이터셋에서 파생된 특징을 향상시켜 예측 모델의 성능을 개선하는 임무를 맡았습니다. 데이터셋에는 다양한 범주형 및 수치형 변수가 포함되어 있으며, 모델의 현재 정확도는 만족스럽지 않습니다. 데이터 과학자는 기존 데이터에서 더 많은 유용한 특징을 추출하기 위해 다양한 특징 엔지니어링 기법을 고려하고 있습니다.",
        "Question": "많은 고유 값을 가진 범주형 변수를 머신 러닝에 적합한 형식으로 변환하는 데 가장 효과적인 특징 엔지니어링 기법은 무엇입니까?",
        "Options": {
            "1": "범주형 변수를 각 고유 값에 대한 이진 열로 변환하기 위해 원-핫 인코딩을 적용합니다.",
            "2": "데이터셋에서 각 범주의 발생에 기반한 빈도 인코딩을 생성합니다.",
            "3": "범주형 변수의 차원 축소를 수행하여 고유 값의 수를 줄입니다.",
            "4": "범주형 변수를 각 고유 범주에 대한 단일 정수 값으로 변환하기 위해 레이블 인코딩을 사용합니다."
        },
        "Correct Answer": "범주형 변수를 각 고유 값에 대한 이진 열로 변환하기 위해 원-핫 인코딩을 적용합니다.",
        "Explanation": "원-핫 인코딩은 많은 고유 값을 가진 범주형 변수에 효과적입니다. 이는 모델이 범주 간의 수치적 관계를 잘못 해석하는 것을 방지합니다. 각 범주는 별도의 이진 열로 표현되어 모델이 독립적으로 학습할 수 있도록 하며, 순서 관계를 암시하지 않습니다.",
        "Other Options": [
            "레이블 인코딩은 의도하지 않은 순서 관계를 도입할 수 있으며, 이는 비순서 범주형 변수에 대해 모델을 오도할 수 있습니다. 많은 고유 범주가 있는 범주형 변수에는 적합하지 않습니다.",
            "PCA와 같은 차원 축소 기법은 일반적으로 범주형 데이터에 직접 적용할 수 없으며, 변환된 수치 표현에 적용할 경우 의미 있는 관계를 보존하지 못할 수 있습니다.",
            "빈도 인코딩은 유용할 수 있지만, 데이터셋의 범주 분포에 따라 편향을 도입할 수 있습니다. 원-핫 인코딩만큼 효과적으로 기본 관계를 포착하지 못할 수 있습니다."
        ]
    },
    {
        "Question Number": "24",
        "Situation": "데이터 과학자가 지난 5년간의 판매 데이터를 포함하는 데이터셋을 분석하여 트렌드와 패턴을 식별하고자 합니다. 과학자는 고객 구매 행동을 더 잘 이해하기 위해 판매 금액의 분포를 시각화하고 싶어합니다. 이 분석을 위해 과학자가 만들어야 할 그래프 유형은 무엇입니까?",
        "Question": "판매 금액의 분포를 시각화하는 데 가장 적합한 그래프는 무엇입니까?",
        "Options": {
            "1": "상자 수염 그림(Box Plot)",
            "2": "히스토그램(Histogram)",
            "3": "선 그래프(Line Chart)",
            "4": "산점도(Scatter Plot)"
        },
        "Correct Answer": "히스토그램",
        "Explanation": "히스토그램은 데이터를 빈으로 나누고 각 빈의 관측 수를 세어 수치 데이터의 분포를 보여주는 데 이상적입니다. 이는 판매 금액의 분포를 시각화하는 데 완벽합니다.",
        "Other Options": [
            "산점도는 일반적으로 두 변수의 값을 표시하는 데 사용됩니다. 판매 금액과 같은 단일 변수의 분포를 보여주는 데는 적합하지 않습니다.",
            "상자 수염 그림은 데이터셋의 중앙값, 사분위수 및 잠재적 이상치를 표시하지만, 히스토그램만큼 분포 형태를 효과적으로 보여주지 않습니다.",
            "선 그래프는 시간에 따른 데이터 포인트를 시각화하는 데 사용되며, 값의 분포를 보여주기보다는 시간에 따른 트렌드를 설명합니다."
        ]
    },
    {
        "Question Number": "25",
        "Situation": "한 의료 기관이 환자 인입 예측을 위해 과거 데이터, 환자 인구 통계 및 계절적 추세를 기반으로 예측 분석 솔루션을 구현하고자 합니다. 이 기관은 인프라 관리의 필요성을 최소화하면서 확장성과 사용 용이성을 보장하는 것을 목표로 하고 있습니다.",
        "Question": "이 문제를 효과적으로 해결하기 위해 머신 러닝 전문가가 추천해야 할 AWS 서비스 조합은 무엇인가요?",
        "Options": {
            "1": "모델 훈련을 위해 Amazon SageMaker를 활용하고, 예측 시각화를 위해 Amazon QuickSight를 사용합니다.",
            "2": "Amazon Forecast를 활용하여 과거 데이터를 기반으로 시계열 모델을 생성하고 예측을 제공합니다.",
            "3": "Amazon SageMaker를 사용하여 모델을 구축하고 훈련한 후, AWS Lambda를 사용하여 추론을 위해 배포합니다.",
            "4": "데이터 처리를 위해 Amazon EMR 클러스터를 구현하고, 모델 훈련을 위해 Apache Spark MLlib를 사용합니다."
        },
        "Correct Answer": "Amazon Forecast를 활용하여 과거 데이터를 기반으로 시계열 모델을 생성하고 예측을 제공합니다.",
        "Explanation": "Amazon Forecast는 시계열 예측을 위해 특별히 설계되어, 조직이 최소한의 운영 오버헤드로 예측 모델을 효율적으로 생성하고 관리할 수 있도록 합니다. 모델 구축의 복잡성을 자동으로 처리하여, 과거 추세를 기반으로 환자 인입을 예측하는 데 가장 적합한 선택입니다.",
        "Other Options": [
            "Amazon SageMaker를 사용하여 모델을 구축하고 AWS Lambda를 사용하여 추론하는 것은 예측 작업에 불필요한 복잡성을 도입합니다. Amazon Forecast는 이 용도에 맞게 설계되어 프로세스를 단순화합니다.",
            "Amazon SageMaker를 훈련에 사용하는 것은 실행 가능한 옵션이지만, Amazon QuickSight는 주로 시각화 도구이며 환자 인입 분석에 필요한 예측 기능을 제공하지 않습니다.",
            "Apache Spark MLlib를 사용하여 모델 훈련을 위해 Amazon EMR 클러스터를 구현하는 것은 더 많은 인프라 관리가 필요하며, Amazon Forecast에 비해 시계열 예측에 최적화되어 있지 않아 효율성이 떨어집니다."
        ]
    },
    {
        "Question Number": "26",
        "Situation": "데이터 과학자가 분류 작업을 위한 머신 러닝 모델을 구축할 준비를 하고 있지만, 레이블이 있는 데이터셋이 예상보다 작다는 것을 발견했습니다. 과학자는 모델을 효과적으로 훈련하기 위해 충분한 레이블 데이터가 있는지 확인해야 합니다.",
        "Question": "데이터 과학자가 레이블 데이터의 적절성을 평가하고 잠재적인 문제를 완화하기 위해 사용할 수 있는 전략은 무엇인가요?",
        "Options": {
            "1": "레이블이 있는 데이터와 레이블이 없는 데이터를 모두 활용하기 위해 반지도 학습 접근 방식을 구현합니다.",
            "2": "교차 검증을 사용하여 모델의 성능을 평가하고 결과에 따라 조정합니다.",
            "3": "잠재 사용자로부터 더 많은 레이블 인스턴스를 수집하기 위해 설문 조사를 실시합니다.",
            "4": "데이터 증강 기법을 사용하여 레이블 데이터셋의 크기를 인위적으로 증가시킵니다."
        },
        "Correct Answer": "데이터 증강 기법을 사용하여 레이블 데이터셋의 크기를 인위적으로 증가시킵니다.",
        "Explanation": "데이터 증강 기법을 통해 데이터 과학자는 기존 레이블 데이터에서 합성 데이터 포인트를 생성하여 데이터셋의 크기를 효과적으로 증가시키고, 더 다양한 훈련 예제를 제공함으로써 모델의 성능을 향상시킬 수 있습니다.",
        "Other Options": [
            "설문 조사를 실시하는 것은 즉각적인 결과를 가져오지 않을 수 있으며, 새로운 인스턴스가 모델 성능에 충분히 유익하거나 관련성이 있을 것이라는 보장을 하지 않습니다.",
            "교차 검증을 사용하여 모델 성능을 평가하는 것은 좋은 관행이지만, 처음에 훈련을 위한 충분한 레이블 데이터가 있는지에 대한 우려를 해결하지는 않습니다.",
            "반지도 학습 접근 방식을 구현하는 것은 유익할 수 있지만, 레이블 데이터셋이 심각하게 작을 경우 데이터 증강이 더 직접적인 해결책이 될 수 있습니다."
        ]
    },
    {
        "Question Number": "27",
        "Situation": "데이터 과학자가 머신 러닝 모델을 위한 데이터셋을 준비하고 있으며, 데이터 내의 기본 패턴과 관계를 이해하고자 합니다. 모델을 구축하기 전에 트렌드, 이상치 및 상관관계를 식별하기 위해 데이터를 시각화하고자 합니다.",
        "Question": "데이터 과학자가 머신 러닝을 위해 데이터를 효과적으로 분석하고 시각화하기 위해 어떤 접근 방식을 취해야 하나요?",
        "Options": {
            "1": "Amazon Redshift를 적용하여 데이터를 저장하고 SQL 쿼리를 사용하여 분석합니다.",
            "2": "Amazon QuickSight를 활용하여 데이터 시각화를 위한 대시보드를 생성합니다.",
            "3": "Amazon SageMaker Data Wrangler를 활용하여 EDA를 수행하고 데이터셋을 시각화합니다.",
            "4": "AWS Glue를 사용하여 데이터를 준비하되, 먼저 시각화하지 않습니다."
        },
        "Correct Answer": "Amazon SageMaker Data Wrangler를 활용하여 EDA를 수행하고 데이터셋을 시각화합니다.",
        "Explanation": "Amazon SageMaker Data Wrangler는 데이터 준비 및 탐색적 데이터 분석(EDA)을 위한 통합 환경을 제공합니다. 데이터 과학자는 데이터 분포, 상관관계 및 모델링 전에 데이터를 이해하는 데 중요한 기타 통계적 통찰력을 시각화할 수 있습니다.",
        "Other Options": [
            "Amazon QuickSight는 주로 대시보드를 생성하는 데 사용되지만, 머신 러닝 준비의 맥락에서 탐색적 데이터 분석을 수행하기 위해 특별히 설계된 것은 아닙니다.",
            "AWS Glue는 데이터 준비 및 ETL 프로세스에 중점을 두고 있으며, 탐색적 데이터 분석을 위한 직접적인 데이터 시각화 기능을 포함하지 않습니다.",
            "Amazon Redshift는 SQL 기반 분석을 허용하는 데이터 웨어하우스 솔루션이지만, 머신 러닝 모델 준비의 맥락에서 효과적인 탐색적 데이터 분석에 필요한 특정 도구와 시각화가 부족합니다."
        ]
    },
    {
        "Question Number": "28",
        "Situation": "소매 회사가 역사적 판매 데이터와 계절적 트렌드를 기반으로 재고 수준을 예측하여 재고 관리를 개선하고자 합니다. 데이터 과학 팀은 이 문제를 머신 러닝을 사용하여 해결할 수 있는 방식으로 정의해야 합니다.",
        "Question": "데이터 과학 팀은 이 재고 관리 문제를 머신 러닝 문제로 어떻게 구성해야 합니까?",
        "Options": {
            "1": "각 제품의 주문 필요 수량을 예측하기 위해 회귀 모델을 생성합니다.",
            "2": "제품을 '재고 있음'과 '재고 없음'으로 분류하기 위해 분류 모델을 개발합니다.",
            "3": "판매 패턴과 재고 수준에 따라 제품을 세분화하기 위해 클러스터링을 구현합니다.",
            "4": "이상 탐지를 사용하여 재고에 영향을 미칠 수 있는 비정상적인 판매 급증을 식별합니다."
        },
        "Correct Answer": "각 제품의 주문 필요 수량을 예측하기 위해 회귀 모델을 생성합니다.",
        "Explanation": "역사적 판매 데이터를 기반으로 재고 수준을 예측하는 문제는 연속 변수를 예측하는 것이 목표인 회귀 문제로 접근하는 것이 가장 적합합니다. 회귀 모델은 이러한 유형의 수치 예측을 효과적으로 처리할 수 있어 이 시나리오에 가장 적합한 구성입니다.",
        "Other Options": [
            "분류는 재고의 범주가 아닌 수량을 예측하는 것이 목표이므로 적절하지 않습니다.",
            "클러스터링은 재고 수준을 예측할 필요를 직접적으로 해결하지 않으며, 유사한 항목을 그룹화하는 데 더 중점을 둡니다.",
            "이상 탐지는 이상치를 식별하는 데 유용하지만, 역사적 트렌드를 기반으로 미래의 재고 필요를 예측하는 방법을 제공하지 않습니다."
        ]
    },
    {
        "Question Number": "29",
        "Situation": "머신 러닝 엔지니어가 이미지를 분류하기 위한 신경망 모델을 설계하고 있습니다. 엔지니어는 적절한 아키텍처, 레이어 및 노드를 선택해야 하며, 최적의 학습률과 활성화 함수도 결정해야 합니다. 이러한 요소를 이해하는 것은 높은 모델 정확도를 달성하는 데 중요합니다.",
        "Question": "다음 구성 중 이미지 분류 작업에서 신경망 모델의 성능을 가장 향상시킬 가능성이 높은 것은 무엇입니까?",
        "Options": {
            "1": "여러 개의 합성곱 레이어, ReLU 활성화 함수 및 0.01의 학습률을 사용하는 깊은 신경망.",
            "2": "하나의 은닉 레이어, 시그모이드 활성화 함수 및 0.1의 학습률을 사용하는 얕은 신경망.",
            "3": "LSTM 레이어, tanh 활성화 함수 및 0.005의 학습률을 사용하는 순환 신경망.",
            "4": "드롭아웃 정규화를 사용하고 출력 레이어에 소프트맥스 활성화 함수를 적용하며 0.001의 학습률을 사용하는 신경망."
        },
        "Correct Answer": "여러 개의 합성곱 레이어, ReLU 활성화 함수 및 0.01의 학습률을 사용하는 깊은 신경망.",
        "Explanation": "여러 개의 합성곱 레이어를 가진 깊은 신경망은 이미지 분류 작업에 적합하며, 데이터의 복잡한 패턴을 포착할 수 있습니다. ReLU 활성화 함수는 소실 기울기 문제를 줄이는 데 선호되며, 0.01의 학습률은 깊은 네트워크를 효율적으로 훈련하는 데 일반적으로 효과적입니다.",
        "Other Options": [
            "하나의 은닉 레이어를 가진 얕은 신경망은 이미지 분류에 덜 효과적이며, 데이터의 복잡성을 포착하지 못할 수 있습니다. 시그모이드 함수는 특히 깊은 네트워크에서 소실 기울기 문제를 겪을 수 있으며, 0.1의 학습률은 안정적인 수렴을 위해 너무 높을 수 있습니다.",
            "LSTM 레이어를 가진 순환 신경망은 주로 순차 데이터에 사용되므로 이미지 분류에 덜 적합합니다. tanh는 유용할 수 있지만, 깊은 네트워크에서는 ReLU만큼 효과적이지 않습니다. 0.005의 학습률은 효율적인 훈련을 위해 너무 낮을 수 있습니다.",
            "드롭아웃 정규화는 과적합 방지에 유익하지만, 소프트맥스는 일반적으로 은닉 레이어가 아닌 다중 클래스 출력 레이어에 사용됩니다. 0.001의 학습률은 특히 깊은 네트워크의 초기 훈련에 대해 너무 보수적일 수 있으며, 수렴 속도를 늦출 수 있습니다."
        ]
    },
    {
        "Question Number": "30",
        "Situation": "데이터 엔지니어가 AWS에서 Apache Spark를 사용하여 머신 러닝 프로젝트를 위한 대규모 데이터 세트를 처리하는 임무를 맡고 있습니다. 데이터 세트는 Amazon S3에 저장되어 있으며 모델 훈련 전에 광범위한 변환이 필요합니다. 엔지니어는 실행 시간과 자원 사용을 최소화하기 위해 데이터 처리 작업을 최적화해야 합니다.",
        "Question": "Apache Spark를 사용하여 데이터 변환 프로세스를 최적화하기 위한 가장 좋은 접근 방식은 무엇입니까?",
        "Options": {
            "1": "모든 데이터를 RDD에 로드하고 최대 병렬성을 보장하기 위해 map 및 reduce 함수를 사용하여 변환을 수행합니다.",
            "2": "처리 시간을 절약하기 위해 Amazon S3에 저장된 데이터에 대해 Spark에 로드하지 않고 직접 데이터 변환을 수행합니다.",
            "3": "Spark의 DataFrame API를 사용하여 메모리 내에서 변환을 수행하고 최적화된 실행을 위해 지연 평가를 활용합니다.",
            "4": "데이터 변환을 위해 Spark SQL 인터페이스만 사용합니다. 이는 DataFrame API보다 더 효율적입니다."
        },
        "Correct Answer": "Spark의 DataFrame API를 사용하여 메모리 내에서 변환을 수행하고 최적화된 실행을 위해 지연 평가를 활용합니다.",
        "Explanation": "Spark의 DataFrame API를 사용하면 메모리 내 계산과 쿼리 최적화를 위한 Catalyst와 같은 최적화를 통해 대규모 데이터 세트를 효율적으로 처리할 수 있습니다. 지연 평가는 데이터에 대한 패스를 줄여 성능을 향상시킵니다.",
        "Other Options": [
            "모든 데이터를 RDD에 로드하면 메모리 사용량이 증가할 수 있으며, DataFrame API의 최적화를 활용하지 못하므로 대규모 데이터 세트에 대해 덜 효율적입니다.",
            "Spark SQL은 효율적일 수 있지만, DataFrame API보다 본질적으로 더 효율적이지 않습니다. 이 옵션은 변환을 위한 DataFrame 사용의 장점을 무시합니다.",
            "Amazon S3에 있는 데이터에 대해 직접 변환을 수행하는 것은 불가능합니다. Spark는 변환을 효과적으로 수행하기 위해 메모리 내에서 데이터에 접근해야 합니다."
        ]
    },
    {
        "Question Number": "31",
        "Situation": "데이터 과학자가 고객 이탈 모델의 예측 성능을 개선하는 임무를 맡았습니다. 팀은 더 높은 정확도를 달성하고 과적합을 방지하기 위해 다양한 앙상블 학습 기법을 평가하고 있습니다. 그들은 결정을 내리기 전에 Bagging과 Boosting 방법의 특성을 이해하고 싶어합니다.",
        "Question": "앙상블 학습에서 Bagging과 Boosting의 차이를 올바르게 요약한 진술은 무엇입니까?",
        "Options": {
            "1": "Boosting은 대체 샘플링을 통해 새로운 훈련 세트를 생성하는 반면, Bagging은 각 훈련 인스턴스의 가중치를 조정하는 데 중점을 둡니다.",
            "2": "Bagging과 Boosting 모두 여러 학습자를 결합하여 과적합을 줄이고 모델 정확도를 개선하는 것을 목표로 합니다.",
            "3": "Bagging은 일반적으로 Boosting보다 더 나은 정확도를 제공하며, Boosting은 주로 과적합을 피하는 데 사용됩니다.",
            "4": "Bagging은 대체 샘플링을 사용하여 여러 훈련 세트를 생성하는 반면, Boosting은 모델이 재훈련될 때 가중치를 변경합니다."
        },
        "Correct Answer": "Bagging은 대체 샘플링을 사용하여 여러 훈련 세트를 생성하는 반면, Boosting은 모델이 재훈련될 때 가중치를 변경합니다.",
        "Explanation": "Bagging은 대체 샘플링을 통해 훈련 데이터의 여러 하위 집합을 생성하여 분산을 줄이고 과적합을 방지하는 데 도움을 줍니다. 반면, Boosting은 잘못 예측된 인스턴스의 가중치를 조정하여 모델을 개선하는 데 중점을 두어 전반적인 정확도를 향상시킵니다.",
        "Other Options": [
            "이 옵션은 Boosting이 대체 샘플링을 사용한다고 잘못 설명하고 있습니다. Boosting은 이러한 방식으로 새로운 훈련 세트를 생성하지 않으며, 대신 이전 오류를 기반으로 인스턴스의 가중치를 조정하는 데 중점을 둡니다.",
            "이 진술은 오해의 소지가 있으며, Bagging은 주로 분산을 줄이고 과적합을 피하는 데 도움을 주지만, Boosting은 가중치가 있는 데이터로 모델을 순차적으로 훈련하여 정확도를 높이는 데 더 중점을 둡니다.",
            "이 옵션은 두 방법 모두 과적합을 줄이는 것을 목표로 한다고 잘못 주장합니다. Bagging은 과적합을 완화하는 데 도움을 주지만, Boosting은 주의 깊게 모니터링하지 않으면 과적합의 위험을 증가시킵니다."
        ]
    },
    {
        "Question Number": "32",
        "Situation": "머신러닝 팀이 Amazon SageMaker를 사용하여 분류 문제의 모델 성능을 최적화하는 작업을 하고 있습니다. 그들은 모델의 정확도를 개선하기 위해 조정하고 싶은 하이퍼파라미터 세트를 가지고 있습니다. 팀은 SageMaker의 내장 기능을 활용하여 조정 프로세스를 자동화하고 프로젝트의 다른 측면에 집중하고 싶어합니다.",
        "Question": "팀이 하이퍼파라미터 조정을 위해 SageMaker를 효과적으로 활용하기 위해 어떤 접근 방식을 취해야 합니까?",
        "Options": {
            "1": "하이퍼파라미터 조정 없이 SageMaker의 내장 알고리즘을 사용하여 최적의 모델 성능을 달성합니다.",
            "2": "고정된 하이퍼파라미터로 여러 모델을 동시에 훈련하고 이후 가장 성능이 좋은 모델을 선택합니다.",
            "3": "각 훈련 작업 후에 하이퍼파라미터를 수동으로 조정하여 모델에 대한 최상의 설정을 찾습니다.",
            "4": "알고리즘을 선택하고 하이퍼파라미터의 범위를 정의하며 조정 프로세스 중 최적화할 메트릭을 지정합니다."
        },
        "Correct Answer": "알고리즘을 선택하고 하이퍼파라미터의 범위를 정의하며 조정 프로세스 중 최적화할 메트릭을 지정합니다.",
        "Explanation": "이 접근 방식은 Amazon SageMaker의 자동 하이퍼파라미터 조정 기능을 활용하여 팀이 하이퍼파라미터 세트, 그 범위 및 성능 메트릭을 정의할 수 있게 합니다. SageMaker는 지정된 메트릭을 기반으로 최상의 하이퍼파라미터 조합을 찾기 위해 여러 훈련 작업을 병렬로 실행합니다.",
        "Other Options": [
            "이 옵션은 수동으로 하이퍼파라미터를 조정하는 것이 비효율적이며, 조정 프로세스를 최적화하도록 설계된 SageMaker의 자동 조정 기능을 활용하지 않는다고 잘못 설명하고 있습니다.",
            "이 옵션은 고정된 하이퍼파라미터로 모델을 훈련하는 것이 SageMaker의 하이퍼파라미터 조정 기능을 활용하지 않는다고 잘못 설명하고 있습니다. 이 기능은 다양한 하이퍼파라미터 구성을 탐색하고 최적의 설정을 찾기 위해 특별히 설계되었습니다.",
            "이 옵션은 내장 알고리즘을 조정 없이 사용하는 것이 모델 성능을 극대화하지 못한다고 잘못 주장합니다. 하이퍼파라미터 조정은 모델을 데이터와 작업의 세부 사항에 맞추는 데 중요합니다."
        ]
    },
    {
        "Question Number": "33",
        "Situation": "의료 제공자가 퇴원 후 30일 이내에 환자 재입원을 예측하는 머신러닝 모델을 개발하고 있습니다. 모델의 성능은 실제 양성 비율과 실제로 양성인 예측 양성 비율 모두에 초점을 맞춰 평가해야 합니다. 의료 팀은 정확성에만 의존하는 것이 전체 그림을 제공하지 않을 수 있음을 이해하고 있습니다. 그들은 정밀도와 재현율 간의 균형을 맞출 수 있는 메트릭을 활용하고 싶어합니다.",
        "Question": "의료 제공자가 모델에서 정밀도와 재현율을 효과적으로 균형 있게 평가하기 위해 어떤 평가 메트릭을 사용해야 합니까?",
        "Options": {
            "1": "정확도",
            "2": "ROC-AUC",
            "3": "평균 제곱 오차",
            "4": "F1 점수"
        },
        "Correct Answer": "F1 점수",
        "Explanation": "F1 점수는 정밀도와 재현율의 조화 평균으로, 거짓 양성과 거짓 음성을 모두 고려해야 할 때 이상적인 메트릭입니다. 이는 두 가지 간의 균형을 제공하여 모델 성능 평가에서 정밀도와 재현율이 모두 소홀히 여겨지지 않도록 합니다.",
        "Other Options": [
            "정확도는 실제 양성과 실제 음성을 모두 고려하여 모델의 전반적인 정확성을 측정하지만, 부정 사례의 수가 지배하는 불균형 데이터 세트에서는 오해를 불러일으킬 수 있어 실제 양성을 식별하는 모델의 능력을 반영하지 못할 수 있습니다.",
            "ROC-AUC는 다양한 임계값에서 진짜 양성 비율과 거짓 양성 비율 간의 균형을 평가하지만, 정밀도나 재현율을 직접 측정하지 않으므로 두 메트릭이 동일하게 중요한 경우에는 덜 적합합니다.",
            "평균 제곱 오차는 주로 회귀 작업에 사용되며 예측 값과 실제 값 간의 평균 제곱 차이를 측정하지만, 정밀도와 재현율에 중점을 둔 분류 작업에는 적용되지 않습니다."
        ]
    },
    {
        "Question Number": "34",
        "Situation": "데이터 과학자가 많은 특성을 가진 대규모 데이터셋을 사용하여 회귀 모델을 작업하고 있습니다. 그들은 과적합에 대해 걱정하고 있으며 모델의 성능을 최적화하고 싶어합니다. 그들은 모델의 일반화 능력을 향상시키기 위해 정규화 기법을 사용하는 것을 고려하고 있습니다.",
        "Question": "데이터 과학자가 회귀 모델에서 L1 정규화를 L2 정규화보다 선호해야 하는 경우는 언제인가요?",
        "Options": {
            "1": "특성이 매우 많지만 모든 특성이 중요할 것이라고 예상할 때.",
            "2": "몇 가지 특성만 관련이 있다고 생각하고 차원을 줄이고 싶을 때.",
            "3": "모든 특성이 예측에 동등하게 기여할 것으로 예상될 때.",
            "4": "계산 효율성이 주요 관심사이며 특성 선택을 피하고 싶을 때."
        },
        "Correct Answer": "몇 가지 특성만 관련이 있다고 생각하고 차원을 줄이고 싶을 때.",
        "Explanation": "L1 정규화는 일부 계수를 0으로 축소할 수 있기 때문에 특성 선택에 효과적이며, 이는 모델의 차원을 효과적으로 줄입니다. 데이터 과학자가 결과에 관련된 특성이 일부만 있다고 의심할 때 특히 유용합니다.",
        "Other Options": [
            "이 옵션은 모든 특성이 동등하게 기여한다고 가정하고 특성 선택을 수행하지 않는 L2 정규화를 사용하는 것을 제안합니다. 이는 몇 가지 특성만 관련이 있다고 믿는 경우에는 적합하지 않습니다.",
            "이 옵션은 특성 선택보다 계산 효율성을 잘못 우선시합니다. L2는 계산적으로 효율적이지만 차원을 줄이거나 특성을 선택하는 목적을 달성하지 못합니다. 이는 몇 가지 특성만 관련이 있을 경우에 중요합니다.",
            "이 옵션은 모든 특성이 중요하다고 간주될 때 L2 정규화가 선호된다고 암시하지만, L1은 데이터 과학자가 관련 없는 특성을 제거하고자 할 때 더 적합합니다."
        ]
    },
    {
        "Question Number": "35",
        "Situation": "한 스타트업이 AWS에서 고객 이탈을 예측하기 위해 머신러닝 모델을 배포하고 있습니다. 그들은 현재 비용이 많이 드는 대규모 Amazon EC2 인스턴스 유형을 사용하고 있으며, 모델 성능을 희생하지 않고 자원 사용을 최적화하고 싶어합니다. 그들은 배포를 위한 최적의 자원을 식별해야 합니다.",
        "Question": "스타트업이 자원을 적절히 조정하기 위해 어떤 단계를 취해야 하나요? (두 가지 선택)",
        "Options": {
            "1": "더 효율적인 자원 관리를 위해 Amazon SageMaker 엔드포인트로 전환합니다.",
            "2": "현재 인스턴스의 CPU 및 메모리 사용량을 모니터링하여 과소 활용을 식별합니다.",
            "3": "모델에 대한 적절한 자원 할당을 보장하기 위해 인스턴스 크기를 늘립니다.",
            "4": "더 큰 부하를 처리하기 위해 여러 인스턴스에 모델을 배포합니다.",
            "5": "AWS Compute Optimizer를 사용하여 인스턴스 유형을 분석하고 적합한 대안을 추천합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "AWS Compute Optimizer를 사용하여 인스턴스 유형을 분석하고 적합한 대안을 추천합니다.",
            "현재 인스턴스의 CPU 및 메모리 사용량을 모니터링하여 과소 활용을 식별합니다."
        ],
        "Explanation": "AWS Compute Optimizer를 사용하면 스타트업이 현재 사용 패턴에 기반한 맞춤형 추천을 받아 성능을 저하시키지 않으면서 더 비용 효율적인 인스턴스 유형을 선택할 수 있습니다. CPU 및 메모리 사용량을 모니터링하면 현재 인스턴스가 과도하게 할당되었는지에 대한 통찰력을 제공하여 적절한 조정 결정을 내릴 수 있습니다.",
        "Other Options": [
            "인스턴스 크기를 늘리는 것은 적절한 조정에 역효과를 줄 수 있으며, 현재 인스턴스가 이미 과도하게 할당된 경우 불필요한 비용을 초래할 수 있습니다.",
            "Amazon SageMaker 엔드포인트로 전환하는 것은 효율성을 개선할 수 있지만, 성능 지표를 먼저 이해하지 않고는 현재 인스턴스의 적절한 조정 요구를 직접적으로 해결하지 못할 수 있습니다.",
            "여러 인스턴스에 모델을 배포하는 것은 중복성을 증가시킬 수 있지만, 비용 절감을 위한 자원 할당 최적화의 근본적인 필요를 해결하지는 않습니다."
        ]
    },
    {
        "Question Number": "36",
        "Situation": "한 소매 회사가 역사적 판매 데이터와 계절적 트렌드를 기반으로 다양한 제품 카테고리에 대한 미래 판매를 예측하고자 합니다.",
        "Question": "데이터 과학자가 이 판매 예측 모델링에 어떤 머신러닝 접근 방식을 사용해야 하나요?",
        "Options": {
            "1": "Amazon SageMaker DeepAR",
            "2": "Amazon SageMaker Linear Learner",
            "3": "Amazon SageMaker K-means",
            "4": "Amazon SageMaker XGBoost"
        },
        "Correct Answer": "Amazon SageMaker DeepAR",
        "Explanation": "Amazon SageMaker DeepAR는 시계열 데이터 예측을 위해 설계되었으며, 역사적 트렌드와 계절성을 기반으로 미래 판매를 예측하는 데 가장 적합한 옵션입니다.",
        "Other Options": [
            "Amazon SageMaker XGBoost는 일반적으로 분류 및 회귀 작업에 사용되지만, 이 시나리오에서 요구되는 시계열 예측에 특화되어 있지 않습니다.",
            "Amazon SageMaker K-means는 데이터를 클러스터로 그룹화하는 클러스터링 알고리즘이지만, 시계열 예측을 위한 예측 기능을 제공하지 않습니다.",
            "Amazon SageMaker Linear Learner는 회귀 작업을 수행할 수 있지만, 계절적 패턴을 고려해야 하는 복잡한 시계열 예측에 특별히 맞춰져 있지 않습니다."
        ]
    },
    {
        "Question Number": "37",
        "Situation": "데이터 과학자가 머신 러닝 모델을 위한 데이터셋을 준비하고 있습니다. 데이터셋에는 결측값, 범주형 변수 및 이상치가 포함된 다양한 특성이 있습니다. 데이터 과학자는 데이터셋이 깨끗하고 모델링에 적합한지 확인해야 합니다.",
        "Question": "모델을 위한 데이터를 정리하고 준비하는 가장 적절한 접근 방식은 무엇입니까?",
        "Options": {
            "1": "이상치를 중앙값으로 대체하고 모든 수치적 특성을 정규화합니다.",
            "2": "범주형 변수를 수치 레이블로 변환하고 결측값이 있는 특성을 삭제합니다.",
            "3": "결측값이 있는 모든 행을 제거하고 나머지 특성을 스케일링합니다.",
            "4": "결측값에 대해 평균 대체를 사용하고 범주형 변수에 대해 원-핫 인코딩을 사용합니다."
        },
        "Correct Answer": "결측값에 대해 평균 대체를 사용하고 범주형 변수에 대해 원-핫 인코딩을 사용합니다.",
        "Explanation": "평균 대체를 사용하면 데이터 포인트를 유지할 수 있어 일반적으로 삭제하는 것보다 낫습니다. 원-핫 인코딩은 범주형 변수를 처리하는 데 효과적이며, 순서 관계를 암시하지 않기 때문에 많은 머신 러닝 알고리즘에 적합합니다.",
        "Other Options": [
            "결측값이 있는 모든 행을 제거하면 특히 데이터셋이 작을 경우 귀중한 데이터 손실로 이어질 수 있습니다. 이 접근 방식은 모델링을 위한 데이터 준비에 최선의 방법이 아닐 수 있습니다.",
            "이상치를 중앙값으로 대체하는 것은 이상치의 근본 원인을 해결하지 않으며, 특성을 정규화하는 것은 사용되는 알고리즘에 따라 필요하지 않을 수 있습니다. 이 접근 방식은 결측값이나 범주형 변수를 포괄적으로 처리하지 않습니다.",
            "범주형 변수를 수치 레이블로 변환하면 의도하지 않은 순서 관계가 발생할 수 있으며, 이는 특정 알고리즘을 오도할 수 있습니다. 결측값이 있는 특성을 삭제하면 상당한 데이터 손실로 이어질 수 있습니다."
        ]
    },
    {
        "Question Number": "38",
        "Situation": "머신 러닝 전문가가 Amazon SageMaker를 사용하여 분류 모델을 개발하고 있습니다. 전문가는 모델이 훈련 데이터에서는 매우 잘 작동하지만 검증 데이터에서는 성능이 저조하여 잠재적인 과적합을 나타내는 것을 발견했습니다. 모델의 일반화를 개선하기 위해 전문가는 과적합을 줄이기 위한 전략을 구현하고자 합니다.",
        "Question": "전문가가 적용해야 할 전략의 조합은 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "모델에 L2 정규화를 적용합니다.",
            "2": "훈련 중 교차 검증 기법을 활용합니다.",
            "3": "더 많은 레이어를 추가하여 모델의 복잡성을 증가시킵니다.",
            "4": "훈련 에포크 수를 줄입니다.",
            "5": "훈련 데이터셋의 크기를 늘립니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "훈련 중 교차 검증 기법을 활용합니다.",
            "모델에 L2 정규화를 적용합니다."
        ],
        "Explanation": "훈련 중 교차 검증 기법을 활용하면 통계 분석 결과가 독립적인 데이터셋에 어떻게 일반화될지를 평가하는 데 도움이 되어 과적합의 위험을 줄일 수 있습니다. L2 정규화를 적용하면 모델의 큰 가중치에 패널티를 부여하여 모델을 단순화하고 과적합을 완화하는 데 도움이 됩니다.",
        "Other Options": [
            "모델의 복잡성을 증가시키기 위해 레이어를 추가하면 문제를 해결하기보다는 과적합을 더욱 초래할 수 있습니다. 복잡한 모델은 훈련 데이터의 노이즈에 맞추기 쉬워지기 때문입니다.",
            "훈련 데이터셋의 크기를 늘리면 모델의 일반화를 개선하는 데 도움이 될 수 있지만, 데이터 가용성에 따라 실현 가능하지 않을 수 있으며 현재 모델의 복잡성이 높다면 과적합을 직접적으로 해결하지 않습니다.",
            "훈련 에포크 수를 줄이면 과적합을 방지하는 데 도움이 될 수 있지만, 모델이 데이터에서 학습할 시간이 충분하지 않으면 과소적합으로 이어질 수 있습니다."
        ]
    },
    {
        "Question Number": "39",
        "Situation": "머신 러닝 전문가가 다양한 분류 모델을 평가하고 있으며, 민감도와 특이성 지표를 기반으로 의사 결정을 위한 최적의 임계값을 선택하고자 합니다. 전문가는 수신자 조작 특성(ROC) 곡선을 생성하고 각 모델에 대한 곡선 아래 면적(AUC)을 계산했습니다. 목표는 민감도와 특이성 간의 균형을 효과적으로 맞추는 임계값을 식별하는 것입니다.",
        "Question": "전문가가 ROC 곡선에서 민감도와 특이성을 최대화하기 위한 최적의 임계값을 결정하는 가장 좋은 접근 방식은 무엇입니까?",
        "Options": {
            "1": "ROC 곡선이 대각선과 교차하는 지점을 식별합니다.",
            "2": "ROC 곡선에서 왼쪽 상단 모서리에 가장 가까운 점을 선택합니다.",
            "3": "거짓 긍정과 관계없이 가장 높은 진양성 비율을 초래하는 임계값을 사용합니다.",
            "4": "모델 중에서 가장 높은 AUC 값을 제공하는 임계값을 선택합니다."
        },
        "Correct Answer": "ROC 곡선에서 왼쪽 상단 모서리에 가장 가까운 점을 선택합니다.",
        "Explanation": "최적의 임계값은 ROC 곡선에서 진양성을 최대화하면서 거짓 긍정을 최소화하는 지점에서 찾을 수 있으며, 이는 그래프의 왼쪽 상단 모서리에 가장 가까운 지점에 해당합니다. 이 지점은 민감도와 특이성 간의 최상의 균형을 나타냅니다.",
        "Other Options": [
            "가장 높은 AUC를 기준으로 임계값을 선택하는 것은 민감도와 특이성 간의 최적 균형을 보장하지 않습니다. 높은 AUC는 모델 성능을 나타내지만 최상의 운영 임계값을 결정하지는 않습니다.",
            "ROC 곡선이 대각선과 교차하는 지점(AUC 0.5)은 구별력이 없는 모델을 나타내므로 민감도와 특이성을 최대화하기 위한 적절한 임계값이 아닙니다.",
            "거짓 긍정을 고려하지 않고 가장 높은 진양성 비율에만 집중하면 실제 시나리오에서 잘 작동하지 않을 수 있는 불균형 모델로 이어질 수 있습니다. 균형 잡힌 접근이 필요합니다."
        ]
    },
    {
        "Question Number": "40",
        "Situation": "머신 러닝 전문가가 전자상거래 플랫폼에 배포된 두 가지 추천 알고리즘을 평가하기 위해 A/B 테스트를 수행하고 있습니다. 목표는 어떤 알고리즘이 사용자들 사이에서 더 높은 전환율을 이끄는지를 결정하는 것입니다. 각 알고리즘은 한 달 동안 무작위로 선택된 사용자 그룹에 제시됩니다.",
        "Question": "A/B 테스트가 유효하고 결과가 신뢰할 수 있도록 보장하기 위해 어떤 조치를 취해야 합니까? (두 가지 선택)",
        "Options": {
            "1": "선택 편향을 줄이기 위해 사용자를 두 알고리즘 중 하나에 무작위로 할당합니다.",
            "2": "결과의 변동성을 최소화하기 위해 두 알고리즘에 동일한 사용자 그룹을 사용합니다.",
            "3": "계절 효과를 고려할 수 있도록 충분한 기간 동안 테스트를 실행합니다.",
            "4": "각 그룹의 샘플 크기가 통계적 유의성을 달성할 수 있을 만큼 충분히 큰지 확인합니다.",
            "5": "테스트가 완료된 후에만 사용자 피드백을 수집하고 분석합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "각 그룹의 샘플 크기가 통계적 유의성을 달성할 수 있을 만큼 충분히 큰지 확인합니다.",
            "선택 편향을 줄이기 위해 사용자를 두 알고리즘 중 하나에 무작위로 할당합니다."
        ],
        "Explanation": "충분히 큰 샘플 크기를 보장하는 것은 통계적 유의성을 달성하는 데 중요하며, 이는 테스트 결과의 의미 있는 해석을 가능하게 합니다. 사용자를 알고리즘에 무작위로 할당하면 선택 편향을 제거하여 두 그룹이 비교 가능하고 결과가 유효하도록 보장합니다.",
        "Other Options": [
            "계절 효과를 고려할 수 있도록 충분한 기간 동안 테스트를 실행하는 것은 중요하지만, 샘플 크기가 너무 작거나 선택 편향이 있는 경우에는 충분하지 않습니다.",
            "두 알고리즘에 동일한 사용자 그룹을 사용하는 것은 편향을 초래하고 결과의 독립성을 저해하여 유효한 A/B 테스트에 필수적인 요소입니다.",
            "테스트가 완료된 후에만 사용자 피드백을 수집하고 분석하는 것은 테스트 자체의 유효성에 영향을 미치지 않지만, A/B 테스트 중 실시간 분석을 위한 권장 관행은 아닙니다."
        ]
    },
    {
        "Question Number": "41",
        "Situation": "데이터 과학자는 역사적 레이블이 있는 데이터를 기반으로 새로운 데이터 포인트의 범주를 정확하게 예측할 수 있는 분류 모델을 구축하는 임무를 맡고 있습니다. 모델은 특성 공간에서 데이터 포인트의 근접성을 기반으로 예측을 해야 합니다.",
        "Question": "데이터 과학자가 이 분류 작업에 사용할 머신 러닝 알고리즘은 무엇입니까?",
        "Options": {
            "1": "Random Forest 알고리즘",
            "2": "K-Nearest Neighbors 알고리즘",
            "3": "Support Vector Machine 알고리즘",
            "4": "Gradient Boosting 알고리즘"
        },
        "Correct Answer": "K-Nearest Neighbors 알고리즘",
        "Explanation": "K-Nearest Neighbors (KNN) 알고리즘은 특성 공간에서 가장 가까운 이웃의 클래스에 따라 새로운 데이터 포인트를 분류하는 감독 학습 방법입니다. 이 알고리즘은 인스턴스의 분류가 K개의 가장 가까운 이웃 중 다수 클래스에 의해 결정되는 분류 작업을 효과적으로 처리합니다.",
        "Other Options": [
            "Support Vector Machine 알고리즘은 주로 고차원 공간에서 클래스를 가장 잘 분리하는 초평면을 찾는 데 사용되며, 근접성을 기반으로 한 직접적인 분류에는 가장 적합하지 않을 수 있습니다.",
            "Random Forest 알고리즘은 분류를 위해 여러 결정 트리를 구성하는 앙상블 방법으로, KNN처럼 근접성을 기반으로 하지 않기 때문에 더 복잡합니다.",
            "Gradient Boosting 알고리즘은 모델을 순차적으로 구축하고 이전 모델의 오류에 집중하는 앙상블 방법으로, 단순한 이웃 기반 분류 작업에는 덜 적합합니다."
        ]
    },
    {
        "Question Number": "42",
        "Situation": "의료 제공자가 환자 데이터를 기반으로 심각한 의학적 상태의 존재를 예측하기 위해 머신 러닝 모델을 개발하고 있습니다. 그들은 잘못된 진단의 수를 최소화하는 것을 우선시하며, 여기서 거짓 부정은 환자에게 심각한 결과를 초래할 수 있습니다. 이 시나리오에서 제공자가 더 중점적으로 집중해야 할 성과 지표는 무엇입니까?",
        "Question": "이 시나리오에서 의료 제공자에게 가장 중요한 지표는 무엇입니까?",
        "Options": {
            "1": "특이도",
            "2": "정밀도",
            "3": "민감도",
            "4": "음성 예측 값"
        },
        "Correct Answer": "민감도",
        "Explanation": "이 시나리오에서 의료 제공자는 민감도에 집중해야 합니다. 민감도는 진양성 비율을 측정하고 잘못된 진단(거짓 부정)을 최소화합니다. 높은 민감도는 상태가 있는 대부분의 환자가 올바르게 식별되도록 보장하며, 이는 환자 안전에 매우 중요합니다.",
        "Other Options": [
            "특이도는 거짓 긍정을 줄이는 데 중점을 두기 때문에 이 경우 덜 중요합니다. 그러나 모든 상태가 있는 환자를 식별하는 것이 우선이므로 민감도가 더 중요합니다.",
            "정밀도는 진양성의 비율을 진양성과 거짓 긍정의 합으로 측정합니다. 중요하지만, 이 맥락에서 거짓 부정을 최소화할 필요를 직접적으로 다루지는 않습니다.",
            "음성 예측 값은 음성 테스트를 받은 대상이 실제로 상태가 없을 가능성을 나타냅니다. 관련이 있지만, 모든 진짜 사례를 식별하는 주요 관심사를 다루지는 않습니다."
        ]
    },
    {
        "Question Number": "43",
        "Situation": "머신 러닝 전문가가 대규모 데이터 세트를 저장하기 위해 Amazon S3를 사용하고 분석을 위해 Amazon Redshift를 사용하는 추천 시스템을 구축하고 있습니다. 데이터 세트에는 사용자 상호작용, 제품 세부정보 및 거래 기록이 포함됩니다. 전문가는 머신 러닝 모델 교육을 위해 데이터에 쉽게 접근할 수 있도록 하고, 실시간 예측을 위해 효율적으로 처리될 수 있도록 해야 합니다.",
        "Question": "데이터 접근성과 ML 처리를 최적화하기 위해 어떤 데이터 저장소 솔루션 조합을 구현해야 합니까? (두 개 선택)",
        "Options": {
            "1": "데이터 웨어하우징을 위해 Amazon Redshift를 구현하고 ETL 프로세스를 위해 Amazon Glue를 사용합니다.",
            "2": "구조화된 쿼리와 빠른 접근을 위해 Amazon RDS에 데이터를 저장합니다.",
            "3": "데이터 저장을 위해 Amazon S3를 활용하고 대규모 데이터 세트를 쿼리하기 위해 Amazon Athena를 사용합니다.",
            "4": "세션 기반 사용자 상호작용 데이터를 저장하기 위해 Amazon DynamoDB를 사용합니다.",
            "5": "데이터 세트 전반에 걸쳐 인덱싱 및 검색 기능을 위해 Amazon Elasticsearch를 활용합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "데이터 저장을 위해 Amazon S3를 활용하고 대규모 데이터 세트를 쿼리하기 위해 Amazon Athena를 사용합니다.",
            "데이터 웨어하우징을 위해 Amazon Redshift를 구현하고 ETL 프로세스를 위해 Amazon Glue를 사용합니다."
        ],
        "Explanation": "데이터 저장을 위해 Amazon S3를 활용하면 대규모 데이터 세트를 확장 가능하고 비용 효율적으로 저장할 수 있으며, Amazon Athena는 서버리스 쿼리 기능을 제공하여 인프라를 프로비저닝하지 않고도 데이터를 쉽게 접근할 수 있게 합니다. 또한, Amazon Redshift를 구현하면 분석을 위한 효율적인 데이터 웨어하우징이 가능하고, Amazon Glue는 ETL 프로세스를 자동화하여 데이터가 머신 러닝 모델을 위해 처리되고 준비되도록 보장합니다.",
        "Other Options": [
            "Amazon RDS에 데이터를 저장하면 Amazon S3를 사용할 때보다 확장성과 유연성이 제한될 수 있으며, 특히 대규모 데이터 세트에 대해서는 분석 작업에 적합하지 않을 수 있습니다.",
            "Amazon DynamoDB를 사용하는 것은 세션 기반 데이터에 유리하지만, 일반적으로 머신 러닝 모델 교육에 필요한 대규모 역사적 데이터 세트를 처리하는 데 가장 적합한 선택이 아닐 수 있습니다.",
            "Amazon Elasticsearch를 활용하는 것은 검색 기능에 좋지만, 머신 러닝에 필요한 구조화된 데이터 분석이나 ETL 프로세스를 위해 주로 설계된 것이 아닙니다."
        ]
    },
    {
        "Question Number": "44",
        "Situation": "데이터 과학 팀이 분류 문제를 작업하고 있지만 모델 교육을 위한 레이블이 있는 데이터의 양에 어려움을 겪고 있습니다. 그들은 데이터 세트가 충분한지 평가하고 레이블이 있는 데이터의 잠재적 부족을 완화하기 위한 전략을 고려하고자 합니다.",
        "Question": "팀이 레이블이 있는 데이터의 충분성을 평가하고 잠재적 완화 전략을 식별하기 위해 어떤 접근 방식을 취해야 합니까?",
        "Options": {
            "1": "모델 성능을 위한 최소 샘플 크기를 결정하기 위해 통계적 힘 분석을 수행합니다.",
            "2": "활성 학습을 구현하여 더 큰 비레이블 데이터 세트에서 레이블링을 위한 가장 유익한 샘플을 반복적으로 선택합니다.",
            "3": "제한된 레이블 데이터 보완을 위해 유사한 작업에 대한 사전 훈련된 모델을 활용하는 전이 학습 기술을 사용합니다.",
            "4": "레이블링 요구 사항을 결정하기 전에 클래스 불균형이 없도록 특성 분포를 분석합니다."
        },
        "Correct Answer": "제한된 레이블 데이터 보완을 위해 유사한 작업에 대한 사전 훈련된 모델을 활용하는 전이 학습 기술을 사용합니다.",
        "Explanation": "전이 학습은 레이블 데이터가 부족할 때 특히 효과적이며, 관련 작업에서 얻은 지식을 모델이 활용할 수 있게 합니다. 이 전략은 제한된 레이블 데이터로도 성능을 향상시킬 수 있어 적절한 완화 접근 방식이 됩니다.",
        "Other Options": [
            "통계적 힘 분석을 수행하는 것은 샘플 크기 요구 사항을 이해하는 데 도움이 될 수 있지만, 레이블 데이터 부족을 관리하는 방법을 직접적으로 다루지는 않습니다.",
            "활성 학습을 구현하는 것은 레이블 데이터 세트를 개선하기 위한 유효한 전략이지만, 시작하기 위해 초기 레이블 데이터 세트가 필요하며, 이는 데이터 부족의 근본적인 문제를 해결하지 못할 수 있습니다.",
            "특성 분포를 분석하는 것은 잠재적 편향을 이해하는 데 중요하지만, 레이블 데이터 부족이나 더 많은 데이터를 획득하기 위한 전략에 대한 직접적인 해결책을 제공하지 않습니다."
        ]
    },
    {
        "Question Number": "45",
        "Situation": "머신 러닝 엔지니어가 높은 컴퓨팅 성능이 필요한 딥 러닝 프로젝트를 위해 EC2 인스턴스를 설정하고 있습니다. 그들은 모델 교육 프로세스를 최적화하기 위해 사용할 최상의 인스턴스 유형과 Amazon Machine Images (AMIs)를 고려하고 있습니다.",
        "Question": "엔지니어가 구현해야 할 전략의 조합은 무엇입니까? (두 개 선택)",
        "Options": {
            "1": "최적의 성능을 위해 p3 인스턴스 유형을 사용합니다.",
            "2": "TensorFlow와 PyTorch가 미리 로드된 AMI를 활용합니다.",
            "3": "CPU 성능 향상을 위해 m5 인스턴스 유형을 선택합니다.",
            "4": "설정을 위해 표준 Amazon Linux AMI를 선택합니다.",
            "5": "EC2에서 GPU 인스턴스에 대한 한도 증가를 요청합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "EC2에서 GPU 인스턴스에 대한 한도 증가를 요청합니다.",
            "TensorFlow와 PyTorch가 미리 로드된 AMI를 활용합니다."
        ],
        "Explanation": "GPU 인스턴스에 대한 한도 증가를 요청하는 것은 필수적이며, 이러한 인스턴스는 고성능 머신 러닝 작업에 필요합니다. 또한, TensorFlow와 PyTorch와 같은 인기 있는 머신 러닝 라이브러리가 미리 로드된 AMI를 사용하면 설정 시간을 크게 줄이고 환경이 딥 러닝 작업에 최적화되도록 보장할 수 있습니다.",
        "Other Options": [
            "표준 Amazon Linux AMI를 선택하면 머신 러닝에 필요한 라이브러리와 도구를 제공하지 않을 수 있어 설정 시간이 늘어나고 과정이 복잡해질 수 있습니다.",
            "p3 인스턴스 유형은 실제로 강력하지만 유일한 옵션은 아니므로 선택지를 제한하지 않고는 최상의 전략으로 간주될 수 없습니다.",
            "m5 인스턴스 유형을 선택하는 것은 CPU 성능에 초점을 맞추지만, 이는 GPU 가속의 이점을 누리는 딥 러닝 작업에는 이상적이지 않습니다."
        ]
    },
    {
        "Question Number": "46",
        "Situation": "데이터 과학자가 고객 피드백을 분석하기 위해 AWS에서 머신 러닝 솔루션을 개발하고 있습니다. 데이터 보안 및 규정 준수를 보장하기 위해 팀은 이 솔루션에 사용되는 데이터 레이크에서 민감한 정보를 관리하기 위한 모범 사례를 구현해야 합니다.",
        "Question": "팀이 데이터 레이크에 저장된 민감한 데이터의 암호화를 시행하기 위해 주로 사용해야 하는 AWS 서비스는 무엇입니까?",
        "Options": {
            "1": "Amazon S3 Object Lock",
            "2": "AWS Key Management Service (KMS)",
            "3": "Amazon Elastic Block Store (EBS)",
            "4": "AWS Secrets Manager"
        },
        "Correct Answer": "AWS Key Management Service (KMS)",
        "Explanation": "AWS Key Management Service (KMS)는 데이터 레이크에서 사용되는 다양한 AWS 서비스의 데이터를 암호화하는 데 사용되는 암호화 키를 관리하고 제어하기 위해 특별히 설계되었습니다. 이는 데이터가 저장 중 및 전송 중에 암호화되도록 강제하여 민감한 정보가 보호되도록 합니다.",
        "Other Options": [
            "Amazon Elastic Block Store (EBS)는 주로 블록 스토리지에 사용되며 일부 암호화 기능을 제공하지만, 다양한 AWS 서비스에서 암호화 키를 관리하는 주요 서비스는 아닙니다.",
            "Amazon S3 Object Lock은 지정된 기간 동안 객체가 삭제되거나 덮어쓰여지는 것을 방지하는 데 사용됩니다. 데이터 보존에 도움이 되지만, 암호화 키를 관리하거나 암호화 기능을 제공하지는 않습니다.",
            "AWS Secrets Manager는 API 키 및 데이터베이스 자격 증명과 같은 비밀을 관리하기 위해 설계되었습니다. 데이터 레이크에 저장된 데이터에 대한 암호화를 제공하지 않으므로 이 특정 요구 사항에 적합하지 않습니다."
        ]
    },
    {
        "Question Number": "47",
        "Situation": "금융 서비스 회사가 실시간으로 거래 데이터를 수집하여 즉각적으로 분석할 수 있는 사기 탐지 시스템을 구축하고 있습니다. 회사는 데이터가 거의 실시간으로 처리되어 발생하는 잠재적인 사기 활동을 식별할 수 있도록 하기를 원합니다. 이 스트리밍 데이터를 효율적으로 수집하고 처리하기 위한 다양한 옵션을 고려하고 있습니다.",
        "Question": "실시간 데이터 수집 및 처리를 위한 요구 사항을 가장 잘 충족하는 솔루션은 무엇입니까?",
        "Options": {
            "1": "Amazon Kinesis Data Analytics를 사용하여 데이터를 수집하고 결과를 Amazon DynamoDB에 직접 저장합니다.",
            "2": "Amazon Kinesis Data Streams를 사용하여 거래 데이터를 수집하고 AWS Lambda를 사용하여 데이터를 실시간으로 처리합니다.",
            "3": "Amazon Kinesis Data Firehose를 구현하여 스트리밍 데이터를 Amazon Redshift에 저장하고 주기적으로 데이터를 쿼리합니다.",
            "4": "Amazon S3를 사용하여 거래 데이터의 배치 업로드를 수행하고 예약된 AWS Glue 작업을 실행하여 데이터를 처리합니다."
        },
        "Correct Answer": "Amazon Kinesis Data Streams를 사용하여 거래 데이터를 수집하고 AWS Lambda를 사용하여 데이터를 실시간으로 처리합니다.",
        "Explanation": "Amazon Kinesis Data Streams를 사용하면 회사가 실시간 스트리밍 데이터를 효율적으로 수집할 수 있으며, AWS Lambda는 도착하는 즉시 데이터를 처리하는 서버리스 컴퓨팅 서비스를 제공하여 실시간 분석 및 사기 탐지에 적합합니다.",
        "Other Options": [
            "Amazon S3를 사용하여 배치 업로드를 수행하는 것은 데이터 수집 및 분석에 지연을 초래하므로 실시간 처리에 적합하지 않습니다.",
            "Amazon Kinesis Data Firehose를 사용하여 데이터를 Amazon Redshift에 저장하는 것은 스트리밍 데이터를 즉시 처리할 수 없으며, 배치 처리에 더 적합합니다.",
            "Amazon Kinesis Data Analytics를 사용하여 데이터를 수집하는 것은 분석에 유용하지만, 필요한 수집 메커니즘을 직접 제공하지 않으며, 이미 수집된 데이터를 처리하는 데 더 적합합니다."
        ]
    },
    {
        "Question Number": "48",
        "Situation": "데이터 과학자가 Amazon SageMaker를 사용하여 고객 행동의 대규모 데이터 세트를 클러스터링하는 임무를 맡고 있습니다. 데이터 세트는 레이블이 없으며 고객 상호작용과 관련된 다양한 특성을 포함하고 있습니다. 목표는 타겟 마케팅 전략을 위한 뚜렷한 고객 세그먼트를 식별하는 것입니다.",
        "Question": "이 데이터 세트에 대해 K-Means 클러스터링을 효과적으로 구현하기 위해 어떤 기술 조합을 사용해야 합니까? (두 가지 선택)",
        "Options": {
            "1": "데이터 세트에서 무작위 샘플링을 사용하여 중심점을 초기화합니다.",
            "2": "엘보우 방법을 사용하여 최적의 클러스터 수를 결정합니다.",
            "3": "클러스터링 정확도를 높이기 위해 감독 학습 알고리즘을 통합합니다.",
            "4": "특성을 표준화하여 평균이 0이고 표준 편차가 1이 되도록 합니다.",
            "5": "클러스터링 전에 PCA와 같은 차원 축소 기법을 적용합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "특성을 표준화하여 평균이 0이고 표준 편차가 1이 되도록 합니다.",
            "엘보우 방법을 사용하여 최적의 클러스터 수를 결정합니다."
        ],
        "Explanation": "특성을 표준화하면 모든 특성이 K-Means 클러스터링의 거리 계산에 동일하게 기여하도록 하여, 범위가 큰 특성이 클러스터 할당에 불균형적으로 영향을 미치는 것을 방지합니다. 엘보우 방법은 설명된 분산을 클러스터 수의 함수로 플로팅하고 '무릎' 지점을 찾는 일반적인 기술입니다.",
        "Other Options": [
            "감독 학습 알고리즘을 통합하는 것은 K-Means에 적합하지 않으며, K-Means는 레이블 정보를 사용하지 않는 비감독 학습 기법입니다.",
            "PCA와 같은 차원 축소 기법을 적용하는 것이 유익할 수 있지만, K-Means 클러스터링에 필수적이지 않으며 이산 그룹을 식별하는 요구 사항을 직접 해결하지 않을 수 있습니다.",
            "무작위 샘플링을 사용하여 중심점을 초기화하는 것은 무작위 초기화로 인해 나쁜 클러스터링 결과를 초래할 수 있습니다. K-Means++와 같은 방법을 사용하는 것이 더 효과적인 초기화를 위해 일반적으로 더 좋습니다."
        ]
    },
    {
        "Question Number": "49",
        "Situation": "머신 러닝 엔지니어가 고객 이탈을 예측하기 위해 Amazon SageMaker를 사용하여 이진 분류 모델을 구축하고 있습니다. 데이터셋에는 이탈한 고객 10,000건과 이탈하지 않은 고객 90,000건이 포함되어 있어 불균형 분포가 발생합니다. 초기 모델의 정확도는 92%이지만, 이탈한 클래스의 재현율은 45%에 불과합니다. 엔지니어는 정밀도를 크게 희생하지 않으면서 재현율을 개선해야 합니다.",
        "Question": "소수 클래스의 재현율을 향상시키기 위해 어떤 전략 조합을 사용해야 합니까? (두 가지 선택)",
        "Options": {
            "1": "강력한 모델 평가를 보장하기 위해 교차 검증을 활용합니다.",
            "2": "소수 클래스를 위한 합성 샘플을 생성하기 위해 SMOTE를 구현합니다.",
            "3": "데이터셋의 균형을 맞추기 위해 다수 클래스의 일부 인스턴스를 제거합니다.",
            "4": "다수 클래스 전용의 별도 모델을 훈련합니다.",
            "5": "재현율을 최적화하기 위해 다른 분류 임계값을 설정합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "소수 클래스를 위한 합성 샘플을 생성하기 위해 SMOTE를 구현합니다.",
            "재현율을 최적화하기 위해 다른 분류 임계값을 설정합니다."
        ],
        "Explanation": "SMOTE를 사용하면 소수 클래스의 합성 인스턴스를 생성할 수 있어 모델이 이탈과 관련된 더 나은 패턴을 학습하는 데 도움이 됩니다. 분류 임계값을 조정하면 정밀도와 재현율 간의 균형에 직접적인 영향을 미쳐 이탈한 클래스의 재현율을 증가시킬 수 있습니다.",
        "Other Options": [
            "다수 클래스의 인스턴스를 제거하면 귀중한 정보가 손실될 수 있으며, 다수 클래스가 이미 상당히 우세할 경우 모델의 효과성이 감소할 수 있습니다.",
            "교차 검증은 모델 평가를 위한 좋은 방법이지만, 불균형 문제를 직접적으로 해결하거나 소수 클래스의 재현율을 개선하지는 않습니다.",
            "다수 클래스 전용의 별도 모델을 훈련하는 것은 소수 클래스의 재현율을 개선하는 데 도움이 되지 않으며, 불균형 문제를 해결하지 않고 전체 솔루션의 복잡성을 증가시킬 수 있습니다."
        ]
    },
    {
        "Question Number": "50",
        "Situation": "머신 러닝 전문가가 AWS 서비스를 사용하여 오디오 파일을 텍스트로 전사하는 작업을 맡고 있습니다. 전문가는 선택한 서비스가 다양한 오디오 형식을 처리하고 정확한 전사 결과를 제공할 수 있는지 평가하고 있습니다.",
        "Question": "전문가는 Amazon Transcribe와 함께 어떤 오디오 형식을 사용할 수 있습니까? (두 가지 선택)",
        "Options": {
            "1": "WAV",
            "2": "CSV",
            "3": "TXT",
            "4": "MP3",
            "5": "AAC"
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "MP3",
            "WAV"
        ],
        "Explanation": "Amazon Transcribe는 MP3 및 WAV와 같은 오디오 형식을 지원하며, 이는 오디오 파일에 일반적으로 사용됩니다. 이러한 형식은 음성 언어를 텍스트로 고품질로 전사할 수 있도록 하여 서비스의 기능에 적합합니다.",
        "Other Options": [
            "TXT는 텍스트 형식이며 전사 입력 오디오 형식으로 사용할 수 없습니다. Amazon Transcribe는 텍스트로 변환할 오디오 파일을 필요로 합니다.",
            "AAC는 현재 Amazon Transcribe의 전사 작업에서 지원되지 않습니다. 인기 있는 오디오 형식이지만, 이 서비스는 AAC를 포함하지 않는 특정 지원 형식을 가지고 있습니다.",
            "CSV는 구조화된 데이터를 위한 데이터 형식이며 오디오 형식이 아닙니다. Amazon Transcribe는 전사 작업을 위해 오디오 입력이 필요하며, CSV는 이 목적에 적합하지 않습니다."
        ]
    },
    {
        "Question Number": "51",
        "Situation": "데이터 과학자가 TensorFlow를 사용하여 머신 러닝 프로젝트를 진행하고 있습니다. 그들은 모델 아키텍처를 정의하기 위해 계산 그래프를 구축하고 있습니다. 과학자는 변수를 저장하고 순차적으로 작업을 수행할 수 있도록 해야 합니다.",
        "Question": "이 시나리오에서 TensorFlow Graph 객체를 사용하는 목적은 무엇입니까?",
        "Options": {
            "1": "사용자 개입 없이 자동으로 훈련 프로세스를 처리하기 위해.",
            "2": "실행하지 않고 신경망의 데이터 흐름을 시각화하기 위해.",
            "3": "작업 없이 모델의 가중치와 편향을 직접 저장하기 위해.",
            "4": "세션에서 실행할 수 있는 일련의 계산을 정의하기 위해."
        },
        "Correct Answer": "세션에서 실행할 수 있는 일련의 계산을 정의하기 위해.",
        "Explanation": "TensorFlow Graph 객체는 수행하려는 계산 및 작업을 정의하기 위한 청사진 역할을 합니다. 이를 통해 모델의 구조를 구축하고 조직할 수 있으며, 이후 TensorFlow 세션에서 실행하여 효율적인 계산 관리를 가능하게 합니다.",
        "Other Options": [
            "이 옵션은 잘못된 것입니다. 가중치와 편향은 모델의 일부이지만, Graph 객체는 이러한 매개변수를 저장하기 위해서만 사용되는 것이 아니라 작업과 계산을 정의하기 위해 사용됩니다.",
            "이 옵션은 잘못된 것입니다. TensorFlow는 전체 훈련 프로세스를 자동으로 처리하지 않으며, 사용자 정의 작업과 훈련 루프에 대한 제어가 필요합니다.",
            "이 옵션은 잘못된 것입니다. TensorFlow는 시각화 도구를 제공하지만, Graph 객체의 주요 목적은 단순히 데이터 흐름을 시각화하는 것이 아니라 계산을 정의하고 실행하는 것입니다."
        ]
    },
    {
        "Question Number": "52",
        "Situation": "데이터 과학자는 Amazon SageMaker를 사용하여 머신 러닝 모델을 구축하는 임무를 맡았습니다. 그들은 입력 데이터의 정규화가 필요하고 회귀 및 분류 작업을 모두 지원하는 이진 분류 문제에 적합한 알고리즘을 선택해야 합니다.",
        "Question": "이 요구 사항에 가장 적합한 Amazon SageMaker 알고리즘은 무엇입니까?",
        "Options": {
            "1": "Linear Learner, 분류 및 회귀 작업을 모두 처리할 수 있습니다.",
            "2": "DeepAR, 시계열 예측을 위해 설계되었습니다.",
            "3": "XGBoost, 대규모 데이터 세트에 최적화되어 있으며 데이터 정규화가 필요하지 않습니다.",
            "4": "K-Means, 주로 분류보다는 클러스터링에 사용됩니다."
        },
        "Correct Answer": "Linear Learner, 분류 및 회귀 작업을 모두 처리할 수 있습니다.",
        "Explanation": "Amazon SageMaker의 Linear Learner 알고리즘은 회귀 및 분류 작업 모두를 위해 특별히 설계되었으며, 최적의 성능을 위해 데이터 정규화가 필요합니다. 이는 시나리오에 명시된 요구 사항과 잘 맞습니다.",
        "Other Options": [
            "XGBoost는 분류 작업에 강력한 알고리즘이지만 정규화가 필요하지 않으며, 정규화에 대한 강조를 고려할 때 요구 사항에 가장 적합하지 않습니다.",
            "K-Means는 클러스터링 알고리즘이며 분류 문제에 적합하지 않습니다. 이 옵션은 데이터 과학자의 작업 요구를 충족하지 않습니다.",
            "DeepAR는 시계열 예측을 위해 설계되었으며 이진 분류 문제에는 적용되지 않으므로 현재 작업에 부적합한 선택입니다."
        ]
    },
    {
        "Question Number": "53",
        "Situation": "데이터 엔지니어링 팀은 IoT 장치에서 스트리밍 데이터를 처리하여 실시간 머신 러닝 추론을 위한 데이터 수집 파이프라인을 개발하는 임무를 맡았습니다. 그들은 Amazon EMR을 활용하여 데이터 처리 작업을 효율적으로 처리하고자 합니다. 팀은 수신 데이터의 양이 증가함에 따라 파이프라인이 원활하게 확장될 수 있도록 해야 합니다.",
        "Question": "팀이 Amazon EMR을 사용하여 스트리밍 데이터 수집 파이프라인을 조정하면서 높은 확장성과 낮은 지연 시간을 보장하기 위해 가장 잘 사용할 수 있는 접근 방식은 무엇입니까?",
        "Options": {
            "1": "Amazon Kinesis Data Streams를 사용하여 스트리밍 데이터를 수집하고 이를 실시간으로 데이터를 처리하는 Amazon EMR 작업과 통합합니다.",
            "2": "IoT 장치에서 Amazon EMR 클러스터로 직접 연결을 설정하고 Apache Spark 스트리밍을 사용하여 데이터를 처리합니다.",
            "3": "AWS IoT Core를 사용하여 데이터를 Amazon S3에 직접 수집한 다음 Amazon EMR 작업을 트리거하여 데이터를 배치로 처리합니다.",
            "4": "AWS Lambda 함수를 활용하여 스트리밍 데이터를 Amazon EMR 클러스터로 푸시하여 실시간으로 처리합니다."
        },
        "Correct Answer": "Amazon Kinesis Data Streams를 사용하여 스트리밍 데이터를 수집하고 이를 실시간으로 데이터를 처리하는 Amazon EMR 작업과 통합합니다.",
        "Explanation": "Amazon Kinesis Data Streams를 사용하면 팀이 낮은 지연 시간으로 높은 처리량의 데이터 수집을 처리할 수 있습니다. 이는 스트리밍 데이터의 실시간 처리를 위해 설계되었으며, Amazon EMR과 통합하면 팀이 실시간 분석을 위한 Spark의 확장 가능한 처리 기능을 활용할 수 있습니다.",
        "Other Options": [
            "데이터를 Amazon S3에 직접 수집한 다음 EMR 작업을 트리거하는 것은 배치 처리로 인해 지연 시간을 초래하므로 실시간 요구 사항에 적합하지 않습니다.",
            "IoT 장치에서 EMR 클러스터로 직접 연결을 설정하는 것은 확장성과 보안 문제로 인해 권장되지 않으며 아키텍처를 복잡하게 만듭니다.",
            "AWS Lambda를 사용하여 스트리밍 데이터를 EMR 클러스터로 푸시하는 것은 비효율적입니다. Lambda는 단기 작업을 위해 설계되었으며 지속적인 스트리밍 데이터 수집에는 적합하지 않습니다."
        ]
    },
    {
        "Question Number": "54",
        "Situation": "머신 러닝 엔지니어는 PyTorch를 사용하여 역사적 데이터를 기반으로 주가를 예측하는 신경망을 개발하는 임무를 맡았습니다. 훈련 프로세스를 최적화하기 위해 엔지니어는 제로로 채워진 다차원 배열 역할을 하는 텐서를 생성해야 하며, 텐서가 그래디언트 계산을 위한 연산을 추적하도록 해야 합니다.",
        "Question": "엔지니어가 PyTorch에서 역전파를 위한 연산 순서를 유지하는 제로로 채워진 텐서를 어떻게 생성할 수 있습니까?",
        "Options": {
            "1": "torch.empty((3, 3), requires_grad=False)",
            "2": "torch.zeros((3, 3), requires_grad=True)",
            "3": "torch.full((3, 3), 0, requires_grad=False)",
            "4": "torch.ones((3, 3), requires_grad=True)"
        },
        "Correct Answer": "torch.zeros((3, 3), requires_grad=True)",
        "Explanation": "올바른 옵션은 제로로 채워진 3x3 텐서를 생성하고 requires_grad=True로 설정하여 그래디언트 추적을 가능하게 합니다. 이는 신경망 훈련 중 역전파에 필수적입니다.",
        "Other Options": [
            "이 옵션은 값을 초기화하지 않은 빈 텐서를 생성하며, requires_grad가 False로 설정되어 있어 그래디언트를 추적하지 않습니다.",
            "이 옵션은 제로 대신 1로 채워진 텐서를 생성하므로 제로로 채워진 텐서의 요구 사항을 충족하지 않습니다.",
            "이 옵션은 제로로 채워진 텐서를 생성하지만 requires_grad가 False로 설정되어 있어 역전파를 위한 연산을 추적하지 않습니다."
        ]
    },
    {
        "Question Number": "55",
        "Situation": "한 회사가 Amazon SageMaker를 사용하여 이미지 분류 모델을 개발하고 있습니다. 데이터는 S3에 저장되어 있으며, 이미지를 여러 카테고리로 정확하게 분류하는 훈련 작업을 생성해야 합니다. 팀은 최적의 성능과 자원 활용을 보장하기 위해 특정 매개변수로 훈련 작업을 구성해야 합니다.",
        "Question": "Amazon SageMaker에서 이미지 분류기를 위한 훈련 작업을 생성하는 데 필요한 구성은 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "훈련 작업이 무한정 실행되지 않도록 최대 실행 시간을 미리 정의된 한도로 설정합니다.",
            "2": "예측할 클래스의 수를 제공하며, 이는 모델의 출력층의 뉴런 수에 해당합니다.",
            "3": "입력 데이터 구성에서 훈련 데이터와 검증 데이터의 S3 위치를 지정합니다.",
            "4": "이미지 분류 작업에 GPU 인스턴스가 필요하지 않으므로 CPU만 지원하는 인스턴스 유형을 선택합니다.",
            "5": "이미지 분류에 적합한 Amazon SageMaker의 미리 구축된 알고리즘 중 하나를 선택합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "입력 데이터 구성에서 훈련 데이터와 검증 데이터의 S3 위치를 지정합니다.",
            "예측할 클래스의 수를 제공하며, 이는 모델의 출력층의 뉴런 수에 해당합니다."
        ],
        "Explanation": "Amazon SageMaker에서 이미지 분류기를 위한 훈련 작업을 생성하려면 훈련 및 검증 데이터가 저장된 S3 위치를 지정하는 것이 필수적입니다. 또한, 클래스 수를 제공하는 것은 모델의 출력층 구성을 정의하므로 이미지를 원하는 카테고리로 올바르게 분류할 수 있도록 보장합니다.",
        "Other Options": [
            "CPU만 지원하는 인스턴스 유형을 선택하는 것은 이미지 분류 작업에 적합하지 않을 수 있으며, 특히 선택한 알고리즘이 성능을 위해 GPU 가속의 이점을 누릴 수 있는 경우에는 더욱 그렇습니다. 일부 알고리즘은 효율적인 훈련을 위해 GPU 인스턴스가 필요합니다.",
            "최대 실행 시간을 설정하는 것은 SageMaker 훈련 작업에 필요한 구성 사항이 아닙니다. 작업 기간을 모니터링하고 관리하는 것은 좋지만, 작업 자체를 생성하는 데 핵심 요구 사항은 아닙니다.",
            "적절한 알고리즘을 선택하는 것은 중요하지만, 이는 훈련 작업 생성 과정과 관련된 구성 세부 사항이 아닙니다. 이 선택은 작업 설정 전에 이루어집니다."
        ]
    },
    {
        "Question Number": "56",
        "Situation": "한 데이터 과학자가 소매 비즈니스의 고객 세분화를 위한 여러 특성을 포함하는 고차원 데이터셋을 다루고 있습니다. 과학자는 데이터를 시각화하고 기계 학습 모델의 계산 복잡성을 줄이기를 원합니다. 그들은 차원 축소를 위해 주성분 분석(Principal Component Analysis, PCA)을 사용하는 것을 고려하고 있습니다.",
        "Question": "이 시나리오에서 PCA를 사용하는 주요 이점은 무엇입니까?",
        "Options": {
            "1": "PCA는 더 나은 정확성을 위해 특성의 수를 증가시키는 데 도움을 줄 수 있습니다.",
            "2": "PCA는 원래 특성이 새로운 데이터셋에 유지될 것이라고 보장합니다.",
            "3": "PCA는 가능한 한 많은 분산을 유지하면서 차원을 줄입니다.",
            "4": "PCA는 모델 훈련 전에 특성 스케일링의 필요성을 제거할 수 있습니다."
        },
        "Correct Answer": "PCA는 가능한 한 많은 분산을 유지하면서 차원을 줄입니다.",
        "Explanation": "PCA의 주요 이점은 데이터셋의 특성 수를 줄이면서 원래 데이터의 분산을 많이 유지할 수 있다는 것입니다. 이는 모델을 단순화하고 시각화를 개선하는 데 도움이 되며, 중요한 정보를 잃지 않도록 합니다.",
        "Other Options": [
            "이 옵션은 PCA가 특성의 수를 줄이기 위해 설계되었기 때문에 잘못된 것입니다. 적절한 정당화 없이 특성을 증가시키는 것은 과적합으로 이어질 수 있습니다.",
            "이 옵션은 PCA가 특성 스케일링의 필요성을 제거하지 않기 때문에 잘못된 것입니다. 사실, PCA를 적용하기 전에 모든 특성이 분석에 동등하게 기여하도록 특성을 스케일링하는 것이 종종 권장됩니다.",
            "이 옵션은 PCA가 원래 특성을 새로운 특성 집합(주성분)으로 변환하며, 이는 원래 특성의 선형 조합이기 때문에 원래 특성이 유지되지 않기 때문에 잘못된 것입니다."
        ]
    },
    {
        "Question Number": "57",
        "Situation": "한 헬스케어 스타트업이 질병 탐지를 위한 의료 이미지를 분류하는 기계 학습 애플리케이션을 개발하고 있습니다. 팀은 이미지를 처리하기 위해 합성곱 신경망(Convolutional Neural Network, CNN)을 사용하는 것을 고려하고 있습니다. 그들은 모델이 이미지의 원시 픽셀 값에서 직접 특징을 효과적으로 학습할 수 있도록 하고, 분류 성능을 개선하기 위해 사전 훈련된 필터를 활용하고자 합니다.",
        "Question": "이 시나리오에서 이미지 분류를 위해 합성곱 신경망을 사용하는 주요 장점은 무엇입니까?",
        "Options": {
            "1": "CNN은 모든 입력 특성이 출력 분류에 동등하게 기여하도록 보장하기 위해 전적으로 완전 연결층만을 사용합니다.",
            "2": "CNN은 훈련을 위해 대량의 레이블이 있는 데이터가 필요하므로 데이터 가용성이 제한된 시나리오에는 덜 적합합니다.",
            "3": "CNN은 그레이스케일 이미지에서만 작동하도록 설계되어 있어 이미지 처리 작업의 범위를 제한합니다.",
            "4": "CNN은 이미지에서 계층적 특징을 자동으로 추출할 수 있어 수동 특징 엔지니어링 없이 효과적인 분류를 가능하게 합니다."
        },
        "Correct Answer": "CNN은 이미지에서 계층적 특징을 자동으로 추출할 수 있어 수동 특징 엔지니어링 없이 효과적인 분류를 가능하게 합니다.",
        "Explanation": "합성곱 신경망은 합성곱 필터를 적용하여 이미지에서 특징을 자동으로 학습하는 데 뛰어나며, 이를 통해 공간적 계층 구조를 포착할 수 있습니다. 이는 이미지 분류 작업에 매우 효과적이며, 특징 위치에 대한 사전 지식이 필요하지 않습니다.",
        "Other Options": [
            "CNN은 대량의 레이블이 있는 데이터셋에서 이점을 얻지만, 사전 훈련된 모델을 활용한 전이 학습과 같은 기술을 활용할 수 있어 데이터 가용성이 제한된 시나리오에도 적합합니다.",
            "CNN은 그레이스케일 이미지뿐만 아니라 컬러 이미지도 처리할 수 있어 제한된 색상 형식 이상의 다양한 이미지 분류 작업에 유연하게 적용할 수 있습니다.",
            "CNN은 주로 합성곱 층과 풀링 층을 사용하며, 완전 연결층은 일반적으로 분류를 위해 마지막에 사용되지만 CNN 아키텍처의 유일한 구성 요소는 아닙니다."
        ]
    },
    {
        "Question Number": "58",
        "Situation": "금융 서비스 회사는 기계 학습 모델 훈련을 위해 대량의 역사적 거래 데이터를 처리해야 합니다. 그들은 데이터를 분석에 적합한 형식으로 변환하고 Amazon SageMaker에 최종적으로 수집하기 위해 분산 데이터 처리 작업을 실행할 수 있는 솔루션이 필요합니다.",
        "Question": "회사가 이러한 배치 데이터 처리 작업을 효율적으로 조정하기 위해 어떤 AWS 서비스를 사용해야 합니까?",
        "Options": {
            "1": "AWS Glue",
            "2": "Amazon EMR",
            "3": "Amazon Redshift",
            "4": "AWS Lambda"
        },
        "Correct Answer": "Amazon EMR",
        "Explanation": "Amazon EMR은 Apache Spark와 같은 프레임워크를 사용하여 대량의 데이터를 처리하도록 설계되어 있어, 기계 학습 모델 준비 및 변환을 위한 배치 작업을 실행하는 데 적합합니다. 이후 Amazon SageMaker에 수집하기 위해서입니다.",
        "Other Options": [
            "AWS Glue는 주로 ETL 서비스로, Amazon EMR에 비해 대규모 배치 처리 작업에 효과적이지 않을 수 있습니다. Amazon EMR은 분산 데이터 처리에 최적화되어 있습니다.",
            "Amazon Redshift는 데이터 웨어하우스 서비스로, 데이터 처리 서비스가 아닙니다. 배치 데이터 처리 작업을 조정하기보다는 데이터를 쿼리하고 분석하는 데 더 적합합니다.",
            "AWS Lambda는 짧은 생명 주기의 프로세스와 이벤트 기반 아키텍처에 더 적합한 서버리스 컴퓨팅 서비스입니다. 대규모 배치 처리 작업을 처리하도록 설계되지 않았습니다."
        ]
    },
    {
        "Question Number": "59",
        "Situation": "데이터 엔지니어는 다양한 출처의 원시 데이터를 분석 및 기계 학습에 적합한 구조화된 형식으로 변환하는 작업을 맡고 있습니다. 엔지니어는 효과적인 데이터 변환 기술을 결정해야 합니다.",
        "Question": "엔지니어가 데이터를 변환하기 위해 사용할 수 있는 기술은 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "데이터 시각화 도구",
            "2": "데이터 정규화",
            "3": "특징 스케일링 방법",
            "4": "Apache Spark SQL",
            "5": "관계형 데이터베이스 관리 시스템"
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Apache Spark SQL",
            "특징 스케일링 방법"
        ],
        "Explanation": "Apache Spark SQL은 대규모 데이터 세트를 쿼리하고 변환하는 데 강력한 도구로, 데이터 변환 솔루션에 적합합니다. 표준화 또는 정규화와 같은 특징 스케일링 방법은 모든 특징이 분석에 동등하게 기여하도록 하여 기계 학습 모델을 준비하는 데 필수적입니다.",
        "Other Options": [
            "데이터 시각화 도구는 주로 데이터를 그래픽적으로 표현하는 데 사용되며, 데이터를 분석에 적합한 구조화된 형식으로 실제로 변환하는 데 도움을 주지 않습니다.",
            "데이터 정규화는 사용할 수 있는 특정 기술이지만, 원시 데이터 소스에서 다양한 데이터 유형과 구조를 변환하기 위한 독립적인 솔루션은 아닙니다.",
            "관계형 데이터베이스 관리 시스템은 주로 구조화된 데이터를 저장하고 관리하는 데 사용되며, 원시 데이터를 구조화된 형식으로 변환하는 데 사용되지 않습니다."
        ]
    },
    {
        "Question Number": "60",
        "Situation": "데이터 과학자는 텍스트 번역 기능이 필요한 다국어 애플리케이션에서 작업하고 있습니다. 그들은 대량의 텍스트를 배치로 번역할 수 있는 솔루션을 구현해야 하며, 특정 용어를 제공하여 번역을 사용자 정의할 수 있는 유연성도 필요합니다. 데이터 과학자는 이 작업에 Amazon Translate를 사용하는 것을 고려하고 있습니다.",
        "Question": "Amazon Translate의 어떤 기능이 데이터 과학자가 전문 용어를 포함하고 정확한 번역을 보장할 수 있도록 합니까?",
        "Options": {
            "1": "번역 메모리",
            "2": "사용자 정의 용어",
            "3": "배치 처리",
            "4": "실시간 번역"
        },
        "Correct Answer": "사용자 정의 용어",
        "Explanation": "사용자 정의 용어는 사용자가 번역에 사용할 특정 용어와 구문을 정의할 수 있게 하여, 번역이 애플리케이션의 전문적인 요구를 충족하도록 보장합니다. 이 기능은 CSV 또는 TMX 형식의 사용자 정의 사전 사용을 지원합니다.",
        "Other Options": [
            "실시간 번역은 입력되는 텍스트를 즉시 번역하는 기능을 의미합니다. 유용하지만, 사용자 정의 용어를 포함할 필요를 구체적으로 다루지 않습니다.",
            "배치 처리는 대량의 텍스트를 한 번에 번역할 수 있는 기능이지만, 전문 용어를 통한 번역의 사용자 정의를 촉진하지 않습니다.",
            "번역 메모리는 일부 번역 서비스에서 이전에 번역된 세그먼트를 저장하여 향후 사용할 수 있도록 하는 기능이지만, 사용자 정의 용어와 같은 전문 용어를 추가하는 메커니즘을 제공하지 않습니다."
        ]
    },
    {
        "Question Number": "61",
        "Situation": "데이터 과학자는 구매 이력, 인구 통계 및 탐색 행동을 포함하는 데이터 세트를 사용하여 전자 상거래 플랫폼의 고객 세분화를 분석하는 임무를 맡았습니다. 초기 k-평균 클러스터링을 수행한 후, 과학자는 최적의 클러스터 수가 불분명하다는 것을 알게 되었고, 이는 고객 세그먼트의 잘못된 해석으로 이어질 수 있습니다. 적절한 클러스터 수를 보다 신뢰성 있게 결정하기 위해 데이터 과학자는 어떤 방법을 활용해야 할까요?",
        "Question": "분석을 위한 최적의 클러스터 수에 대한 명확한 통찰력을 제공할 수 있는 기술은 무엇인가요?",
        "Options": {
            "1": "고객 세그먼트를 예측하기 위해 랜덤 포레스트 분류기를 구현합니다.",
            "2": "클러스터링 전에 데이터 세트를 정규화하기 위해 특성 스케일링을 적용합니다.",
            "3": "차원 축소를 위해 주성분 분석을 수행합니다.",
            "4": "클러스터의 응집력과 분리를 평가하기 위해 실루엣 점수를 사용합니다."
        },
        "Correct Answer": "클러스터의 응집력과 분리를 평가하기 위해 실루엣 점수를 사용합니다.",
        "Explanation": "실루엣 점수는 객체가 자신의 클러스터에 얼마나 유사한지를 다른 클러스터와 비교하여 측정합니다. 높은 실루엣 점수는 더 잘 정의된 클러스터를 나타내며, 탐색적 데이터 분석에서 최적의 클러스터 수를 결정하는 효과적인 방법입니다.",
        "Other Options": [
            "랜덤 포레스트 분류기를 구현하는 것은 클러스터 수를 결정하는 데 직접적인 도움이 되지 않으며, 탐색적 클러스터링 분석보다는 감독 학습 작업에 더 적합합니다.",
            "특성 스케일링을 적용하는 것은 클러스터링 알고리즘에 중요하지만 최적의 클러스터 수에 대한 통찰력을 제공하지 않습니다. 이는 분석 기술이 아닌 전처리 단계입니다.",
            "주성분 분석을 수행하는 것은 차원 축소에 도움이 될 수 있지만 최적의 클러스터 수를 식별하는 데는 도움이 되지 않습니다. 이는 시각화 및 특성 추출에 사용되며 클러스터링 평가에는 사용되지 않습니다."
        ]
    },
    {
        "Question Number": "62",
        "Situation": "소매 회사는 고객의 다양한 제품과의 상호작용을 분석하여 제품 추천 시스템을 개선하려고 합니다. 목표는 레이블이 없는 데이터를 기반으로 고객 행동 및 선호도에 따라 제품 간의 유사성을 발견하는 것입니다. 회사는 이 솔루션을 효과적으로 구현하기 위해 AWS 서비스를 활용하고자 합니다.",
        "Question": "회사가 이 목표를 달성하기 위해 어떤 접근 방식을 사용해야 할까요?",
        "Options": {
            "1": "과거 판매 데이터를 기반으로 유사한 제품을 판매 지표 및 고객 평가에 따라 그룹화하기 위해 전통적인 클러스터링 모델을 훈련합니다.",
            "2": "Amazon Rekognition을 사용하여 제품 이미지를 분석하고 시각적 특성을 기반으로 제품 간의 유사성 탐지를 위한 특성을 추출합니다.",
            "3": "Amazon SageMaker 이미지 분류 알고리즘을 구현하여 제품을 이미지에 따라 분류한 후 유사한 제품을 추천합니다.",
            "4": "Amazon SageMaker Object2Vec를 활용하여 제품을 특성 벡터로 표현하여 고객 상호작용을 기반으로 유사성 분석을 가능하게 합니다."
        },
        "Correct Answer": "Amazon SageMaker Object2Vec를 활용하여 제품을 특성 벡터로 표현하여 고객 상호작용을 기반으로 유사성 분석을 가능하게 합니다.",
        "Explanation": "Amazon SageMaker Object2Vec를 사용하면 회사는 제품을 특성 벡터로 변환하여 고객 상호작용을 기반으로 관계와 유사성을 효과적으로 포착할 수 있습니다. 이는 레이블이 없는 데이터로 제품 유사성을 이해하는 목표에 이상적입니다.",
        "Other Options": [
            "이미지 분류 알고리즘을 구현하는 것은 제품을 순수하게 시각적 특성에 따라 분류하는 데 중점을 두며, 고객 상호작용 및 행동에 기반한 유사성을 직접적으로 발견하지 않습니다.",
            "Amazon Rekognition은 이미지 분석 및 객체 감지를 위해 특별히 설계되었으며, 제품 유사성을 위한 고객 상호작용 데이터를 분석하는 데 적합하지 않으며, 필요한 비지도 학습 측면이 부족합니다.",
            "판매 데이터에 대한 전통적인 클러스터링 모델을 훈련하는 것은 제품 관계에 대한 통찰력을 제공할 수 있지만 고객 상호작용 데이터를 효과적으로 활용하지 않으며 유사성 탐지를 위한 비지도 학습의 이점을 활용하지 않습니다."
        ]
    },
    {
        "Question Number": "63",
        "Situation": "소매 회사는 고객 구매 행동을 예측하기 위한 머신 러닝 모델을 향상시키기 위해 다양한 유형의 데이터를 수집하고 있습니다. 데이터 소스에는 거래 기록, 웹 로그 및 고객 피드백이 포함됩니다. 데이터 엔지니어는 모델의 정확성과 통찰력을 개선하기 위해 필수 데이터 소스를 식별하고 집계하는 임무를 맡았습니다. 데이터 엔지니어가 가장 먼저 고려해야 할 주요 데이터 소스는 무엇인가요?",
        "Question": "고객 구매 행동 모델의 예측 정확성을 향상시키기 위해 가장 관련성이 높은 주요 데이터 소스는 무엇인가요?",
        "Options": {
            "1": "웹사이트에서 사용자 상호작용을 캡처하는 웹 로그",
            "2": "설문조사 및 리뷰에서 수집된 고객 피드백",
            "3": "고객 구매를 상세히 기록한 역사적 거래 기록",
            "4": "다양한 플랫폼의 소셜 미디어 참여 데이터"
        },
        "Correct Answer": "고객 구매를 상세히 기록한 역사적 거래 기록",
        "Explanation": "역사적 거래 기록은 고객 구매 행동에 대한 직접적인 통찰력을 제공하므로 구매와 관련된 정확한 예측 모델을 구축하는 데 가장 중요한 데이터 소스입니다.",
        "Other Options": [
            "고객 피드백은 고객 만족도 및 선호도를 이해하는 데 유용하지만 구매 행동을 직접적으로 나타내지는 않습니다.",
            "웹 로그는 사용자 상호작용 및 관심에 대한 통찰력을 제공하지만 실제 구매 행동과는 직접적인 상관관계가 없습니다.",
            "소셜 미디어 참여 데이터는 브랜드 감정 및 관심을 반영할 수 있지만 거래 기록에 비해 구매 행동과의 직접적인 관련성이 낮습니다."
        ]
    },
    {
        "Question Number": "64",
        "Situation": "한 회사가 음성 기반 애플리케이션을 개발하고 있으며, 이 애플리케이션은 텍스트를 음성으로 변환하는 기능이 필요합니다. 이 애플리케이션은 사용자가 생성한 텍스트를 여러 언어로 자연스럽게 들리는 음성으로 변환해야 합니다. 머신 러닝 전문가는 이 요구 사항을 충족할 수 있는 적절한 AWS 서비스를 선택해야 합니다.",
        "Question": "전문가가 이 기능을 효과적으로 구현하기 위해 어떤 AWS 서비스를 사용해야 합니까?",
        "Options": {
            "1": "Amazon Rekognition은 이미지 분석 및 객체 감지를 위한 것입니다.",
            "2": "Amazon Lex는 대화형 인터페이스와 챗봇을 만들기 위해 사용됩니다.",
            "3": "Amazon Polly는 텍스트를 여러 언어로 생동감 있는 음성으로 변환합니다.",
            "4": "AWS Lambda는 서버를 프로비저닝하지 않고 이벤트에 응답하여 코드를 실행합니다."
        },
        "Correct Answer": "Amazon Polly는 텍스트를 여러 언어로 생동감 있는 음성으로 변환합니다.",
        "Explanation": "Amazon Polly는 고급 딥 러닝 기술을 사용하여 텍스트를 자연스럽게 들리는 음성으로 변환하도록 특별히 설계되었습니다. 여러 언어와 음성 스타일을 지원하여 텍스트-음성 변환 기능이 필요한 애플리케이션에 이상적인 선택입니다.",
        "Other Options": [
            "Amazon Rekognition은 얼굴 인식 및 객체 감지와 같은 이미지 및 비디오 분석에 중점을 두고 있으며, 텍스트-음성 변환 기능에는 적용되지 않습니다.",
            "Amazon Lex는 대화형 인터페이스와 챗봇을 구축하는 데 사용되며, 음성 인식을 포함할 수 있지만 텍스트-음성 변환 기능을 직접 제공하지는 않습니다.",
            "AWS Lambda는 이벤트에 응답하여 코드를 실행하는 서버리스 컴퓨팅 서비스입니다. 다른 서비스와 통합할 수 있지만, 자체적으로 텍스트-음성 변환 기능을 제공하지는 않습니다."
        ]
    },
    {
        "Question Number": "65",
        "Situation": "제품 관리자가 고객 세분화 문제에 대한 머신 러닝 솔루션을 구현할지 여부를 결정하고자 합니다. 관리자는 데이터의 특성과 비즈니스 요구 사항에 따라 머신 러닝의 적합성을 평가해야 합니다.",
        "Question": "어떤 시나리오가 머신 러닝을 사용해야 함을 나타냅니까? (두 가지 선택)",
        "Options": {
            "1": "원하는 결과가 명확한 분류 작업이며 규칙이 잘 정의되어 있습니다.",
            "2": "문제를 간단한 휴리스틱이나 규칙 기반 시스템으로 해결할 수 있습니다.",
            "3": "데이터셋이 작고 전통적인 통계 방법이 충분합니다.",
            "4": "데이터에 전통적인 방법으로 모델링하기 어려운 복잡한 패턴이 있습니다.",
            "5": "문제가 과거 데이터를 기반으로 고객 행동을 예측하는 것입니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "문제가 과거 데이터를 기반으로 고객 행동을 예측하는 것입니다.",
            "데이터에 전통적인 방법으로 모델링하기 어려운 복잡한 패턴이 있습니다."
        ],
        "Explanation": "머신 러닝은 과거 데이터를 기반으로 결과를 예측해야 할 필요가 있는 시나리오와 데이터가 전통적인 방법으로 쉽게 포착할 수 없는 복잡한 패턴을 보일 때 특히 적합합니다. 고객 세분화에서는 복잡한 행동 패턴을 이해하기 위해 종종 머신 러닝이 제공하는 고급 기술이 필요합니다.",
        "Other Options": [
            "이 옵션은 잘못되었습니다. 작은 데이터셋은 일반적으로 전통적인 통계 방법이 효과적일 수 있음을 의미하며, 이러한 경우 머신 러닝이 큰 이점을 제공하지 않을 수 있습니다.",
            "이 옵션은 잘못되었습니다. 분류 작업이 때때로 머신 러닝의 이점을 받을 수 있지만, 규칙이 잘 정의되고 간단할 경우 미리 정의된 규칙으로도 효과적으로 처리할 수 있습니다.",
            "이 옵션은 잘못되었습니다. 간단한 휴리스틱이나 규칙 기반 시스템은 일반적으로 머신 러닝이 유익한 상황을 나타내지 않으며, 머신 러닝은 문제가 복잡하고 적응형 솔루션이 필요할 때 가장 가치가 있습니다."
        ]
    }
]