[
    {
        "Question Number": "1",
        "Situation": "Une entreprise doit accorder à un membre spécifique de l'équipe l'accès à un bucket Amazon S3 tout en restreignant l'accès à certains objets au sein du bucket. L'administrateur IAM souhaite maintenir une gestion simplifiée tout en s'assurant que le membre de l'équipe dispose uniquement des autorisations nécessaires.",
        "Question": "Quel type de politique l'administrateur doit-il utiliser, et quel format ARN de ressource doit-il spécifier pour limiter l'accès aux objets dans le bucket ? (Choisissez deux.)",
        "Options": {
            "1": "Utiliser une politique inline et spécifier l'ARN comme arn:aws:s3:::bucket-name/*",
            "2": "Utiliser une politique gérée par le client et spécifier l'ARN comme arn:aws:s3:::bucket-name",
            "3": "Utiliser une politique gérée par AWS et spécifier l'ARN comme arn:aws:s3:::bucket-name/*",
            "4": "Utiliser une politique inline et spécifier l'ARN comme arn:aws:s3:::bucket-name",
            "5": "Utiliser une politique de bucket et spécifier l'ARN comme arn:aws:s3:::bucket-name/specific-object-key"
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Utiliser une politique inline et spécifier l'ARN comme arn:aws:s3:::bucket-name/*",
            "Utiliser une politique de bucket et spécifier l'ARN comme arn:aws:s3:::bucket-name/specific-object-key"
        ],
        "Explanation": "Une politique inline est une politique intégrée à une seule identité IAM (un utilisateur, un groupe ou un rôle). Cela permettrait à l'administrateur d'accorder des autorisations spécifiques à un seul utilisateur, ce qui est la nécessité dans ce cas. L'ARN 'arn:aws:s3:::bucket-name/*' accorderait l'accès à tous les objets dans le bucket. Une politique de bucket est une politique basée sur la ressource – elle vous permet de créer une politique et de l'attacher directement au bucket S3. L'ARN 'arn:aws:s3:::bucket-name/specific-object-key' restreindrait l'accès à un objet spécifique dans le bucket.",
        "Other Options": [
            "Utiliser une politique gérée par le client et spécifier l'ARN comme 'arn:aws:s3:::bucket-name' ne restreindrait pas l'accès à des objets spécifiques dans le bucket. Au lieu de cela, cela accorderait l'accès à l'ensemble du bucket.",
            "Utiliser une politique gérée par AWS et spécifier l'ARN comme 'arn:aws:s3:::bucket-name/*' ne serait pas idéal car les politiques gérées par AWS sont conçues pour fournir des autorisations pour des cas d'utilisation courants et sont gérées par AWS. Cela peut ne pas fournir le contrôle granulaire requis dans ce scénario.",
            "Utiliser une politique inline et spécifier l'ARN comme 'arn:aws:s3:::bucket-name' ne restreindrait pas l'accès à des objets spécifiques dans le bucket. Au lieu de cela, cela accorderait l'accès à l'ensemble du bucket."
        ]
    },
    {
        "Question Number": "2",
        "Situation": "Une entreprise souhaite gérer les autorisations pour un grand nombre d'utilisateurs IAM provenant de différentes équipes au sein de l'organisation. Ils ont besoin d'une structure qui permet une attribution facile des autorisations pour chaque équipe sans avoir à attribuer des politiques individuelles à chaque utilisateur. De plus, ils veulent empêcher qu'un utilisateur individuel soit référencé directement dans les politiques de ressources.",
        "Question": "Quelle fonctionnalité IAM serait la solution la PLUS efficace pour répondre à ces exigences ?",
        "Options": {
            "1": "Créer des rôles IAM individuels pour chaque utilisateur avec des politiques spécifiques à l'équipe attachées.",
            "2": "Utiliser des groupes IAM pour organiser les utilisateurs par équipe et attacher des politiques spécifiques à chaque groupe.",
            "3": "Mettre en place un seul rôle IAM pour tous les utilisateurs et s'appuyer sur AWS Organizations pour gérer les autorisations.",
            "4": "Attribuer des politiques inline à chaque utilisateur en fonction de leurs autorisations spécifiques à l'équipe."
        },
        "Correct Answer": "Utiliser des groupes IAM pour organiser les utilisateurs par équipe et attacher des politiques spécifiques à chaque groupe.",
        "Explanation": "Utiliser des groupes IAM est la solution la plus efficace car cela permet à l'entreprise de gérer les autorisations au niveau de l'équipe plutôt qu'individuellement. En créant des groupes pour chaque équipe, l'entreprise peut attacher des politiques qui définissent les autorisations pour tous les utilisateurs de ce groupe. Cela simplifie la gestion des autorisations, car tout changement dans la politique s'appliquera automatiquement à tous les utilisateurs du groupe. De plus, les groupes IAM empêchent les utilisateurs individuels d'être référencés directement dans les politiques de ressources, ce qui correspond à l'exigence de l'entreprise.",
        "Other Options": [
            "Créer des rôles IAM individuels pour chaque utilisateur avec des politiques spécifiques à l'équipe attachées entraînerait une structure complexe et ingérable, surtout avec un grand nombre d'utilisateurs. Cette approche nécessiterait des mises à jour constantes et la gestion de chaque rôle, ce qui est inefficace.",
            "Mettre en place un seul rôle IAM pour tous les utilisateurs et s'appuyer sur AWS Organizations pour gérer les autorisations ne fournit pas la granularité nécessaire pour des autorisations spécifiques à l'équipe. Cela entraînerait que tous les utilisateurs aient les mêmes autorisations, ce qui ne répond pas à l'exigence de gestion des autorisations par équipe.",
            "Attribuer des politiques inline à chaque utilisateur en fonction de leurs autorisations spécifiques à l'équipe n'est pas évolutif. Les politiques inline sont attachées directement aux utilisateurs, ce qui rend difficile la gestion collective des autorisations pour une équipe. Cette approche entraînerait également des redondances et une augmentation de la charge administrative."
        ]
    },
    {
        "Question Number": "3",
        "Situation": "Une entreprise configure un VPC dans AWS avec des sous-réseaux privés et publics. Ils doivent permettre l'accès à Internet pour les instances dans le sous-réseau public tout en maintenant les instances dans le sous-réseau privé isolées de l'accès direct à Internet.",
        "Question": "Quelles étapes l'entreprise doit-elle suivre pour configurer l'accès à Internet pour le sous-réseau public et garantir un routage sécurisé au sein du VPC ?",
        "Options": {
            "1": "Attacher une passerelle Internet (IGW) au VPC, associer une table de routage avec le sous-réseau public qui dirige le trafic 0.0.0.0/0 vers l'IGW, et attribuer des adresses IPv4 publiques aux instances dans le sous-réseau public.",
            "2": "Configurer une passerelle NAT dans le sous-réseau privé, l'attacher au VPC, et créer une table de routage qui dirige le trafic 0.0.0.0/0 du sous-réseau public vers la passerelle NAT.",
            "3": "Créer une passerelle Internet (IGW) et l'attacher à chaque instance dans le sous-réseau public individuellement pour fournir un accès à Internet, tout en utilisant la table de routage par défaut pour le routage.",
            "4": "Utiliser une connexion de Peering VPC entre les sous-réseaux privés et publics pour router le trafic Internet, et s'assurer que toutes les instances dans les deux sous-réseaux ont des adresses IPv4 publiques pour la connectivité."
        },
        "Correct Answer": "Attacher une passerelle Internet (IGW) au VPC, associer une table de routage avec le sous-réseau public qui dirige le trafic 0.0.0.0/0 vers l'IGW, et attribuer des adresses IPv4 publiques aux instances dans le sous-réseau public.",
        "Explanation": "Pour permettre l'accès à Internet pour les instances dans le sous-réseau public, l'entreprise doit attacher une passerelle Internet (IGW) au VPC. L'IGW permet la communication entre les instances dans le sous-réseau public et Internet. De plus, une table de routage doit être associée au sous-réseau public qui dirige tout le trafic sortant (0.0.0.0/0) vers l'IGW. Enfin, les instances dans le sous-réseau public doivent avoir des adresses IPv4 publiques pour être accessibles depuis Internet. Cette configuration garantit que les instances peuvent envoyer et recevoir du trafic depuis Internet tout en maintenant le sous-réseau privé isolé.",
        "Other Options": [
            "Configurer une passerelle NAT dans le sous-réseau privé est incorrect pour fournir un accès à Internet au sous-réseau public. Une passerelle NAT est utilisée pour permettre aux instances dans un sous-réseau privé d'initier un trafic sortant vers Internet tout en empêchant le trafic entrant depuis Internet, ce qui ne s'applique pas au sous-réseau public.",
            "Créer une passerelle Internet (IGW) et l'attacher à chaque instance dans le sous-réseau public individuellement est incorrect. Une IGW doit être attachée au VPC dans son ensemble, et non à des instances individuelles. De plus, la table de routage doit être configurée pour diriger le trafic vers l'IGW, plutôt que de s'appuyer sur la table de routage par défaut.",
            "Utiliser une connexion de Peering VPC entre les sous-réseaux privés et publics n'est pas une méthode valide pour router le trafic Internet. Le Peering VPC est utilisé pour connecter deux VPC, et non pour permettre l'accès à Internet. De plus, les instances dans le sous-réseau privé ne devraient pas avoir d'adresses IPv4 publiques si elles doivent rester isolées de l'accès direct à Internet."
        ]
    },
    {
        "Question Number": "4",
        "Situation": "Une entreprise de vente au détail multinationale étend sa présence en ligne en Europe et en Asie. Elle souhaite garantir un accès à faible latence à sa base de données clients pour les utilisateurs dans ces nouvelles régions tout en respectant les exigences de souveraineté des données.",
        "Question": "Quelle stratégie architecturale AWS le concepteur de solutions devrait-il recommander pour répondre à ces exigences ? (Choisissez deux.)",
        "Options": {
            "1": "Déployer une seule instance Amazon RDS dans la région AWS principale et utiliser Amazon CloudFront pour mettre en cache les requêtes de base de données à l'échelle mondiale.",
            "2": "Configurer Amazon Aurora Global Database avec des répliques de lecture secondaires dans les régions Europe et Asie.",
            "3": "Utiliser Amazon DynamoDB avec des tables globales activées pour la réplication automatique entre les régions.",
            "4": "Mettre en œuvre une connexion VPN vers le centre de données sur site dans chaque nouvelle région et répliquer la base de données manuellement.",
            "5": "Utiliser AWS DataSync pour automatiser la réplication des données entre les régions."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Configurer Amazon Aurora Global Database avec des répliques de lecture secondaires dans les régions Europe et Asie.",
            "Utiliser Amazon DynamoDB avec des tables globales activées pour la réplication automatique entre les régions."
        ],
        "Explanation": "Configurer Amazon Aurora Global Database avec des répliques de lecture secondaires dans les régions Europe et Asie est une réponse correcte car cela permet des lectures à faible latence et une récupération après sinistre. Les données sont répliquées dans plusieurs régions, ce qui garantit la souveraineté des données et un accès à faible latence. Utiliser Amazon DynamoDB avec des tables globales activées pour la réplication automatique entre les régions est également correct. Les tables globales répliquent vos données dans plusieurs régions AWS pour vous donner un accès rapide et local aux données pour vos applications distribuées à l'échelle mondiale, garantissant ainsi un accès à faible latence et la souveraineté des données.",
        "Other Options": [
            "Déployer une seule instance Amazon RDS dans la région AWS principale et utiliser Amazon CloudFront pour mettre en cache les requêtes de base de données à l'échelle mondiale n'est pas une solution viable car CloudFront est un réseau de distribution de contenu, pas un service de mise en cache de base de données. Il n'est pas conçu pour mettre en cache les requêtes de base de données.",
            "Mettre en œuvre une connexion VPN vers le centre de données sur site dans chaque nouvelle région et répliquer la base de données manuellement n'est pas une solution efficace. Cela nécessiterait un effort manuel significatif et ne fournirait pas l'accès à faible latence requis pour les utilisateurs dans les nouvelles régions.",
            "Utiliser AWS DataSync pour automatiser la réplication des données entre les régions n'est pas la meilleure solution car DataSync est principalement utilisé pour transférer des données entre le stockage sur site et AWS ou entre les services de stockage AWS. Il ne fournit pas l'accès à faible latence requis pour les utilisateurs dans les nouvelles régions."
        ]
    },
    {
        "Question Number": "5",
        "Situation": "Une plateforme de commerce électronique mondiale connaît des pics de trafic importants lors des événements de vente, avec des millions d'utilisateurs accédant à la plateforme simultanément depuis différentes régions. Pour garantir une expérience fluide pour tous les utilisateurs, la plateforme doit gérer de forts volumes de trafic sans compromettre la latence ou la disponibilité.",
        "Question": "Quelle stratégie parmi les suivantes répondrait le mieux à ces exigences ?",
        "Options": {
            "1": "Utiliser un seul centre de données avec des serveurs puissants",
            "2": "Mettre en œuvre une architecture distribuée multi-régions pour servir les utilisateurs depuis l'emplacement le plus proche",
            "3": "Compter uniquement sur la mise en cache des données au niveau de la base de données",
            "4": "Ajouter plus de CPU et de mémoire à leurs serveurs d'application principaux"
        },
        "Correct Answer": "Mettre en œuvre une architecture distribuée multi-régions pour servir les utilisateurs depuis l'emplacement le plus proche",
        "Explanation": "Mettre en œuvre une architecture distribuée multi-régions permet à la plateforme de commerce électronique de gérer de forts volumes de trafic en répartissant la charge sur plusieurs serveurs situés dans différentes régions géographiques. Cette approche minimise la latence en servant les utilisateurs depuis le centre de données le plus proche, améliorant ainsi les temps de réponse et garantissant une haute disponibilité. Elle offre également de la redondance ; si une région rencontre des problèmes, d'autres peuvent continuer à servir les utilisateurs, maintenant ainsi la performance globale de la plateforme lors des pics de trafic.",
        "Other Options": [
            "Utiliser un seul centre de données avec des serveurs puissants ne gérerait pas efficacement les pics de trafic importants, car cela crée un point de défaillance unique et peut entraîner une latence accrue pour les utilisateurs situés loin de ce centre de données. Cette approche limite la scalabilité et ne fournit pas de redondance.",
            "Compter uniquement sur la mise en cache des données au niveau de la base de données peut améliorer les performances mais ne résout pas le problème du volume de trafic élevé dans différentes régions. La mise en cache peut réduire la charge sur la base de données, mais si les serveurs d'application ou l'infrastructure réseau ne peuvent pas gérer le trafic entrant, les utilisateurs peuvent toujours rencontrer des retards ou des pannes.",
            "Ajouter plus de CPU et de mémoire à leurs serveurs d'application principaux peut fournir un coup de pouce temporaire en termes de performance, mais cela ne résout pas le problème sous-jacent de scalabilité et de latence pour les utilisateurs situés loin du serveur. Cette approche peut entraîner des rendements décroissants et ne fournit pas la distribution géographique nécessaire pour gérer efficacement les pics de trafic mondiaux."
        ]
    },
    {
        "Question Number": "6",
        "Situation": "Une entreprise déploie une application critique sur AWS et souhaite garantir une haute disponibilité et une récupération rapide en cas de défaillances de l'infrastructure. Elle envisage différentes stratégies de basculement pour minimiser les temps d'arrêt lors des pannes.",
        "Question": "Laquelle des stratégies de basculement suivantes est la mieux adaptée pour maintenir la disponibilité du service avec un minimum de temps d'arrêt ? (Choisissez deux.)",
        "Options": {
            "1": "Utiliser une stratégie de basculement active-active à travers plusieurs zones de disponibilité pour garantir que le trafic est automatiquement dirigé vers des ressources saines.",
            "2": "Utiliser une stratégie de sauvegarde et de restauration qui sauvegarde périodiquement l'état de l'application et le restaure lorsqu'une défaillance se produit.",
            "3": "Utiliser une stratégie de basculement en attente chaude, où seule une petite partie des ressources fonctionne dans une région de secours, et la capacité totale est augmentée au besoin.",
            "4": "Utiliser une stratégie de basculement à lumière pilote avec une infrastructure minimale fonctionnant dans la région secondaire, en augmentant les ressources uniquement lorsqu'une défaillance se produit.",
            "5": "Mettre en œuvre une stratégie de basculement à froid où aucune ressource ne fonctionne dans la région de secours jusqu'à ce qu'une défaillance se produise, puis déployer complètement les ressources."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Utiliser une stratégie de basculement active-active à travers plusieurs zones de disponibilité pour garantir que le trafic est automatiquement dirigé vers des ressources saines.",
            "Utiliser une stratégie de basculement en attente chaude, où seule une petite partie des ressources fonctionne dans une région de secours, et la capacité totale est augmentée au besoin."
        ],
        "Explanation": "Une stratégie de basculement active-active est une méthode très efficace pour maintenir la disponibilité du service avec un minimum de temps d'arrêt. Elle consiste à exécuter des instances de l'application dans plusieurs zones de disponibilité simultanément. Si une instance échoue, le trafic est automatiquement redirigé vers les autres instances actives, garantissant ainsi une disponibilité continue du service. Une stratégie de basculement en attente chaude aide également à minimiser le temps d'arrêt. Dans cette stratégie, une version réduite de l'application fonctionne toujours dans la région de secours. En cas de défaillance, le système peut rapidement augmenter sa capacité pour gérer la charge complète, réduisant ainsi le temps d'arrêt subi par les utilisateurs.",
        "Other Options": [
            "Une stratégie de sauvegarde et de restauration, bien qu'utile pour la récupération des données, n'est pas la meilleure option pour maintenir la disponibilité du service avec un minimum de temps d'arrêt. La restauration à partir d'une sauvegarde peut être un processus long, entraînant des périodes prolongées d'inactivité.",
            "Une stratégie de basculement à lumière pilote implique de garder une version minimale de l'environnement fonctionnant dans la région secondaire. Bien que cette stratégie puisse être efficace, elle peut ne pas être aussi rapide à augmenter sa capacité que la stratégie de basculement en attente chaude, ce qui peut entraîner des périodes d'inactivité plus longues.",
            "Une stratégie de basculement à froid implique de ne pas avoir de ressources fonctionnant dans la région de secours jusqu'à ce qu'une défaillance se produise. Cette stratégie peut entraîner les périodes d'inactivité les plus longues, car les ressources doivent être complètement déployées après une défaillance, ce qui peut prendre un temps considérable."
        ]
    },
    {
        "Question Number": "7",
        "Situation": "Une startup développe un système d'enchères en temps réel pour des publicités en ligne qui nécessite une latence extrêmement faible et un débit élevé pour le traitement des enchères. Le système doit également être hautement disponible et évolutif sans intervention manuelle.",
        "Question": "Quelle solution de base de données AWS le concepteur de solutions devrait-il recommander pour répondre à ces exigences ?",
        "Options": {
            "1": "Amazon RDS for MySQL avec IOPS provisionnées",
            "2": "Amazon DynamoDB avec mode de capacité à la demande",
            "3": "Amazon ElastiCache for Redis dans une configuration en cluster",
            "4": "Amazon Aurora Serverless avec optimisation en mémoire"
        },
        "Correct Answer": "Amazon DynamoDB avec mode de capacité à la demande",
        "Explanation": "Amazon DynamoDB est un service de base de données NoSQL entièrement géré qui fournit des temps de réponse en millisecondes à un chiffre, ce qui le rend idéal pour les applications nécessitant une latence extrêmement faible. Son mode de capacité à la demande permet à la base de données de s'adapter automatiquement en fonction du trafic, garantissant un débit élevé sans intervention manuelle. Cela est particulièrement bénéfique pour un système d'enchères en temps réel où le nombre d'enchères peut fluctuer de manière significative. De plus, DynamoDB est conçu pour une haute disponibilité et durabilité, ce qui correspond parfaitement aux exigences du système de la startup.",
        "Other Options": [
            "Amazon RDS for MySQL avec IOPS provisionnées est un service de base de données relationnelle qui peut fournir des performances élevées, mais il peut ne pas atteindre la même latence faible que DynamoDB pour des charges de travail à haute vitesse. De plus, RDS nécessite plus de gestion pour l'évolutivité et la disponibilité par rapport à DynamoDB.",
            "Amazon ElastiCache for Redis dans une configuration en cluster est un magasin de données en mémoire qui peut fournir une faible latence, mais il est principalement utilisé pour le cache plutôt que comme base de données principale. Il ne fournit pas intrinsèquement les fonctionnalités de durabilité et de persistance requises pour un système d'enchères.",
            "Amazon Aurora Serverless avec optimisation en mémoire est une base de données relationnelle qui peut évoluer automatiquement, mais elle peut ne pas fournir le même niveau de faible latence et de haut débit que DynamoDB, en particulier sous des charges de travail imprévisibles typiques dans des scénarios d'enchères en temps réel."
        ]
    },
    {
        "Question Number": "8",
        "Situation": "Une entreprise développe une application sans serveur utilisant des fonctions AWS Lambda. L'application doit traiter des images téléchargées par les utilisateurs et stocker les résultats dans une base de données. L'architecture doit garantir que chaque image est traitée exactement une fois, même si la même image est téléchargée plusieurs fois.",
        "Question": "Quelle combinaison de services AWS le concepteur de solutions devrait-il utiliser pour atteindre cette exigence ? (Choisissez DEUX.)",
        "Options": {
            "1": "Amazon S3",
            "2": "Amazon DynamoDB avec écritures conditionnelles",
            "3": "Amazon Simple Queue Service (SQS)",
            "4": "Amazon Simple Notification Service (SNS)",
            "5": "AWS Step Functions"
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Amazon S3",
            "Amazon DynamoDB avec écritures conditionnelles"
        ],
        "Explanation": "Amazon S3 peut être utilisé pour stocker les images téléchargées par les utilisateurs. Il peut également déclencher des fonctions AWS Lambda lorsqu'une nouvelle image est téléchargée, qui peut ensuite traiter l'image. Amazon DynamoDB avec écritures conditionnelles peut être utilisé pour stocker les résultats du traitement des images. Les écritures conditionnelles garantissent qu'un élément est écrit dans la table uniquement si la condition spécifiée est remplie. Dans ce cas, la condition pourrait être que l'image n'a pas été traitée auparavant, garantissant que chaque image est traitée exactement une fois.",
        "Other Options": [
            "Amazon Simple Queue Service (SQS) est un service de mise en file d'attente de messages entièrement géré qui vous permet de découpler et d'évoluer des microservices, des systèmes distribués et des applications sans serveur. Cependant, il ne prévient pas intrinsèquement le traitement d'un même message plus d'une fois.",
            "Amazon Simple Notification Service (SNS) est un service de messagerie entièrement géré pour la communication entre applications (A2A) et entre applications et personnes (A2P). Cependant, il ne prévient pas intrinsèquement le traitement d'un même message plus d'une fois.",
            "AWS Step Functions est un service de flux de travail sans serveur qui vous permet de coordonner plusieurs services AWS en flux de travail sans serveur. Bien qu'il puisse être utilisé pour orchestrer des fonctions AWS Lambda, il ne prévient pas intrinsèquement l'exécution de la même fonction plus d'une fois pour la même entrée."
        ]
    },
    {
        "Question Number": "9",
        "Situation": "Une entreprise met en place un groupe Auto Scaling pour ses instances EC2 et souhaite s'assurer qu'elle peut mettre à jour les configurations sans avoir à recréer l'ensemble de la configuration.",
        "Question": "Quelle option devraient-ils choisir, et pourquoi ?",
        "Options": {
            "1": "Utiliser des configurations de lancement, car elles prennent en charge la gestion des versions et permettent des mises à jour sans recréation.",
            "2": "Utiliser des modèles de lancement, car ils prennent en charge la gestion des versions, permettant des mises à jour de configuration sans créer un nouveau modèle.",
            "3": "Utiliser des configurations de lancement, car elles sont plus faciles à gérer et disposent de fonctionnalités de gestion des versions intégrées.",
            "4": "Utiliser des modèles de lancement, car ils prennent en charge les mises à jour en direct directement au sein du groupe Auto Scaling sans gestion des versions."
        },
        "Correct Answer": "Utiliser des modèles de lancement, car ils prennent en charge la gestion des versions, permettant des mises à jour de configuration sans créer un nouveau modèle.",
        "Explanation": "Les modèles de lancement sont l'option recommandée pour configurer des groupes Auto Scaling dans AWS car ils prennent en charge la gestion des versions. Cela signifie que lorsque vous devez mettre à jour des configurations, vous pouvez créer une nouvelle version du modèle de lancement sans avoir à recréer l'ensemble de la configuration. Cette fonctionnalité permet une plus grande flexibilité et une gestion plus facile des configurations au fil du temps, ce qui est idéal pour les environnements nécessitant des mises à jour ou des changements fréquents.",
        "Other Options": [
            "Utiliser des configurations de lancement, car elles prennent en charge la gestion des versions et permettent des mises à jour sans recréation. - Cette option est incorrecte car les configurations de lancement ne prennent pas en charge la gestion des versions. Une fois qu'une configuration de lancement est créée, elle ne peut pas être modifiée ; toute mise à jour nécessite la création d'une nouvelle configuration de lancement.",
            "Utiliser des configurations de lancement, car elles sont plus faciles à gérer et disposent de fonctionnalités de gestion des versions intégrées. - Cette option est incorrecte car les configurations de lancement n'ont pas de fonctionnalités de gestion des versions intégrées. Elles sont moins flexibles que les modèles de lancement, ce qui peut entraîner une surcharge de gestion lorsque des mises à jour sont nécessaires.",
            "Utiliser des modèles de lancement, car ils prennent en charge les mises à jour en direct directement au sein du groupe Auto Scaling sans gestion des versions. - Cette option est trompeuse car bien que les modèles de lancement prennent en charge la gestion des versions, ils ne prennent pas en charge les mises à jour en direct directement au sein du groupe Auto Scaling. Les mises à jour nécessitent la création d'une nouvelle version du modèle, qui est ensuite utilisée pour de nouvelles instances."
        ]
    },
    {
        "Question Number": "10",
        "Situation": "Une application de santé doit gérer des millions de requêtes par seconde, en distribuant le trafic entrant sur plusieurs instances Amazon EC2 pour un traitement efficace. En raison des exigences de conformité, l'application doit également prendre en charge le chiffrement de bout en bout pour un transfert de données sécurisé. De plus, l'application doit fonctionner avec une latence ultra-faible car elle traite des données médicales sensibles au temps.",
        "Question": "Quelle solution de répartition de charge AWS répondrait le mieux à ces exigences ?",
        "Options": {
            "1": "Application Load Balancer (ALB) avec terminaison SSL",
            "2": "Network Load Balancer (NLB) avec écouteurs TCP et TLS",
            "3": "Classic Load Balancer avec écouteurs HTTP et HTTPS",
            "4": "Amazon CloudFront avec mise en cache HTTPS"
        },
        "Correct Answer": "Network Load Balancer (NLB) avec écouteurs TCP et TLS",
        "Explanation": "Le Network Load Balancer (NLB) est conçu pour gérer des millions de requêtes par seconde tout en maintenant une latence ultra-faible, ce qui le rend idéal pour le traitement de données médicales sensibles au temps. Il fonctionne au niveau de transport (couche 4) et peut distribuer efficacement le trafic TCP sur plusieurs instances EC2. De plus, le NLB prend en charge les écouteurs TLS, ce qui permet le chiffrement de bout en bout, répondant ainsi aux exigences de conformité pour un transfert de données sécurisé. Cette combinaison de haut débit, de faible latence et de prise en charge du chiffrement fait du NLB le meilleur choix pour cette application de santé.",
        "Other Options": [
            "L'Application Load Balancer (ALB) avec terminaison SSL est principalement conçu pour le trafic HTTP/HTTPS et fonctionne à la couche 7. Bien qu'il prenne en charge la terminaison SSL, il peut introduire une latence supplémentaire en raison de son traitement au niveau de l'application, ce qui n'est pas idéal pour des exigences de latence ultra-faible.",
            "Le Classic Load Balancer avec écouteurs HTTP et HTTPS est une option plus ancienne qui ne fournit pas le même niveau de performance et d'évolutivité que le NLB. Il fonctionne à la fois à la couche 4 et à la couche 7 mais manque des fonctionnalités avancées et des optimisations trouvées dans le NLB, ce qui le rend moins adapté pour gérer efficacement des millions de requêtes par seconde.",
            "Amazon CloudFront avec mise en cache HTTPS est un réseau de distribution de contenu (CDN) qui peut mettre en cache du contenu à des emplacements périphériques, ce qui est bénéfique pour la livraison de contenu statique. Cependant, ce n'est pas un répartiteur de charge et ne distribue pas directement le trafic sur les instances EC2, ce qui le rend inadapté à l'exigence de distribution du trafic entrant pour le traitement des données médicales."
        ]
    },
    {
        "Question Number": "11",
        "Situation": "Une entreprise mondiale de commerce électronique souhaite garantir une haute disponibilité et une tolérance aux pannes pour son site web en dirigeant le trafic vers plusieurs régions. Elle souhaite basculer automatiquement vers une région de secours si la région principale devient indisponible.",
        "Question": "Quelle configuration Amazon Route 53 l'entreprise devrait-elle utiliser pour atteindre un basculement DNS résilient, et quelle fonctionnalité permet cette fonctionnalité ?",
        "Options": {
            "1": "Utiliser le routage pondéré Route 53 pour distribuer le trafic entre les régions en fonction des poids définis et configurer des vérifications de santé pour le basculement.",
            "2": "Utiliser le routage basé sur la latence Route 53 pour diriger les utilisateurs vers la région avec la latence la plus basse, avec des vérifications de santé pour basculer vers une autre région si nécessaire.",
            "3": "Utiliser le routage par géolocalisation Route 53 pour diriger le trafic en fonction de l'emplacement de l'utilisateur et configurer des vérifications de santé pour rediriger les utilisateurs si une région échoue.",
            "4": "Utiliser le routage de basculement Route 53 pour diriger le trafic vers une région principale et rediriger automatiquement vers une région secondaire en cas de défaillance, en utilisant des vérifications de santé pour surveiller la disponibilité de la région principale."
        },
        "Correct Answer": "Utiliser le routage de basculement Route 53 pour diriger le trafic vers une région principale et rediriger automatiquement vers une région secondaire en cas de défaillance, en utilisant des vérifications de santé pour surveiller la disponibilité de la région principale.",
        "Explanation": "Le routage de basculement Route 53 est spécifiquement conçu pour des scénarios où la haute disponibilité est critique. Il vous permet de désigner une ressource principale (dans ce cas, la région principale) et une ressource secondaire (la région de secours). Si les vérifications de santé déterminent que la région principale est indisponible, Route 53 redirige automatiquement le trafic vers la région secondaire. Cette configuration garantit que les utilisateurs subissent une interruption minimale et que le site web reste accessible même si une région échoue.",
        "Other Options": [
            "L'utilisation du routage pondéré Route 53 distribue le trafic en fonction des poids définis, mais ne fournit pas intrinsèquement de basculement automatique. Bien que des vérifications de santé puissent être configurées, cette option n'est pas spécifiquement conçue pour des scénarios de basculement, ce qui la rend moins adaptée aux besoins de l'entreprise.",
            "Le routage basé sur la latence Route 53 dirige les utilisateurs vers la région avec la latence la plus basse, ce qui est bénéfique pour la performance mais ne fournit pas de mécanisme de basculement direct. Bien que des vérifications de santé puissent être mises en œuvre, cette option est principalement axée sur l'optimisation de l'expérience utilisateur plutôt que sur l'assurance de disponibilité pendant les pannes.",
            "Le routage par géolocalisation Route 53 dirige le trafic en fonction de l'emplacement de l'utilisateur, ce qui est utile pour cibler des régions spécifiques mais ne fournit pas de capacités de basculement automatique. Bien que des vérifications de santé puissent être configurées, cette méthode de routage ne priorise pas la disponibilité de la même manière que le routage de basculement."
        ]
    },
    {
        "Question Number": "12",
        "Situation": "Une entreprise conçoit une application sans serveur pour traiter les téléchargements des utilisateurs et les transformer en un format spécifique. L'application doit s'adapter automatiquement pour accueillir un trafic fluctuant et gérer plusieurs téléchargements de fichiers simultanément. L'entreprise souhaite éviter de gérer des serveurs et une infrastructure tout en s'assurant que les processus de transformation sont réalisés rapidement et de manière fiable.",
        "Question": "Quels services AWS l'entreprise devrait-elle utiliser pour mettre en œuvre cette solution ? (Choisissez deux.)",
        "Options": {
            "1": "Utiliser AWS Lambda pour déclencher les fonctions de traitement lorsqu'un fichier est téléchargé sur Amazon S3, et utiliser Amazon SQS pour mettre en file d'attente les tâches de transformation.",
            "2": "Utiliser AWS Fargate pour exécuter des travaux de traitement conteneurisés, permettant une mise à l'échelle automatique en fonction du nombre de téléchargements.",
            "3": "Utiliser Amazon EC2 pour gérer l'infrastructure et traiter les fichiers manuellement.",
            "4": "Utiliser les notifications d'événements Amazon S3 pour déclencher des fonctions AWS Lambda pour le traitement de chaque fichier téléchargé.",
            "5": "Utiliser Amazon S3 pour traiter directement les téléchargements, sans avoir besoin de déclencher d'autres fonctions ou services."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Utiliser AWS Lambda pour déclencher les fonctions de traitement lorsqu'un fichier est téléchargé sur Amazon S3, et utiliser Amazon SQS pour mettre en file d'attente les tâches de transformation.",
            "Utiliser les notifications d'événements Amazon S3 pour déclencher des fonctions AWS Lambda pour le traitement de chaque fichier téléchargé."
        ],
        "Explanation": "AWS Lambda est un service de calcul sans serveur qui exécute votre code en réponse à des événements, tels que des changements de données dans un compartiment Amazon S3. Cela en fait un choix approprié pour l'exigence de l'entreprise de traiter les téléchargements des utilisateurs et de les transformer en un format spécifique sans gérer de serveurs. Amazon SQS est un service de mise en file d'attente de messages entièrement géré qui vous permet de découpler et de mettre à l'échelle des microservices, des systèmes distribués et des applications sans serveur. SQS élimine la complexité et la surcharge associées à la gestion et à l'exploitation de middleware orienté message, et permet aux développeurs de se concentrer sur des travaux différenciés. L'utilisation des notifications d'événements Amazon S3 en conjonction avec AWS Lambda permet à l'entreprise de déclencher des fonctions de traitement immédiatement après qu'un fichier a été téléchargé, répondant ainsi à l'exigence de transformations rapides et fiables.",
        "Other Options": [
            "AWS Fargate est un moteur de calcul sans serveur pour conteneurs. Bien qu'il permette une mise à l'échelle automatique, il est plus complexe et moins direct que l'utilisation d'AWS Lambda pour ce cas d'utilisation spécifique. Il nécessiterait également que l'entreprise gère des applications conteneurisées, ce qu'elle souhaite éviter.",
            "Amazon EC2 est un service web qui fournit une capacité de calcul redimensionnable dans le cloud. Il est conçu pour faciliter le calcul cloud à l'échelle web, mais nécessite une gestion manuelle de l'infrastructure, ce que l'entreprise souhaite éviter.",
            "Amazon S3 est un service de stockage, il n'a pas la capacité de traiter directement les téléchargements ou de les transformer en un format spécifique. Il peut stocker et récupérer n'importe quelle quantité de données, mais ne peut pas effectuer de calculs ou de transformations sur ces données."
        ]
    },
    {
        "Question Number": "13",
        "Situation": "Une entreprise développe une application de commerce électronique et doit mettre en œuvre une architecture orientée événements pour gérer les commandes des clients, le traitement des paiements et les mises à jour de l'inventaire. Elle souhaite s'assurer que le système est hautement disponible, évolutif et découplé.",
        "Question": "Quelle architecture l'entreprise devrait-elle utiliser pour atteindre ces objectifs ?",
        "Options": {
            "1": "Utiliser Amazon SQS pour découpler les services et garantir le traitement asynchrone des événements. Utiliser AWS Lambda pour traiter les événements, et Amazon SNS pour diffuser les événements à plusieurs abonnés pour des notifications efficaces.",
            "2": "Utiliser des instances Amazon EC2 avec une file d'attente de messages, où chaque instance EC2 traite les événements et envoie des mises à jour à une base de données Amazon RDS.",
            "3": "Utiliser Amazon DynamoDB Streams pour capturer les données d'événements, et configurer AWS Step Functions pour orchestrer les flux de travail pour le traitement des événements.",
            "4": "Utiliser Amazon S3 pour stocker les données d'événements, et configurer une instance EC2 pour interroger le bucket S3 à la recherche de nouveaux événements à traiter."
        },
        "Correct Answer": "Utiliser Amazon SQS pour découpler les services et garantir le traitement asynchrone des événements. Utiliser AWS Lambda pour traiter les événements, et Amazon SNS pour diffuser les événements à plusieurs abonnés pour des notifications efficaces.",
        "Explanation": "Cette option met en œuvre efficacement une architecture orientée événements qui est hautement disponible, évolutive et découplée. Amazon SQS (Simple Queue Service) permet une communication asynchrone entre les services, ce qui aide à les découpler. AWS Lambda peut traiter des événements sans avoir besoin de gérer des serveurs, permettant une mise à l'échelle automatique en fonction du nombre d'événements entrants. De plus, Amazon SNS (Simple Notification Service) peut diffuser des messages à plusieurs abonnés, garantissant que divers composants de l'application peuvent réagir aux événements de manière efficace. Cette combinaison offre une solution robuste pour gérer les commandes des clients, le traitement des paiements et les mises à jour de l'inventaire de manière évolutive.",
        "Other Options": [
            "Utiliser des instances Amazon EC2 avec une file d'attente de messages introduit plus de complexité et de surcharge de gestion. Les instances EC2 nécessitent un provisionnement, une mise à l'échelle et une maintenance, ce qui contredit l'objectif d'avoir une architecture hautement disponible et évolutive. De plus, cette option ne tire pas parti des capacités sans serveur, ce qui peut entraîner des inefficacités dans l'utilisation des ressources.",
            "Utiliser Amazon DynamoDB Streams et AWS Step Functions est une option viable mais peut ne pas être aussi simple que la première option. Bien que DynamoDB Streams puisse capturer les changements dans la base de données, cela nécessite une configuration et une gestion supplémentaires. AWS Step Functions sont utiles pour orchestrer des flux de travail mais peuvent ajouter une complexité inutile pour des tâches de traitement d'événements simples par rapport à l'approche directe orientée événements utilisant SQS et Lambda.",
            "Utiliser Amazon S3 pour stocker les données d'événements et interroger une instance EC2 pour de nouveaux événements n'est pas une solution idéale pour une architecture orientée événements. L'interrogation introduit une latence et peut entraîner des inefficacités, car le système attendrait que les événements soient traités plutôt que de réagir en temps réel. Cette approche manque également des avantages de découplage et d'évolutivité offerts par les files d'attente de messages et les fonctions sans serveur."
        ]
    },
    {
        "Question Number": "14",
        "Situation": "Une grande plateforme de commerce électronique connaît un trafic élevé, surtout pendant les événements de vente, ce qui entraîne une augmentation significative du nombre de connexions à la base de données. Pour optimiser les performances de la base de données et prévenir la surcharge, elle décide d'utiliser un service proxy pour gérer efficacement ces connexions.",
        "Question": "Quel service AWS devraient-ils mettre en œuvre pour gérer efficacement les connexions à la base de données, et quels avantages cela offre-t-il en termes de mise à l'échelle et de basculement ?",
        "Options": {
            "1": "Amazon RDS Proxy, car il regroupe et partage les connexions à la base de données, réduisant ainsi la surcharge sur la base de données et améliorant l'évolutivité de l'application.",
            "2": "AWS App Mesh, qui gère la communication de service à service mais ne se spécialise pas dans la gestion des connexions à la base de données.",
            "3": "Amazon API Gateway, car il fournit un proxy pour les requêtes API, mais il est principalement conçu pour les API RESTful, pas pour les connexions à la base de données.",
            "4": "AWS Direct Connect, qui fournit une connexion réseau dédiée mais ne gère ni ne regroupe les connexions à la base de données."
        },
        "Correct Answer": "Amazon RDS Proxy, car il regroupe et partage les connexions à la base de données, réduisant ainsi la surcharge sur la base de données et améliorant l'évolutivité de l'application.",
        "Explanation": "Amazon RDS Proxy est spécifiquement conçu pour gérer efficacement les connexions à la base de données. Il regroupe et partage les connexions à la base de données, ce qui réduit le nombre de connexions ouvertes et la surcharge associée sur le serveur de base de données. Cela est particulièrement bénéfique pendant les périodes de fort trafic, comme les événements de vente, car cela permet à l'application de se mettre à l'échelle plus efficacement sans surcharger la base de données. De plus, RDS Proxy fournit des capacités de basculement, permettant aux applications de se reconnecter automatiquement à une base de données de secours en cas de défaillance, améliorant ainsi la disponibilité et la fiabilité.",
        "Other Options": [
            "AWS App Mesh est un maillage de services qui gère la communication de service à service, mais il ne se spécialise pas dans la gestion des connexions à la base de données. Il se concentre sur la communication entre microservices plutôt que sur le regroupement ou la gestion des connexions à la base de données.",
            "Amazon API Gateway est conçu pour créer, publier, maintenir, surveiller et sécuriser des API à n'importe quelle échelle. Bien qu'il agisse comme un proxy pour les requêtes API, il n'est pas destiné à gérer les connexions à la base de données, qui est le besoin principal dans ce scénario.",
            "AWS Direct Connect fournit une connexion réseau dédiée de vos locaux à AWS, ce qui peut améliorer la bande passante et réduire la latence. Cependant, il ne gère ni ne regroupe les connexions à la base de données, ce qui le rend inadapté au besoin spécifique d'optimiser les performances de la base de données pendant les périodes de fort trafic."
        ]
    },
    {
        "Question Number": "15",
        "Situation": "Une entreprise de biotechnologie exécute des charges de travail intensives en calcul pour le séquençage de l'ADN, qui nécessite des ressources de calcul seulement quelques heures par jour. Elle souhaite minimiser les coûts tout en s'assurant que ses travaux peuvent être terminés pendant ces fenêtres de temps.",
        "Question": "Quelle option d'achat optimiserait le mieux les coûts pour cette charge de travail ?",
        "Options": {
            "1": "Instances réservées avec un engagement de 1 an",
            "2": "Plans d'économies avec un engagement de 3 ans",
            "3": "Instances Spot avec allocation optimisée de capacité",
            "4": "Instances à la demande avec mise à l'échelle automatique programmée"
        },
        "Correct Answer": "Instances Spot avec allocation optimisée de capacité",
        "Explanation": "Les instances Spot permettent aux utilisateurs de tirer parti de la capacité de calcul inutilisée à des prix nettement inférieurs par rapport aux instances à la demande ou réservées. Étant donné que l'entreprise de biotechnologie ne nécessite des ressources de calcul que pendant quelques heures chaque jour, l'utilisation d'instances Spot peut réduire considérablement les coûts, surtout si elle peut tolérer des interruptions. L'allocation optimisée de capacité garantit que les instances Spot sont plus susceptibles d'être disponibles lorsque nécessaire, ce qui en fait un choix approprié pour leurs charges de travail intensives en calcul qui ont des fenêtres de temps spécifiques.",
        "Other Options": [
            "Les instances réservées avec un engagement de 1 an ne seraient pas rentables pour des charges de travail qui ne sont nécessaires que pendant quelques heures chaque jour, car elles nécessitent un engagement à payer pour la capacité indépendamment de l'utilisation.",
            "Les plans d'économies avec un engagement de 3 ans impliquent également un engagement financier à long terme qui peut ne pas correspondre à la nature sporadique de la charge de travail, ce qui peut entraîner un gaspillage de ressources et de coûts.",
            "Les instances à la demande avec mise à l'échelle automatique programmée offriraient de la flexibilité, mais elles sont généralement plus coûteuses que les instances Spot et n'offrent pas le même niveau d'économies, surtout pour des charges de travail qui peuvent être exécutées de manière intermittente."
        ]
    },
    {
        "Question Number": "16",
        "Situation": "Une entreprise de services financiers met en œuvre une nouvelle application qui exige un chiffrement continu et sans faille entre les appareils clients et les serveurs backend. De plus, l'application doit utiliser une adresse IP statique pour faciliter le filtrage IP afin d'améliorer la sécurité.",
        "Question": "Quel type de chargeur d'équilibrage AWS l'entreprise devrait-elle déployer pour répondre à ces exigences, et quelles sont les principales raisons de ce choix ?",
        "Options": {
            "1": "Application Load Balancer (ALB) pour sa capacité à effectuer un routage basé sur le contenu et à gérer la terminaison SSL.",
            "2": "Network Load Balancer (NLB) en raison de son fonctionnement au niveau 4, de son support pour les adresses IP statiques et de sa capacité à maintenir un chiffrement de bout en bout via le transfert TCP.",
            "3": "Classic Load Balancer (CLB) car il prend en charge HTTPS et peut gérer des sessions persistantes pour des connexions sécurisées.",
            "4": "Application Load Balancer (ALB) car il offre des adresses IP statiques et garantit un débit élevé."
        },
        "Correct Answer": "Network Load Balancer (NLB) en raison de son fonctionnement au niveau 4, de son support pour les adresses IP statiques et de sa capacité à maintenir un chiffrement de bout en bout via le transfert TCP.",
        "Explanation": "Le Network Load Balancer (NLB) est le meilleur choix pour ce scénario car il fonctionne au niveau 4 du modèle OSI, ce qui lui permet de gérer efficacement le trafic TCP. Il prend en charge les adresses IP statiques, ce qui est essentiel pour l'exigence de l'entreprise en matière de filtrage IP. De plus, le NLB peut maintenir un chiffrement de bout en bout en transférant le trafic TCP sans le déchiffrer, garantissant ainsi que les données restent sécurisées entre les appareils clients et les serveurs backend. Cela correspond parfaitement au besoin d'un chiffrement continu et sans faille.",
        "Other Options": [
            "Application Load Balancer (ALB) est principalement conçu pour le trafic de niveau 7 (couche application) et excelle dans le routage basé sur le contenu et la terminaison SSL. Cependant, il ne prend pas en charge les adresses IP statiques nativement, ce qui est une exigence critique dans ce cas.",
            "Classic Load Balancer (CLB) prend en charge HTTPS et peut gérer des sessions persistantes, mais il fonctionne à la fois au niveau 4 et au niveau 7. Il n'a pas la capacité de fournir des adresses IP statiques et est généralement considéré comme moins efficace que le NLB pour des scénarios à fort débit, ce qui le rend moins adapté aux exigences décrites.",
            "Application Load Balancer (ALB) n'offre pas directement d'adresses IP statiques, ce qui est une exigence clé pour le filtrage IP. Bien qu'il offre un débit élevé et des capacités de routage avancées, il ne répond pas aussi efficacement au besoin de maintenir un chiffrement de bout en bout que le NLB."
        ]
    },
    {
        "Question Number": "17",
        "Situation": "Un fournisseur de soins de santé conçoit une application pour garantir un service ininterrompu et protéger les données critiques des patients. L'application doit rester opérationnelle malgré les pannes de composants, mais en cas de catastrophe, le fournisseur souhaite également une stratégie pour récupérer des données vitales.",
        "Question": "Laquelle des approches suivantes répond le mieux à ces exigences ? (Choisissez deux.)",
        "Options": {
            "1": "Mettre en œuvre une haute disponibilité en déployant des ressources sur plusieurs zones de disponibilité, garantissant un temps d'arrêt minimal lors des pannes de composants et une récupération plus rapide.",
            "2": "Se concentrer sur la tolérance aux pannes en configurant des ressources en mode actif-actif sur plusieurs serveurs, de sorte que l'application continue sans interruption même si un composant échoue.",
            "3": "Développer un plan de reprise après sinistre (DR) en programmant des sauvegardes périodiques et en établissant des serveurs de secours dans une région séparée, permettant à l'application d'être restaurée en cas de catastrophe régionale.",
            "4": "Combiner haute disponibilité et reprise après sinistre en déployant sur plusieurs zones de disponibilité et en programmant des sauvegardes régulières, pour maintenir le temps de fonctionnement et protéger les données lors de pannes ou de catastrophes.",
            "5": "Utiliser un déploiement dans une seule zone de disponibilité avec des instantanés automatisés pour garantir la récupération des données en cas de panne de serveur."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Mettre en œuvre une haute disponibilité en déployant des ressources sur plusieurs zones de disponibilité, garantissant un temps d'arrêt minimal lors des pannes de composants et une récupération plus rapide.",
            "Combiner haute disponibilité et reprise après sinistre en déployant sur plusieurs zones de disponibilité et en programmant des sauvegardes régulières, pour maintenir le temps de fonctionnement et protéger les données lors de pannes ou de catastrophes."
        ],
        "Explanation": "La première réponse correcte concerne la mise en œuvre de la haute disponibilité. Cette approche garantit que l'application reste opérationnelle même si un ou plusieurs composants échouent. En déployant des ressources sur plusieurs zones de disponibilité, l'application peut continuer à fonctionner avec un temps d'arrêt minimal lors des pannes de composants et se rétablir plus rapidement. La deuxième réponse correcte combine haute disponibilité et reprise après sinistre. Cette approche garantit non seulement le temps de fonctionnement de l'application lors des pannes de composants, mais protège également les données critiques des patients en programmant des sauvegardes régulières. En cas de catastrophe, les données peuvent être récupérées, assurant ainsi la continuité de l'application.",
        "Other Options": [
            "Se concentrer sur la tolérance aux pannes en configurant des ressources en mode actif-actif sur plusieurs serveurs n'est pas suffisant. Bien que cela garantisse que l'application continue sans interruption même si un composant échoue, cela ne fournit pas de stratégie pour la récupération des données en cas de catastrophe.",
            "Développer un plan de reprise après sinistre (DR) en programmant des sauvegardes périodiques et en établissant des serveurs de secours dans une région séparée est une bonne stratégie pour la récupération des données. Cependant, cela ne garantit pas le service ininterrompu de l'application en cas de pannes de composants.",
            "Utiliser un déploiement dans une seule zone de disponibilité avec des instantanés automatisés peut garantir la récupération des données en cas de panne de serveur. Cependant, cela ne fournit pas de haute disponibilité ou de tolérance aux pannes, car une panne dans la seule zone de disponibilité pourrait entraîner un temps d'arrêt de l'application."
        ]
    },
    {
        "Question Number": "18",
        "Situation": "Une plateforme de trading financier est hébergée sur des instances Amazon EC2 et nécessite un volume EBS capable de supporter des IOPS extrêmement élevés (opérations d'entrée/sortie par seconde) pour une base de données sensible à la latence et à haute fréquence. La plateforme a besoin de jusqu'à 250 000 IOPS et d'un débit élevé pour des performances optimales.",
        "Question": "Quel type de volume EBS répondrait le mieux à ces exigences ?",
        "Options": {
            "1": "General Purpose SSD (gp3)",
            "2": "Provisioned IOPS SSD (io2)",
            "3": "Throughput Optimized HDD (st1)",
            "4": "Cold HDD (sc1)"
        },
        "Correct Answer": "Provisioned IOPS SSD (io2)",
        "Explanation": "Le type de volume Provisioned IOPS SSD (io2) est spécifiquement conçu pour les applications intensives en I/O qui nécessitent des performances élevées et une faible latence. Il peut supporter jusqu'à 256 000 IOPS par volume, ce qui le rend adapté aux exigences de la plateforme de trading financier de jusqu'à 250 000 IOPS. De plus, les volumes io2 offrent un débit élevé et sont optimisés pour les charges de travail sensibles à la latence, ce qui en fait le meilleur choix pour une base de données à haute fréquence.",
        "Other Options": [
            "Les volumes General Purpose SSD (gp3) peuvent fournir jusqu'à 16 000 IOPS et conviennent à une variété de charges de travail, mais ils ne répondent pas à l'exigence de 250 000 IOPS nécessaire pour cette application spécifique.",
            "Les volumes Throughput Optimized HDD (st1) sont conçus pour des charges de travail nécessitant un débit élevé plutôt que des IOPS élevés. Ils ne conviennent pas aux applications sensibles à la latence comme une base de données à haute fréquence, car ils ne peuvent fournir qu'un maximum de 500 IOPS par volume.",
            "Les volumes Cold HDD (sc1) sont destinés aux données rarement accessibles et offrent les performances les plus faibles parmi les types de volumes EBS, avec un maximum de 250 IOPS par volume. Cela les rend inadaptés aux applications à haute performance et sensibles à la latence."
        ]
    },
    {
        "Question Number": "19",
        "Situation": "Une institution financière exploite des applications critiques pour la mission qui nécessitent une connectivité stable, à large bande et à faible latence entre ses centres de données sur site et AWS pour soutenir le traitement des données en temps réel et les activités de trading. Ils souhaitent s'assurer que tous les transferts de données se font par une connexion sécurisée et privée qui contourne Internet public, protégeant ainsi contre les risques de sécurité potentiels et la variabilité des performances.",
        "Question": "Quelles options répondraient le mieux à leurs exigences ? (Choisissez deux.)",
        "Options": {
            "1": "Utiliser une ligne louée à haute vitesse d'un fournisseur de télécommunications directement vers AWS",
            "2": "Établir un VPN Site-à-Site AWS sur Internet public",
            "3": "Déployer AWS Direct Connect pour une connexion réseau privée et dédiée",
            "4": "Mettre en place un protocole de transfert de fichiers (FTP) crypté pour des synchronisations de données périodiques",
            "5": "Implémenter AWS Transit Gateway avec Direct Connect Gateway pour une connectivité multi-régionale"
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Utiliser une ligne louée à haute vitesse d'un fournisseur de télécommunications directement vers AWS",
            "Déployer AWS Direct Connect pour une connexion réseau privée et dédiée"
        ],
        "Explanation": "Utiliser une ligne louée à haute vitesse d'un fournisseur de télécommunications directement vers AWS et déployer AWS Direct Connect pour une connexion réseau privée et dédiée sont les meilleures options pour cette institution financière. Ces options fournissent une connexion stable, à large bande et à faible latence qui contourne Internet public, ce qui est crucial pour le traitement des données en temps réel et les activités de trading de l'institution. AWS Direct Connect, en particulier, fournit une connexion réseau dédiée des centres de données sur site de l'institution vers AWS, garantissant une connexion sécurisée et fiable.",
        "Other Options": [
            "Établir un VPN Site-à-Site AWS sur Internet public n'est pas la meilleure option car il utilise toujours Internet public, ce qui peut entraîner une variabilité des performances et des risques de sécurité potentiels.",
            "Mettre en place un protocole de transfert de fichiers (FTP) crypté pour des synchronisations de données périodiques ne répond pas à l'exigence de traitement des données en temps réel et des activités de trading, car il est conçu pour des transferts de données périodiques, et non en temps réel.",
            "Implémenter AWS Transit Gateway avec Direct Connect Gateway pour une connectivité multi-régionale n'est pas nécessairement requis pour les besoins de l'institution. Bien qu'il fournisse une connectivité multi-régionale, il ne fournit pas intrinsèquement la connexion à large bande et à faible latence requise pour le traitement des données en temps réel et les activités de trading."
        ]
    },
    {
        "Question Number": "20",
        "Situation": "Imaginez que vous gérez une application qui traite des fichiers vidéo pour le transcodage et qui a une demande fluctuante. Pour garantir un traitement résilient et efficace, vous utilisez Amazon SQS pour la mise en file d'attente des messages et des Groupes de Mise à l'Échelle Automatique (ASG) pour votre pool de travailleurs. Cependant, certains messages échouent occasionnellement et nécessitent un traitement spécial pour éviter une surcharge du système.",
        "Question": "Quelle approche devriez-vous mettre en œuvre pour améliorer la résilience et garantir que les messages échoués sont traités efficacement ?",
        "Options": {
            "1": "Utiliser une file de messages en échec (Dead-Letter Queue, DLQ) au sein de SQS pour capturer les messages problématiques qui échouent au traitement plusieurs fois.",
            "2": "Configurer des politiques de mise à l'échelle ASG pour n'ajouter des instances que lorsque l'utilisation du CPU dépasse 80%.",
            "3": "Utiliser Amazon RDS pour stocker et réessayer les messages échoués jusqu'à ce qu'ils soient traités avec succès.",
            "4": "Mettre en place des Alarmes CloudWatch pour vous notifier chaque fois qu'un message échoue, afin que vous puissiez le retraiter manuellement."
        },
        "Correct Answer": "Utiliser une file de messages en échec (Dead-Letter Queue, DLQ) au sein de SQS pour capturer les messages problématiques qui échouent au traitement plusieurs fois.",
        "Explanation": "Une file de messages en échec (DLQ) est spécifiquement conçue pour gérer les messages qui ne peuvent pas être traités avec succès après un nombre spécifié de tentatives. En utilisant une DLQ, vous pouvez isoler ces messages problématiques pour une enquête plus approfondie sans impacter le traitement des autres messages dans la file. Cette approche améliore la résilience de votre application en empêchant les échecs de traitement des messages de submerger votre système et permet un débogage et un traitement plus faciles des messages échoués.",
        "Other Options": [
            "Configurer des politiques de mise à l'échelle ASG pour n'ajouter des instances que lorsque l'utilisation du CPU dépasse 80% ne traite pas directement le problème du traitement des messages échoués. Bien que cela puisse aider à gérer l'allocation des ressources, cela ne fournit pas de mécanisme pour gérer les messages qui échouent à être traités, ce qui est le problème central dans ce scénario.",
            "Utiliser Amazon RDS pour stocker et réessayer les messages échoués jusqu'à ce qu'ils soient traités avec succès n'est pas une solution optimale. RDS est un service de base de données relationnelle et n'est pas conçu pour la mise en file d'attente des messages. Cette approche introduirait une complexité et une latence inutiles, car elle nécessiterait une logique supplémentaire pour gérer l'état des messages et leurs réessais.",
            "Mettre en place des Alarmes CloudWatch pour vous notifier chaque fois qu'un message échoue créerait une approche réactive plutôt qu'une approche proactive. Bien que cela puisse vous aider à surveiller les échecs, cela ne fournit pas un moyen automatisé de gérer les messages échoués, ce qui est essentiel pour maintenir la résilience et l'efficacité du système."
        ]
    },
    {
        "Question Number": "21",
        "Situation": "Une entreprise de services financiers déploie une application qui nécessite un chiffrement rapide et ininterrompu entre les clients et les instances backend, ainsi que la possibilité d'utiliser une adresse IP statique pour le filtrage.",
        "Question": "Quel type de répartiteur de charge AWS est le plus adapté à ce scénario, et pourquoi ?",
        "Options": {
            "1": "Application Load Balancer (ALB), car il permet le routage basé sur le contenu et fournit la terminaison SSL.",
            "2": "Network Load Balancer (NLB), car il fonctionne au niveau 4, prend en charge les IP statiques et permet un chiffrement ininterrompu avec le transfert TCP.",
            "3": "Classic Load Balancer (CLB), car il est compatible avec HTTPS et prend en charge les sessions persistantes pour des connexions sécurisées.",
            "4": "Application Load Balancer (ALB), car il prend en charge les adresses IP statiques et fournit un haut débit."
        },
        "Correct Answer": "Network Load Balancer (NLB), car il fonctionne au niveau 4, prend en charge les IP statiques et permet un chiffrement ininterrompu avec le transfert TCP.",
        "Explanation": "Le Network Load Balancer (NLB) est le choix le plus adapté à ce scénario car il fonctionne au niveau 4 (couche de transport) du modèle OSI, ce qui lui permet de gérer le trafic TCP directement. Cette capacité lui permet de maintenir un chiffrement ininterrompu entre les clients et les instances backend, car il peut transférer des paquets TCP sans les déchiffrer. De plus, le NLB prend en charge les adresses IP statiques, ce qui est essentiel pour le filtrage. Cette combinaison de fonctionnalités rend le NLB idéal pour les applications nécessitant des connexions rapides et sécurisées avec des IP statiques.",
        "Other Options": [
            "L'Application Load Balancer (ALB) n'est pas adapté car, bien qu'il fournisse la terminaison SSL et le routage basé sur le contenu, il fonctionne au niveau 7 (couche application), ce qui signifie qu'il déchiffrerait le trafic, ce qui pourrait compromettre l'exigence de chiffrement ininterrompu.",
            "Le Classic Load Balancer (CLB) n'est pas le meilleur choix car, bien qu'il prenne en charge HTTPS, c'est une technologie plus ancienne qui ne fournit pas le même niveau de performance et de fonctionnalités que le NLB. Il ne prend également pas en charge les IP statiques de la même manière que le NLB.",
            "L'Application Load Balancer (ALB) est incorrectement déclaré comme prenant en charge les adresses IP statiques ; il ne fournit pas directement d'IP statiques. Au lieu de cela, il utilise des IP dynamiques et nécessite des configurations supplémentaires (comme l'utilisation d'un NLB en amont) pour atteindre la fonctionnalité d'IP statique."
        ]
    },
    {
        "Question Number": "22",
        "Situation": "Une entreprise possède un bucket S3 nommé \"secretcatproject\" qui contient des données sensibles. L'entreprise doit permettre l'accès à ce bucket à des utilisateurs spécifiques d'un compte partenaire tout en s'assurant que les données restent sécurisées contre l'accès public.",
        "Question": "Quelle méthode l'entreprise devrait-elle utiliser pour accorder l'accès nécessaire tout en empêchant l'accès non autorisé par des utilisateurs anonymes ?",
        "Options": {
            "1": "Définir la politique du bucket pour permettre l'accès public à tous les utilisateurs afin de simplifier la gestion des accès.",
            "2": "Utiliser une politique de bucket S3 qui spécifie les rôles IAM du compte partenaire comme principaux avec la permission d'accéder au bucket.",
            "3": "Activer \"Bloquer l'accès public\" sur le bucket et utiliser des listes de contrôle d'accès (ACL) pour gérer l'accès pour le compte partenaire.",
            "4": "Attacher une politique IAM directement au bucket pour contrôler l'accès des utilisateurs dans le compte partenaire."
        },
        "Correct Answer": "Utiliser une politique de bucket S3 qui spécifie les rôles IAM du compte partenaire comme principaux avec la permission d'accéder au bucket.",
        "Explanation": "Utiliser une politique de bucket S3 pour spécifier les rôles IAM du compte partenaire comme principaux permet un contrôle granulaire sur qui peut accéder au bucket. Cette méthode garantit que seuls les utilisateurs désignés du compte partenaire peuvent accéder aux données sensibles, tout en empêchant tout accès public. Les politiques de bucket sont des outils puissants pour gérer les permissions et peuvent être adaptées pour répondre à des exigences de sécurité spécifiques, ce qui en fait la méthode la plus sécurisée et appropriée pour la situation décrite.",
        "Other Options": [
            "Définir la politique du bucket pour permettre l'accès public à tous les utilisateurs exposerait les données sensibles à quiconque sur Internet, ce qui est contraire à l'exigence de garder les données sécurisées contre l'accès public.",
            "Activer 'Bloquer l'accès public' sur le bucket et utiliser des listes de contrôle d'accès (ACL) n'est pas la meilleure pratique pour gérer l'accès. Bien que cela empêche l'accès public, les ACL peuvent être complexes et moins gérables que les politiques de bucket, surtout lorsqu'il s'agit d'accès inter-comptes. Les politiques de bucket sont généralement préférées à cet effet.",
            "Attacher une politique IAM directement au bucket n'est pas possible, car les politiques IAM sont attachées aux utilisateurs, groupes ou rôles IAM, et non directement aux buckets S3. Le contrôle d'accès pour les buckets S3 est géré par des politiques de bucket ou des ACL, rendant cette option incorrecte."
        ]
    },
    {
        "Question Number": "23",
        "Situation": "Une entreprise de santé doit sauvegarder les données des patients sur AWS à des fins de récupération après sinistre. Pour réduire les coûts, elle nécessite une solution qui minimise les coûts de stockage tout en garantissant la conservation à long terme des sauvegardes. Elle souhaite également avoir la possibilité de récupérer les données dans quelques heures si nécessaire.",
        "Question": "Quelles stratégies de sauvegarde répondraient le mieux à ces exigences ? (Choisissez deux.)",
        "Options": {
            "1": "Stocker les sauvegardes dans Amazon S3 Standard",
            "2": "Utiliser Amazon S3 Glacier Flexible Retrieval pour le stockage d'archives",
            "3": "Stocker les sauvegardes dans Amazon S3 Standard-IA",
            "4": "Utiliser des instantanés Amazon EBS stockés dans la même région",
            "5": "Mettre en œuvre AWS Backup avec des politiques de cycle de vie pour transférer les sauvegardes vers des classes de stockage à coût réduit"
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Utiliser Amazon S3 Glacier Flexible Retrieval pour le stockage d'archives",
            "Mettre en œuvre AWS Backup avec des politiques de cycle de vie pour transférer les sauvegardes vers des classes de stockage à coût réduit"
        ],
        "Explanation": "Amazon S3 Glacier Flexible Retrieval est une solution économique pour le stockage de données à long terme et permet la récupération des données dans quelques heures, ce qui correspond aux exigences de l'entreprise. AWS Backup avec des politiques de cycle de vie permet la transition automatique des sauvegardes vers des classes de stockage à coût réduit après une certaine période, ce qui peut réduire considérablement les coûts de stockage au fil du temps.",
        "Other Options": [
            "Stocker les sauvegardes dans Amazon S3 Standard n'est pas la solution la plus économique pour la conservation des données à long terme. Bien qu'elle offre une grande durabilité, disponibilité et performance, son coût est plus élevé par rapport à d'autres classes de stockage comme S3 Glacier ou S3 Standard-IA.",
            "Stocker les sauvegardes dans Amazon S3 Standard-IA (Infrequent Access) pourrait être une solution économique pour les données qui sont moins fréquemment accessibles, mais cela peut ne pas offrir le même niveau d'économies de coûts pour le stockage à long terme que S3 Glacier ou AWS Backup avec des politiques de cycle de vie.",
            "Utiliser des instantanés Amazon EBS stockés dans la même région ne minimise pas nécessairement les coûts de stockage, surtout pour la conservation à long terme. De plus, stocker les sauvegardes dans la même région ne fournit pas la redondance géographique souvent souhaitée pour les besoins de récupération après sinistre."
        ]
    },
    {
        "Question Number": "24",
        "Situation": "Un site d'actualités stocke des fichiers multimédias dans Amazon S3. Ces fichiers sont fréquemment accessibles dans les 7 premiers jours après leur téléchargement, mais voient très peu d'accès après cette période. Le site souhaite réduire les coûts de stockage en fonction de ces modèles d'accès.",
        "Question": "Quelle configuration de stockage optimiserait le mieux les coûts ?",
        "Options": {
            "1": "Stocker tous les fichiers dans S3 Standard",
            "2": "Stocker les fichiers dans S3 Intelligent-Tiering",
            "3": "Déplacer les fichiers vers S3 Standard-IA après 7 jours",
            "4": "Utiliser S3 Glacier pour tous les fichiers multimédias"
        },
        "Correct Answer": "Déplacer les fichiers vers S3 Standard-IA après 7 jours",
        "Explanation": "Déplacer les fichiers vers S3 Standard-IA (Infrequent Access) après 7 jours est la meilleure option car elle est conçue pour les données qui sont moins fréquemment accessibles mais nécessitent un accès rapide lorsque cela est nécessaire. Étant donné que les fichiers multimédias sont fréquemment accessibles dans les 7 premiers jours et voient peu d'accès par la suite, la transition vers Standard-IA après cette période réduira considérablement les coûts de stockage tout en permettant un accès rapide si nécessaire. S3 Standard-IA offre des coûts de stockage inférieurs par rapport à S3 Standard, ce qui en fait une solution économique pour le modèle d'accès décrit.",
        "Other Options": [
            "Stocker tous les fichiers dans S3 Standard n'optimiserait pas les coûts puisque S3 Standard est plus cher que S3 Standard-IA pour les données peu fréquemment accessibles. Cette option ne tire pas parti des coûts inférieurs disponibles pour les données qui ne sont pas fréquemment accessibles après les 7 premiers jours.",
            "Stocker les fichiers dans S3 Intelligent-Tiering pourrait être une option viable, mais cela entraîne des coûts supplémentaires en raison de la surveillance et du passage automatique entre les niveaux. Étant donné que le modèle d'accès est prévisible (accès fréquent dans les 7 premiers jours et peu fréquent par la suite), déplacer manuellement les fichiers vers Standard-IA après 7 jours est plus économique que d'utiliser Intelligent-Tiering.",
            "Utiliser S3 Glacier pour tous les fichiers multimédias n'est pas approprié car Glacier est conçu pour le stockage d'archives et a des temps de récupération qui peuvent varier de quelques minutes à plusieurs heures. Cela ne répondrait pas à l'exigence d'un accès rapide aux fichiers qui pourraient encore être nécessaires peu après le téléchargement initial."
        ]
    },
    {
        "Question Number": "25",
        "Situation": "Une entreprise a récemment créé un nouveau compte AWS, et le fondateur utilise actuellement l'utilisateur root pour gérer les ressources au sein du compte. L'utilisateur root a un contrôle total et illimité sur toutes les ressources du compte, et par défaut, aucun autre utilisateur n'a de permissions jusqu'à ce qu'elles soient accordées explicitement. Pour une meilleure sécurité, le fondateur souhaite déléguer des responsabilités à d'autres membres de l'équipe en créant des utilisateurs IAM avec des permissions spécifiques au lieu d'utiliser le compte root pour les tâches quotidiennes.",
        "Question": "Quelles actions le fondateur devrait-il entreprendre pour garantir que le compte AWS reste sécurisé tout en gérant l'accès de manière efficace ? (Choisissez deux.)",
        "Options": {
            "1": "Continuer à utiliser l'utilisateur root pour toutes les tâches administratives quotidiennes et créer des utilisateurs IAM avec un accès en lecture seule pour les membres de l'équipe.",
            "2": "Activer l'authentification multi-facteurs (MFA) sur le compte root, créer des utilisateurs IAM pour chaque membre de l'équipe avec les permissions nécessaires, et éviter d'utiliser le compte root pour les activités régulières.",
            "3": "Partager les identifiants du compte root avec les membres de l'équipe et configurer des groupes IAM pour organiser les permissions.",
            "4": "Créer un utilisateur root séparé pour chaque membre de l'équipe afin de leur donner un accès direct au compte AWS.",
            "5": "Faire régulièrement tourner les clés d'accès root et limiter l'utilisation du compte root aux tâches essentielles."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Activer l'authentification multi-facteurs (MFA) sur le compte root, créer des utilisateurs IAM pour chaque membre de l'équipe avec les permissions nécessaires, et éviter d'utiliser le compte root pour les activités régulières.",
            "Faire régulièrement tourner les clés d'accès root et limiter l'utilisation du compte root aux tâches essentielles."
        ],
        "Explanation": "Activer l'authentification multi-facteurs (MFA) sur le compte root ajoute une couche de sécurité supplémentaire en exigeant deux formes d'identification pour se connecter. Créer des utilisateurs IAM pour chaque membre de l'équipe permet au fondateur de déléguer des responsabilités et de gérer l'accès de manière efficace en accordant des permissions spécifiques à chaque utilisateur. De cette manière, le compte root, qui a un contrôle total sur toutes les ressources, n'est pas utilisé pour les activités régulières, réduisant ainsi le risque de modifications accidentelles ou de violations de sécurité. Faire régulièrement tourner les clés d'accès root est une autre bonne pratique pour maintenir la sécurité. Cela garantit que même si une clé est compromise, elle ne sera valide que pour une période limitée. Limiter l'utilisation du compte root aux tâches essentielles minimise également le risque de modifications accidentelles ou de violations de sécurité.",
        "Other Options": [
            "Continuer à utiliser l'utilisateur root pour toutes les tâches administratives quotidiennes n'est pas une bonne pratique car cela augmente le risque de modifications accidentelles ou de violations de sécurité. Créer des utilisateurs IAM avec un accès en lecture seule pour les membres de l'équipe limite leur capacité à effectuer des tâches nécessaires.",
            "Partager les identifiants du compte root avec les membres de l'équipe est un risque de sécurité grave. Cela leur donne un contrôle total et illimité sur toutes les ressources du compte. Configurer des groupes IAM pour organiser les permissions est une bonne pratique, mais cela doit être fait avec des utilisateurs IAM, pas avec le compte root.",
            "Créer un utilisateur root séparé pour chaque membre de l'équipe n'est pas possible. AWS permet seulement un utilisateur root par compte. De plus, donner un accès direct au compte AWS aux membres de l'équipe est un risque de sécurité grave."
        ]
    },
    {
        "Question Number": "26",
        "Situation": "Une entreprise souhaite créer une application de service client capable d'analyser les retours des clients pour identifier les thèmes et sentiments clés, puis de convertir cette analyse en un résumé audio pour l'accessibilité.",
        "Question": "Quelle combinaison de services gérés par AWS serait la plus appropriée pour ces tâches, et pourquoi ? (Choisissez deux.)",
        "Options": {
            "1": "Amazon SageMaker et Amazon Rekognition, car ils permettent un modélisation avancée de l'apprentissage machine et des capacités de reconnaissance d'image.",
            "2": "Amazon Comprehend et Amazon Polly, car Comprehend peut analyser le texte pour en extraire les thèmes et sentiments, tandis que Polly peut convertir le texte en discours naturel.",
            "3": "AWS Glue et Amazon Athena, pour traiter les données des retours et effectuer des requêtes complexes sur des données structurées.",
            "4": "Amazon Translate et Amazon Lex, pour traduire les retours des clients dans différentes langues et construire des interfaces conversationnelles.",
            "5": "Amazon Transcribe et Amazon Translate, pour transcrire les retours oraux et les traduire en plusieurs langues."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Amazon SageMaker et Amazon Rekognition, car ils permettent un modélisation avancée de l'apprentissage machine et des capacités de reconnaissance d'image.",
            "Amazon Comprehend et Amazon Polly, car Comprehend peut analyser le texte pour en extraire les thèmes et sentiments, tandis que Polly peut convertir le texte en discours naturel."
        ],
        "Explanation": "Amazon SageMaker est un service entièrement géré qui offre à chaque développeur et scientifique des données la possibilité de construire, former et déployer rapidement des modèles d'apprentissage machine (ML). SageMaker élimine le travail lourd de chaque étape du processus d'apprentissage machine pour faciliter le développement de modèles de haute qualité. Amazon Rekognition facilite l'ajout d'analyses d'images et de vidéos à vos applications en utilisant une technologie d'apprentissage profond éprouvée et hautement évolutive qui ne nécessite aucune expertise en apprentissage machine pour être utilisée. Amazon Comprehend utilise l'apprentissage machine pour trouver des insights et des relations dans le texte. Il peut identifier la langue du texte ; extraire des phrases clés, des lieux, des personnes, des marques ou des événements ; comprendre si le texte est positif ou négatif ; analyser le texte en utilisant la tokenisation et les parties du discours ; et organiser automatiquement une collection de fichiers texte par sujet. Amazon Polly est un service qui transforme le texte en discours réaliste, permettant de créer des applications qui parlent et de construire de nouvelles catégories de produits activés par la parole.",
        "Other Options": [
            "AWS Glue et Amazon Athena sont utilisés pour des travaux ETL (Extraire, Transformer, Charger) et pour interroger des données, pas pour l'analyse des sentiments ou la conversion texte-parole.",
            "Amazon Translate et Amazon Lex sont utilisés pour la traduction linguistique et la construction d'interfaces conversationnelles, pas pour l'analyse des sentiments ou la conversion texte-parole.",
            "Amazon Transcribe et Amazon Translate sont utilisés pour transcrire les retours oraux et les traduire en plusieurs langues, pas pour l'analyse des sentiments ou la conversion texte-parole."
        ]
    },
    {
        "Question Number": "27",
        "Situation": "Votre équipe conçoit une application hautement résiliente qui repose sur une base de données backend pour une récupération rapide des données et une durabilité.",
        "Question": "Quelle fonctionnalité d'Amazon DynamoDB améliorerait le mieux la résilience et garantirait la disponibilité des données en cas de pannes régionales ?",
        "Options": {
            "1": "DynamoDB Streams, permettant la réplication en temps réel des changements vers d'autres services AWS.",
            "2": "DynamoDB Global Tables, permettant la réplication multi-régionale pour un basculement automatique et une résilience inter-régionale.",
            "3": "DynamoDB Accelerator (DAX), fournissant un cache en mémoire pour accélérer les temps de lecture lors des pics de charge.",
            "4": "DynamoDB Auto Scaling, ajustant dynamiquement le débit de lecture et d'écriture pour correspondre aux pics de demande."
        },
        "Correct Answer": "DynamoDB Global Tables, permettant la réplication multi-régionale pour un basculement automatique et une résilience inter-régionale.",
        "Explanation": "DynamoDB Global Tables fournissent une solution entièrement gérée pour déployer des bases de données entièrement répliquées et multi-régionales. Cette fonctionnalité garantit que votre application peut continuer à fonctionner même en cas de panne régionale, car elle réplique automatiquement les données à travers plusieurs régions AWS. Cette réplication permet un basculement automatique, ce qui signifie que si une région devient indisponible, l'application peut passer sans problème à une autre région où les données sont toujours accessibles, améliorant ainsi la résilience et garantissant la disponibilité des données.",
        "Other Options": [
            "DynamoDB Streams permet la réplication en temps réel des changements vers d'autres services AWS, mais ne fournit pas de réplication multi-régionale ou de capacités de basculement automatique. Il est plus adapté aux architectures basées sur les événements plutôt qu'à garantir la résilience contre les pannes régionales.",
            "DynamoDB Accelerator (DAX) est conçu pour améliorer les performances de lecture en fournissant un cache en mémoire, ce qui peut aider lors des pics de charge mais ne traite pas la question de la disponibilité des données en cas de pannes régionales.",
            "DynamoDB Auto Scaling ajuste le débit de lecture et d'écriture en fonction des pics de demande, ce qui est bénéfique pour la performance et la gestion des coûts, mais cela n'améliore pas la résilience ou ne garantit pas la disponibilité des données à travers les régions en cas de pannes."
        ]
    },
    {
        "Question Number": "28",
        "Situation": "Une entreprise doit s'assurer que les données sensibles stockées dans Amazon S3 sont chiffrées au repos en utilisant des clés gérées par le client.",
        "Question": "Quel service l'entreprise devrait-elle utiliser pour gérer les clés de chiffrement ?",
        "Options": {
            "1": "AWS Certificate Manager (ACM)",
            "2": "AWS Key Management Service (AWS KMS)",
            "3": "Amazon S3 Server-Side Encryption with AES-256",
            "4": "AWS Secrets Manager"
        },
        "Correct Answer": "AWS Key Management Service (AWS KMS)",
        "Explanation": "AWS Key Management Service (AWS KMS) est spécifiquement conçu pour gérer les clés de chiffrement et fournit un moyen centralisé de créer, gérer et contrôler l'utilisation des clés cryptographiques à travers les services AWS. En utilisant AWS KMS, vous pouvez créer des clés gérées par le client (CMK) qui peuvent être utilisées pour chiffrer les données stockées dans Amazon S3. Cela permet un contrôle granulaire sur qui peut utiliser les clés et comment elles peuvent être utilisées, garantissant que les données sensibles sont chiffrées au repos conformément aux exigences de sécurité de l'entreprise.",
        "Other Options": [
            "AWS Certificate Manager (ACM) est principalement utilisé pour gérer les certificats SSL/TLS pour sécuriser les sites web et les applications. Il ne fournit pas de fonctionnalité pour gérer les clés de chiffrement pour les données au repos dans des services comme Amazon S3.",
            "Amazon S3 Server-Side Encryption with AES-256 offre un chiffrement au repos, mais utilise par défaut des clés gérées par AWS. Bien qu'il puisse également utiliser des clés gérées par le client, il ne fournit pas les capacités de gestion des clés qu'offre AWS KMS, ce qui fait d'AWS KMS le choix le plus approprié pour gérer les clés de chiffrement.",
            "AWS Secrets Manager est conçu pour gérer des secrets tels que des clés API, des identifiants de base de données et d'autres informations sensibles. Il n'est pas destiné à gérer les clés de chiffrement pour les données au repos dans Amazon S3."
        ]
    },
    {
        "Question Number": "29",
        "Situation": "Une application de médias sociaux dispose d'une base de données MySQL qui reçoit fréquemment des demandes de lecture pour du contenu populaire. Pour réduire les coûts de la base de données et améliorer les temps de réponse, ils souhaitent mettre en œuvre une couche de mise en cache pour décharger les lectures de la base de données.",
        "Question": "Quelle stratégie de mise en cache serait la plus rentable pour ce scénario ?",
        "Options": {
            "1": "Utiliser Amazon S3 pour mettre en cache le contenu fréquemment consulté",
            "2": "Mettre en œuvre une mise en cache en mémoire avec un cache géré comme Amazon ElastiCache",
            "3": "Créer plusieurs répliques de lecture de la base de données MySQL",
            "4": "Utiliser un système de traitement par lots pour précalculer les requêtes populaires"
        },
        "Correct Answer": "Mettre en œuvre une mise en cache en mémoire avec un cache géré comme Amazon ElastiCache",
        "Explanation": "Mettre en œuvre une mise en cache en mémoire avec un service géré comme Amazon ElastiCache est la stratégie la plus rentable pour ce scénario car elle permet un accès rapide aux données fréquemment demandées, réduisant ainsi considérablement la charge sur la base de données MySQL. Les caches en mémoire stockent les données dans la RAM, ce qui offre des temps de lecture beaucoup plus rapides par rapport aux solutions de stockage sur disque. Cette approche peut gérer efficacement un trafic de lecture élevé et peut être mise à l'échelle selon les besoins, ce qui la rend idéale pour les applications avec des demandes de lecture fréquentes pour du contenu populaire.",
        "Other Options": [
            "Utiliser Amazon S3 pour mettre en cache le contenu fréquemment consulté n'est pas adapté car S3 est principalement un service de stockage d'objets, optimisé pour la durabilité et la disponibilité plutôt que pour la vitesse. Accéder aux données depuis S3 implique une latence plus élevée par rapport à la mise en cache en mémoire, ce qui le rend moins efficace pour réduire les temps de réponse dans un scénario à forte lecture.",
            "Créer plusieurs répliques de lecture de la base de données MySQL peut améliorer les performances de lecture en répartissant la charge sur plusieurs répliques, mais cela ne traite pas la rentabilité aussi efficacement que la mise en cache. Chaque réplique entraîne des coûts supplémentaires pour le stockage et la maintenance, et bien qu'elle puisse aider à la scalabilité des lectures, elle ne fournit pas les mêmes avantages de vitesse qu'un cache en mémoire.",
            "Utiliser un système de traitement par lots pour précalculer les requêtes populaires n'est pas une solution de mise en cache directe et peut ne pas fournir un accès en temps réel au contenu fréquemment consulté. Bien qu'il puisse réduire la charge sur la base de données en précalculant les résultats, il n'offre pas les temps de réponse immédiats qu'un cache en mémoire fournirait pour des demandes de contenu dynamique."
        ]
    },
    {
        "Question Number": "30",
        "Situation": "Une entreprise construit une plateforme de support client et souhaite utiliser les services AWS pour analyser les retours des clients et générer des réponses vocales automatisées. Ils souhaitent extraire des informations clés à partir des données textuelles et convertir les réponses textuelles en discours.",
        "Question": "Quels services AWS l'entreprise devrait-elle utiliser pour atteindre ces objectifs ?",
        "Options": {
            "1": "Utiliser Amazon Polly pour convertir le texte en discours et Amazon Comprehend pour analyser le sentiment des clients et extraire des phrases clés des retours.",
            "2": "Utiliser Amazon Lex pour créer un chatbot conversationnel et Amazon Polly pour la conversion de texte en discours.",
            "3": "Utiliser Amazon S3 pour stocker les retours et AWS Lambda pour analyser le texte et générer du discours.",
            "4": "Utiliser Amazon Transcribe pour convertir le discours en texte et Amazon Rekognition pour l'analyse des sentiments."
        },
        "Correct Answer": "Utiliser Amazon Polly pour convertir le texte en discours et Amazon Comprehend pour analyser le sentiment des clients et extraire des phrases clés des retours.",
        "Explanation": "Cette option est correcte car Amazon Polly est spécifiquement conçu pour convertir le texte en discours réaliste, ce qui correspond à l'objectif de l'entreprise de générer des réponses vocales automatisées. De plus, Amazon Comprehend est un service de traitement du langage naturel (NLP) qui peut analyser les données textuelles pour extraire des informations telles que le sentiment et les phrases clés, ce qui le rend idéal pour analyser les retours des clients.",
        "Other Options": [
            "Cette option est incorrecte car Amazon Lex est utilisé pour créer des interfaces conversationnelles (chatbots) et n'est pas principalement axé sur l'analyse des données textuelles pour le sentiment ou l'extraction de phrases clés. Bien qu'Amazon Polly soit inclus pour la conversion de discours, cela ne traite pas de l'analyse des retours des clients.",
            "Cette option est incorrecte car Amazon S3 est un service de stockage et ne fournit aucune capacité d'analyse. AWS Lambda peut être utilisé pour l'informatique sans serveur mais nécessiterait des services supplémentaires pour l'analyse de texte et la génération de discours, ce qui le rend moins efficace que la réponse correcte.",
            "Cette option est incorrecte car Amazon Transcribe est utilisé pour convertir le discours en texte, ce qui n'est pas pertinent pour l'objectif de l'entreprise d'analyser les retours textuels. De plus, Amazon Rekognition est un service d'analyse d'images et de vidéos, pas adapté pour l'analyse des sentiments des données textuelles."
        ]
    },
    {
        "Question Number": "31",
        "Situation": "Une entreprise souhaite stocker des données sensibles dans Amazon S3 et doit s'assurer qu'AWS n'a pas accès aux données en clair. Elle veut également avoir un contrôle total sur la gestion des clés et le traitement du chiffrement.",
        "Question": "Quelle méthode de chiffrement l'entreprise devrait-elle utiliser pour répondre à ces exigences ?",
        "Options": {
            "1": "Chiffrement côté serveur avec des clés gérées par S3 (SSE-S3)",
            "2": "Chiffrement côté serveur avec des clés gérées par AWS KMS (SSE-KMS)",
            "3": "Chiffrement côté client",
            "4": "Chiffrement côté serveur avec des clés fournies par le client (SSE-C)"
        },
        "Correct Answer": "Chiffrement côté client",
        "Explanation": "Le chiffrement côté client permet à l'entreprise de chiffrer les données avant de les envoyer à Amazon S3, garantissant qu'AWS n'a pas accès aux données en clair. Cette méthode donne à l'entreprise un contrôle total sur le processus de chiffrement et la gestion des clés, car elle peut utiliser ses propres clés de chiffrement et algorithmes pour sécuriser les données avant de les télécharger sur S3. Cela répond à l'exigence de garantir qu'AWS n'a pas accès aux données en clair.",
        "Other Options": [
            "Le chiffrement côté serveur avec des clés gérées par S3 (SSE-S3) utilise les propres clés d'Amazon pour gérer le chiffrement, ce qui signifie qu'AWS a accès aux données en clair, ne répondant donc pas à l'exigence de l'entreprise.",
            "Le chiffrement côté serveur avec des clés gérées par AWS KMS (SSE-KMS) permet un meilleur contrôle sur la gestion des clés par rapport à SSE-S3, mais AWS a toujours accès aux données en clair car les processus de chiffrement et de déchiffrement se déroulent côté serveur.",
            "Le chiffrement côté serveur avec des clés fournies par le client (SSE-C) permet aux clients de fournir leurs propres clés pour le chiffrement, mais AWS gère toujours les processus de chiffrement et de déchiffrement, ce qui signifie qu'AWS pourrait potentiellement accéder aux données en clair, ce qui ne répond pas à l'exigence de l'entreprise."
        ]
    },
    {
        "Question Number": "32",
        "Situation": "Un site d'actualités mondial avec des millions de lecteurs dans le monde entier utilise Amazon CloudFront pour livrer efficacement du contenu avec une faible latence. L'équipe du site souhaite ajouter des fonctionnalités qui personnalisent le contenu en fonction du pays du spectateur, telles que des points forts d'actualités locales, et doit également mettre en œuvre des tests A/B pour tester différents agencements d'articles. La solution doit fonctionner dans des emplacements de périphérie pour garantir une expérience fluide et à faible latence pour les spectateurs du monde entier.",
        "Question": "Quel service AWS et quelle configuration l'architecte de solutions devrait-il recommander ?",
        "Options": {
            "1": "Déployer AWS Lambda dans un VPC avec des règles de routage basées sur le pays pour la personnalisation du contenu",
            "2": "Utiliser des fonctions Lambda@Edge, déclenchées par des événements de demande de visualiseur CloudFront et de demande d'origine, pour personnaliser le contenu en fonction du pays et effectuer des tests A/B dans des emplacements de périphérie",
            "3": "Lancer des instances Amazon EC2 dans plusieurs régions avec un contenu spécifique à chaque pays stocké localement sur chaque instance",
            "4": "Configurer Amazon CloudFront avec des comportements de cache spécifiques à chaque pays pour servir un contenu personnalisé par pays"
        },
        "Correct Answer": "Utiliser des fonctions Lambda@Edge, déclenchées par des événements de demande de visualiseur CloudFront et de demande d'origine, pour personnaliser le contenu en fonction du pays et effectuer des tests A/B dans des emplacements de périphérie",
        "Explanation": "L'utilisation de Lambda@Edge permet au site de faire fonctionner du code plus près des utilisateurs dans les emplacements de périphérie CloudFront, ce qui minimise la latence et améliore l'expérience utilisateur. En déclenchant des fonctions sur des événements de demande de visualiseur et de demande d'origine, le site peut personnaliser dynamiquement le contenu en fonction du pays de l'utilisateur et mettre en œuvre des tests A/B pour différents agencements. Cette solution est efficace et tire parti des capacités de CloudFront pour livrer rapidement et efficacement du contenu personnalisé.",
        "Other Options": [
            "Déployer AWS Lambda dans un VPC avec des règles de routage basées sur le pays ne serait pas optimal car cela introduirait de la latence en nécessitant que le trafic soit routé via le VPC au lieu d'être directement aux emplacements de périphérie. Cette configuration n'utilise pas efficacement les avantages de faible latence de CloudFront.",
            "Bien que l'utilisation de fonctions Lambda@Edge soit la bonne approche, cette option ne précise pas l'utilisation des événements CloudFront, qui sont essentiels pour déclencher les fonctions au bon moment. Par conséquent, elle manque de détails nécessaires pour une solution complète.",
            "Lancer des instances Amazon EC2 dans plusieurs régions serait inefficace et coûteux. Cela nécessiterait la gestion de plusieurs instances et la synchronisation du contenu entre elles, ce qui complique l'architecture et ne tire pas parti des avantages de l'informatique en périphérie pour la livraison de contenu à faible latence."
        ]
    },
    {
        "Question Number": "33",
        "Situation": "Une entreprise de streaming vidéo doit connecter ses services de livraison de contenu, hébergés sur AWS, à son réseau de siège dans une autre ville. L'entreprise nécessite une connexion à haut débit pour le transfert de gros fichiers vidéo et une faible latence pour éviter les problèmes de mise en mémoire tampon pendant la lecture vidéo. Elle souhaite également que la connexion soit privée pour garantir que le contenu vidéo sensible ne soit pas exposé à Internet public et recherche une solution rentable pour atteindre ces objectifs.",
        "Question": "Quelle approche répondrait le mieux à ses besoins ?",
        "Options": {
            "1": "AWS PrivateLink pour créer un lien privé pour le contenu vidéo directement vers son siège",
            "2": "AWS Direct Connect pour établir une connexion privée à haut débit entre AWS et son réseau sur site",
            "3": "Un circuit MPLS point à point d'un fournisseur de télécommunications pour créer une connexion privée vers AWS",
            "4": "Utiliser un service Internet géré avec des VPN dédiés pour un transfert de données sécurisé"
        },
        "Correct Answer": "AWS Direct Connect pour établir une connexion privée à haut débit entre AWS et son réseau sur site",
        "Explanation": "AWS Direct Connect fournit une connexion réseau dédiée du siège de l'entreprise à AWS, ce qui est idéal pour des exigences à haut débit et à faible latence. Ce service permet une connexion privée qui ne traverse pas Internet public, garantissant que le contenu vidéo sensible reste sécurisé. Direct Connect peut gérer efficacement de gros transferts de données, ce qui en fait une solution rentable pour le transfert de gros fichiers vidéo sans risque de mise en mémoire tampon pendant la lecture.",
        "Other Options": [
            "AWS PrivateLink est conçu pour connecter des services de manière sécurisée au sein d'AWS et ne fournit pas de connexion directe aux réseaux sur site. Il est plus adapté pour accéder aux services AWS de manière privée plutôt que de transférer de gros fichiers entre AWS et un réseau externe.",
            "Un circuit MPLS point à point d'un fournisseur de télécommunications peut fournir une connexion privée, mais il peut ne pas être aussi rentable ou flexible qu'AWS Direct Connect. De plus, la configuration et la gestion des circuits MPLS peuvent être plus complexes et ne garantissent pas le même niveau de performance que Direct Connect.",
            "Utiliser un service Internet géré avec des VPN dédiés peut fournir une connexion sécurisée, mais cela n'offre généralement pas le même niveau de débit et de faible latence qu'AWS Direct Connect. Les VPN sur Internet peuvent également introduire des variations de performance, ce qui pourrait entraîner des problèmes de mise en mémoire tampon pendant la lecture vidéo."
        ]
    },
    {
        "Question Number": "34",
        "Situation": "Une entreprise de fabrication collecte des données de capteurs dans ses installations sur site et a besoin d'archiver ces données dans AWS pour un stockage et une analyse à long terme. Elle souhaite minimiser les coûts tout en nécessitant un moyen fluide de transférer les données vers le cloud avec un minimum d'efforts manuels.",
        "Question": "Quelle option de stockage hybride répondrait le mieux à ces exigences ?",
        "Options": {
            "1": "AWS Direct Connect",
            "2": "AWS Storage Gateway",
            "3": "Amazon S3 avec Transfer Acceleration",
            "4": "AWS DataSync"
        },
        "Correct Answer": "AWS Storage Gateway",
        "Explanation": "AWS Storage Gateway est conçu spécifiquement pour les solutions de stockage hybride dans le cloud, permettant aux applications sur site d'utiliser le stockage cloud de manière transparente. Il offre un moyen de transférer des données vers AWS avec un minimum d'efforts manuels, ce qui le rend idéal pour l'archivage des données de capteurs. Il prend en charge diverses configurations, telles que les passerelles de fichiers, de volumes et de bandes, ce qui peut aider à minimiser les coûts tout en garantissant que les données sont facilement accessibles pour l'analyse dans le cloud.",
        "Other Options": [
            "AWS Direct Connect fournit une connexion réseau dédiée entre les installations sur site et AWS, ce qui peut améliorer la bande passante et réduire les coûts de transfert de données. Cependant, il ne fournit pas un moyen fluide de gérer et de transférer les données automatiquement, car il nécessite une configuration et une gestion supplémentaires.",
            "Amazon S3 avec Transfer Acceleration accélère le transfert de fichiers vers S3 sur de longues distances, mais ne fournit pas de solution de stockage hybride. Il est plus adapté au transfert de fichiers qu'à l'intégration des données sur site avec le stockage cloud de manière transparente.",
            "AWS DataSync est un service qui automatise le déplacement des données entre le stockage sur site et les services de stockage AWS. Bien qu'il soit efficace pour transférer de grandes quantités de données, il peut nécessiter une configuration et une gestion plus manuelles par rapport à AWS Storage Gateway, qui est plus intégré dans les flux de travail existants."
        ]
    },
    {
        "Question Number": "35",
        "Situation": "Une entreprise de médias stocke de grands fichiers vidéo dans Amazon S3. Les vidéos sont fréquemment consultées peu après leur téléchargement, mais sont rarement consultées après un mois. L'entreprise souhaite optimiser les coûts de stockage sans compromettre les performances d'accès pour les vidéos récemment téléchargées.",
        "Question": "Quelle classe de stockage S3 le solutions architect devrait-il recommander ?",
        "Options": {
            "1": "S3 Standard",
            "2": "S3 Intelligent-Tiering",
            "3": "S3 Standard-Infrequent Access (S3 Standard-IA)",
            "4": "S3 One Zone-Infrequent Access (S3 One Zone-IA)"
        },
        "Correct Answer": "S3 Intelligent-Tiering",
        "Explanation": "S3 Intelligent-Tiering est la meilleure option pour ce scénario car il déplace automatiquement les données entre deux niveaux d'accès (fréquent et peu fréquent) en fonction des modèles d'accès changeants. Étant donné que les vidéos sont fréquemment consultées peu après leur téléchargement mais rarement après un mois, cette classe de stockage optimisera les coûts en déplaçant les données vers le niveau d'accès peu fréquent après la période d'accès initiale, sans compromettre les performances d'accès pour les vidéos récemment téléchargées.",
        "Other Options": [
            "S3 Standard n'est pas l'option la plus rentable pour ce cas d'utilisation car il est conçu pour les données fréquemment consultées et ne permet pas d'économies de coûts pour les données qui deviennent peu fréquemment consultées après une courte période.",
            "S3 Standard-Infrequent Access (S3 Standard-IA) n'est pas idéal car, bien qu'il soit moins cher pour les données peu fréquemment consultées, il entraîne des frais de récupération et n'est pas optimisé pour les données fréquemment consultées peu après leur téléchargement.",
            "S3 One Zone-Infrequent Access (S3 One Zone-IA) n'est également pas adapté car il stocke les données dans une seule zone de disponibilité, ce qui pose un risque de perte de données en cas de défaillance de la zone de disponibilité. De plus, il est conçu pour les données peu fréquemment consultées, ce qui ne correspond pas au besoin d'accès rapide peu après le téléchargement."
        ]
    },
    {
        "Question Number": "36",
        "Situation": "Une entreprise de développement de logiciels déploie une application basée sur des microservices utilisant des conteneurs Docker. L'application nécessite un déploiement automatisé, une mise à l'échelle et une gestion des conteneurs à travers un cluster d'instances EC2.",
        "Question": "Quels services AWS le solutions architect devrait-il recommander pour orchestrer l'application conteneurisée ? (Choisissez deux.)",
        "Options": {
            "1": "Amazon Elastic Container Service (ECS)",
            "2": "AWS Lambda",
            "3": "Amazon EC2 Auto Scaling",
            "4": "AWS Batch",
            "5": "Amazon Elastic Kubernetes Service (EKS)"
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Amazon Elastic Container Service (ECS)",
            "Amazon Elastic Kubernetes Service (EKS)"
        ],
        "Explanation": "Amazon Elastic Container Service (ECS) et Amazon Elastic Kubernetes Service (EKS) sont tous deux des services AWS spécifiquement conçus pour orchestrer des applications conteneurisées. ECS est un service haute performance et hautement évolutif qui vous permet d'exécuter et de gérer des applications compatibles avec Docker sur un cluster d'instances Amazon EC2. EKS, quant à lui, est un service géré qui facilite l'exécution de Kubernetes sur AWS sans avoir besoin d'installer, d'exploiter et de maintenir votre propre plan de contrôle ou vos nœuds Kubernetes. Les deux services offrent un déploiement automatisé, une mise à l'échelle et une gestion des conteneurs, ce qui est exactement ce que nécessite le scénario de la question.",
        "Other Options": [
            "AWS Lambda est un service de calcul sans serveur qui vous permet d'exécuter votre code sans provisionner ou gérer des serveurs. Bien qu'il puisse être utilisé en conjonction avec des applications conteneurisées, ce n'est pas un service spécifiquement conçu pour orchestrer des conteneurs.",
            "Amazon EC2 Auto Scaling est un service qui vous aide à maintenir la disponibilité de l'application et vous permet d'ajouter ou de supprimer automatiquement des instances EC2 selon les conditions que vous définissez. Bien qu'il puisse être utilisé pour faire évoluer les instances EC2 sous-jacentes, il ne fournit pas de capacités d'orchestration de conteneurs.",
            "AWS Batch est un service qui permet aux professionnels de l'informatique de planifier et d'exécuter des travaux de traitement par lots. Bien qu'il puisse exécuter des travaux qui sont conteneurisés, il n'est pas spécifiquement conçu pour orchestrer des applications conteneurisées."
        ]
    },
    {
        "Question Number": "37",
        "Situation": "Un site de commerce électronique a besoin d'une solution économique pour acheminer le trafic vers différentes applications en fonction des noms de domaine, et il souhaite éviter des frais supplémentaires pour un routage complexe.",
        "Question": "Quel service réseau AWS répondrait le mieux à cette exigence ?",
        "Options": {
            "1": "Utiliser AWS Global Accelerator pour le routage mondial",
            "2": "Déployer Amazon Route 53 pour le routage basé sur DNS",
            "3": "Utiliser un Application Load Balancer avec un routage basé sur le chemin",
            "4": "Configurer le VPC Peering pour un routage direct du trafic"
        },
        "Correct Answer": "Déployer Amazon Route 53 pour le routage basé sur DNS",
        "Explanation": "Amazon Route 53 est un service web de système de noms de domaine (DNS) évolutif et hautement disponible qui peut acheminer le trafic en fonction des noms de domaine. Il permet un routage basé sur DNS économique, idéal pour diriger les utilisateurs vers différentes applications en fonction du domaine qu'ils accèdent. Ce service peut gérer des politiques de routage telles que le routage simple, le routage pondéré, le routage basé sur la latence, et plus encore, sans encourir de frais supplémentaires pour des configurations de routage complexes. Il est spécifiquement conçu à cet effet, ce qui en fait le meilleur choix pour les besoins du site de commerce électronique.",
        "Other Options": [
            "AWS Global Accelerator est conçu pour améliorer la disponibilité et les performances des applications en dirigeant le trafic vers des points de terminaison optimaux en fonction de la santé, de la géographie et des politiques de routage. Cependant, il entraîne des coûts supplémentaires et est plus adapté aux applications mondiales qu'au routage simple basé sur le domaine.",
            "Un Application Load Balancer avec un routage basé sur le chemin est principalement utilisé pour distribuer le trafic d'application entrant entre plusieurs cibles, telles que des instances EC2, en fonction du chemin de la requête. Bien qu'il puisse acheminer le trafic efficacement, ce n'est pas la solution la plus économique pour le routage basé sur les noms de domaine, car cela implique une configuration supplémentaire et des coûts potentiels.",
            "Le VPC Peering permet un routage direct du trafic réseau entre deux VPC (Clouds Privés Virtuels) mais ne gère pas le routage basé sur les noms de domaine. Il est plus adapté à la communication réseau interne qu'à la direction du trafic externe en fonction des noms de domaine, ce qui en fait un choix inapproprié pour les exigences du site de commerce électronique."
        ]
    },
    {
        "Question Number": "38",
        "Situation": "Une startup est préoccupée par ses dépenses en base de données et souhaite surveiller les coûts au fil du temps. Elle veut mettre en place des alertes de coût pour rester dans le budget et analyser les tendances de dépenses pour identifier des économies potentielles.",
        "Question": "Quelle combinaison d'outils de gestion des coûts AWS devraient-ils utiliser ? (Choisissez deux.)",
        "Options": {
            "1": "AWS Trusted Advisor et AWS Cost Explorer",
            "2": "AWS Budgets et AWS Cost Explorer",
            "3": "AWS Cost and Usage Report et AWS Support",
            "4": "AWS Trusted Advisor et AWS Budgets",
            "5": "AWS Cost Anomaly Detection et AWS Budgets"
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "AWS Budgets et AWS Cost Explorer",
            "AWS Cost Anomaly Detection et AWS Budgets"
        ],
        "Explanation": "AWS Budgets permet aux utilisateurs de définir des budgets de coûts et d'utilisation personnalisés qui les alertent lorsque leurs coûts ou leur utilisation dépassent (ou sont prévus pour dépasser) le montant budgété. Cela aiderait la startup à surveiller les coûts et à rester dans le budget. AWS Cost Explorer permet aux utilisateurs de visualiser, comprendre et gérer leurs coûts et leur utilisation AWS au fil du temps. Cela aiderait la startup à analyser les tendances de dépenses et à identifier des économies potentielles. AWS Cost Anomaly Detection analyse automatiquement vos données de coûts et d'utilisation pour détecter des modèles de dépenses inhabituels, fournissant une couche supplémentaire de gestion des coûts.",
        "Other Options": [
            "AWS Trusted Advisor et AWS Cost Explorer : Bien qu'AWS Cost Explorer soit un outil correct, AWS Trusted Advisor fournit principalement des conseils en temps réel pour aider à provisionner des ressources selon les meilleures pratiques AWS, pas spécifiquement pour la gestion des coûts.",
            "AWS Cost and Usage Report et AWS Support : AWS Cost and Usage Report fournit des données complètes sur les coûts, mais il ne fournit pas la fonctionnalité d'alerte dont la startup a besoin. AWS Support est un service d'assistance technique et n'aide pas directement à la gestion des coûts.",
            "AWS Trusted Advisor et AWS Budgets : Bien qu'AWS Budgets soit un outil correct, AWS Trusted Advisor fournit principalement des conseils en temps réel pour aider à provisionner des ressources selon les meilleures pratiques AWS, pas spécifiquement pour la gestion des coûts."
        ]
    },
    {
        "Question Number": "39",
        "Situation": "Une application d'entreprise nécessite un accès à faible latence aux données stockées dans Amazon S3. Les données sont accessibles par des utilisateurs de divers emplacements géographiques à travers le monde. L'entreprise souhaite améliorer la vitesse d'accès aux données pour les utilisateurs en mettant en cache les données fréquemment consultées plus près d'eux.",
        "Question": "Quel service AWS le solutions architecte devrait-il utiliser pour répondre à cette exigence ?",
        "Options": {
            "1": "Amazon CloudFront",
            "2": "AWS Global Accelerator",
            "3": "Amazon Route 53",
            "4": "AWS Direct Connect"
        },
        "Correct Answer": "Amazon CloudFront",
        "Explanation": "Amazon CloudFront est un service de réseau de distribution de contenu (CDN) qui met en cache le contenu dans des emplacements périphériques à travers le monde. En utilisant CloudFront, les données fréquemment consultées stockées dans Amazon S3 peuvent être mises en cache plus près des utilisateurs, réduisant ainsi considérablement la latence et améliorant la vitesse d'accès. Lorsqu'un utilisateur demande des données, CloudFront les sert depuis l'emplacement périphérique le plus proche, ce qui améliore les performances pour les utilisateurs situés dans diverses régions géographiques.",
        "Other Options": [
            "AWS Global Accelerator améliore la disponibilité et les performances des applications en dirigeant le trafic vers des points de terminaison optimaux, mais il ne met pas en cache le contenu. Il est plus adapté à l'amélioration des performances des applications TCP et UDP qu'à la mise en cache de contenu statique provenant de S3.",
            "Amazon Route 53 est un service web de système de noms de domaine (DNS) évolutif qui fournit l'enregistrement de domaine, le routage DNS et la vérification de la santé. Bien qu'il aide à diriger les utilisateurs vers les ressources les plus proches, il ne met pas en cache les données ni n'améliore directement la vitesse d'accès aux données.",
            "AWS Direct Connect fournit une connexion réseau dédiée de vos locaux à AWS, ce qui peut améliorer la bande passante et réduire la latence pour le transfert de données. Cependant, il ne met pas en cache les données ni ne fournit un mécanisme de distribution de contenu, ce qui le rend inadapté à l'exigence de mise en cache des données fréquemment consultées."
        ]
    },
    {
        "Question Number": "40",
        "Situation": "Une entreprise conçoit une application web à plusieurs niveaux qui fonctionnera sur AWS. L'application se compose d'un niveau web frontal, d'un niveau de logique métier et d'un niveau de base de données. L'entreprise exige une haute disponibilité et une tolérance aux pannes pour l'application.",
        "Question": "Quelle architecture le solutions architect devrait-il recommander ?",
        "Options": {
            "1": "Déployer tous les niveaux dans une seule zone de disponibilité avec Auto Scaling et équilibrage de charge.",
            "2": "Déployer les niveaux web et de logique métier dans plusieurs zones de disponibilité et le niveau de base de données dans une seule zone de disponibilité avec Multi-AZ RDS.",
            "3": "Déployer le niveau web dans plusieurs zones de disponibilité, le niveau de logique métier dans une seule zone de disponibilité, et le niveau de base de données en utilisant Amazon DynamoDB.",
            "4": "Déployer tous les niveaux dans plusieurs régions AWS pour garantir une disponibilité mondiale."
        },
        "Correct Answer": "Déployer les niveaux web et de logique métier dans plusieurs zones de disponibilité et le niveau de base de données dans une seule zone de disponibilité avec Multi-AZ RDS.",
        "Explanation": "Cette option offre une haute disponibilité et une tolérance aux pannes en déployant les niveaux web et de logique métier dans plusieurs zones de disponibilité (AZ). Cela garantit que si une AZ tombe en panne, l'application peut toujours fonctionner en utilisant les ressources des autres AZ. De plus, l'utilisation de Multi-AZ pour le niveau de base de données avec Amazon RDS améliore la disponibilité et la durabilité en répliquant automatiquement la base de données sur une instance de secours dans une autre AZ, permettant ainsi un basculement en cas de panne. Cette architecture équilibre efficacement le besoin de haute disponibilité tout en gérant les coûts et la complexité.",
        "Other Options": [
            "Déployer tous les niveaux dans une seule zone de disponibilité avec Auto Scaling et équilibrage de charge ne fournit pas de haute disponibilité ni de tolérance aux pannes, car une défaillance dans cette AZ mettrait hors service l'ensemble de l'application.",
            "Déployer les niveaux web et de logique métier dans plusieurs zones de disponibilité et le niveau de base de données dans une seule zone de disponibilité avec Multi-AZ RDS est partiellement correct, mais cela n'exploite pas pleinement les avantages de la haute disponibilité pour le niveau de base de données puisqu'il est seulement dans une AZ. La base de données devrait également être dans plusieurs AZ pour une tolérance aux pannes complète.",
            "Déployer le niveau web dans plusieurs zones de disponibilité, le niveau de logique métier dans une seule zone de disponibilité, et le niveau de base de données en utilisant Amazon DynamoDB ne fournit pas de tolérance aux pannes pour le niveau de logique métier, qui est critique pour l'application. Bien que DynamoDB soit hautement disponible, l'architecture dans son ensemble manque de redondance dans le niveau de logique métier."
        ]
    },
    {
        "Question Number": "41",
        "Situation": "Une équipe de développement déploie de nouvelles versions de leur API et souhaite les tester en production avec un impact minimal sur les utilisateurs finaux. Ils décident d'utiliser des déploiements canari pour diriger un petit pourcentage du trafic de production vers la nouvelle version avant un déploiement complet.",
        "Question": "Quelle stratégie de déploiement soutiendrait le mieux cette approche de test et comment ?",
        "Options": {
            "1": "Point de terminaison optimisé pour le bord, car il dirige le trafic via CloudFront et offre une latence réduite pour un public mondial.",
            "2": "Point de terminaison régional, car il permet au trafic de rester dans la même région AWS pour des applications spécifiques à la région.",
            "3": "Point de terminaison privé, garantissant que l'API est accessible uniquement au sein d'un VPC pour des tests internes.",
            "4": "Déploiement de stage avec version canari, permettant un déploiement contrôlé de la nouvelle version de l'API tout en augmentant progressivement le trafic vers celle-ci."
        },
        "Correct Answer": "Déploiement de stage avec version canari, permettant un déploiement contrôlé de la nouvelle version de l'API tout en augmentant progressivement le trafic vers celle-ci.",
        "Explanation": "Un déploiement de stage avec version canari est spécifiquement conçu pour des scénarios où de nouvelles versions d'une application ou d'une API doivent être testées en production avec un risque minimal. Cette stratégie permet à l'équipe de développement de diriger un petit pourcentage de trafic vers la nouvelle version, de surveiller ses performances et d'augmenter progressivement le trafic si la nouvelle version fonctionne bien. Ce déploiement contrôlé minimise l'impact sur les utilisateurs finaux et permet un retour rapide en arrière en cas de problèmes.",
        "Other Options": [
            "Le point de terminaison optimisé pour le bord est principalement axé sur la réduction de la latence pour les utilisateurs mondiaux en dirigeant le trafic via CloudFront. Bien qu'il améliore les performances, il ne soutient pas intrinsèquement la stratégie de déploiement canari, qui nécessite un mécanisme pour contrôler la distribution du trafic entre les versions.",
            "Le point de terminaison régional est adapté aux applications qui doivent conserver le trafic au sein d'une région AWS spécifique. Cependant, il ne fournit pas la fonctionnalité nécessaire pour les déploiements canari, qui nécessitent la capacité de déplacer progressivement le trafic entre différentes versions d'une API.",
            "Le point de terminaison privé limite l'accès à l'API à l'intérieur d'un Cloud Privé Virtuel (VPC), ce qui le rend adapté aux tests internes. Cependant, il ne facilite pas la stratégie de déploiement canari, qui implique d'exposer la nouvelle version à un sous-ensemble d'utilisateurs externes pour recueillir des retours et surveiller les performances."
        ]
    },
    {
        "Question Number": "42",
        "Situation": "Une organisation de recherche en génomique effectue des analyses de séquences ADN à grande échelle sur AWS. Les charges de travail nécessitent une puissance de calcul élevée et doivent évoluer rapidement pour gérer des demandes de traitement intenses. L'équipe doit s'assurer que l'application peut évoluer dynamiquement pour répondre aux besoins de performance maximale tout en optimisant les coûts opérationnels pendant les périodes de faible demande.",
        "Question": "Quelle approche répondrait le mieux à ces exigences de haute performance et d'efficacité des coûts ?",
        "Options": {
            "1": "Provisionner des instances EC2 avec le maximum de vCPU et de mémoire pour les charges de travail de pointe et réduire manuellement.",
            "2": "Utiliser un groupe Auto Scaling avec des instances EC2 optimisées pour le calcul et configurer une politique de mise à l'échelle basée sur l'utilisation du CPU.",
            "3": "Mettre en place des fonctions Amazon Lambda pour gérer toutes les tâches de calcul de manière sans serveur.",
            "4": "Exécuter une seule instance EC2 avec une grande quantité de stockage et allouer manuellement des ressources selon les besoins."
        },
        "Correct Answer": "Utiliser un groupe Auto Scaling avec des instances EC2 optimisées pour le calcul et configurer une politique de mise à l'échelle basée sur l'utilisation du CPU.",
        "Explanation": "Utiliser un groupe Auto Scaling avec des instances EC2 optimisées pour le calcul permet à l'organisation d'ajuster automatiquement le nombre d'instances en fonction de la charge de travail. Cette approche garantit que pendant les besoins de performance maximale, des instances supplémentaires peuvent être provisionnées pour gérer les demandes de calcul accrues, tandis que pendant les périodes de faible demande, les instances peuvent être supprimées pour optimiser les coûts. La politique de mise à l'échelle basée sur l'utilisation du CPU est efficace car elle corrèle directement les actions de mise à l'échelle avec l'utilisation réelle des ressources, garantissant que l'application peut répondre dynamiquement aux changements de charge de travail de manière efficace.",
        "Other Options": [
            "Provisionner des instances EC2 avec le maximum de vCPU et de mémoire pour les charges de travail de pointe et réduire manuellement n'est pas efficace. Cette approche conduit à une sur-provisionnement pendant les périodes de faible demande, entraînant des coûts inutiles. La mise à l'échelle manuelle est également sujette à des erreurs humaines et peut ne pas répondre assez rapidement aux changements de charge de travail.",
            "Mettre en place des fonctions Amazon Lambda pour gérer toutes les tâches de calcul de manière sans serveur peut ne pas être adapté pour des analyses de séquences ADN à haute performance qui nécessitent une puissance de calcul et une mémoire significatives. Lambda a des limitations sur le temps d'exécution et l'allocation des ressources, ce qui peut ne pas répondre aux besoins des charges de travail génomiques intensives.",
            "Exécuter une seule instance EC2 avec une grande quantité de stockage et allouer manuellement des ressources selon les besoins n'est pas une solution évolutive. Cette approche ne permet pas une mise à l'échelle dynamique, ce qui est crucial pour gérer efficacement des charges de travail variables. De plus, s'appuyer sur une seule instance crée un point de défaillance unique et peut entraîner des goulets d'étranglement en matière de performance."
        ]
    },
    {
        "Question Number": "43",
        "Situation": "Une entreprise de services financiers génère et stocke chaque jour de grands volumes de données clients sur site. En raison de réglementations strictes et d'exigences de conformité, elle doit conserver ces données localement, mais souhaite transférer des données plus anciennes et rarement consultées vers AWS pour réduire les coûts de stockage. Elle a besoin d'une solution qui puisse étendre sans problème son infrastructure de stockage actuelle vers AWS, permettant l'accès aux données archivées sans perturber ses applications ou flux de travail existants.",
        "Question": "Quel service AWS répondrait le mieux aux exigences de l'entreprise ?",
        "Options": {
            "1": "Amazon S3 avec des politiques de cycle de vie",
            "2": "AWS Direct Connect",
            "3": "AWS Storage Gateway",
            "4": "Amazon EBS Snapshot Export"
        },
        "Correct Answer": "AWS Storage Gateway",
        "Explanation": "AWS Storage Gateway est conçu pour s'intégrer sans problème aux environnements sur site avec le stockage cloud. Il fournit une solution de stockage cloud hybride qui permet aux entreprises de conserver leurs données localement tout en étendant leurs capacités de stockage vers AWS. Dans ce scénario, l'entreprise de services financiers peut utiliser le Storage Gateway pour transférer des données plus anciennes et rarement consultées vers AWS, garantissant ainsi la conformité avec les exigences réglementaires tout en réduisant les coûts de stockage. Le service permet un accès facile aux données archivées sans perturber les applications ou flux de travail existants, ce qui en fait la meilleure option pour les besoins de l'entreprise.",
        "Other Options": [
            "Amazon S3 avec des politiques de cycle de vie est un service de stockage qui permet aux utilisateurs de gérer le cycle de vie de leurs données, mais il ne fournit pas l'intégration fluide avec l'infrastructure sur site que l'entreprise nécessite. Il nécessiterait des étapes supplémentaires pour déplacer les données de l'environnement sur site vers S3, ce qui pourrait perturber les flux de travail existants.",
            "AWS Direct Connect est un service qui fournit une connexion réseau dédiée entre l'environnement sur site et AWS. Bien qu'il puisse améliorer la bande passante et réduire la latence pour le transfert de données, il ne répond pas directement au besoin d'une solution de stockage hybride qui permet un accès fluide aux données archivées.",
            "Amazon EBS Snapshot Export permet aux utilisateurs d'exporter des instantanés EBS vers S3, mais il est principalement axé sur la sauvegarde et la récupération des volumes EBS plutôt que sur la fourniture d'une solution de stockage hybride. Il ne facilite pas l'accès continu aux données archivées de la manière dont AWS Storage Gateway le fait."
        ]
    },
    {
        "Question Number": "44",
        "Situation": "Une entreprise a à la fois une connexion VPN et un lien AWS Direct Connect établis entre son environnement sur site et son VPC AWS. Pour une transmission de données hautement sécurisée, elle souhaite s'assurer que tout le trafic reste crypté pendant qu'il traverse le réseau.",
        "Question": "Quelle approche garantirait le mieux une communication cryptée pour toutes les données échangées entre son centre de données et AWS ?",
        "Options": {
            "1": "Compter uniquement sur AWS Direct Connect car il fournit un lien privé et dédié, éliminant le besoin de cryptage supplémentaire.",
            "2": "Configurer un VPN sur AWS Direct Connect pour crypter les données sur une connexion privée, garantissant un cryptage de bout en bout.",
            "3": "Utiliser une passerelle Internet (IGW) avec HTTPS pour sécuriser les données pendant leur transit sur Internet.",
            "4": "Activer AWS Shield sur Direct Connect pour crypter le trafic et prévenir les accès non autorisés."
        },
        "Correct Answer": "Configurer un VPN sur AWS Direct Connect pour crypter les données sur une connexion privée, garantissant un cryptage de bout en bout.",
        "Explanation": "Bien qu'AWS Direct Connect fournisse un lien privé et dédié entre l'environnement sur site et AWS, il ne crypte pas intrinsèquement les données transmises. Pour garantir que toutes les données échangées restent cryptées, la configuration d'un VPN sur le lien Direct Connect est la meilleure approche. Cette configuration permet une communication sécurisée et cryptée tout en profitant de la bande passante dédiée et de la latence réduite de Direct Connect. Le VPN ajoute une couche de sécurité supplémentaire en cryptant les paquets de données, garantissant que même si le lien privé était compromis, les données resteraient sécurisées.",
        "Other Options": [
            "Compter uniquement sur AWS Direct Connect n'est pas suffisant pour garantir le cryptage. Bien qu'il fournisse une connexion privée, il ne crypte pas les données en transit, les rendant vulnérables à l'interception.",
            "Cette option est en fait la bonne réponse. Configurer un VPN sur AWS Direct Connect est la meilleure approche pour garantir une communication cryptée.",
            "Utiliser une passerelle Internet (IGW) avec HTTPS n'est pas applicable dans ce scénario puisque la question spécifie un désir de connexion privée entre le centre de données et AWS. Une IGW est utilisée pour l'accès public à Internet, et bien que HTTPS fournisse un cryptage, cela ne répond pas à l'exigence d'une connexion privée et sécurisée."
        ]
    },
    {
        "Question Number": "45",
        "Situation": "Une entreprise souhaite améliorer la sécurité de son environnement AWS en détectant des activités inhabituelles et non autorisées à travers plusieurs comptes. Elle envisage Amazon GuardDuty pour surveiller et identifier les menaces potentielles en utilisant l'IA/ML et l'intelligence des menaces.",
        "Question": "Comment Amazon GuardDuty aide-t-il à détecter les menaces de sécurité, et comment les résultats sont-ils gérés ?",
        "Options": {
            "1": "GuardDuty analyse les journaux DNS, VPC flow et CloudTrail, envoyant les résultats directement à l'utilisateur root pour un examen manuel.",
            "2": "GuardDuty utilise l'IA/ML sur les journaux DNS, VPC flow et CloudTrail, créant des résultats qui peuvent déclencher des réponses automatisées via CloudWatch Events, telles que des notifications SNS ou des invocations Lambda pour des actions de remédiation.",
            "3": "GuardDuty surveille uniquement le trafic d'un seul compte, nécessitant que les utilisateurs examinent manuellement les journaux pour détecter les menaces inter-comptes.",
            "4": "GuardDuty utilise des règles statiques pour détecter l'activité et ne notifie que pour les anomalies réseau dans les journaux VPC flow."
        },
        "Correct Answer": "GuardDuty utilise l'IA/ML sur les journaux DNS, VPC flow et CloudTrail, créant des résultats qui peuvent déclencher des réponses automatisées via CloudWatch Events, telles que des notifications SNS ou des invocations Lambda pour des actions de remédiation.",
        "Explanation": "Amazon GuardDuty exploite l'intelligence artificielle (IA) et l'apprentissage automatique (ML) pour analyser diverses sources de données, y compris les journaux DNS, les journaux VPC flow et les journaux CloudTrail. Cette analyse aide à identifier des modèles inhabituels et des menaces potentielles à la sécurité. Lorsque GuardDuty détecte une menace, il génère des résultats qui peuvent être intégrés avec des services AWS comme CloudWatch Events. Cette intégration permet des réponses automatisées, telles que l'envoi de notifications via Amazon SNS ou l'invocation de fonctions AWS Lambda pour des actions de remédiation, renforçant ainsi la posture de sécurité de l'environnement AWS.",
        "Other Options": [
            "Bien que GuardDuty analyse les journaux DNS, VPC flow et CloudTrail, il n'envoie pas les résultats directement à l'utilisateur root pour un examen manuel. Au lieu de cela, les résultats sont générés automatiquement et peuvent être intégrés avec d'autres services AWS pour des réponses automatisées.",
            "GuardDuty peut surveiller plusieurs comptes via AWS Organizations, permettant une détection centralisée des menaces à travers toute une organisation plutôt que juste un compte. Il ne nécessite pas que les utilisateurs examinent manuellement les journaux pour détecter les menaces inter-comptes.",
            "GuardDuty ne repose pas uniquement sur des règles statiques ; il utilise l'IA et le ML pour détecter un large éventail d'activités, pas seulement des anomalies réseau dans les journaux VPC flow. Il analyse divers types de journaux pour identifier de manière exhaustive les menaces potentielles."
        ]
    },
    {
        "Question Number": "46",
        "Situation": "Une entreprise de services financiers est en train de passer d'une architecture monolithique à des microservices pour mieux gérer les transactions des clients. L'entreprise souhaite mettre en œuvre des microservices sans état pour garantir une haute disponibilité, une scalabilité et une tolérance aux pannes.",
        "Question": "Quelle approche de conception l'entreprise devrait-elle adopter pour garantir des microservices résilients et faiblement couplés ?",
        "Options": {
            "1": "Concevoir chaque microservice pour qu'il soit sans état, ce qui signifie qu'il ne conserve aucune information de session entre les requêtes, et stocker l'état dans un cache distribué tel qu'Amazon ElastiCache pour des performances et une durabilité optimales.",
            "2": "Concevoir chaque microservice pour maintenir l'état de session au sein du service lui-même, afin que l'état puisse être facilement accessible par d'autres services sans systèmes externes.",
            "3": "Mettre en œuvre une base de données monolithique qui stocke toutes les données de session pour les microservices, afin que le système puisse y accéder de manière centralisée pour maintenir l'état entre les services.",
            "4": "Utiliser Amazon RDS avec un déploiement multi-AZ pour gérer l'état de session de chaque microservice, garantissant la cohérence et la disponibilité des données."
        },
        "Correct Answer": "Concevoir chaque microservice pour qu'il soit sans état, ce qui signifie qu'il ne conserve aucune information de session entre les requêtes, et stocker l'état dans un cache distribué tel qu'Amazon ElastiCache pour des performances et une durabilité optimales.",
        "Explanation": "Concevoir chaque microservice pour qu'il soit sans état est crucial pour atteindre une haute disponibilité, une scalabilité et une tolérance aux pannes. Les microservices sans état ne conservent pas d'informations de session, ce qui leur permet d'être facilement répliqués et scalés horizontalement. En stockant l'état dans un cache distribué comme Amazon ElastiCache, l'entreprise peut s'assurer que les données sont accessibles et durables sans coupler les services à un système de gestion d'état spécifique. Cette approche favorise un faible couplage entre les services, car ils peuvent fonctionner indépendamment sans dépendre d'un état partagé.",
        "Other Options": [
            "Concevoir chaque microservice pour maintenir l'état de session au sein du service lui-même contredit le principe de l'absence d'état. Cette approche peut entraîner un couplage étroit entre les services, rendant difficile leur scalabilité et leur gestion de manière indépendante, et peut également créer des défis en matière de tolérance aux pannes et de récupération.",
            "Mettre en œuvre une base de données monolithique pour stocker toutes les données de session centralise la gestion de l'état, ce qui va à l'encontre de l'objectif de décentralisation de l'architecture des microservices. Cela peut créer un point de défaillance unique et limiter la scalabilité et la résilience du système, car tous les services dépendraient de la disponibilité de la base de données monolithique.",
            "Utiliser Amazon RDS avec un déploiement multi-AZ pour la gestion de l'état de session introduit une dépendance à une base de données relationnelle, ce qui peut entraîner des goulets d'étranglement et une réduction des performances. Bien que cela garantisse la cohérence et la disponibilité des données, cela ne s'aligne pas avec le principe de conception sans état que les microservices devraient suivre, augmentant ainsi le couplage entre les services."
        ]
    },
    {
        "Question Number": "47",
        "Situation": "Une organisation exige que les clés de chiffrement utilisées pour protéger les données sensibles soient automatiquement renouvelées chaque année.",
        "Question": "Quelle fonctionnalité AWS l'organisation peut-elle utiliser pour répondre à cette exigence ?",
        "Options": {
            "1": "Configurer une politique de cycle de vie dans Amazon S3",
            "2": "Activer le renouvellement automatique des clés dans AWS KMS",
            "3": "Utiliser Amazon GuardDuty pour surveiller l'utilisation des clés",
            "4": "Activer le chiffrement en transit à l'aide d'AWS Certificate Manager (ACM)"
        },
        "Correct Answer": "Activer le renouvellement automatique des clés dans AWS KMS",
        "Explanation": "AWS Key Management Service (KMS) offre la possibilité de renouveler automatiquement les clés de chiffrement. En activant le renouvellement automatique des clés, l'organisation peut s'assurer que les clés utilisées pour chiffrer les données sensibles sont renouvelées chaque année sans intervention manuelle. Cette fonctionnalité aide à maintenir les meilleures pratiques de sécurité en changeant régulièrement les clés de chiffrement, réduisant ainsi le risque de compromission des clés.",
        "Other Options": [
            "Configurer une politique de cycle de vie dans Amazon S3 est lié à la gestion du cycle de vie de stockage des objets dans S3, comme la transition des objets vers différentes classes de stockage ou leur suppression après une certaine période. Cela ne concerne pas le renouvellement automatique des clés de chiffrement.",
            "Utiliser Amazon GuardDuty pour surveiller l'utilisation des clés est axé sur la détection des menaces et la surveillance des activités malveillantes dans les comptes AWS. Bien que cela puisse aider à identifier les accès non autorisés ou les anomalies dans l'utilisation des clés, cela ne fournit pas de mécanisme pour le renouvellement des clés.",
            "Activer le chiffrement en transit à l'aide d'AWS Certificate Manager (ACM) concerne la sécurisation des données lors de leur transmission sur le réseau. C'est important pour protéger les données en transit, mais cela ne répond pas à l'exigence de renouvellement automatique des clés de chiffrement."
        ]
    },
    {
        "Question Number": "48",
        "Situation": "Une entreprise conçoit un VPC personnalisé dans la région us-east-1 avec une architecture à trois niveaux, comprenant un niveau web, un niveau application et un niveau base de données. Ils nécessitent que chaque niveau soit isolé à travers trois zones de disponibilité (AZ) et ont besoin d'un accès contrôlé pour les ressources publiques et privées. L'entreprise souhaite également activer le support DNS pour la résolution des noms d'hôtes internes au sein du VPC.",
        "Question": "Quelle configuration l'entreprise devrait-elle mettre en œuvre pour répondre à ces exigences tout en garantissant un accès public contrôlé et une fonctionnalité DNS interne ?",
        "Options": {
            "1": "Attribuer un bloc CIDR /16 au VPC, utiliser des sous-réseaux privés pour chaque niveau dans chaque AZ, configurer une passerelle NAT dans chaque AZ pour l'accès Internet sortant depuis les sous-réseaux privés, et activer enableDnsHostnames et enableDnsSupport pour la fonctionnalité DNS.",
            "2": "Utiliser un bloc CIDR /24 pour le VPC, créer un sous-réseau public dans chaque AZ pour le niveau web, déployer une passerelle Internet pour un accès public direct, et désactiver enableDnsSupport pour empêcher la résolution des noms d'hôtes internes.",
            "3": "Allouer un bloc CIDR /28 au VPC, configurer des sous-réseaux publics uniquement pour tous les niveaux, utiliser un Bastion Host pour l'accès Internet, et désactiver enableDnsHostnames pour restreindre la fonctionnalité DNS aux adresses IP privées uniquement.",
            "4": "Configurer le VPC avec un bloc CIDR /20, mettre en place des sous-réseaux privés dans chaque AZ pour le niveau web, utiliser des instances NAT pour le trafic sortant, et désactiver enableDnsHostnames pour une sécurité accrue."
        },
        "Correct Answer": "Attribuer un bloc CIDR /16 au VPC, utiliser des sous-réseaux privés pour chaque niveau dans chaque AZ, configurer une passerelle NAT dans chaque AZ pour l'accès Internet sortant depuis les sous-réseaux privés, et activer enableDnsHostnames et enableDnsSupport pour la fonctionnalité DNS.",
        "Explanation": "Cette option répond à toutes les exigences énoncées dans le scénario. En attribuant un bloc CIDR /16, l'entreprise s'assure d'un espace d'adresses IP suffisant pour son architecture à trois niveaux. L'utilisation de sous-réseaux privés pour chaque niveau dans chaque AZ fournit l'isolement et la sécurité nécessaires. La passerelle NAT permet aux instances dans les sous-réseaux privés d'accéder à Internet pour des mises à jour ou des services externes tout en les rendant inaccessibles depuis Internet public. L'activation de enableDnsHostnames et enableDnsSupport garantit que les ressources internes peuvent résoudre des noms d'hôtes, facilitant ainsi la communication au sein du VPC.",
        "Other Options": [
            "Utiliser un bloc CIDR /24 pour le VPC est insuffisant pour une architecture à trois niveaux qui s'étend sur plusieurs AZ, car cela limite le nombre d'adresses IP disponibles. Créer des sous-réseaux publics pour le niveau web l'exposerait directement à Internet, ce qui ne correspond pas à l'exigence d'accès contrôlé. Désactiver enableDnsSupport empêcherait la résolution des noms d'hôtes internes, ce qui est une exigence critique.",
            "Allouer un bloc CIDR /28 est beaucoup trop petit pour un VPC qui doit prendre en charge plusieurs niveaux à travers trois AZ, ce qui entraînerait une épuisement des adresses IP. Configurer des sous-réseaux publics pour tous les niveaux contredit l'exigence d'isolement et d'accès contrôlé. De plus, désactiver enableDnsHostnames restreindrait la fonctionnalité DNS, empêchant la résolution des noms d'hôtes internes.",
            "Configurer le VPC avec un bloc CIDR /20 fournit plus d'adresses IP qu'un /28 mais n'est toujours pas optimal pour une architecture à trois niveaux. Configurer des sous-réseaux privés uniquement pour le niveau web ne fournit pas l'isolement nécessaire pour les niveaux application et base de données. Utiliser des instances NAT au lieu de passerelles NAT peut entraîner des problèmes de performance et une surcharge de gestion. Désactiver enableDnsHostnames restreindrait encore la fonctionnalité DNS, ce qui n'est pas acceptable compte tenu des exigences."
        ]
    },
    {
        "Question Number": "49",
        "Situation": "Une startup surveille de près ses dépenses mensuelles sur AWS pour éviter les dépassements de budget et mettre en place des alertes si les dépenses dépassent ses limites prévisionnelles. De plus, la startup souhaite analyser les tendances des modèles de dépenses au fil du temps pour identifier des opportunités d'économies potentielles et optimiser son utilisation d'AWS.",
        "Question": "Quelle combinaison d'outils de gestion des coûts AWS répondrait le mieux à ces exigences ?",
        "Options": {
            "1": "Utiliser AWS Budgets pour mettre en place des alertes de dépenses et AWS Cost Explorer pour analyser les modèles de dépenses et les tendances au fil du temps.",
            "2": "Mettre en œuvre AWS Trusted Advisor pour identifier des recommandations d'économies et utiliser le rapport AWS Cost and Usage pour un suivi détaillé des coûts.",
            "3": "Activer le rapport AWS Cost and Usage pour un suivi complet et s'abonner à AWS Support pour des informations supplémentaires sur la gestion des coûts.",
            "4": "Utiliser AWS Cost Explorer pour visualiser les tendances des coûts et AWS Trusted Advisor pour recevoir des recommandations régulières sur l'optimisation des coûts."
        },
        "Correct Answer": "Utiliser AWS Budgets pour mettre en place des alertes de dépenses et AWS Cost Explorer pour analyser les modèles de dépenses et les tendances au fil du temps.",
        "Explanation": "Cette option répond directement aux exigences de la startup en lui permettant de mettre en place des alertes pour les limites de dépenses à l'aide d'AWS Budgets, ce qui aide à prévenir les dépassements de budget. De plus, AWS Cost Explorer fournit des outils puissants pour analyser les modèles de dépenses et les tendances au fil du temps, permettant à la startup d'identifier des opportunités d'économies et d'optimiser efficacement son utilisation d'AWS.",
        "Other Options": [
            "La mise en œuvre d'AWS Trusted Advisor pour des recommandations d'économies est utile, mais elle ne fournit pas la capacité de mettre en place des alertes de dépenses. Le rapport AWS Cost and Usage est détaillé mais se concentre davantage sur les données brutes que sur l'analyse des tendances, rendant cette combinaison moins efficace pour les besoins de la startup.",
            "Activer le rapport AWS Cost and Usage est bénéfique pour un suivi complet des coûts, mais s'abonner à AWS Support ne fournit pas directement d'informations sur la gestion des coûts. Cette option manque de la fonctionnalité d'alerte proactive qu'offre AWS Budgets, qui est cruciale pour surveiller les dépenses.",
            "Utiliser AWS Cost Explorer pour visualiser les tendances est un bon choix, mais se fier uniquement à AWS Trusted Advisor pour des recommandations ne fournit pas le mécanisme d'alerte nécessaire pour la gestion du budget. Cette combinaison ne répond pas entièrement à l'exigence de la startup de surveiller et d'alerter sur les limites de dépenses."
        ]
    },
    {
        "Question Number": "50",
        "Situation": "Une entreprise met en place un VPC sécurisé dans AWS et doit activer l'accès Internet sortant pour les instances dans un sous-réseau privé. Elle envisage d'utiliser soit une instance NAT, soit une passerelle NAT.",
        "Question": "Laquelle des options suivantes décrit correctement les principales différences entre les instances NAT et les passerelles NAT, en particulier en ce qui concerne les configurations de sécurité et la maintenance ?",
        "Options": {
            "1": "Les instances NAT prennent en charge l'utilisation de groupes de sécurité et sont hautement disponibles, tandis que les passerelles NAT ne prennent pas en charge les groupes de sécurité et s'appuient sur des ACL réseau pour le filtrage du trafic.",
            "2": "Les passerelles NAT offrent une disponibilité, une bande passante plus élevées et nécessitent moins de maintenance que les instances NAT, mais elles ne prennent en charge que les ACL réseau pour le filtrage du trafic, pas les groupes de sécurité.",
            "3": "Les instances NAT offrent une mise à l'échelle automatique et une haute disponibilité au sein d'une zone de disponibilité, ce qui les rend idéales pour les charges de travail de production.",
            "4": "Les passerelles NAT permettent une utilisation polyvalente, comme agir en tant qu'hôte bastion, ce qui n'est pas possible avec les instances NAT en raison des restrictions de gestion d'AWS."
        },
        "Correct Answer": "Les passerelles NAT offrent une disponibilité, une bande passante plus élevées et nécessitent moins de maintenance que les instances NAT, mais elles ne prennent en charge que les ACL réseau pour le filtrage du trafic, pas les groupes de sécurité.",
        "Explanation": "Les passerelles NAT sont conçues pour fournir une solution gérée et hautement disponible pour activer l'accès Internet sortant pour les instances dans un sous-réseau privé. Elles s'adaptent automatiquement aux besoins en bande passante du trafic, ce qui les rend adaptées aux charges de travail de production. De plus, les passerelles NAT nécessitent peu de maintenance car elles sont gérées par AWS, contrairement aux instances NAT, qui nécessitent une configuration, une mise à l'échelle et une maintenance manuelles. Bien que les passerelles NAT ne prennent pas en charge les groupes de sécurité, elles peuvent être contrôlées à l'aide d'ACL réseau, ce qui constitue une différence clé par rapport aux instances NAT qui prennent en charge les groupes de sécurité.",
        "Other Options": [
            "Les instances NAT prennent en charge l'utilisation de groupes de sécurité et sont hautement disponibles, tandis que les passerelles NAT ne prennent pas en charge les groupes de sécurité et s'appuient sur des ACL réseau pour le filtrage du trafic. Cette affirmation est incorrecte car, bien que les instances NAT prennent en charge les groupes de sécurité, les passerelles NAT ne prennent pas du tout en charge les groupes de sécurité, s'appuyant uniquement sur des ACL réseau pour le filtrage du trafic. De plus, les passerelles NAT sont conçues pour une haute disponibilité.",
            "Les instances NAT offrent une mise à l'échelle automatique et une haute disponibilité au sein d'une zone de disponibilité, ce qui les rend idéales pour les charges de travail de production. Cette affirmation est incorrecte car les instances NAT ne fournissent pas de mise à l'échelle automatique ; elles nécessitent une intervention manuelle pour se mettre à l'échelle et ne sont pas intrinsèquement hautement disponibles à moins d'être configurées avec plusieurs instances à travers des zones de disponibilité.",
            "Les passerelles NAT permettent une utilisation polyvalente, comme agir en tant qu'hôte bastion, ce qui n'est pas possible avec les instances NAT en raison des restrictions de gestion d'AWS. Cette affirmation est incorrecte car les passerelles NAT ne peuvent pas agir en tant qu'hôtes bastions ; elles sont spécifiquement conçues pour la fonctionnalité NAT. Les hôtes bastions sont généralement des instances EC2 configurées pour permettre un accès sécurisé aux instances dans des sous-réseaux privés."
        ]
    },
    {
        "Question Number": "51",
        "Situation": "Une entreprise de vente au détail, ShopSmart, stocke des données clients, y compris des informations personnelles identifiables (PII), dans des buckets Amazon S3. Pour se conformer aux réglementations sur la confidentialité des données, elle a besoin d'une solution capable d'identifier et de classer automatiquement les informations sensibles. De plus, elle souhaite avoir la possibilité de créer des règles personnalisées pour détecter des modèles de données uniques spécifiques à son activité. ShopSmart envisage Amazon Macie pour répondre à ces besoins.",
        "Question": "Comment Amazon Macie aide-t-il à garantir la sécurité et la confidentialité des données pour les informations sensibles dans les buckets S3, et quelles options sont disponibles pour créer des identifiants de données ?",
        "Options": {
            "1": "Amazon Macie ne fournit que des identifiants de données prédéfinis, limitant son utilisation à des types de données spécifiques, tels que les informations financières et les dossiers de santé, sans options de personnalisation pour d'autres modèles de données sensibles.",
            "2": "Amazon Macie utilise l'apprentissage automatique et des identifiants de données gérés pour la découverte et la classification automatisées des données sensibles, y compris les PII et les informations financières. Il permet également la création d'identifiants de données personnalisés à l'aide d'expressions régulières et de proximité de mots-clés, permettant une identification des données plus granulaire en fonction des besoins organisationnels uniques.",
            "3": "Amazon Macie se concentre principalement sur la surveillance du trafic réseau pour des modèles inhabituels, fournissant des alertes sur le mouvement des données mais ne permettant pas d'identifier directement les informations sensibles stockées dans les buckets S3.",
            "4": "Amazon Macie s'appuie uniquement sur AWS Security Hub pour la découverte et la classification des données, nécessitant que les utilisateurs mettent en place des règles EventBridge personnalisées pour détecter et classer les données en fonction de critères prédéfinis."
        },
        "Correct Answer": "Amazon Macie utilise l'apprentissage automatique et des identifiants de données gérés pour la découverte et la classification automatisées des données sensibles, y compris les PII et les informations financières. Il permet également la création d'identifiants de données personnalisés à l'aide d'expressions régulières et de proximité de mots-clés, permettant une identification des données plus granulaire en fonction des besoins organisationnels uniques.",
        "Explanation": "Amazon Macie est conçu pour aider les organisations à découvrir, classer et protéger automatiquement les données sensibles stockées dans Amazon S3. Il utilise des algorithmes d'apprentissage automatique pour identifier et classer les informations sensibles, y compris les informations personnellement identifiables (PII) et les données financières. De plus, Macie offre la flexibilité de créer des identifiants de données personnalisés, qui peuvent être adaptés pour répondre à des exigences commerciales spécifiques. Cela se fait par l'utilisation d'expressions régulières et de proximité de mots-clés, permettant aux organisations de définir des modèles uniques pertinents pour leurs opérations, renforçant ainsi leurs efforts de sécurité des données et de conformité.",
        "Other Options": [
            "Amazon Macie ne limite pas sa fonctionnalité uniquement aux identifiants de données prédéfinis. Il offre à la fois des identifiants de données gérés et la possibilité de créer des identifiants personnalisés, permettant une détection plus large des données sensibles.",
            "Amazon Macie ne se concentre pas principalement sur la surveillance du trafic réseau. Au contraire, sa fonction principale est d'identifier et de classer les données sensibles dans les buckets S3, ce qui en fait un outil clé pour la confidentialité et la sécurité des données.",
            "Amazon Macie fonctionne de manière indépendante en termes de découverte et de classification des données. Bien qu'il puisse s'intégrer à AWS Security Hub pour une gestion de la sécurité plus large, il ne s'appuie pas uniquement sur cela pour ses fonctionnalités principales."
        ]
    },
    {
        "Question Number": "52",
        "Situation": "Une entreprise de jeux en ligne a des utilisateurs dans le monde entier et souhaite minimiser la latence en déployant son application plus près des utilisateurs finaux. De plus, elle souhaite optimiser les coûts en évitant les frais de transfert de données inter-régions lorsque des utilisateurs de différentes régions accèdent à l'application.",
        "Question": "Quelle approche les aiderait le mieux à répondre à ces exigences ?",
        "Options": {
            "1": "Déployer toutes les ressources dans une seule région AWS et utiliser CloudFront pour le caching",
            "2": "Déployer des ressources dans plusieurs zones de disponibilité dans une région AWS",
            "3": "Déployer l'application dans plusieurs régions AWS en fonction des emplacements des utilisateurs",
            "4": "Utiliser une seule zone de disponibilité et s'appuyer sur le routage DNS global"
        },
        "Correct Answer": "Déployer l'application dans plusieurs régions AWS en fonction des emplacements des utilisateurs",
        "Explanation": "Déployer l'application dans plusieurs régions AWS permet à l'entreprise de jeux de placer ses ressources plus près des utilisateurs finaux, réduisant ainsi considérablement la latence. En ayant des instances dans diverses régions, les utilisateurs peuvent se connecter au serveur le plus proche, ce qui minimise le temps nécessaire pour que les données voyagent. De plus, cette approche aide à éviter les frais de transfert de données inter-régions, car les utilisateurs accédant à l'application depuis leur région locale ne supporteront pas les coûts associés au transfert de données entre les régions.",
        "Other Options": [
            "Déployer toutes les ressources dans une seule région AWS et utiliser CloudFront pour le caching peut aider à réduire la latence dans une certaine mesure, mais cela ne résout pas le problème des frais de transfert de données inter-régions lorsque des utilisateurs de différentes régions accèdent à l'application. CloudFront peut mettre en cache le contenu mais ne peut pas atténuer complètement la latence pour tous les utilisateurs dans le monde.",
            "Déployer des ressources dans plusieurs zones de disponibilité dans une région AWS améliore la disponibilité et la tolérance aux pannes, mais ne réduit pas significativement la latence pour les utilisateurs situés loin de cette région. Cela n'aide pas non plus avec les coûts de transfert de données inter-régions, car tous les utilisateurs accéderaient toujours à la même région.",
            "Utiliser une seule zone de disponibilité et s'appuyer sur le routage DNS global ne minimiserait pas efficacement la latence pour les utilisateurs situés loin de cette zone. Bien que le routage DNS puisse diriger les utilisateurs vers le point de terminaison le plus proche, cela ne résout pas le problème de la haute latence pour les utilisateurs géographiquement éloignés de la seule zone de disponibilité, ni n'aborde les frais de transfert de données inter-régions."
        ]
    },
    {
        "Question Number": "53",
        "Situation": "Une entreprise d'analyse de données exécute des travaux de traitement à grande échelle pour ses clients, mais la demande varie considérablement tout au long de la semaine. L'entreprise souhaite une solution de calcul rentable qui lui permettra de gérer ces charges de travail tout en minimisant les coûts pendant les périodes de faible demande.",
        "Question": "Quelle approche optimiserait le mieux les coûts pour cette charge de travail ? (Choisissez deux.)",
        "Options": {
            "1": "Utiliser des instances EC2 à la demande et démarrer manuellement les instances au besoin",
            "2": "Utiliser des instances réservées pour un nombre fixe d'instances EC2",
            "3": "Déployer un groupe Auto Scaling avec des instances Spot pour les travaux de traitement",
            "4": "Utiliser AWS Lambda pour exécuter tous les travaux de traitement à la demande",
            "5": "Mettre en œuvre des plans d'économies EC2 pour réduire les coûts des charges de travail prévisibles"
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Déployer un groupe Auto Scaling avec des instances Spot pour les travaux de traitement",
            "Utiliser AWS Lambda pour exécuter tous les travaux de traitement à la demande"
        ],
        "Explanation": "Déployer un groupe Auto Scaling avec des instances Spot pour les travaux de traitement est une solution rentable pour des charges de travail variables. Les instances Spot sont disponibles avec jusqu'à 90 % de réduction par rapport aux prix à la demande et sont idéales pour les applications avec des horaires de début et de fin flexibles, ou qui peuvent supporter des interruptions. Auto Scaling garantit que l'entreprise dispose de la bonne capacité pour gérer la charge à tout moment, optimisant ainsi les coûts. Utiliser AWS Lambda pour exécuter tous les travaux de traitement à la demande est également une bonne option car cela permet à l'entreprise d'exécuter du code sans provisionner ou gérer des serveurs, et vous ne payez que pour le temps de calcul que vous consommez, ce qui peut être très rentable pour des charges de travail sporadiques.",
        "Other Options": [
            "Utiliser des instances EC2 à la demande et démarrer manuellement les instances au besoin n'est pas la solution la plus rentable pour des charges de travail variables. Bien que cela offre de la flexibilité, cela ne tire pas parti des économies de coûts des instances Spot ou d'AWS Lambda.",
            "Utiliser des instances réservées pour un nombre fixe d'instances EC2 n'est pas idéal pour des charges de travail variables car cela ne fournit pas la flexibilité de monter ou descendre en fonction de la demande. Les instances réservées offrent une réduction significative par rapport aux prix à la demande, mais elles nécessitent un engagement d'un ou trois ans, ce qui peut ne pas convenir aux charges de travail variables.",
            "Mettre en œuvre des plans d'économies EC2 pour réduire les coûts des charges de travail prévisibles n'est pas la meilleure option pour ce scénario. Les plans d'économies offrent une réduction sur votre utilisation de calcul AWS, mais ils nécessitent un engagement à un montant d'utilisation constant (mesuré en $/heure) pendant 1 ou 3 ans, ce qui peut ne pas convenir aux charges de travail variables."
        ]
    },
    {
        "Question Number": "54",
        "Situation": "Une application de reporting utilisée par une équipe d'analyse doit gérer un volume élevé de requêtes de lecture pour générer des insights rapidement et efficacement. Bien que la base de données ait une source unique pour les opérations d'écriture, elle doit supporter un trafic de lecture élevé avec une faible latence, même lorsque l'instance principale traite une charge de travail importante. L'équipe souhaite une configuration qui puisse équilibrer la charge de lecture et fournir un accès ininterrompu à la base de données pour les requêtes d'analyse.",
        "Question": "Quelle stratégie de réplication de base de données atteindrait le mieux cet objectif ?",
        "Options": {
            "1": "Activer le déploiement Multi-AZ pour la base de données principale, permettant un basculement automatique vers une instance de secours pour une disponibilité améliorée",
            "2": "Utiliser des réplicas de lecture pour décharger le trafic de lecture de la base de données principale, distribuant la charge de travail et réduisant la latence des requêtes de lecture",
            "3": "Déployer une configuration active-active multi-régions pour soutenir une haute disponibilité et équilibrer le trafic de lecture et d'écriture à travers différentes régions",
            "4": "Configurer la base de données pour une réplication synchrone uniquement afin d'assurer la cohérence des données pendant les périodes de fort trafic de lecture"
        },
        "Correct Answer": "Utiliser des réplicas de lecture pour décharger le trafic de lecture de la base de données principale, distribuant la charge de travail et réduisant la latence des requêtes de lecture",
        "Explanation": "Utiliser des réplicas de lecture est la stratégie la plus efficace pour gérer de grands volumes de requêtes de lecture dans ce scénario. Les réplicas de lecture permettent à la base de données principale de se concentrer sur les opérations d'écriture tout en distribuant les requêtes de lecture sur plusieurs réplicas. Cette configuration non seulement équilibre la charge de lecture mais réduit également la latence, car les requêtes de lecture peuvent être traitées par des réplicas optimisés pour les opérations de lecture. De plus, si l'instance principale est sous une forte charge, les réplicas de lecture peuvent toujours fournir un accès ininterrompu aux données, garantissant que les requêtes d'analyse peuvent être exécutées rapidement et efficacement.",
        "Other Options": [
            "Activer le déploiement Multi-AZ améliore principalement la disponibilité et les capacités de basculement mais ne répond pas spécifiquement au besoin de gérer un fort trafic de lecture. Cela fournit une instance de secours pour le basculement mais ne distribue pas la charge de lecture, ce qui est crucial pour les exigences de l'équipe d'analyse.",
            "Déployer une configuration active-active multi-régions peut fournir une haute disponibilité et un équilibrage de charge, mais cela est plus complexe et peut introduire de la latence en raison de la synchronisation des données entre les régions. Cette option n'est pas nécessaire pour le scénario donné, qui se concentre sur la gestion efficace du trafic de lecture plutôt que sur l'équilibrage des opérations de lecture et d'écriture entre les régions.",
            "Configurer la base de données pour une réplication synchrone assure la cohérence des données mais peut introduire de la latence pendant les périodes de fort trafic de lecture. La réplication synchrone nécessite que tous les réplicas confirment la réception des données avant que la principale puisse continuer, ce qui peut ralentir les opérations de lecture et ne répond pas efficacement au besoin d'accès à faible latence pour les lectures."
        ]
    },
    {
        "Question Number": "55",
        "Situation": "Une startup stocke les données des utilisateurs sur Amazon S3 et souhaite optimiser les coûts de stockage en mettant en œuvre des politiques de cycle de vie des données. Les données sont fréquemment accessibles pendant les 30 premiers jours et rarement accessibles par la suite, mais elles doivent être conservées pendant 5 ans pour des raisons de conformité.",
        "Question": "Quelle politique de cycle de vie des données serait la plus rentable ?",
        "Options": {
            "1": "Stocker les données dans S3 Standard et les déplacer vers Glacier après 30 jours",
            "2": "Stocker les données dans S3 Intelligent-Tiering tout au long de leur cycle de vie",
            "3": "Déplacer les données vers S3 Standard-IA après 30 jours, puis vers Glacier Deep Archive après un an",
            "4": "Stocker toutes les données dans S3 Standard et les supprimer après 5 ans"
        },
        "Correct Answer": "Déplacer les données vers S3 Standard-IA après 30 jours, puis vers Glacier Deep Archive après un an",
        "Explanation": "Cette option est la plus rentable car elle utilise S3 Standard pendant les 30 premiers jours lorsque les données sont fréquemment accessibles, garantissant des performances et des coûts optimaux pour les données actives. Après 30 jours, déplacer les données vers S3 Standard-IA (Infrequent Access) réduit les coûts de stockage pour les données qui sont rarement accessibles mais qui doivent encore être conservées. Enfin, la transition vers Glacier Deep Archive après un an offre le coût de stockage le plus bas pour la conservation à long terme, ce qui correspond à l'exigence de conserver les données pendant 5 ans pour des raisons de conformité. Cette stratégie équilibre efficacement les besoins en coûts et en accès tout au long du cycle de vie des données.",
        "Other Options": [
            "Stocker les données dans S3 Standard et les déplacer vers Glacier après 30 jours : Cette option entraîne des coûts plus élevés pendant les 30 premiers jours car elle maintient les données dans S3 Standard, qui est plus coûteux que Standard-IA. De plus, passer à Glacier après 30 jours peut ne pas être optimal puisque les données devront encore être conservées pendant 5 ans, et Glacier n'est pas conçu pour un accès fréquent.",
            "Stocker les données dans S3 Intelligent-Tiering tout au long de leur cycle de vie : Bien que S3 Intelligent-Tiering déplace automatiquement les données entre deux niveaux d'accès en fonction des modèles d'accès changeants, cela peut ne pas être la solution la plus rentable pour ce cas d'utilisation spécifique. Étant donné que les données sont fréquemment accessibles pendant les 30 premiers jours et rarement accessibles par la suite, une approche plus adaptée (comme le déplacement vers Standard-IA) permettrait probablement d'économiser davantage par rapport aux frais d'Intelligent-Tiering.",
            "Stocker toutes les données dans S3 Standard et les supprimer après 5 ans : Cette option est la moins rentable car elle maintient toutes les données dans S3 Standard pendant toute la durée, ce qui est la classe de stockage la plus coûteuse. De plus, elle ne tire pas parti des options de stockage à coût réduit pour les données qui sont peu accessibles après les 30 premiers jours."
        ]
    },
    {
        "Question Number": "56",
        "Situation": "Une entreprise de commerce électronique souhaite protéger ses données de transaction en cas de défaillance du système. Pour limiter la perte de données potentielle, elle a fixé un objectif de point de récupération (RPO) strict de 5 minutes, ce qui signifie qu'elle ne peut se permettre de perdre que jusqu'à 5 minutes de données en cas de panne. Elle a besoin d'une solution qui maintienne la réplication des données à jour pour atteindre ce RPO minimal.",
        "Question": "Laquelle des approches suivantes répondrait le mieux à cette exigence de RPO ?",
        "Options": {
            "1": "Prendre des instantanés horaires de la base de données pour fournir des points de récupération réguliers, permettant une restauration jusqu'à la dernière sauvegarde horaire",
            "2": "Mettre en œuvre une réplication continue des données vers une base de données secondaire, garantissant des mises à jour quasi en temps réel et minimisant la perte de données potentielle",
            "3": "Sauvegarder les données sur Amazon S3 toutes les 10 minutes, créant des points de récupération réguliers qui peuvent être restaurés au besoin",
            "4": "Utiliser des sauvegardes complètes hebdomadaires avec des sauvegardes incrémentielles quotidiennes pour capturer les changements de données de manière rentable"
        },
        "Correct Answer": "Mettre en œuvre une réplication continue des données vers une base de données secondaire, garantissant des mises à jour quasi en temps réel et minimisant la perte de données potentielle",
        "Explanation": "La réplication continue des données permet des mises à jour en temps réel ou quasi en temps réel de la base de données principale vers une base de données secondaire. Cette approche garantit que tout changement apporté à la base de données principale est immédiatement reflété dans la base de données secondaire, minimisant ainsi la perte de données potentielle à quelques secondes ou minutes, ce qui correspond parfaitement à l'objectif de point de récupération (RPO) strict de 5 minutes fixé par l'entreprise de commerce électronique. Cette méthode est la plus efficace pour répondre à l'exigence de maintenir la réplication des données à jour.",
        "Other Options": [
            "Prendre des instantanés horaires de la base de données ne répondrait pas à l'exigence de RPO de 5 minutes, car cela permettrait la perte de jusqu'à 59 minutes de données si une défaillance se produisait juste après le dernier instantané.",
            "Sauvegarder les données sur Amazon S3 toutes les 10 minutes ne répondrait pas suffisamment au RPO de 5 minutes, car il pourrait encore y avoir une perte potentielle de jusqu'à 9 minutes de données si une défaillance se produisait juste avant la prochaine sauvegarde.",
            "Utiliser des sauvegardes complètes hebdomadaires avec des sauvegardes incrémentielles quotidiennes n'est pas adapté à un RPO de 5 minutes, car cette méthode entraînerait une perte de données significative, potentiellement jusqu'à 24 heures, selon le moment où la dernière sauvegarde incrémentielle a été effectuée."
        ]
    },
    {
        "Question Number": "57",
        "Situation": "Une grande plateforme de commerce électronique doit mettre en œuvre une architecture orientée événements pour gérer les mises à jour d'inventaire, le traitement des commandes et les notifications aux clients. La plateforme doit garantir que le système est hautement disponible, résilient aux pannes et capable de s'adapter automatiquement en fonction du trafic.",
        "Question": "Quel design d'architecture devrait être mis en œuvre pour atteindre ces objectifs ? (Choisissez deux.)",
        "Options": {
            "1": "Utiliser Amazon SQS pour découpler les services et garantir un traitement asynchrone des messages, et utiliser Amazon SNS pour diffuser des événements à plusieurs abonnés. Mettre en œuvre AWS Lambda pour traiter les événements et s'adapter automatiquement.",
            "2": "Utiliser des instances Amazon EC2 exécutant une application personnalisée pour gérer les messages des sources d'événements, et configurer Amazon Route 53 pour acheminer le trafic en fonction de la charge.",
            "3": "Utiliser Amazon RDS avec un déploiement multi-zones de disponibilité pour gérer le traitement des événements, et stocker les messages dans Amazon DynamoDB pour la scalabilité.",
            "4": "Utiliser Amazon Kinesis Data Streams pour gérer les données d'événements en temps réel, et s'intégrer à Amazon Elasticsearch Service pour interroger les données.",
            "5": "Mettre en œuvre AWS Step Functions pour orchestrer les workflows de traitement des événements et utiliser Amazon MQ pour le courtage de messages."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Utiliser Amazon SQS pour découpler les services et garantir un traitement asynchrone des messages, et utiliser Amazon SNS pour diffuser des événements à plusieurs abonnés. Mettre en œuvre AWS Lambda pour traiter les événements et s'adapter automatiquement.",
            "Utiliser Amazon Kinesis Data Streams pour gérer les données d'événements en temps réel, et s'intégrer à Amazon Elasticsearch Service pour interroger les données."
        ],
        "Explanation": "Amazon SQS et SNS sont utilisés pour découpler les services et diffuser des événements à plusieurs abonnés, respectivement. Cela garantit une haute disponibilité et une résilience aux pannes. AWS Lambda est sans serveur et s'adapte automatiquement en fonction de la charge de travail, ce qui le rend adapté au traitement des événements. D'autre part, Amazon Kinesis Data Streams est conçu pour gérer les données d'événements en temps réel, ce qui est crucial pour une plateforme de commerce électronique. Amazon Elasticsearch Service permet une interrogation efficace de ces données.",
        "Other Options": [
            "Utiliser des instances Amazon EC2 exécutant une application personnalisée pour gérer les messages des sources d'événements, et configurer Amazon Route 53 pour acheminer le trafic en fonction de la charge n'est pas la meilleure option. Bien que les instances EC2 puissent être utilisées pour exécuter des applications et que Route 53 puisse aider à distribuer la charge, cette approche ne fournit pas intrinsèquement l'architecture orientée événements, la haute disponibilité, la résilience aux pannes et l'adaptation automatique requises.",
            "Utiliser Amazon RDS avec un déploiement multi-zones de disponibilité pour gérer le traitement des événements, et stocker les messages dans Amazon DynamoDB pour la scalabilité n'est pas idéal. Bien que RDS et DynamoDB soient des services AWS robustes, ils ne sont pas conçus pour des architectures orientées événements. RDS est un service de base de données relationnelle, pas un service de traitement d'événements, et DynamoDB, bien que scalable, n'est pas conçu pour le messaging d'événements.",
            "Mettre en œuvre AWS Step Functions pour orchestrer les workflows de traitement des événements et utiliser Amazon MQ pour le courtage de messages n'est pas le meilleur choix. Bien que Step Functions puisse orchestrer des workflows et qu'Amazon MQ puisse courtier des messages, ils ne fournissent pas intrinsèquement la haute disponibilité, la résilience aux pannes et l'adaptation automatique requises pour ce scénario."
        ]
    },
    {
        "Question Number": "58",
        "Situation": "Une entreprise déploie une application de calcul haute performance sur Amazon EC2 et souhaite optimiser pour la latence réseau la plus basse possible et la meilleure performance en paquets par seconde parmi ses instances. En même temps, elle a une autre application qui nécessite une disponibilité maximale et une résilience en isolant chaque instance sur des racks différents.",
        "Question": "Quels types de groupes de placement l'entreprise devrait-elle utiliser pour ces applications, et pourquoi ?",
        "Options": {
            "1": "Utilisez des Cluster Placement Groups pour l'application haute performance afin d'atteindre une faible latence et un haut débit, et des Spread Placement Groups pour l'application qui nécessite une haute disponibilité et une isolation entre les racks.",
            "2": "Utilisez des Spread Placement Groups pour les deux applications afin d'assurer la résilience et d'isoler les instances sur plusieurs racks.",
            "3": "Utilisez des Partition Placement Groups pour l'application haute performance afin de fournir un haut débit et des Cluster Placement Groups pour l'application isolée afin de réduire la latence.",
            "4": "Utilisez des Cluster Placement Groups pour les deux applications afin de minimiser la latence et d'augmenter la performance parmi les instances."
        },
        "Correct Answer": "Utilisez des Cluster Placement Groups pour l'application haute performance afin d'atteindre une faible latence et un haut débit, et des Spread Placement Groups pour l'application qui nécessite une haute disponibilité et une isolation entre les racks.",
        "Explanation": "Les Cluster Placement Groups sont conçus pour fournir une faible latence et un haut débit en plaçant les instances proches les unes des autres au sein d'une seule zone de disponibilité. Cela est idéal pour les applications de calcul haute performance qui nécessitent une communication rapide entre les instances. D'autre part, les Spread Placement Groups garantissent que les instances sont placées sur différents racks, ce qui améliore la disponibilité et la résilience en réduisant le risque de pannes simultanées. Cela rend les Spread Placement Groups adaptés aux applications qui doivent être isolées les unes des autres pour maintenir une haute disponibilité.",
        "Other Options": [
            "Utiliser des Spread Placement Groups pour les deux applications garantirait la résilience et l'isolation, mais cela n'optimiserait pas la faible latence et le haut débit pour l'application haute performance, qui est une exigence critique.",
            "Utiliser des Partition Placement Groups pour l'application haute performance est incorrect car les Partition Placement Groups sont conçus pour des applications nécessitant une haute disponibilité et une tolérance aux pannes, pas spécifiquement pour une faible latence et un haut débit. De plus, utiliser des Cluster Placement Groups pour l'application isolée ne fournirait pas la résilience nécessaire entre les racks.",
            "Utiliser des Cluster Placement Groups pour les deux applications optimiserait la faible latence et la performance, mais cela ne fournirait pas l'isolation et la résilience nécessaires pour l'application qui nécessite une haute disponibilité, car toutes les instances seraient placées dans le même rack."
        ]
    },
    {
        "Question Number": "59",
        "Situation": "Une entreprise développe une application qui exposera des API aux clients via une interface web. L'entreprise doit s'assurer que les API peuvent évoluer automatiquement en fonction de la demande, gérer les pics de trafic et fournir une gestion efficace des API.",
        "Question": "Quel service AWS l'entreprise devrait-elle utiliser pour y parvenir, et quels principes de conception devraient être suivis pour garantir l'évolutivité et la résilience ?",
        "Options": {
            "1": "Utilisez Amazon API Gateway pour créer et gérer les API, et combinez-le avec AWS Lambda pour un calcul sans état afin de gérer des charges de travail imprévisibles. Mettez en œuvre des stratégies de mise en cache pour réduire la latence et améliorer la performance.",
            "2": "Utilisez Amazon EC2 pour héberger les API et gérer le trafic avec un groupe Auto Scaling, tout en stockant les données dans Amazon RDS pour une haute disponibilité.",
            "3": "Utilisez AWS Fargate pour gérer des conteneurs Docker exécutant les API, et implémentez des appels API directs à Amazon DynamoDB pour stocker les données de l'application.",
            "4": "Utilisez AWS Elastic Load Balancer pour acheminer le trafic API vers des instances EC2, et stockez les données API dans Amazon S3 pour une haute évolutivité."
        },
        "Correct Answer": "Utilisez Amazon API Gateway pour créer et gérer les API, et combinez-le avec AWS Lambda pour un calcul sans état afin de gérer des charges de travail imprévisibles. Mettez en œuvre des stratégies de mise en cache pour réduire la latence et améliorer la performance.",
        "Explanation": "Amazon API Gateway est spécifiquement conçu pour créer, publier et gérer des API à grande échelle. Il peut gérer automatiquement les pics de trafic et fournit des fonctionnalités intégrées pour la mise en cache, le throttling et la surveillance. Lorsqu'il est combiné avec AWS Lambda, qui permet l'exécution sans serveur du code, l'application peut évoluer automatiquement en fonction de la demande sans avoir besoin de provisionner des serveurs. Cette combinaison prend en charge le calcul sans état, ce qui est idéal pour gérer des charges de travail imprévisibles. Les stratégies de mise en cache peuvent encore améliorer la performance en réduisant le nombre d'appels effectués aux services backend, améliorant ainsi les temps de réponse et réduisant les coûts.",
        "Other Options": [
            "Utiliser Amazon EC2 pour héberger les API nécessite une gestion manuelle des instances et des configurations de mise à l'échelle, ce qui peut compliquer l'architecture et peut ne pas gérer les pics de trafic aussi efficacement que les solutions sans serveur. Bien que les groupes Auto Scaling puissent aider, ils impliquent toujours plus de surcharge par rapport à l'approche sans serveur.",
            "AWS Fargate est une bonne option pour gérer des conteneurs, mais cela ajoute de la complexité par rapport à l'utilisation d'API Gateway et Lambda. Les appels API directs à DynamoDB peuvent fonctionner, mais sans les fonctionnalités de gestion des API d'API Gateway, la solution peut manquer de l'évolutivité et des capacités de surveillance nécessaires.",
            "AWS Elastic Load Balancer peut distribuer le trafic vers des instances EC2, mais cette configuration nécessite toujours de gérer ces instances et de les mettre à l'échelle manuellement. Stocker les données API dans Amazon S3 n'est pas adapté aux réponses API dynamiques, car S3 est principalement destiné au stockage d'objets et ne fournit pas le même niveau de performance et de capacités de requête qu'une solution de base de données."
        ]
    },
    {
        "Question Number": "60",
        "Situation": "Une entreprise connaît une augmentation du trafic client sur son application web et doit faire évoluer son infrastructure pour gérer la charge. Elle envisage à la fois des options de mise à l'échelle horizontale et verticale.",
        "Question": "Quelle est une différence clé entre la mise à l'échelle horizontale et verticale, et laquelle serait plus adaptée pour minimiser les interruptions d'application ?",
        "Options": {
            "1": "La mise à l'échelle verticale consiste à augmenter la taille de l'instance, nécessitant un redémarrage, ce qui peut provoquer des interruptions temporaires, tandis que la mise à l'échelle horizontale consiste à ajouter plus d'instances sans besoin de redémarrage, évitant ainsi les interruptions.",
            "2": "La mise à l'échelle horizontale ajoute plus de ressources à la même instance, ce qui augmente la capacité sans interruption, tandis que la mise à l'échelle verticale ajoute de nouvelles instances pour gérer plus de trafic.",
            "3": "La mise à l'échelle verticale nécessite une modification de l'application pour chaque nouvelle taille d'instance, tandis que la mise à l'échelle horizontale ne nécessite aucune modification de l'application.",
            "4": "La mise à l'échelle horizontale a une limite stricte sur le nombre d'instances qui peuvent être ajoutées, tandis que la mise à l'échelle verticale offre une capacité illimitée."
        },
        "Correct Answer": "La mise à l'échelle verticale consiste à augmenter la taille de l'instance, nécessitant un redémarrage, ce qui peut provoquer des interruptions temporaires, tandis que la mise à l'échelle horizontale consiste à ajouter plus d'instances sans besoin de redémarrage, évitant ainsi les interruptions.",
        "Explanation": "La différence clé entre la mise à l'échelle horizontale et verticale réside dans la manière dont les ressources sont ajoutées pour gérer une charge accrue. La mise à l'échelle verticale (également connue sous le nom de 'scaling up') consiste à améliorer les ressources du serveur existant, telles que le CPU, la RAM ou le stockage. Ce processus nécessite souvent un redémarrage du serveur, ce qui peut entraîner un temps d'arrêt temporaire de l'application. En revanche, la mise à l'échelle horizontale (ou 'scaling out') consiste à ajouter plus d'instances ou de serveurs pour répartir la charge. Cette méthode permet à l'application de continuer à fonctionner sans interruption, ce qui la rend plus adaptée pour minimiser les interruptions pendant les périodes de trafic accru.",
        "Other Options": [
            "Cette option indique incorrectement que la mise à l'échelle horizontale ajoute des ressources à la même instance, ce qui n'est pas exact. La mise à l'échelle horizontale ajoute plus d'instances plutôt que d'augmenter la capacité d'une seule instance.",
            "Cette option est incorrecte car elle suggère que la mise à l'échelle verticale nécessite une modification de l'application pour chaque nouvelle taille d'instance. En réalité, la mise à l'échelle verticale ne nécessite pas de modifications de l'application elle-même, mais elle nécessite un redémarrage, ce qui peut provoquer des interruptions.",
            "Cette option est trompeuse car elle affirme que la mise à l'échelle horizontale a une limite stricte sur les instances, ce qui n'est pas universellement vrai. Bien qu'il puisse y avoir des limites pratiques basées sur l'infrastructure ou les capacités du fournisseur de cloud, la mise à l'échelle horizontale est généralement plus flexible que la mise à l'échelle verticale, qui est limitée par la capacité maximale d'un seul serveur."
        ]
    },
    {
        "Question Number": "61",
        "Situation": "Lors de la configuration d'une connexion VPN IPSEC entre deux sites d'entreprise, laquelle des affirmations suivantes décrit correctement le rôle de la Phase 1 et de la Phase 2 de l'IKE (Internet Key Exchange) dans l'établissement d'une connexion sécurisée ?",
        "Question": "Lesquelles des affirmations suivantes sont vraies concernant la Phase 1 et la Phase 2 de l'IKE ? (Choisissez deux.)",
        "Options": {
            "1": "La Phase 1 de l'IKE établit un tunnel sécurisé en utilisant le chiffrement symétrique, tandis que la Phase 2 de l'IKE utilise le chiffrement asymétrique pour le transfert de données en vrac à travers le tunnel.",
            "2": "La Phase 1 de l'IKE est responsable de l'authentification et de l'établissement d'une connexion sécurisée avec un chiffrement asymétrique, de la mise en place d'une clé symétrique et de la création de l'Association de Sécurité IKE (SA) ; la Phase 2 de l'IKE utilise ensuite cette clé pour un transfert de données en vrac rapide et chiffré, créant la SA IPSEC.",
            "3": "La Phase 1 de l'IKE établit directement la SA IPSEC en utilisant des clés symétriques échangées sur un réseau public, tandis que la Phase 2 de l'IKE gère la ré-authentification de chaque session.",
            "4": "Les Phases 1 et 2 de l'IKE utilisent toutes deux le chiffrement asymétrique tout au long du processus de configuration de la connexion et de transfert de données pour garantir le plus haut niveau de sécurité.",
            "5": "La Phase 1 de l'IKE négocie les paramètres pour le tunnel IPSEC, et la Phase 2 de l'IKE gère le chiffrement réel des données transmises."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "La Phase 1 de l'IKE est responsable de l'authentification et de l'établissement d'une connexion sécurisée avec un chiffrement asymétrique, de la mise en place d'une clé symétrique et de la création de l'Association de Sécurité IKE (SA) ; la Phase 2 de l'IKE utilise ensuite cette clé pour un transfert de données en vrac rapide et chiffré, créant la SA IPSEC.",
            "La Phase 1 de l'IKE négocie les paramètres pour le tunnel IPSEC, et la Phase 2 de l'IKE gère le chiffrement réel des données transmises."
        ],
        "Explanation": "La Phase 1 de l'IKE est responsable de l'authentification des pairs, de l'établissement d'une connexion sécurisée et de la mise en place d'une clé symétrique pour le chiffrement des données. Elle utilise le chiffrement asymétrique pour ces tâches afin d'assurer la sécurité. Une fois cela fait, elle crée l'Association de Sécurité IKE (SA). La Phase 2 de l'IKE utilise ensuite la clé symétrique mise en place lors de la Phase 1 pour un transfert de données en vrac rapide et chiffré. Elle crée la SA IPSEC qui est utilisée pour le transfert réel des données. La Phase 1 négocie également les paramètres pour le tunnel IPSEC, et la Phase 2 gère le chiffrement réel des données transmises.",
        "Other Options": [
            "La Phase 1 de l'IKE utilise le chiffrement asymétrique pour l'établissement de la connexion sécurisée et la mise en place de la clé symétrique, et non le chiffrement symétrique. La Phase 2 de l'IKE utilise la clé symétrique de la Phase 1 pour le transfert de données, et non le chiffrement asymétrique.",
            "La Phase 1 de l'IKE n'établit pas directement la SA IPSEC, elle établit la SA IKE. La SA IPSEC est établie lors de la Phase 2. De plus, les clés symétriques ne sont pas échangées sur un réseau public, elles sont mises en place de manière sécurisée en utilisant le chiffrement asymétrique lors de la Phase 1.",
            "Bien que la Phase 1 de l'IKE utilise le chiffrement asymétrique pour l'établissement de la connexion sécurisée et la mise en place de la clé symétrique, la Phase 2 utilise la clé symétrique de la Phase 1 pour le transfert de données, et non le chiffrement asymétrique."
        ]
    },
    {
        "Question Number": "62",
        "Situation": "Une entreprise financière doit stocker des données de transaction critiques dans une solution de stockage hautement disponible et résiliente pour garantir la durabilité et l'accessibilité des données. Elle souhaite également protéger les données contre la suppression accidentelle et les récupérer rapidement en cas de catastrophe.",
        "Question": "Quelle configuration dans Amazon S3 répond le mieux à ces exigences ? (Choisissez deux.)",
        "Options": {
            "1": "Utilisez la classe de stockage Amazon S3 Standard avec la versionnage activé et la réplication inter-régions pour protéger contre les suppressions accidentelles et garantir la disponibilité des données dans plusieurs régions.",
            "2": "Utilisez Amazon S3 Glacier pour un stockage à faible coût et activez le verrouillage des objets pour prévenir les suppressions accidentelles tout en conservant un accès rapide aux données.",
            "3": "Stockez les données dans Amazon S3 Intelligent-Tiering pour réduire les coûts, en comptant sur AWS Backup pour la récupération après sinistre à travers les régions.",
            "4": "Utilisez Amazon S3 One Zone-Infrequent Access pour stocker les données dans une seule zone de disponibilité et activez le versionnage pour protéger contre la perte de données.",
            "5": "Activez la suppression par authentification multi-facteurs (MFA) sur les buckets Amazon S3 pour fournir une couche de protection supplémentaire contre les suppressions accidentelles."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Utilisez la classe de stockage Amazon S3 Standard avec la versionnage activé et la réplication inter-régions pour protéger contre les suppressions accidentelles et garantir la disponibilité des données dans plusieurs régions.",
            "Activez la suppression par authentification multi-facteurs (MFA) sur les buckets Amazon S3 pour fournir une couche de protection supplémentaire contre les suppressions accidentelles."
        ],
        "Explanation": "La classe de stockage Amazon S3 Standard offre une durabilité, une disponibilité et des performances élevées pour le stockage d'objets fréquemment accessibles. Lorsque le versionnage est activé, elle conserve toutes les versions d'un objet (y compris toutes les écritures et suppressions) dans le bucket. La réplication inter-régions permet la copie automatique et asynchrone d'objets entre des buckets dans différentes régions, ce qui peut aider à répondre aux exigences de conformité et à minimiser la latence. La suppression par authentification multi-facteurs (MFA) ajoute une couche de sécurité supplémentaire en exigeant une MFA pour supprimer une version d'objet ou suspendre le versionnage sur le bucket.",
        "Other Options": [
            "Amazon S3 Glacier est une classe de stockage sécurisée, durable et à faible coût pour l'archivage de données et la sauvegarde à long terme. Cependant, elle ne fournit pas un accès rapide aux données car les temps de récupération peuvent aller de quelques minutes à plusieurs heures.",
            "Amazon S3 Intelligent-Tiering est conçu pour optimiser les coûts en déplaçant automatiquement les données vers le niveau d'accès le plus rentable, sans impact sur les performances ni surcharge opérationnelle. AWS Backup peut être utilisé pour la récupération après sinistre, mais cette option ne protège pas contre les suppressions accidentelles.",
            "Amazon S3 One Zone-Infrequent Access est une option à moindre coût pour les données rarement accessibles, mais elle stocke les données dans une seule zone de disponibilité, ce qui est moins résilient et ne répond pas à l'exigence de haute disponibilité."
        ]
    },
    {
        "Question Number": "63",
        "Situation": "Une entreprise de services financiers migre son application sur site vers AWS. L'application se compose d'un niveau web, d'un niveau d'application et d'un niveau de base de données. L'entreprise exige une isolation stricte entre les niveaux pour des raisons de sécurité et de conformité. Elle doit également optimiser l'adressage IP pour accueillir une croissance future.",
        "Question": "Quelle architecture réseau le concepteur de solutions devrait-il concevoir pour répondre à ces exigences ? (Choisissez deux.)",
        "Options": {
            "1": "Déployez tous les niveaux dans un seul sous-réseau public avec des groupes de sécurité contrôlant l'accès.",
            "2": "Utilisez un seul sous-réseau privé pour tous les niveaux avec des ACL réseau pour l'isolation.",
            "3": "Créez des sous-réseaux privés séparés pour chaque niveau à travers plusieurs zones de disponibilité, en utilisant un VPC avec des blocs CIDR qui permettent une expansion future.",
            "4": "Placez le niveau web dans un sous-réseau public et les niveaux d'application et de base de données dans un seul sous-réseau privé avec des plages IP qui se chevauchent.",
            "5": "Mettez en œuvre plusieurs sous-réseaux privés pour chaque niveau au sein d'un VPC et utilisez le peering VPC pour isoler le trafic entre les niveaux."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Créez des sous-réseaux privés séparés pour chaque niveau à travers plusieurs zones de disponibilité, en utilisant un VPC avec des blocs CIDR qui permettent une expansion future.",
            "Mettez en œuvre plusieurs sous-réseaux privés pour chaque niveau au sein d'un VPC et utilisez le peering VPC pour isoler le trafic entre les niveaux."
        ],
        "Explanation": "Créer des sous-réseaux privés séparés pour chaque niveau à travers plusieurs zones de disponibilité permet une isolation stricte entre les niveaux, ce qui est une exigence pour l'entreprise. Utiliser un VPC avec des blocs CIDR qui permettent une expansion future aide à optimiser l'adressage IP pour accueillir une croissance future. La mise en œuvre de plusieurs sous-réseaux privés pour chaque niveau au sein d'un VPC et l'utilisation du peering VPC pour isoler le trafic entre les niveaux fournissent également l'isolation et la sécurité requises.",
        "Other Options": [
            "Déployer tous les niveaux dans un seul sous-réseau public avec des groupes de sécurité contrôlant l'accès n'est pas une bonne pratique car cela ne fournit pas l'isolation requise entre les niveaux et expose l'application à des risques de sécurité potentiels.",
            "Utiliser un seul sous-réseau privé pour tous les niveaux avec des ACL réseau pour l'isolation ne fournit pas l'isolation requise entre les niveaux car tous les niveaux se trouvent dans le même sous-réseau.",
            "Placer le niveau web dans un sous-réseau public et les niveaux d'application et de base de données dans un seul sous-réseau privé avec des plages IP qui se chevauchent ne fournit pas l'isolation requise entre les niveaux et peut causer des conflits IP en raison des plages IP qui se chevauchent."
        ]
    },
    {
        "Question Number": "64",
        "Situation": "Une entreprise utilise Amazon RDS pour sa base de données et nécessite le chiffrement des données pour des raisons de conformité. L'entreprise souhaite s'assurer que les données sont chiffrées à la fois au repos et en transit, et que les clés de chiffrement sont gérées de manière sécurisée. De plus, elle utilise Oracle comme moteur de base de données.",
        "Question": "Quelle approche satisferait le mieux ces exigences de sécurité ? (Choisissez deux.)",
        "Options": {
            "1": "Utiliser le SSL/TLS intégré d'RDS pour le chiffrement en transit et activer le Transparent Data Encryption (TDE) pour le chiffrement au repos au sein du moteur de base de données Oracle.",
            "2": "Activer Amazon RDS pour utiliser des clés gérées par KMS pour le chiffrement au repos et configurer le SSL/TLS pour gérer le chiffrement en transit.",
            "3": "Intégrer CloudHSM avec Amazon RDS pour gérer les clés de chiffrement pour Oracle, en s'assurant qu'AWS n'a pas accès aux clés, et activer le SSL/TLS pour le chiffrement en transit.",
            "4": "Utiliser les paramètres de chiffrement par défaut d'RDS et compter sur le chiffrement des volumes EBS pour les données au repos, sans configuration supplémentaire pour le chiffrement en transit.",
            "5": "Mettre en œuvre un chiffrement au niveau de l'application pour gérer le chiffrement des données avant qu'elles ne soient envoyées à RDS et utiliser des connexions VPN pour le chiffrement en transit."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Utiliser le SSL/TLS intégré d'RDS pour le chiffrement en transit et activer le Transparent Data Encryption (TDE) pour le chiffrement au repos au sein du moteur de base de données Oracle.",
            "Activer Amazon RDS pour utiliser des clés gérées par KMS pour le chiffrement au repos et configurer le SSL/TLS pour gérer le chiffrement en transit."
        ],
        "Explanation": "La première réponse correcte consiste à utiliser le SSL/TLS intégré d'RDS pour le chiffrement en transit et à activer le Transparent Data Encryption (TDE) pour le chiffrement au repos au sein du moteur de base de données Oracle. Le SSL/TLS est un protocole qui garantit la transmission sécurisée des données sur les réseaux, et le TDE est une fonctionnalité d'Oracle qui fournit un chiffrement des données au repos. La deuxième réponse correcte est d'activer Amazon RDS pour utiliser des clés gérées par KMS pour le chiffrement au repos et de configurer le SSL/TLS pour gérer le chiffrement en transit. Amazon Key Management Service (KMS) est un service géré qui facilite la création et le contrôle des clés de chiffrement utilisées pour chiffrer vos données.",
        "Other Options": [
            "L'intégration de CloudHSM avec Amazon RDS pour gérer les clés de chiffrement pour Oracle et l'activation du SSL/TLS pour le chiffrement en transit n'est pas nécessaire car AWS KMS peut gérer la gestion des clés pour RDS, et c'est plus simple et plus rentable.",
            "Utiliser les paramètres de chiffrement par défaut d'RDS et compter sur le chiffrement des volumes EBS pour les données au repos, sans configuration supplémentaire pour le chiffrement en transit, n'est pas suffisant car cela ne garantit pas le chiffrement en transit.",
            "Mettre en œuvre un chiffrement au niveau de l'application pour gérer le chiffrement des données avant qu'elles ne soient envoyées à RDS et utiliser des connexions VPN pour le chiffrement en transit n'est pas la meilleure approche car cela ajoute une complexité et une surcharge inutiles. Il est plus efficace d'utiliser les services AWS intégrés pour le chiffrement au repos et en transit."
        ]
    },
    {
        "Question Number": "65",
        "Situation": "Une société de production vidéo stocke des milliers de fichiers vidéo, qui sont rarement consultés après la production initiale. Elle souhaite une solution de stockage économique qui lui permette d'archiver ces fichiers tout en pouvant les récupérer en quelques minutes si nécessaire.",
        "Question": "Quels services de stockage AWS répondraient le mieux à ces exigences ? (Choisissez deux.)",
        "Options": {
            "1": "Amazon EFS",
            "2": "Amazon S3 Glacier Instant Retrieval",
            "3": "Amazon FSx for Windows File Server",
            "4": "Amazon EBS Provisioned IOPS",
            "5": "Amazon S3 Intelligent-Tiering"
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Amazon S3 Glacier Instant Retrieval",
            "Amazon S3 Intelligent-Tiering"
        ],
        "Explanation": "Amazon S3 Glacier Instant Retrieval est une solution de stockage économique pour l'archivage des données. Elle est conçue pour le stockage à long terme de données qui sont rarement consultées, mais qui peuvent être récupérées en quelques minutes si nécessaire. Cela en fait un choix approprié pour la société de production vidéo. Amazon S3 Intelligent-Tiering est un autre choix approprié car il déplace automatiquement les données vers le niveau d'accès le plus économique, sans impact sur les performances ni surcharge opérationnelle. Il est idéal pour les données avec des modèles d'accès inconnus ou changeants, ce qui en fait un bon choix pour le stockage de fichiers vidéo rarement consultés.",
        "Other Options": [
            "Amazon EFS (Elastic File System) est un service de stockage de fichiers à utiliser avec Amazon EC2. Bien qu'il puisse techniquement être utilisé pour stocker des fichiers vidéo, ce n'est pas la solution la plus économique pour des données rarement consultées.",
            "Amazon FSx for Windows File Server fournit un système de fichiers Microsoft Windows natif entièrement géré. Ce n'est pas la solution la plus économique pour stocker des fichiers vidéo rarement consultés, et il est plus adapté aux charges de travail d'entreprise nécessitant des systèmes de fichiers Windows.",
            "Amazon EBS (Elastic Block Store) Provisioned IOPS est un type de stockage conçu pour fournir des performances à 10 % près des IOPS provisionnés 99,9 % du temps. Cela est plus adapté aux charges de travail nécessitant des performances élevées plutôt qu'à un stockage à long terme économique de données rarement consultées."
        ]
    }
]