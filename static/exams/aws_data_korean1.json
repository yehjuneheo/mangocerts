[
    {
        "Question Number": "4",
        "Situation": "데이터 엔지니어링 팀은 Amazon RDS 데이터베이스 인스턴스에 대한 안전한 액세스를 구성하는 임무를 맡았습니다. 그들은 신뢰할 수 있는 소스만 데이터베이스에 연결할 수 있도록 IP 주소의 허용 목록을 만들어야 합니다. 팀은 이 보안 조치를 구현하기 위한 다양한 방법을 고려하고 있습니다.",
        "Question": "다음 방법 중 Amazon RDS 인스턴스에 대한 연결을 제한하기 위해 IP 주소의 허용 목록을 효과적으로 생성할 수 있는 방법은 무엇입니까?",
        "Options": {
            "1": "AWS WAF를 활성화하여 RDS 인스턴스에 대한 수신 트래픽을 필터링합니다.",
            "2": "AWS IAM 정책을 사용하여 RDS 데이터베이스에 대한 액세스를 제한합니다.",
            "3": "AWS Lambda 함수를 구성하여 동적 IP 주소 허용 목록 관리를 수행합니다.",
            "4": "VPC에서 보안 그룹을 구현하여 특정 IP 주소를 허용합니다."
        },
        "Correct Answer": "VPC에서 보안 그룹을 구현하여 특정 IP 주소를 허용합니다.",
        "Explanation": "VPC에서 보안 그룹을 사용하는 것은 Amazon RDS 인스턴스에 대한 수신 및 송신 트래픽을 네트워크 수준에서 직접 제어하므로 IP 주소의 허용 목록을 생성하는 가장 효과적인 방법입니다. 이를 통해 지정된 IP 주소만 데이터베이스에 접근할 수 있어 보안이 강화됩니다.",
        "Other Options": [
            "AWS IAM 정책은 서비스 수준에서 권한을 관리하는 데 사용되며 IP 주소를 기반으로 네트워크 액세스를 제한하는 기능을 제공하지 않습니다.",
            "AWS WAF는 일반적인 웹 공격으로부터 웹 애플리케이션을 보호하기 위해 설계되었지만, WAF 기능이 없는 RDS 인스턴스에 직접 트래픽을 필터링하는 데 적합하지 않습니다.",
            "AWS Lambda 함수는 다양한 자동화 작업에 사용될 수 있지만, RDS에 대한 IP 주소 허용 목록을 구현하는 직접적인 방법은 아니며 추가적인 복잡성과 관리가 필요합니다."
        ]
    },
    {
        "Question Number": "5",
        "Situation": "데이터 엔지니어링 팀은 Amazon Athena를 사용하여 여러 데이터 소스를 분석 워크플로에 통합하는 임무를 맡았습니다. 그들은 Amazon S3에 저장된 데이터와 관계형 데이터베이스에 있는 데이터를 모두 쿼리하여 포괄적인 보고서를 생성해야 합니다. 이를 위해 Athena의 기능을 사용하여 이러한 다양한 데이터 소스에 효율적으로 접근하고자 합니다.",
        "Question": "팀이 Amazon Athena를 사용하여 S3 데이터와 관계형 데이터베이스를 모두 쿼리할 수 있도록 하는 최선의 접근 방식은 무엇입니까?",
        "Options": {
            "1": "AWS Glue 서비스를 활용하여 관계형 데이터베이스의 모든 데이터를 S3로 이동한 다음 Athena를 사용하여 데이터를 쿼리합니다.",
            "2": "Amazon Redshift를 설정하여 관계형 데이터베이스에서 데이터를 복사하고 Athena를 사용하여 Redshift 데이터를 직접 쿼리합니다.",
            "3": "Athena Query Federation SDK를 사용하여 관계형 데이터베이스에 대한 사용자 지정 데이터 커넥터를 생성하기 위해 AWS Lambda 함수를 구현합니다.",
            "4": "Amazon QuickSight를 사용하여 S3와 관계형 데이터베이스의 데이터를 시각화하고 Athena를 사용하지 않습니다."
        },
        "Correct Answer": "Athena Query Federation SDK를 사용하여 관계형 데이터베이스에 대한 사용자 지정 데이터 커넥터를 생성하기 위해 AWS Lambda 함수를 구현합니다.",
        "Explanation": "AWS Lambda 함수를 Athena Query Federation SDK와 함께 사용하면 데이터 중복이나 이동 없이 S3 데이터와 함께 관계형 데이터베이스를 직접 쿼리할 수 있어 효율성과 유연성을 극대화할 수 있습니다.",
        "Other Options": [
            "AWS Glue를 사용하여 데이터를 S3로 이동하는 것은 유효한 ETL 전략일 수 있지만, 쿼리하기 전에 데이터를 전송해야 하므로 불필요한 복잡성과 지연을 초래합니다.",
            "Amazon Redshift를 설정하면 추가 비용과 오버헤드가 발생하며, 데이터가 Redshift로 복사되어야 하므로 Athena의 연합 쿼리 기능을 직접 활용하지 않습니다.",
            "Amazon QuickSight는 시각화 도구이며 쿼리 레이어로 작동하지 않으므로 여러 데이터 소스에서 복잡한 쿼리를 수행하기 위해 Athena의 필요성을 직접 대체할 수 없습니다."
        ]
    },
    {
        "Question Number": "6",
        "Situation": "데이터 엔지니어링 팀은 Amazon Redshift에 저장된 대규모 데이터 세트를 변환하여 보다 구조화된 방식으로 분석을 제공하는 임무를 맡았습니다. 그들은 보고 목적으로 데이터를 효율적으로 추출, 변환 및 로드하기 위해 SQL 쿼리를 작성해야 합니다.",
        "Question": "Amazon Redshift의 분석 테이블에 데이터를 효율적으로 변환하고 로드하는 데 가장 적합한 SQL 쿼리 구조는 무엇입니까?",
        "Options": {
            "1": "SELECT column1, column2 FROM source_table WHERE condition GROUP BY column1, column2;",
            "2": "CREATE TABLE analytics_table AS SELECT column1, column2 FROM source_table WHERE condition;",
            "3": "UPDATE analytics_table SET column1 = value1 WHERE condition;",
            "4": "INSERT INTO analytics_table SELECT column1, column2 FROM source_table WHERE condition ORDER BY column1;"
        },
        "Correct Answer": "CREATE TABLE analytics_table AS SELECT column1, column2 FROM source_table WHERE condition;",
        "Explanation": "가장 좋은 옵션은 CREATE TABLE AS SELECT 문을 사용하는 것으로, SELECT 쿼리의 결과에서 새로운 테이블을 직접 생성할 수 있습니다. 이 접근 방식은 추출과 변환을 한 단계에서 효과적으로 결합하여 분석 테이블로의 데이터 로딩 프로세스를 최적화합니다.",
        "Other Options": [
            "첫 번째 옵션은 데이터를 로드하지 않기 때문에 잘못된 것입니다. 단순히 데이터를 검색할 뿐 새로운 테이블에 저장하지 않습니다.",
            "두 번째 옵션은 결과를 분석 테이블에 삽입하기 전에 정렬하려고 시도하므로 잘못된 것이며, 이는 불필요하고 데이터 로딩의 비효율성을 초래할 수 있습니다.",
            "네 번째 옵션은 분석 테이블의 기존 레코드만 업데이트하므로 초기 데이터 변환이나 로딩 프로세스를 촉진하지 않습니다."
        ]
    },
    {
        "Question Number": "7",
        "Situation": "조직이 AWS에서 호스팅되는 새로운 애플리케이션을 구현하고 있으며, 이 애플리케이션은 민감한 데이터에 대한 접근이 필요합니다. 애플리케이션은 권한이 있는 사용자만 접근할 수 있도록 보장해야 하며, 무단 접근의 위험을 최소화해야 합니다. 조직은 이 데이터에 대한 접근을 보호하기 위해 다양한 인증 방법을 고려하고 있습니다.",
        "Question": "애플리케이션에 대해 세밀한 접근 제어를 허용하면서 가장 높은 수준의 보안을 제공하는 인증 방법은 무엇입니까?",
        "Options": {
            "1": "조직 내 사용자 역할에 따라 권한을 부여하는 역할 기반 인증을 사용합니다.",
            "2": "강력한 비밀번호 정책과 정기적인 비밀번호 변경을 통해 비밀번호 기반 인증을 구현합니다.",
            "3": "강화된 보안을 위해 비밀번호 기반 인증과 함께 다단계 인증을 활용합니다.",
            "4": "인증된 장치만 애플리케이션에 접근할 수 있도록 보장하기 위해 인증서 기반 인증을 사용합니다."
        },
        "Correct Answer": "인증된 장치만 애플리케이션에 접근할 수 있도록 보장하기 위해 인증서 기반 인증을 사용합니다.",
        "Explanation": "인증서 기반 인증은 사용자 또는 장치의 신원을 확인하기 위해 디지털 인증서를 요구함으로써 강력한 보안 메커니즘을 제공합니다. 이 방법은 비밀번호 기반 방법에 비해 무단 접근의 위험을 크게 줄이며, 타협하기 어려운 암호화 기술에 의존합니다.",
        "Other Options": [
            "비밀번호 기반 인증은 사용자가 강력한 비밀번호 정책을 일관되게 따르지 않을 경우 피싱 및 무차별 대입 공격과 같은 공격에 취약합니다.",
            "역할 기반 인증은 권한 관리를 효과적으로 수행하지만, 인증서 기반 방법만큼 사용자 또는 장치의 신원을 안전하게 검증하지는 않습니다.",
            "다단계 인증은 보안을 강화하지만 여전히 비밀번호에 의존하므로 타협될 수 있습니다. 인증서 기반 인증은 비밀번호의 필요성을 완전히 없애므로 더 안전한 옵션입니다."
        ]
    },
    {
        "Question Number": "8",
        "Situation": "데이터 엔지니어는 AWS Glue DataBrew를 사용하여 데이터 준비 파이프라인에서 데이터 품질을 보장할 책임이 있습니다. 그들은 들어오는 데이터의 이상을 식별하고 수정하기 위한 규칙을 구현하고자 합니다. 엔지니어는 효과적인 데이터 품질 규칙을 설정하기 위해 무엇을 해야 합니까?",
        "Question": "데이터 엔지니어가 AWS Glue DataBrew에서 데이터 품질 규칙을 정의하기 위해 어떤 접근 방식을 취해야 합니까?",
        "Options": {
            "1": "데이터 수집 전에 데이터 품질을 검증하기 위해 사용자 정의 Python 스크립트를 생성합니다.",
            "2": "DataBrew의 내장 데이터 품질 메트릭 및 시각화를 활용합니다.",
            "3": "AWS Management Console을 사용하여 데이터를 수동으로 검사하고 변경 사항을 적용합니다.",
            "4": "데이터 품질 메트릭을 모니터링하기 위해 AWS Lambda 함수를 설정합니다."
        },
        "Correct Answer": "DataBrew의 내장 데이터 품질 메트릭 및 시각화를 활용합니다.",
        "Explanation": "AWS Glue DataBrew는 데이터 엔지니어가 데이터의 문제를 신속하게 식별하고 이를 효과적으로 해결하기 위한 규칙을 정의하는 데 도움이 되는 내장 데이터 품질 메트릭 및 시각화를 제공합니다. 이 옵션은 최적의 데이터 품질 관리를 위해 DataBrew의 기능을 직접 활용합니다.",
        "Other Options": [
            "사용자 정의 Python 스크립트를 생성하는 것은 불필요한 복잡성을 추가하며 DataBrew의 기존 기능과 원활하게 통합되지 않을 수 있습니다.",
            "데이터를 수동으로 검사하는 것은 시간이 많이 소요되며 대규모 데이터 세트에 대해 확장 가능하지 않아 데이터 품질을 보장하는 비효율적인 방법입니다.",
            "AWS Lambda 함수를 설정하는 것은 이벤트 기반 작업에 더 적합하며 DataBrew가 제공하는 직접적인 데이터 품질 기능을 제공하지 않습니다."
        ]
    },
    {
        "Question Number": "9",
        "Situation": "데이터 엔지니어는 실시간 스트리밍 애플리케이션을 위한 강력한 데이터 수집 파이프라인을 설계하는 임무를 맡고 있습니다. 이 애플리케이션은 들어오는 데이터를 여러 하위 서비스에 효율적으로 분배하여 처리할 수 있는 기능이 필요합니다. 엔지니어는 아키텍처가 다양한 작업 부하를 처리하고 높은 처리량을 유지할 수 있도록 해야 합니다.",
        "Question": "이 시나리오에서 스트리밍 데이터 분배를 위한 팬아웃을 관리하는 가장 좋은 접근 방식은 무엇입니까?",
        "Options": {
            "1": "여러 소비자가 데이터를 병렬로 처리할 수 있도록 Amazon Kinesis Data Streams를 활용합니다.",
            "2": "모든 하위 서비스가 데이터를 가져오기 위해 폴링할 단일 Amazon SQS 큐를 구현합니다.",
            "3": "여러 구독자에게 메시지를 방송하기 위해 Amazon SNS를 사용하여 데이터를 처리합니다.",
            "4": "도착하는 데이터를 각 하위 서비스에 직접 푸시하기 위해 AWS Lambda를 사용합니다."
        },
        "Correct Answer": "여러 소비자가 데이터를 병렬로 처리할 수 있도록 Amazon Kinesis Data Streams를 활용합니다.",
        "Explanation": "Amazon Kinesis Data Streams를 사용하면 여러 소비자가 동일한 스트림에서 동시에 읽을 수 있어 효율적인 팬아웃이 가능합니다. 이 아키텍처는 높은 처리량을 지원하며 다양한 작업 부하에 맞춰 확장할 수 있어 실시간 스트리밍 애플리케이션에 적합합니다.",
        "Other Options": [
            "단일 Amazon SQS 큐를 구현하는 것은 모든 하위 서비스가 동일한 큐를 폴링해야 하므로 진정한 팬아웃 기능을 제공하지 않으며, 이는 잠재적인 병목 현상과 지연을 초래할 수 있습니다.",
            "AWS Lambda를 사용하여 각 서비스에 직접 데이터를 푸시하는 것은 복잡성을 증가시키고 하위 서비스의 수가 시간이 지남에 따라 변경될 경우 확장성 문제를 초래할 수 있습니다.",
            "Amazon SNS를 사용하여 메시지를 방송하는 것은 팬아웃에 적합하지만 스트리밍 데이터를 효율적으로 처리하지 않으며, Kinesis는 일반적으로 지속적인 데이터 수집 및 처리를 위해 선호됩니다."
        ]
    },
    {
        "Question Number": "10",
        "Situation": "한 금융 서비스 회사가 사기 탐지를 위한 거래 모니터링을 위한 실시간 분석 플랫폼을 구축하고 있습니다. 이 플랫폼은 상태가 있는 데이터 거래와 상태가 없는 데이터 거래를 효율적으로 처리해야 합니다. 데이터 엔지니어링 팀은 거래 데이터의 수집 및 변환을 구현하기 위해 다양한 AWS 서비스를 평가하고 있습니다.",
        "Question": "상태가 있는 거래와 상태가 없는 거래 모두의 처리를 가장 잘 지원하는 접근 방식의 조합은 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "상태가 있는 상호작용을 처리하기 위해 Amazon DynamoDB Streams를 사용하고, 거래 데이터의 상태가 없는 처리를 위해 AWS Lambda를 활용합니다.",
            "2": "거래 데이터에 대한 상태가 있는 변환이 필요한 ETL 프로세스를 위해 Amazon Glue를 활용합니다.",
            "3": "거래 데이터의 실시간 수집을 위해 Amazon Kinesis Data Streams를 활용하고, 여러 레코드 간의 상태를 유지합니다.",
            "4": "거래 데이터의 메시지 큐잉을 위해 Amazon SQS를 사용하며, 이는 본질적으로 상태가 없는 거래만 지원합니다.",
            "5": "거래 데이터의 배치 처리를 위해 AWS Lambda 함수를 Amazon S3와 함께 구현하며, 각 함수 호출은 상태가 없습니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "거래 데이터의 실시간 수집을 위해 Amazon Kinesis Data Streams를 활용하고, 여러 레코드 간의 상태를 유지합니다.",
            "상태가 있는 상호작용을 처리하기 위해 Amazon DynamoDB Streams를 사용하고, 거래 데이터의 상태가 없는 처리를 위해 AWS Lambda를 활용합니다."
        ],
        "Explanation": "Amazon Kinesis Data Streams를 사용하면 데이터의 실시간 처리가 가능하며, 레코드 간의 상태를 추적할 수 있어 실시간 분석에 필수적입니다. 또한, Amazon DynamoDB Streams는 상태가 있는 데이터의 변화를 캡처하는 데 사용될 수 있으며, AWS Lambda는 이러한 변화를 상태가 없는 방식으로 처리할 수 있어 이 조합이 요구 사항에 이상적입니다.",
        "Other Options": [
            "AWS Lambda 함수를 Amazon S3와 함께 배치 처리하는 것은 실시간 분석에 최적이 아니며, S3는 일반적으로 배치 작업에 사용되며 호출 간 상태를 유지하지 않습니다.",
            "Amazon SQS를 사용하는 것은 주로 메시지 큐잉을 위한 것이며, 상태가 있는 거래를 지원하지 않으며, 상태가 없는 메시지 처리를 위해 설계되었습니다.",
            "Amazon Glue를 활용하는 것은 ETL 프로세스에 더 적합하며, 실시간 수집을 위해 설계되지 않아 즉각적인 거래 처리 요구 사항에 덜 효과적입니다."
        ]
    },
    {
        "Question Number": "11",
        "Situation": "한 소매 회사가 Amazon Redshift를 통한 분석, DynamoDB를 통한 빠른 NoSQL 데이터 접근, AWS Lake Formation을 통한 데이터 레이크 관리의 조합을 포함하는 새로운 데이터 아키텍처를 구현할 계획입니다. 회사는 성능과 비용 효율성을 최적화하기 위해 이러한 데이터 저장소의 스키마를 설계할 전략이 필요합니다.",
        "Question": "회사가 Amazon Redshift, DynamoDB 및 AWS Lake Formation의 스키마를 설계할 때 최적의 성능과 통합을 보장하기 위해 어떤 접근 방식을 취해야 합니까?",
        "Options": {
            "1": "Redshift에 대해 정규화된 스키마를 만들고, DynamoDB에 대해 관계형 모델을 사용하며, Lake Formation에 대해 그리드 구조를 사용합니다.",
            "2": "Redshift에 대해 스노우플레이크 스키마를 구현하고, DynamoDB에 대해 문서 기반 모델을 사용하며, Lake Formation에 대해 파티션 테이블을 사용합니다.",
            "3": "Redshift에 대해 비정규화된 스키마를 설계하고, DynamoDB에 대해 와이드 컬럼 저장소 모델을 사용하며, Lake Formation에 대해 계층 구조를 사용합니다.",
            "4": "Redshift에 대해 스타 스키마를 사용하고, DynamoDB에 대해 키-값 쌍 구조를 사용하며, Lake Formation에 대해 평면 파일 구조를 사용합니다."
        },
        "Correct Answer": "Redshift에 대해 스타 스키마를 사용하고, DynamoDB에 대해 키-값 쌍 구조를 사용하며, Lake Formation에 대해 평면 파일 구조를 사용합니다.",
        "Explanation": "이 접근 방식은 데이터를 스타 스키마로 구성하여 Redshift의 성능을 극대화하며, 이는 분석 쿼리에 효과적입니다. DynamoDB에서 키-값 쌍 구조를 사용하면 데이터에 빠르게 접근할 수 있으며, Lake Formation에서 평면 파일을 활용하면 다양한 데이터 형식에 대한 유연성과 원시 데이터에 대한 쉬운 접근을 제공합니다.",
        "Other Options": [
            "Redshift에 대한 스노우플레이크 스키마는 조인을 복잡하게 하고 분석 쿼리의 성능을 저하시킬 수 있습니다. DynamoDB에 대한 문서 기반 모델은 일반적으로 키-값 접근을 지원하므로 최적이 아닙니다.",
            "Redshift의 비정규화된 스키마는 데이터 중복과 저장 비용 증가를 초래할 수 있어 분석 처리에 덜 효율적입니다. DynamoDB에 대한 와이드 컬럼 저장소 모델은 키-값 접근을 위해 설계되었으므로 적합하지 않습니다.",
            "Redshift에 대한 정규화된 스키마는 복잡한 조인과 느린 쿼리로 이어질 수 있으므로 권장되지 않습니다. 또한, DynamoDB에 대한 관계형 모델은 주로 NoSQL 작업을 위해 설계되었으므로 적합하지 않으며, Lake Formation의 그리드 구조는 전통적인 데이터 레이크 설계와 일치하지 않습니다."
        ]
    },
    {
        "Question Number": "12",
        "Situation": "한 금융 서비스 회사가 규정 준수 및 감사 목적으로 애플리케이션 데이터를 기록해야 합니다. 그들은 신뢰할 수 있는 로그 저장소를 제공하면서 나중에 로그를 쉽게 분석할 수 있는 솔루션을 구현하고자 합니다.",
        "Question": "최소한의 관리 오버헤드로 로그 데이터를 저장하고 분석하기 위한 서버리스 솔루션을 제공하는 AWS 서비스는 무엇입니까?",
        "Options": {
            "1": "로그 저장을 위한 Amazon RDS와 읽기 복제본",
            "2": "로그 처리를 위한 Amazon CloudWatch Logs와 Lambda",
            "3": "로그 분석을 위한 Amazon DynamoDB와 Streams",
            "4": "분석을 위한 Amazon S3와 Amazon Athena"
        },
        "Correct Answer": "로그 처리를 위한 Amazon CloudWatch Logs와 Lambda",
        "Explanation": "Amazon CloudWatch Logs는 로그 저장 및 관리를 위해 설계되었으며, AWS Lambda와 직접 통합되어 실시간으로 로그를 처리할 수 있어 효율적이고 서버리스 솔루션입니다.",
        "Other Options": [
            "Amazon S3는 훌륭한 저장 솔루션이지만 쿼리를 위해 Amazon Athena와 같은 추가 서비스가 필요하므로 아키텍처에 복잡성을 더합니다.",
            "Amazon RDS는 주로 관계형 데이터베이스 서비스이며, 로그를 저장할 수 있지만 로그 전용으로 설계되지 않았으므로 더 많은 관리 및 확장 고려가 필요합니다.",
            "Amazon DynamoDB는 NoSQL 데이터베이스로 로그 데이터에 사용할 수 있지만, 분석을 위한 Streams 사용은 불필요한 복잡성을 추가하며 로그 관리에 직접적으로 적합하지 않습니다."
        ]
    },
    {
        "Question Number": "13",
        "Situation": "한 회사가 AWS Glue를 사용하여 ETL 프로세스를 관리하고 있으며, AWS Glue Data Catalog를 통해 데이터 소스의 메타데이터를 유지하고 있습니다. 데이터 엔지니어는 카탈로그가 기본 데이터 소스에서 발생하는 최신 스키마 변경 사항으로 항상 최신 상태를 유지하도록 하고 싶어합니다. 그들은 이러한 스키마 변경 사항을 Glue Data Catalog에 자동으로 반영하는 솔루션을 구현해야 합니다.",
        "Question": "기본 데이터 소스의 스키마 변경 사항이 AWS Glue Data Catalog에 자동으로 반영되도록 보장하기 위한 가장 효과적인 솔루션은 무엇입니까?",
        "Options": {
            "1": "AWS CloudFormation을 사용하여 Glue Data Catalog 스키마를 관리합니다.",
            "2": "정기적으로 AWS Glue 크롤러를 실행하도록 예약된 Lambda 함수를 구성합니다.",
            "3": "매 데이터 로드 후 AWS Glue API를 수동으로 호출하여 스키마를 업데이트합니다.",
            "4": "기존 테이블을 업데이트하는 옵션으로 AWS Glue 크롤러를 설정합니다."
        },
        "Correct Answer": "기존 테이블을 업데이트하는 옵션으로 AWS Glue 크롤러를 설정합니다.",
        "Explanation": "기존 테이블을 업데이트하는 옵션으로 AWS Glue 크롤러를 설정하면 크롤러가 기본 데이터 소스의 스키마 변경 사항을 자동으로 감지하고 Glue Data Catalog에 적용할 수 있습니다. 이를 통해 수동 개입 없이 메타데이터가 항상 최신 상태로 유지됩니다.",
        "Other Options": [
            "매 데이터 로드 후 AWS Glue API를 수동으로 호출하여 스키마를 업데이트하는 것은 비효율적이며 인적 오류가 발생할 수 있습니다. 이는 지속적인 모니터링과 수동 업데이트가 필요하므로 최신 카탈로그를 유지하는 데 이상적이지 않습니다.",
            "정기적으로 AWS Glue 크롤러를 실행하도록 예약된 Lambda 함수를 구성하는 것은 발생하는 즉시 스키마 변경 사항을 포착하지 못할 수 있습니다. 이는 실제 변경과 카탈로그 업데이트 간의 지연을 초래하여 불일치를 초래할 수 있습니다.",
            "AWS CloudFormation을 사용하여 Glue Data Catalog 스키마를 관리하는 것은 동적 스키마 변경에 적합하지 않습니다. CloudFormation은 코드로서의 인프라를 위해 설계되었으며 데이터 변경에 따른 지속적인 업데이트를 위한 것이 아닙니다."
        ]
    },
    {
        "Question Number": "14",
        "Situation": "한 회사가 Amazon S3를 사용하여 중요한 비즈니스 데이터를 저장하고 DynamoDB를 사용하여 사용자 세션 상태를 관리하고 있습니다. 데이터 엔지니어는 데이터 무결성을 보장하고 데이터 수명 주기를 효율적으로 관리하는 임무를 맡고 있습니다. 회사는 S3 객체에 대한 버전 관리와 DynamoDB의 오래된 항목에 대한 자동 만료를 요구합니다.",
        "Question": "데이터 엔지니어가 데이터 관리를 향상시키기 위해 구현해야 할 행동 조합은 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "스토리지 비용을 줄이기 위해 S3 버킷에서 버전 관리를 비활성화합니다.",
            "2": "DynamoDB 항목에 TTL 속성을 설정하여 지정된 시간 후에 자동으로 만료되도록 합니다.",
            "3": "TTL 없이 항목의 변경 사항을 추적하기 위해 DynamoDB Streams를 사용합니다.",
            "4": "스토리지를 관리하기 위해 S3 객체의 이전 버전을 수동으로 삭제합니다.",
            "5": "모든 객체 버전을 유지하기 위해 S3 버킷에서 버전 관리를 활성화합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "모든 객체 버전을 유지하기 위해 S3 버킷에서 버전 관리를 활성화합니다.",
            "DynamoDB 항목에 TTL 속성을 설정하여 지정된 시간 후에 자동으로 만료되도록 합니다."
        ],
        "Explanation": "S3 버킷에서 버전 관리를 활성화하면 회사가 모든 객체 버전을 유지할 수 있어 데이터 복구 및 감사에 필수적입니다. DynamoDB 항목에 TTL 속성을 설정하면 구식 항목의 자동 만료가 가능해져 스토리지를 관리하고 비용을 효율적으로 줄이는 데 도움이 됩니다.",
        "Other Options": [
            "S3 버킷에서 버전 관리를 비활성화하는 것은 이전 버전을 복구할 수 있는 능력을 없애므로 비효율적입니다. 이는 데이터 손실로 이어질 수 있습니다.",
            "S3 객체의 이전 버전을 수동으로 삭제하는 것은 비효율적이며 노동 집약적입니다. 이는 자동화된 데이터 관리를 제공하는 버전 관리의 이점을 활용하지 못합니다.",
            "DynamoDB Streams를 사용하는 것은 항목의 변경 사항만 추적하며 항목의 수명 주기를 관리하거나 자동 만료를 지원하지 않으므로 효과적인 데이터 관리에 필요합니다."
        ]
    },
    {
        "Question Number": "15",
        "Situation": "데이터 엔지니어가 Amazon S3에 저장된 대규모 데이터 세트를 쿼리해야 하는 분석 프로젝트에 참여하고 있습니다. 팀은 Amazon Athena를 사용하여 즉석 쿼리를 수행하고 향후 분석에 재사용할 수 있는 뷰를 생성하고자 합니다. 데이터 엔지니어는 생성된 뷰가 성능과 비용 효율성을 최적화하도록 해야 합니다.",
        "Question": "데이터 엔지니어가 Amazon Athena를 사용하여 데이터를 쿼리하고 뷰를 생성할 때 최적의 성능을 보장하기 위해 따라야 할 관행은 무엇입니까?",
        "Options": {
            "1": "최적화를 고려하지 않고 Athena에서 테이블 생성을 위한 기본 설정을 활용합니다.",
            "2": "CSV 파일을 소스 데이터로 사용하여 Amazon Athena를 사용하고 데이터를 파티셔닝하지 않고 뷰를 생성합니다.",
            "3": "데이터를 JSON 형식으로 저장하고 이러한 파일 위에 직접 뷰를 생성하여 쿼리를 쉽게 합니다.",
            "4": "모든 데이터 파일을 Parquet 형식으로 변환하여 쿼리 비용을 줄이고 성능을 개선합니다."
        },
        "Correct Answer": "모든 데이터 파일을 Parquet 형식으로 변환하여 쿼리 비용을 줄이고 성능을 개선합니다.",
        "Explanation": "데이터 파일을 Parquet 형식으로 변환하는 것은 Amazon Athena를 사용할 때 모범 사례로, Parquet는 더 나은 압축과 더 빠른 쿼리 성능을 제공하는 열 저장 형식으로, 데이터 검색 시 비용 절감과 효율성을 개선합니다.",
        "Other Options": [
            "데이터를 JSON 형식으로 저장하면 비효율적인 쿼리와 높은 비용이 발생할 수 있으며, JSON의 비구조적 특성으로 인해 Athena에서 Parquet와 같은 열 형식보다 덜 최적화됩니다.",
            "파티셔닝 없이 CSV 파일을 사용하는 것은 쿼리 성능 저하와 높은 비용을 초래할 수 있습니다. Athena는 스캔된 데이터 양에 따라 요금을 부과하며, CSV 파일은 열 형식보다 쿼리에 덜 효율적입니다.",
            "최적화 없이 기본 설정을 활용하는 것은 파티셔닝 및 효율적인 파일 형식 선택과 같은 잠재적인 성능 개선을 간과하여 쿼리 속도와 비용에 상당한 영향을 미칠 수 있습니다."
        ]
    },
    {
        "Question Number": "16",
        "Situation": "데이터 엔지니어가 Amazon Athena를 사용하여 Amazon S3에 저장된 대규모 데이터 세트를 분석하고 있습니다. 팀은 데이터 보안과 데이터에 대한 쿼리 실행과 관련된 비용에 대해 우려하고 있습니다. 그들은 민감한 데이터에 대한 접근이 통제되면서 쿼리 비용을 최적화하고자 합니다.",
        "Question": "Amazon Athena를 사용할 때 데이터 보안을 강화하고 비용을 최소화하는 데 가장 적합한 전략은 무엇입니까?",
        "Options": {
            "1": "S3 버킷 정책을 사용하고 비최적화된 쿼리를 실행합니다.",
            "2": "액세스 제어 목록을 만들고 비압축 데이터 형식을 사용합니다.",
            "3": "IAM 정책을 적용하고 S3에 저장된 데이터를 압축합니다.",
            "4": "암호화를 구현하고 데이터를 파티셔닝하지 않습니다."
        },
        "Correct Answer": "IAM 정책을 적용하고 S3에 저장된 데이터를 압축합니다.",
        "Explanation": "IAM 정책을 적용하면 권한이 있는 사용자만 민감한 데이터에 접근할 수 있으며, 데이터를 압축하면 쿼리 중에 스캔되는 데이터 양이 줄어들어 비용이 낮아지고 성능이 향상됩니다.",
        "Other Options": [
            "데이터를 압축하지 않고 액세스 제어 목록을 생성하면 스캔되는 데이터 양이 줄어들지 않아 비용이 증가할 수 있습니다.",
            "S3 버킷 정책을 사용하고 비최적화된 쿼리를 실행하면 보안이 강화되지 않으며, 더 많은 데이터가 스캔되어 비용이 증가할 가능성이 높습니다.",
            "암호화를 구현하는 것은 중요하지만, 데이터를 파티셔닝하지 않으면 불필요하게 더 큰 데이터 세트를 스캔하게 되어 비용이 증가할 수 있습니다."
        ]
    },
    {
        "Question Number": "17",
        "Situation": "한 회사가 애플리케이션을 AWS로 마이그레이션하고 있으며 사용자 접근 관리를 위한 IAM 정책을 구현하는 과정에 있습니다. 보안 팀은 사용할 정책 유형을 검토하고 효과적인 거버넌스를 위해 AWS 관리형 정책과 고객 관리형 정책 간의 차이를 이해할 필요가 있습니다.",
        "Question": "AWS 관리형 정책과 고객 관리형 정책 간의 주요 차이점은 무엇입니까?",
        "Options": {
            "1": "AWS 관리형 정책은 수정할 수 없지만 고객 관리형 정책은 언제든지 편집할 수 있습니다.",
            "2": "AWS 관리형 정책은 특정 서비스에 대해 고객 관리형 정책보다 더 자세한 권한을 제공합니다.",
            "3": "AWS 관리형 정책은 IAM 역할에만 연결할 수 있지만, 고객 관리형 정책은 역할과 사용자 모두에 연결할 수 있습니다.",
            "4": "AWS 관리형 정책은 AWS에 의해 생성되고 유지 관리되며, 고객 관리형 정책은 사용자가 생성하고 유지 관리합니다."
        },
        "Correct Answer": "AWS 관리형 정책은 AWS에 의해 생성되고 유지 관리되며, 고객 관리형 정책은 사용자가 생성하고 유지 관리합니다.",
        "Explanation": "AWS 관리형 정책은 AWS에 의해 생성되고 유지 관리되는 미리 정의된 정책으로, 여러 사용자 또는 역할에 대한 권한 관리를 쉽게 할 수 있습니다. 고객 관리형 정책은 IAM 사용자가 특정 요구에 맞게 생성하는 사용자 정의 정책으로, 권한 정의에 있어 더 큰 유연성을 제공합니다.",
        "Other Options": [
            "AWS 관리형 정책은 IAM 역할과 사용자 모두에 연결할 수 있으며, 단지 역할에만 연결할 수 있는 것은 아닙니다. 따라서 이 옵션은 관리형 정책의 연결 기능을 잘못 표현하고 있어 부정확합니다.",
            "AWS 관리형 정책은 권한 관리를 단순화하기 위해 설계되었으며, 고객 관리형 정책보다 더 자세한 권한을 본질적으로 제공하지 않습니다. 고객 관리형 정책은 필요에 따라 매우 자세할 수 있습니다. 따라서 이 옵션은 부정확합니다.",
            "AWS 관리형 정책은 직접 수정할 수 없지만, 고객 관리형 정책은 편집할 수 있습니다. 이 옵션은 AWS 관리형 정책이 변경 가능하다는 잘못된 인상을 주기 때문에 오해의 소지가 있습니다."
        ]
    },
    {
        "Question Number": "18",
        "Situation": "데이터 엔지니어링 팀은 Amazon S3에서 민감한 데이터를 관리하는 임무를 맡고 있으며, 오직 권한이 있는 사용자와 서비스만 접근할 수 있도록 보장해야 합니다. 그들은 IAM 정책을 사용하여 권한을 효과적으로 관리하기 위해 세분화된 접근 제어를 구현하고자 합니다. 팀은 또한 S3에 저장된 다양한 데이터 세트에 대한 안전한 접근을 촉진하기 위해 특정 접근 지점을 설정해야 합니다.",
        "Question": "여러 접근 지점을 지원하면서 S3 데이터에 대한 접근을 제어하기 위해 IAM 정책을 구현하는 데 가장 좋은 솔루션은 무엇입니까?",
        "Options": {
            "1": "특정 S3 접근 지점에 대한 접근을 허용하는 정책이 포함된 IAM 역할을 생성하고 필요에 따라 이러한 역할을 사용자와 서비스에 할당합니다.",
            "2": "모든 사용자와 서비스에 대한 접근을 관리하기 위해 S3 버킷에 연결된 단일 IAM 정책을 사용하여 그들이 S3 버킷을 통해서만 데이터에 접근하도록 합니다.",
            "3": "모든 사용자에게 공개 접근을 허용하는 S3 버킷에 대한 리소스 기반 정책을 설정하되, 특정 IP 주소에 대한 접근은 제한합니다.",
            "4": "S3 버킷에 대한 전체 접근을 부여하지만 수행하는 모든 작업에 대해 다단계 인증을 요구하는 IAM 사용자 정책을 구현합니다."
        },
        "Correct Answer": "특정 S3 접근 지점에 대한 접근을 허용하는 정책이 포함된 IAM 역할을 생성하고 필요에 따라 이러한 역할을 사용자와 서비스에 할당합니다.",
        "Explanation": "이 옵션은 특정 접근 지점에 맞춤화된 IAM 역할과 정책을 활용하여 S3 데이터에 대한 세분화된 접근 제어를 가능하게 합니다. 이를 통해 권한이 있는 엔티티만 지정된 데이터 세트에 접근할 수 있도록 보장하여 데이터 보안 및 거버넌스의 모범 사례에 부합합니다.",
        "Other Options": [
            "이 옵션은 세분화된 접근 제어에 필요한 세부 사항이 부족합니다. 모든 사용자에 대한 단일 정책은 민감한 데이터에 대한 무단 접근을 초래할 수 있습니다.",
            "공개 접근을 허용하는 리소스 기반 정책은 보안 요구와 반대됩니다. 공개 접근을 허용하면 무단 데이터 노출의 위험이 증가합니다.",
            "이 옵션은 다단계 인증을 통해 보안을 추가하지만, 접근 지점을 통한 특정 S3 데이터 세트에 대한 세분화된 접근 제어 요구 사항을 해결하지 않습니다."
        ]
    },
    {
        "Question Number": "19",
        "Situation": "금융 서비스 회사는 Amazon S3에 저장된 민감한 고객 데이터에 대한 데이터 보존 정책을 구현해야 합니다. 그들은 규제 요구 사항을 준수하면서 저장 비용을 최소화하고자 합니다. 데이터는 7년 동안 보존되어야 하며, 그 이후에는 저비용 저장 솔루션으로 아카이브되어야 합니다.",
        "Question": "회사의 요구 사항에 맞춰 데이터 보존 및 아카이빙을 관리하는 최상의 접근 방식은 무엇입니까?",
        "Options": {
            "1": "Amazon RDS를 사용하여 데이터를 저장하고 자동 백업을 설정하여 7년 동안 백업을 보존한 후 삭제합니다.",
            "2": "AWS Backup 계획을 구현하여 데이터를 Amazon S3에 백업하고 7년 후 백업을 삭제합니다.",
            "3": "매년 데이터를 Amazon EBS 볼륨에 수동으로 복사하고 7년 동안 볼륨을 보존한 후 삭제합니다.",
            "4": "S3 Lifecycle 정책을 설정하여 7년 후 데이터를 S3 Glacier로 전환하고 Glacier 금고를 장기 저장을 위해 구성합니다."
        },
        "Correct Answer": "S3 Lifecycle 정책을 설정하여 7년 후 데이터를 S3 Glacier로 전환하고 Glacier 금고를 장기 저장을 위해 구성합니다.",
        "Explanation": "S3 Lifecycle 정책을 사용하면 지정된 보존 기간 후에 데이터를 S3 Glacier와 같은 저비용 저장 클래스로 자동 전환할 수 있습니다. 이는 데이터 보존에 대한 규정 준수 요구 사항을 충족하면서 저장 비용을 효과적으로 최소화합니다.",
        "Other Options": [
            "Amazon EBS 볼륨에 데이터를 수동으로 복사하는 것은 비효율적이며 확장성이 없으며, 지속적인 수동 개입이 필요하고 S3의 내장된 라이프사이클 관리 기능을 활용하지 않습니다.",
            "이 목적을 위해 Amazon RDS를 사용하는 것은 불필요한 복잡성과 비용을 초래합니다. RDS는 S3와 같은 장기 데이터 보존을 위해 설계되지 않았으며, 자동 백업은 비용 효율적인 아카이빙 옵션을 제공하지 않습니다.",
            "AWS Backup은 AWS 리소스를 백업하기 위한 서비스이지만, S3에 백업하고 7년 후 삭제하는 것은 S3의 라이프사이클 관리 기능을 활용하지 않으며, 가장 비용 효율적인 장기 솔루션이 아닐 수 있습니다."
        ]
    },
    {
        "Question Number": "20",
        "Situation": "데이터 엔지니어링 팀은 여러 데이터 소스, 특히 관계형 데이터베이스에 연결되는 데이터 수집 파이프라인의 컨테이너 사용을 최적화하는 임무를 맡고 있습니다. 그들은 선택한 컨테이너 오케스트레이션 플랫폼이 들어오는 데이터 부하에 따라 효율적으로 확장할 수 있고, 변환 목적으로 데이터 소스에 원활하게 연결될 수 있기를 원합니다.",
        "Question": "다음 옵션 중 성능 요구 사항을 최적화하면서 데이터 소스에 대한 연결성을 보장하는 데 가장 적합한 것은 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "컨테이너화된 애플리케이션에서 다양한 데이터 소스에 연결하기 위해 ODBC를 활용합니다.",
            "2": "서버리스 컨테이너를 위해 Fargate 실행 유형으로 Amazon Elastic Container Service (Amazon ECS)를 구현합니다.",
            "3": "데이터 수집을 처리하기 위해 Amazon EKS 내에 AWS Lambda 함수를 배포합니다.",
            "4": "컨테이너 내에서 관계형 데이터베이스에 연결하기 위해 JDBC를 활용합니다.",
            "5": "컨테이너 오케스트레이션을 위해 Amazon Elastic Kubernetes Service (Amazon EKS)를 사용합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "컨테이너 오케스트레이션을 위해 Amazon Elastic Kubernetes Service (Amazon EKS)를 사용합니다.",
            "서버리스 컨테이너를 위해 Fargate 실행 유형으로 Amazon Elastic Container Service (Amazon ECS)를 구현합니다."
        ],
        "Explanation": "Amazon EKS를 사용하면 고급 오케스트레이션 기능과 복잡한 마이크로서비스 관리를 개선할 수 있으며, Amazon ECS와 Fargate는 서버 인프라를 관리할 필요 없이 컨테이너가 자동으로 확장될 수 있도록 보장합니다. 이는 데이터 수집 시나리오에서 성능 최적화에 매우 중요합니다.",
        "Other Options": [
            "JDBC는 관계형 데이터베이스에 연결하는 일반적인 방법이지만, 컨테이너 오케스트레이션이나 성능 최적화를 직접적으로 다루지 않으므로 질문의 초점과 관련성이 떨어집니다.",
            "ODBC는 다양한 데이터 소스에 연결하는 데 유용하지만, JDBC와 마찬가지로 컨테이너 사용이나 오케스트레이션을 최적화하지 않으므로 시나리오의 주요 관심사와는 관련이 없습니다.",
            "Amazon EKS 내에 AWS Lambda 함수를 배포하는 것은 표준 관행이 아닙니다. 일반적으로 Lambda는 서버리스 기능을 위해 독립적으로 사용되며, EKS 컨텍스트 내에서 사용하는 것은 컨테이너 성능이나 확장성을 향상시키지 않습니다."
        ]
    },
    {
        "Question Number": "21",
        "Situation": "소매 회사는 Amazon SageMaker를 사용하여 Amazon S3에 저장된 고객 구매 데이터를 기반으로 머신 러닝 모델을 훈련합니다. 데이터 과학 팀은 모델 훈련에 사용된 데이터의 계보를 추적하고 데이터 거버넌스 정책을 준수해야 합니다.",
        "Question": "데이터 과학 팀이 머신 러닝 모델의 데이터 계보를 설정하는 데 가장 도움이 되는 AWS 서비스는 무엇입니까?",
        "Options": {
            "1": "AWS Data Pipeline을 활용하여 데이터 흐름을 관리하고 데이터 세트에 적용된 변환을 기록합니다.",
            "2": "Amazon SageMaker ML Lineage Tracking을 사용하여 모델 훈련 과정 전반에 걸쳐 데이터 계보를 캡처하고 시각화합니다.",
            "3": "Amazon Athena를 활용하여 S3 데이터를 쿼리하고 시간에 따른 데이터 사용에 대한 보고서를 생성합니다.",
            "4": "AWS CloudTrail을 구현하여 데이터 접근 및 S3 버킷의 변경과 관련된 API 호출을 모니터링합니다."
        },
        "Correct Answer": "Amazon SageMaker ML Lineage Tracking을 사용하여 모델 훈련 과정 전반에 걸쳐 데이터 계보를 캡처하고 시각화합니다.",
        "Explanation": "Amazon SageMaker ML Lineage Tracking은 머신 러닝 워크플로우에서 데이터 계보를 추적하기 위해 특별히 설계되었습니다. 사용자는 훈련 작업에 대한 메타데이터를 캡처할 수 있으며, 여기에는 사용된 데이터 세트, 변환 및 생성된 모델이 포함되어 데이터 거버넌스 및 준수를 위한 포괄적인 데이터 계보를 제공합니다.",
        "Other Options": [
            "AWS CloudTrail은 주로 AWS 계정 활동 및 API 호출을 모니터링하고 기록하는 데 중점을 두지만, 머신 러닝 워크플로우에서 데이터 계보 추적을 구체적으로 다루지 않습니다.",
            "AWS Data Pipeline은 데이터 워크플로우를 조정하는 데 유용하지만, 머신 러닝 모델에 특정한 계보 추적 기능을 본질적으로 제공하지 않습니다. 데이터 이동 및 변환에 더 중점을 둡니다.",
            "Amazon Athena는 SQL을 사용하여 S3의 데이터를 분석할 수 있는 쿼리 서비스이지만, 계보 추적 기능을 제공하지 않습니다. 모델 훈련에 사용된 데이터의 관계 및 변환을 캡처하는 데 설계되지 않았습니다."
        ]
    },
    {
        "Question Number": "22",
        "Situation": "데이터 엔지니어링 팀은 Amazon S3에 저장된 JSON 파일, Amazon RDS의 관계형 데이터, Amazon DynamoDB의 반구조적 데이터를 포함한 다양한 데이터 소스를 관리하는 임무를 맡고 있습니다. 팀은 데이터 발견 및 분석 접근을 용이하게 하기 위해 포괄적인 데이터 카탈로그를 생성해야 합니다. 이들은 이 프로세스를 자동화하기 위해 AWS Glue를 사용하는 것을 고려하고 있습니다.",
        "Question": "팀이 데이터 소스의 스키마를 발견하고 AWS Glue 데이터 카탈로그를 효율적으로 채우기 위한 최선의 접근 방식은 무엇입니까?",
        "Options": {
            "1": "크롤러를 사용하지 않고 AWS Glue 데이터 카탈로그에서 각 데이터 소스의 스키마를 수동으로 정의합니다.",
            "2": "S3와 RDS에서 데이터를 읽고 스키마를 추론하여 AWS Glue 데이터 카탈로그에 기록하는 맞춤형 애플리케이션을 개발합니다.",
            "3": "AWS Lambda 함수를 예약하여 RDS와 DynamoDB에서 주기적으로 데이터를 추출한 다음, 추출된 데이터를 사용하여 AWS Glue 데이터 카탈로그를 생성하고 업데이트합니다.",
            "4": "AWS Glue 크롤러를 사용하여 S3, RDS 및 DynamoDB의 데이터 스키마를 자동으로 발견하고 AWS Glue 데이터 카탈로그를 채웁니다."
        },
        "Correct Answer": "AWS Glue 크롤러를 사용하여 S3, RDS 및 DynamoDB의 데이터 스키마를 자동으로 발견하고 AWS Glue 데이터 카탈로그를 채웁니다.",
        "Explanation": "AWS Glue 크롤러는 다양한 데이터 소스의 스키마를 발견하고 AWS Glue 데이터 카탈로그를 채우는 프로세스를 자동화하도록 특별히 설계되었습니다. 이 방법은 수작업 노력을 줄이고 카탈로그가 최신 스키마 변경 사항으로 업데이트되도록 보장합니다.",
        "Other Options": [
            "이 옵션은 각 데이터 소스의 스키마를 수동으로 정의하는 것이 시간이 많이 걸리고 오류가 발생하기 쉬우므로 잘못된 것입니다. 특히 여러 소스를 다루고 잠재적인 스키마 변경이 있을 때 더욱 그렇습니다.",
            "이 옵션은 맞춤형 애플리케이션을 개발하는 데 상당한 개발 및 유지 관리 노력이 필요하므로 잘못된 것입니다. 이는 카탈로그화에 지연을 초래할 수 있으며 AWS Glue 크롤러를 사용하는 것만큼 효율적이지 않을 수 있습니다.",
            "이 옵션은 AWS Lambda 함수를 예약하여 데이터를 추출하는 것이 본질적으로 스키마를 발견하거나 정의하지 않기 때문에 잘못된 것입니다. 이는 불필요한 복잡성을 추가하고 AWS Glue 크롤러의 자동화 기능을 활용하지 않습니다."
        ]
    },
    {
        "Question Number": "23",
        "Situation": "데이터 엔지니어는 데이터 레이크에 수집되는 데이터의 품질을 보장하는 임무를 맡고 있습니다. 엔지니어는 데이터 세트의 무결성을 유지하기 위해 들어오는 데이터의 완전성과 정확성을 검증하는 솔루션을 구현해야 합니다.",
        "Question": "데이터 엔지니어가 들어오는 데이터에 대해 완전성, 일관성, 정확성 및 무결성을 보장하기 위해 데이터 검증 검사를 수행하는 데 사용할 수 있는 AWS 서비스는 무엇입니까?",
        "Options": {
            "1": "Amazon Glue DataBrew",
            "2": "Amazon Redshift Spectrum",
            "3": "Amazon QuickSight",
            "4": "AWS Glue ETL"
        },
        "Correct Answer": "Amazon Glue DataBrew",
        "Explanation": "Amazon Glue DataBrew는 사용자가 코드를 작성하지 않고도 데이터를 정리하고 정규화할 수 있는 데이터 준비 서비스입니다. 데이터 검증 기능이 포함되어 있어 데이터 수집 과정에서 데이터의 완전성, 일관성, 정확성 및 무결성을 보장하는 데 적합합니다.",
        "Other Options": [
            "AWS Glue ETL은 주로 데이터를 추출, 변환 및 로드하는 데 중점을 두고 있습니다. 일부 데이터 품질 검사를 수행할 수 있지만 DataBrew와 같은 광범위한 데이터 검증 작업을 위해 특별히 설계된 것은 아닙니다.",
            "Amazon QuickSight는 데이터 시각화 및 보고 기능을 제공하는 비즈니스 인텔리전스 서비스입니다. 데이터 수집 중 무결성이나 청결성을 검증하는 기능을 제공하지 않습니다.",
            "Amazon Redshift Spectrum은 SQL을 사용하여 S3의 데이터를 쿼리할 수 있지만 데이터 로드 과정에서 완전성 및 정확성과 같은 데이터 품질 속성을 검증하는 기능이 내장되어 있지 않습니다."
        ]
    },
    {
        "Question Number": "24",
        "Situation": "의료 제공자는 HIPAA 규정을 준수하여 민감한 환자 데이터를 처리하고 있습니다. 제공자는 관계형 데이터 저장을 위해 Amazon RDS를 사용하고 데이터 레이크 저장을 위해 Amazon S3를 사용합니다. 규정 준수를 보장하고 환자 프라이버시를 보호하기 위해 조직은 제3자 분석 팀과 데이터를 공유하기 전에 민감한 필드에 데이터 마스킹 및 익명화 기술을 구현해야 합니다.",
        "Question": "데이터 엔지니어링 팀이 HIPAA 규정을 준수하기 위해 민감한 환자 데이터에 데이터 마스킹 및 익명화를 적용하는 데 사용할 수 있는 AWS 서비스는 무엇입니까?",
        "Options": {
            "1": "AWS Lake Formation을 활용하여 데이터 접근을 관리하고 민감한 데이터에 대한 세분화된 데이터 접근 제어를 적용합니다.",
            "2": "Amazon Macie를 구현하여 Amazon S3 및 RDS에서 민감한 데이터를 발견, 분류 및 보호합니다.",
            "3": "AWS Lambda를 사용하여 데이터 검색 중 데이터 마스킹 및 익명화를 위한 맞춤형 함수를 생성합니다.",
            "4": "AWS Glue DataBrew를 활용하여 데이터 처리 파이프라인 내에서 민감한 데이터를 변환하고 마스킹합니다."
        },
        "Correct Answer": "AWS Glue DataBrew를 활용하여 데이터 처리 파이프라인 내에서 민감한 데이터를 변환하고 마스킹합니다.",
        "Explanation": "AWS Glue DataBrew는 데이터를 변환하고 마스킹할 수 있는 사용자 친화적인 인터페이스를 제공하여 HIPAA와 같은 데이터 프라이버시 규정을 준수해야 하는 데이터 준비 작업에 적합합니다. 데이터 공유 전에 민감한 필드가 마스킹되도록 ETL 워크플로우에 통합할 수 있습니다.",
        "Other Options": [
            "Amazon Macie는 주로 데이터 발견 및 분류에 중점을 두고 있지만 데이터 마스킹이나 변환 기능을 직접 제공하지 않으므로 이 특정 요구 사항에 적합하지 않습니다.",
            "AWS Lambda는 다양한 작업을 위한 맞춤형 함수를 생성하는 데 사용할 수 있지만 데이터 마스킹을 위한 내장 기능을 제공하지 않으며 필요한 논리를 구현하기 위해 추가 개발 노력이 필요합니다.",
            "AWS Lake Formation은 데이터 거버넌스 및 관리, 접근 제어 설정 등을 위해 주로 사용되지만 민감한 데이터의 변환이나 마스킹을 직접 처리하지 않습니다."
        ]
    },
    {
        "Question Number": "25",
        "Situation": "한 금융 서비스 회사가 시스템 건강 및 성능 메트릭을 모니터링하기 위해 Amazon CloudWatch를 사용하고 있습니다. CPU 사용률이 80%를 초과할 때 적시에 알림을 받을 수 있도록 보장하고 싶어합니다. 이 회사는 이 시나리오에 대한 알림 설정 방법을 고려하고 있습니다.",
        "Question": "CPU 사용률이 80%를 초과할 때 알림을 받기 위해 회사가 어떤 방법을 사용해야 합니까?",
        "Options": {
            "1": "CPU 사용률이 80%를 초과할 때 SNS 알림을 트리거하는 CloudWatch 경고를 생성합니다.",
            "2": "알림 없이 CPU 사용률을 시각화하는 대시보드를 생성하는 CloudFormation 스택을 구현합니다.",
            "3": "CloudTrail을 사용하여 CPU 사용률 이벤트를 기록하고 80%를 초과하는 항목을 수동으로 확인합니다.",
            "4": "CPU 사용률을 매시간 확인하고 80%를 초과할 경우 이메일을 보내는 예약된 Lambda 함수를 설정합니다."
        },
        "Correct Answer": "CPU 사용률이 80%를 초과할 때 SNS 알림을 트리거하는 CloudWatch 경고를 생성합니다.",
        "Explanation": "CloudWatch 경고를 사용하여 SNS 알림을 트리거하는 것은 CPU 사용률 임계값에 기반한 실시간 알림 요구 사항을 직접 해결합니다. 이 방법은 효율적이고 자동화되어 적시에 알림을 보장합니다.",
        "Other Options": [
            "CPU 사용률을 매시간 확인하는 예약된 Lambda 함수는 한 시간에 한 번만 실행되므로 적시에 알림을 제공하지 못할 수 있으며, 사용률의 중요한 급증을 놓칠 수 있습니다.",
            "CloudTrail을 사용하여 CPU 사용률 이벤트를 기록하는 것은 사전 예방적 알림을 제공하지 않으며, 수동 확인이 필요하므로 비효율적이며 즉각적인 알림에 적합하지 않습니다.",
            "CloudFormation 스택이 대시보드를 생성할 수 있지만, 사전 예방적 알림 메커니즘을 제공하지 않으므로 사전 예방적 알림 요구 사항을 충족하지 못합니다."
        ]
    },
    {
        "Question Number": "26",
        "Situation": "데이터 엔지니어는 분석 목적으로 다양한 소스에서 Amazon S3 버킷으로 데이터를 수집하는 책임이 있습니다. 데이터는 실시간 스트리밍 및 배치 형식으로 제공되며, 일부 소스는 자주 업데이트를 보내고 다른 소스는 덜 자주 데이터를 보냅니다. 데이터 엔지니어는 효율적인 처리와 역사적 데이터 보존을 보장하기 위해 최상의 수집 전략을 결정해야 합니다.",
        "Question": "데이터 엔지니어가 역사적 데이터가 보존되도록 하면서 실시간 데이터와 배치 데이터를 효과적으로 관리하기 위해 어떤 수집 패턴을 구현해야 합니까?",
        "Options": {
            "1": "실시간 데이터를 Amazon Redshift 클러스터에 수집하여 즉시 분석하고 배치 데이터를 S3에 저장하여 나중에 처리합니다.",
            "2": "실시간 데이터를 위해 Amazon Kinesis Data Stream을 구현하고 AWS Batch를 사용하여 배치 데이터를 주간 단위로 처리합니다.",
            "3": "Amazon S3 이벤트를 사용하여 데이터가 업로드되자마자 실시간 및 배치 데이터 처리를 위한 AWS Glue 작업을 트리거합니다.",
            "4": "AWS Lambda를 사용하여 S3 버킷에 도착하는 실시간 데이터를 처리하고 배치 데이터 수집을 위해 AWS Glue 작업을 매일 밤 실행하도록 예약합니다."
        },
        "Correct Answer": "AWS Lambda를 사용하여 S3 버킷에 도착하는 실시간 데이터를 처리하고 배치 데이터 수집을 위해 AWS Glue 작업을 매일 밤 실행하도록 예약합니다.",
        "Explanation": "이 옵션은 실시간 데이터와 배치 데이터를 효율적으로 처리할 수 있게 해줍니다. AWS Lambda는 스트리밍 데이터의 즉각적인 처리를 처리할 수 있으며, 예약된 AWS Glue 작업은 시스템 성능에 영향을 주지 않고 더 큰 배치 프로세스를 처리할 수 있습니다.",
        "Other Options": [
            "이 옵션은 AWS Batch가 배치 작업을 위해 설계되었기 때문에 배치 데이터의 효율적인 처리를 제공하지 않을 수 있으며, 예약된 작업에 비해 데이터 가용성에 지연을 초래할 수 있습니다.",
            "이 옵션은 S3 이벤트를 활용하지만, 배치 데이터 수집에 가장 효율적이지 않을 수 있으며, 주기적으로 대량의 배치 데이터가 업로드될 경우 불필요한 처리 트리거를 초래할 수 있습니다.",
            "이 옵션은 실시간 데이터를 직접 Amazon Redshift에 수집하는 것이 비용 증가 및 잠재적인 성능 문제를 초래할 수 있기 때문에 이상적이지 않을 수 있습니다."
        ]
    },
    {
        "Question Number": "27",
        "Situation": "데이터 엔지니어는 다양한 소스에서 데이터를 수집하고 변환한 후 Amazon Redshift에 로드하여 분석할 수 있는 데이터 파이프라인을 설계하는 임무를 맡고 있습니다. 엔지니어는 AWS 서비스를 사용하여 강력한 ETL 솔루션을 만드는 것을 고려하고 있습니다.",
        "Question": "이 시나리오에 대한 효율적인 ETL 파이프라인 생성을 가장 잘 지원하는 AWS 서비스 조합은 무엇입니까?",
        "Options": {
            "1": "AWS Data Pipeline, Amazon S3, 및 AWS Lambda",
            "2": "Amazon Kinesis Data Firehose, AWS Glue, 및 Amazon S3",
            "3": "AWS Glue, Amazon Kinesis, 및 Amazon Redshift",
            "4": "Amazon EMR, Amazon RDS, 및 AWS Batch"
        },
        "Correct Answer": "AWS Glue, Amazon Kinesis, 및 Amazon Redshift",
        "Explanation": "ETL을 위한 AWS Glue, 실시간 데이터 스트리밍을 위한 Amazon Kinesis, 데이터 웨어하우징을 위한 Amazon Redshift의 조합은 데이터를 효과적으로 수집, 변환 및 저장하는 포괄적인 솔루션을 제공하여 확장성과 성능을 보장합니다.",
        "Other Options": [
            "AWS Data Pipeline은 주로 배치 처리를 위해 설계되었으며, Kinesis와 같은 실시간 데이터 수집 기능을 본질적으로 제공하지 않으므로 이 시나리오에 덜 적합합니다.",
            "Amazon EMR은 빅데이터 처리에 적합하지만 Kinesis만큼 실시간 데이터 수집을 효과적으로 처리하지 않으며, AWS Batch는 실시간 ETL보다는 작업 예약에 더 많이 사용됩니다.",
            "Amazon Kinesis Data Firehose는 데이터 수집에 좋은 선택이지만, AWS Glue가 제공하는 변환 기능을 제공하지 않으므로 이 파이프라인에 필수적입니다."
        ]
    },
    {
        "Question Number": "28",
        "Situation": "데이터 엔지니어링 팀은 다양한 쿼리 요구에 맞춰 Amazon Redshift 클러스터를 최적화하는 임무를 맡고 있으며, 효율적인 데이터 처리와 최소한의 지연 시간을 보장해야 합니다. 이들은 구조화된 데이터와 반구조화된 데이터를 모두 처리하며, Amazon S3에 저장된 대규모 데이터 세트를 직접 쿼리할 수 있는 솔루션이 필요합니다.",
        "Question": "다음 중 팀이 Redshift 클러스터에 데이터를 로드하지 않고 Amazon S3에서 직접 데이터를 쿼리할 수 있도록 가장 잘 지원하는 기능은 무엇입니까?",
        "Options": {
            "1": "Enhanced VPC Routing을 활용하여 Redshift와 S3 간의 데이터 전송을 최적화합니다.",
            "2": "사전 로드된 데이터로 더 나은 성능을 위해 SQL Client Tools를 활용합니다.",
            "3": "Kinesis Data Streams에서 데이터를 처리하기 위해 Redshift Streaming Ingestion을 구현합니다.",
            "4": "Redshift Spectrum을 사용하여 Redshift에 로드하지 않고 S3의 데이터에 대해 쿼리를 실행합니다."
        },
        "Correct Answer": "Redshift Spectrum을 사용하여 Redshift에 로드하지 않고 S3의 데이터에 대해 쿼리를 실행합니다.",
        "Explanation": "Redshift Spectrum은 Amazon S3에 저장된 데이터에 대해 SQL 쿼리를 직접 실행할 수 있게 해주며, Redshift 클러스터에 로드할 필요가 없어 S3에 저장된 대규모 데이터 세트를 쿼리하는 데 이상적입니다.",
        "Other Options": [
            "Enhanced VPC Routing은 주로 데이터 흐름을 관리하며 S3 데이터의 직접 쿼리를 지원하지 않습니다.",
            "SQL Client Tools는 Redshift에 연결하는 데 사용되지만 S3 데이터에 대해 직접 쿼리하는 메커니즘을 제공하지 않습니다.",
            "Redshift Streaming Ingestion은 스트리밍 데이터를 수집하는 데 중점을 두며 S3의 기존 데이터를 쿼리하는 문제를 다루지 않습니다."
        ]
    },
    {
        "Question Number": "29",
        "Situation": "소매 회사는 사용자 프로필 및 제품 재고 데이터에 자주 접근해야 하는 온라인 쇼핑 플랫폼을 분석하고 있습니다. 이 애플리케이션은 읽기 작업에 대해 낮은 지연 시간을 보장해야 하며, 데이터는 정기적으로 업데이트됩니다. 팀은 이러한 접근 패턴을 지원하기 위해 다양한 저장 솔루션을 평가하고 있습니다.",
        "Question": "낮은 지연 시간과 빈번한 데이터 업데이트의 필요성을 고려할 때 이 시나리오에 가장 적합한 저장 솔루션은 무엇입니까?",
        "Options": {
            "1": "프로비저닝된 처리량 모드의 Amazon DynamoDB",
            "2": "업데이트를 위한 이벤트 기반 트리거가 있는 Amazon S3",
            "3": "읽기 작업 확장을 위한 읽기 복제본이 있는 Amazon RDS",
            "4": "자주 접근하는 데이터를 캐시하기 위한 Amazon ElastiCache"
        },
        "Correct Answer": "자주 접근하는 데이터를 캐시하기 위한 Amazon ElastiCache",
        "Explanation": "Amazon ElastiCache는 낮은 지연 시간의 데이터 접근을 위해 설계되었으며, 자주 접근하는 데이터를 캐시하여 읽기 중심의 작업에 대한 성능을 향상시킵니다. 메모리에서 직접 요청을 처리하여 기본 데이터 저장소의 부하를 효과적으로 줄여주며, 설명된 접근 패턴에 이상적입니다.",
        "Other Options": [
            "프로비저닝된 처리량 모드의 Amazon DynamoDB는 높은 트래픽을 처리하고 낮은 지연 시간을 제공할 수 있지만, ElastiCache와 같은 캐싱 레이어가 필요한 경우에는 극도로 낮은 지연 시간에 덜 효과적일 수 있습니다.",
            "이벤트 기반 트리거가 있는 Amazon S3는 대규모 데이터 저장 및 처리에 적합하지만, 데이터베이스 서비스가 아닌 객체 저장소이기 때문에 빈번한 읽기 작업에 대한 낮은 지연 시간 접근을 제공하지 않습니다.",
            "읽기 복제본이 있는 Amazon RDS는 읽기 작업을 확장하는 데 도움이 될 수 있지만, 여전히 데이터베이스 쿼리와 관련된 지연 시간이 발생하며 ElastiCache와 같은 인메모리 캐싱 솔루션만큼 빠르지 않습니다."
        ]
    },
    {
        "Question Number": "30",
        "Situation": "데이터 엔지니어는 대량의 스트리밍 데이터를 저장하기 위해 Amazon S3에 데이터 레이크를 설계하는 임무를 맡고 있습니다. 그들은 데이터 검색 성능을 최적화하고 효율적인 저장 비용을 보장해야 합니다. 다양한 인덱싱, 파티셔닝 및 압축 전략을 고려하고 있습니다.",
        "Question": "다음 중 Amazon S3의 데이터 레이크에 대해 최고의 성능과 비용 최적화를 제공할 전략은 무엇입니까?",
        "Options": {
            "1": "날짜별로 데이터를 파티셔닝하고 Snappy 압축을 사용하여 Parquet 형식을 사용합니다.",
            "2": "모든 데이터를 JSON 형식으로 저장하고 파티셔닝이나 압축을 하지 않습니다.",
            "3": "압축 없이 비파티셔닝 텍스트 파일에 데이터를 저장합니다.",
            "4": "사용자 ID별로 데이터를 파티셔닝하고 Gzip 압축을 사용하여 CSV 형식을 사용합니다."
        },
        "Correct Answer": "날짜별로 데이터를 파티셔닝하고 Snappy 압축을 사용하여 Parquet 형식을 사용합니다.",
        "Explanation": "날짜별로 데이터를 파티셔닝하면 시간 기반 데이터에 대한 효율적인 쿼리가 가능하며, Parquet 형식은 열 지향적이고 분석 쿼리에 최적화되어 있어 성능이 향상됩니다. Snappy 압축은 저장 비용 절감과 읽기 성능 간의 좋은 균형을 제공하여 이 사용 사례에 가장 적합한 선택입니다.",
        "Other Options": [
            "파티셔닝이나 압축 없이 JSON 형식으로 데이터를 저장하면 비효율적인 데이터 검색과 더 큰 파일 크기 및 최적화 부족으로 인해 높은 저장 비용이 발생합니다.",
            "사용자 ID별로 파티셔닝하면 시계열 데이터에 대한 효율적인 쿼리를 제공하지 못할 수 있으며, CSV 형식을 사용하면 열 지향 저장소의 이점을 활용하지 못해 성능이 느려질 수 있습니다.",
            "압축 없이 비파티셔닝 텍스트 파일에 데이터를 저장하면 데이터 검색 시 성능이 저하되고 파일 크기가 커져 비용이 증가합니다."
        ]
    },
    {
        "Question Number": "31",
        "Situation": "한 회사가 AWS Lambda와 Amazon API Gateway를 사용하여 서버리스 애플리케이션을 구축하고 있습니다. 이 애플리케이션은 인증된 사용자만 API 메서드를 호출할 수 있도록 하고, Lambda 함수가 DynamoDB 테이블에 접근할 수 있는 필요한 권한을 가져야 합니다. 데이터 엔지니어는 이를 달성하기 위해 IAM 역할과 정책을 구성해야 합니다.",
        "Question": "데이터 엔지니어가 API와 Lambda 함수를 적절하게 보호하기 위해 구현해야 할 솔루션은 무엇인가요?",
        "Options": {
            "1": "API Gateway를 IAM 역할을 사용하여 인증하도록 구성하고 모든 Lambda 함수가 특정 권한 없이 DynamoDB에 접근할 수 있도록 허용합니다.",
            "2": "Lambda 함수에서 DynamoDB 테이블에 연결하기 위해 액세스 키가 있는 IAM 사용자를 사용하고, 인증 없이 API Gateway를 활성화합니다.",
            "3": "DynamoDB 테이블에 대한 모든 작업을 허용하는 IAM 정책을 설정하고 이를 Lambda 함수 역할에 연결하며, API Gateway는 사용자 정의 인증자를 사용하여 보호합니다.",
            "4": "DynamoDB 테이블에 접근할 수 있는 권한을 가진 Lambda 함수용 IAM 역할을 생성하고, API Gateway에서 사용자 인증을 위해 Amazon Cognito를 사용합니다."
        },
        "Correct Answer": "DynamoDB 테이블에 접근할 수 있는 권한을 가진 Lambda 함수용 IAM 역할을 생성하고, API Gateway에서 사용자 인증을 위해 Amazon Cognito를 사용합니다.",
        "Explanation": "이 옵션은 접근 관리를 위한 안전한 방법을 제공합니다. DynamoDB 테이블에 접근하기 위해 필요한 최소 권한을 가진 Lambda 함수용 특정 IAM 역할을 생성하고, 사용자 인증을 위해 Amazon Cognito를 사용함으로써 인증된 사용자만 API 메서드를 호출할 수 있도록 보장합니다. 이는 보안 및 접근 관리에 대한 모범 사례를 따릅니다.",
        "Other Options": [
            "이 옵션은 Lambda 함수에서 액세스 키가 있는 IAM 사용자를 사용하는 것이 보안 위험으로 인해 권장되지 않기 때문에 잘못된 것입니다. 또한 인증 없이 API Gateway를 활성화하면 API가 무단 접근에 노출됩니다.",
            "이 옵션은 DynamoDB 테이블에 대한 모든 작업을 허용하는 IAM 정책을 설정하는 것이 최소 권한 원칙을 따르지 않기 때문에 잘못된 것입니다. 사용자 정의 인증자를 사용하는 것이 일부 접근 제어를 제공할 수 있지만, Lambda 함수에 대한 안전한 IAM 역할의 필요성을 해결하지 않습니다.",
            "이 옵션은 특정 권한 없이 모든 Lambda 함수가 DynamoDB에 접근할 수 있도록 허용하는 것이 최소 권한 원칙을 위반하기 때문에 잘못된 것입니다. 또한 API Gateway 인증을 위해 IAM 역할만 사용하는 것은 사용자 수준의 인증을 제공하지 않으며 API를 무단 접근에 노출시킬 수 있습니다."
        ]
    },
    {
        "Question Number": "32",
        "Situation": "데이터 엔지니어가 PostgreSQL 호환 Amazon Redshift 데이터베이스를 관리하는 임무를 맡았습니다. 엔지니어는 데이터베이스에 연결하고, 새 데이터베이스를 생성하고, 기존 데이터베이스를 삭제하는 등의 여러 작업을 수행해야 합니다. 엔지니어는 데이터베이스 관리에 대한 모범 사례를 따르기를 원합니다.",
        "Question": "데이터 엔지니어가 데이터베이스에 연결하고 'mydb'라는 새 데이터베이스를 생성한 다음 필요 시 'mydb'를 삭제하기 위해 사용해야 할 올바른 명령 순서는 무엇인가요?",
        "Options": {
            "1": "psql -h my-cluster.cduijjmc4xkx.us-west-2.redshift.amazonaws.com -U masteruser -d dev; CREATE DATABASE mydb; DROP DATABASE mydb;",
            "2": "CREATE DATABASE mydb; psql -h my-cluster.cduijjmc4xkx.us-west-2.redshift.amazonaws.com -U masteruser -d dev; DROP DATABASE mydb;",
            "3": "psql -h my-cluster.cduijjmc4xkx.us-west-2.redshift.amazonaws.com -U masteruser -d dev; DROP DATABASE mydb; CREATE DATABASE mydb;",
            "4": "DROP DATABASE mydb; psql -h my-cluster.cduijjmc4xkx.us-west-2.redshift.amazonaws.com -U masteruser -d dev; CREATE DATABASE mydb;"
        },
        "Correct Answer": "psql -h my-cluster.cduijjmc4xkx.us-west-2.redshift.amazonaws.com -U masteruser -d dev; CREATE DATABASE mydb; DROP DATABASE mydb;",
        "Explanation": "올바른 순서는 psql 명령을 사용하여 데이터베이스에 연결한 후 데이터베이스를 생성하고, 필요 시 데이터베이스를 삭제하는 것입니다. 이 순서는 데이터베이스 작업에 필요한 논리적 흐름을 따릅니다.",
        "Other Options": [
            "이 옵션은 연결을 설정하기 전에 데이터베이스를 생성하려고 시도하므로 잘못된 것입니다. CREATE DATABASE 명령은 연결된 세션 내에서 실행되어야 합니다.",
            "이 옵션은 데이터베이스를 삭제하는 것으로 시작하는데, 데이터베이스가 아직 존재하지 않는 경우 불필요할 뿐만 아니라 이 작업을 수행하려면 연결이 필요합니다. 또한, 연결 없이 삭제한 후 데이터베이스를 생성하는 것은 유효하지 않습니다.",
            "이 옵션은 데이터베이스 연결로 시작하지만 존재하기 전에 데이터베이스를 삭제하려고 시도하므로 오류가 발생합니다. 올바른 접근 방식은 연결을 설정한 후 데이터베이스를 생성하는 것입니다."
        ]
    },
    {
        "Question Number": "33",
        "Situation": "한 회사가 AWS 서비스를 사용하여 IoT 장치에서 스트리밍 데이터를 수집하고 처리해야 합니다. 그들은 데이터 수집 파이프라인에서 높은 처리량과 낮은 대기 시간을 달성하는 것에 특히 관심이 있습니다.",
        "Question": "데이터 수집을 위한 높은 처리량과 낮은 대기 시간 특성을 달성하는 데 도움이 되는 작업은 무엇인가요? (두 가지 선택)",
        "Options": {
            "1": "Amazon Simple Notification Service (SNS)를 활용하여 메시지를 Lambda 함수로 푸시하여 처리합니다.",
            "2": "Amazon Kinesis Data Firehose를 활용하여 최소한의 대기 시간으로 스트리밍 데이터를 목적지에 전달합니다.",
            "3": "Amazon SQS 큐를 설정하여 들어오는 데이터를 버퍼링한 다음 AWS Lambda로 처리합니다.",
            "4": "Amazon Kinesis Data Streams를 사용하여 실시간으로 데이터를 수집하고 처리하며 자동 확장 기능을 제공합니다.",
            "5": "AWS Glue를 구현하여 스트리밍 데이터를 변환한 후 Amazon S3에 저장합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Amazon Kinesis Data Streams를 사용하여 실시간으로 데이터를 수집하고 처리하며 자동 확장 기능을 제공합니다.",
            "Amazon Kinesis Data Firehose를 활용하여 최소한의 대기 시간으로 스트리밍 데이터를 목적지에 전달합니다."
        ],
        "Explanation": "Amazon Kinesis Data Streams는 실시간 데이터 처리를 가능하게 하며 높은 처리량을 처리할 수 있도록 확장할 수 있습니다. 마찬가지로 Amazon Kinesis Data Firehose는 낮은 대기 시간으로 스트리밍 데이터 전달을 위해 설계되어 있어, 두 서비스 모두 데이터 수집에서 높은 처리량과 낮은 대기 시간 요구 사항에 최적입니다.",
        "Other Options": [
            "AWS Glue는 주로 데이터 변환 서비스이며 실시간 데이터 수집에 최적화되어 있지 않으므로 높은 처리량과 낮은 대기 시간 요구 사항을 직접적으로 해결하지 않습니다.",
            "Amazon SNS는 pub/sub 메시징에 유용하지만 스트리밍 데이터의 높은 처리량 수집에는 적합하지 않으며, 이는 Kinesis 서비스가 더 잘 처리합니다.",
            "Amazon SQS는 메시지를 버퍼링할 수 있지만, 그 처리 모델은 Kinesis Data Streams 및 Firehose에 비해 추가적인 대기 시간을 초래하므로, 거의 실시간 데이터 수집을 위해 설계된 Kinesis 서비스보다 효과적이지 않습니다."
        ]
    },
    {
        "Question Number": "34",
        "Situation": "소매 회사는 여러 데이터 소스에서 일일 재고 업데이트를 자동화하고 임계값에 도달했을 때 재고 관리 팀에 경고를 보내고자 합니다. 이 프로세스를 간소화하고 수동 개입을 최소화하기 위해 AWS 서비스를 사용하는 것을 고려하고 있습니다.",
        "Question": "회사가 재고 업데이트 및 알림을 자동화하기 위해 어떤 단계를 수행해야 합니까? (두 가지 선택)",
        "Options": {
            "1": "AWS Glue 작업을 예약하여 재고 데이터를 추출, 변환 및 로드(ETL)하여 Amazon Redshift에 보고용으로 저장합니다.",
            "2": "재고 수준을 확인하고 임계값을 초과할 때 경고를 보내는 Lambda 함수를 시작하는 Amazon CloudWatch Event를 생성합니다.",
            "3": "매일 재고 데이터를 처리하는 AWS Lambda 함수를 트리거하는 Amazon EventBridge 규칙을 생성합니다.",
            "4": "다양한 소스에서 재고 데이터를 집계하는 AWS Step Functions 워크플로를 호출하는 Amazon EventBridge Scheduler를 설정합니다.",
            "5": "Amazon EventBridge를 사용하여 특정 이벤트를 모니터링하고 Amazon SNS를 통해 재고 관리 팀에 직접 알림을 보냅니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "매일 재고 데이터를 처리하는 AWS Lambda 함수를 트리거하는 Amazon EventBridge 규칙을 생성합니다.",
            "Amazon EventBridge를 사용하여 특정 이벤트를 모니터링하고 Amazon SNS를 통해 재고 관리 팀에 직접 알림을 보냅니다."
        ],
        "Explanation": "Amazon EventBridge를 사용하여 매일 Lambda 함수를 트리거하면 수동 개입 없이 재고 데이터를 자동으로 처리할 수 있습니다. 또한, Amazon SNS를 통해 알림을 보내는 EventBridge를 활용하면 재고 관리 팀이 임계값에 도달했을 때 신속하게 통보받아 빠른 대응이 가능합니다.",
        "Other Options": [
            "Amazon EventBridge Scheduler를 설정하여 AWS Step Functions 워크플로를 호출하면 데이터 집계를 자동화할 수 있지만, 간단한 데이터 처리 작업에 비해 불필요한 복잡성을 초래할 수 있습니다.",
            "ETL 프로세스를 위한 AWS Glue 작업은 더 큰 데이터 세트에 유용하지만, 일일 재고 업데이트에는 과도할 수 있어 간단한 작업에 Lambda를 사용하는 것보다 비효율적입니다.",
            "Amazon CloudWatch Event를 생성하는 것은 일부 모니터링 작업에 유용할 수 있지만, Amazon EventBridge와 같은 수준의 통합 및 이벤트 기반 기능을 제공하지 않아 이 시나리오에 대한 효과적인 솔루션이 아닙니다."
        ]
    },
    {
        "Question Number": "35",
        "Situation": "데이터 엔지니어링 팀은 Amazon Athena를 사용하여 S3에 저장된 대규모 지리공간 데이터를 분석하고 있습니다. 그들은 복잡한 데이터 유형을 처리하고 쿼리 성능을 시간에 따라 추적할 수 있도록 쿼리를 최적화해야 합니다. 또한 특정 계산을 위해 사용자 정의 함수를 사용하고 S3 Requester Pays 버킷의 데이터에 접근해야 합니다.",
        "Question": "팀이 지리공간 쿼리를 효율적으로 관리하고 Athena의 기능을 활용하기 위해 어떤 단계를 수행해야 합니까?",
        "Options": {
            "1": "향후 쿼리에서 더 빠르게 접근할 수 있도록 DynamoDB에 쿼리 결과를 저장합니다.",
            "2": "복잡성을 줄이기 위해 쿼리 결과를 단순 데이터 유형으로만 제한합니다.",
            "3": "쿼리 성능을 개선하기 위해 쿼리 기록 보존을 비활성화합니다.",
            "4": "AWS Lambda와 함께 스칼라 UDF를 사용하여 지리공간 데이터에 대한 복잡한 계산을 수행합니다."
        },
        "Correct Answer": "AWS Lambda와 함께 스칼라 UDF를 사용하여 지리공간 데이터에 대한 복잡한 계산을 수행합니다.",
        "Explanation": "AWS Lambda와 함께 스칼라 UDF를 사용하면 팀이 Athena 쿼리 내에서 지리공간 데이터에 대한 복잡한 계산을 직접 수행할 수 있어 AWS Lambda의 효율적인 처리 능력을 활용하고 분석 능력을 향상시킬 수 있습니다.",
        "Other Options": [
            "DynamoDB에 쿼리 결과를 저장하는 것은 Athena의 아키텍처와 호환되지 않으며, 결과는 S3에 저장됩니다.",
            "쿼리 결과를 단순 데이터 유형으로만 제한하면 지리공간 분석에 중요한 복잡한 데이터 유형을 다루는 능력이 저해됩니다.",
            "쿼리 기록 보존을 비활성화하면 팀이 시간에 따라 쿼리 성능을 추적하고 최적화하는 능력이 줄어들어 효율성을 유지하는 데 필수적입니다."
        ]
    },
    {
        "Question Number": "36",
        "Situation": "금융 서비스 회사는 Amazon CloudWatch Logs를 사용하여 로그 관리를 중앙 집중화했습니다. 회사는 로그 내의 민감한 정보, 예를 들어 신용 카드 번호와 사회 보장 번호가 규제 요구 사항을 준수하기 위해 암호화되어야 합니다. 그들은 CloudWatch Logs에 저장되기 전에 민감한 데이터를 자동으로 암호화하는 솔루션을 구현하고자 합니다.",
        "Question": "로그 내의 민감한 정보가 저장 시 암호화되도록 보장하기 위해 어떤 AWS 서비스를 사용해야 합니까?",
        "Options": {
            "1": "암호화 규칙이 있는 AWS WAF",
            "2": "암호화가 활성화된 Amazon RDS",
            "3": "AWS KMS와 함께 사용하는 AWS Lambda",
            "4": "서버 측 암호화가 있는 Amazon S3"
        },
        "Correct Answer": "AWS KMS와 함께 사용하는 AWS Lambda",
        "Explanation": "AWS Lambda는 로그 데이터를 처리하고 AWS Key Management Service(KMS)와 통합하여 민감한 정보를 CloudWatch Logs에 전송하기 전에 암호화할 수 있어 데이터 보호 규정을 준수할 수 있습니다.",
        "Other Options": [
            "Amazon RDS는 주로 데이터베이스 서비스이며, 데이터 저장 시 암호화할 수 있지만 CloudWatch Logs에 특별히 저장된 로그 데이터에는 적용되지 않습니다.",
            "AWS WAF는 웹 애플리케이션을 일반적인 위협으로부터 보호하기 위해 설계된 웹 애플리케이션 방화벽이지만, 로그 저장 시 암호화를 처리하지 않습니다.",
            "서버 측 암호화가 있는 Amazon S3는 S3에 저장된 데이터를 암호화하는 유효한 옵션이지만, CloudWatch Logs는 로그 저장을 위해 S3를 직접 사용하지 않으므로 이 옵션은 관련이 없습니다."
        ]
    },
    {
        "Question Number": "37",
        "Situation": "데이터 엔지니어링 팀은 고객 데이터가 분석 플랫폼으로 흐르는 품질을 보장하는 임무를 맡고 있습니다. 그들은 들어오는 데이터의 이상을 자동으로 감지하고 분석을 위한 시각적 통찰력을 제공할 수 있는 데이터 품질 규칙을 구현하고자 합니다. 팀은 이 목적을 위해 AWS Glue DataBrew를 활용하기로 결정했습니다.",
        "Question": "AWS Glue DataBrew의 어떤 기능이 데이터 품질 규칙 정의에 가장 유용할까요? (두 가지 선택)",
        "Options": {
            "1": "데이터 분포를 이해하기 위한 데이터 프로파일링",
            "2": "내장된 데이터 품질 규칙 및 경고",
            "3": "자동화된 ETL 작업 스케줄링 기능",
            "4": "데이터 변환을 위한 시각적 레시피 생성",
            "5": "사용자 정의 처리를 위한 AWS Lambda와의 통합"
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "데이터 분포를 이해하기 위한 데이터 프로파일링",
            "내장된 데이터 품질 규칙 및 경고"
        ],
        "Explanation": "데이터 프로파일링은 사용자가 데이터의 특성과 분포를 조사할 수 있게 하여 이상을 식별하고 데이터 품질을 보장하는 데 중요합니다. 또한, 내장된 데이터 품질 규칙 및 경고는 데이터 문제를 자동으로 감지하여 고품질 데이터를 유지하기 위한 사전 예방적 접근 방식을 제공합니다.",
        "Other Options": [
            "자동화된 ETL 작업 스케줄링 기능은 데이터 품질을 직접적으로 향상시키기보다는 데이터 변환 작업을 일정에 따라 실행하는 데 중점을 둡니다.",
            "데이터 변환을 위한 시각적 레시피 생성은 데이터의 품질을 평가하거나 보장하기보다는 주로 데이터를 변환하는 데 목적이 있습니다.",
            "사용자 정의 처리를 위한 AWS Lambda와의 통합은 사용자 정의 기능에 대한 유연성을 제공하지만 데이터 품질 규칙 정의와 직접적으로 관련이 없습니다."
        ]
    },
    {
        "Question Number": "38",
        "Situation": "데이터 엔지니어는 Amazon API Gateway와 상호작용하는 Lambda 함수에 대한 다양한 AWS 리소스에 대한 안전한 접근을 설정하는 책임이 있습니다. 엔지니어는 함수가 필요한 리소스에 접근할 수 있도록 하면서 최소 권한 원칙이 적용되도록 해야 합니다.",
        "Question": "Lambda 함수와 API Gateway에 대한 안전한 접근을 보장하기 위한 IAM 역할 구성의 조합은 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "Lambda 함수의 역할 접근을 허용하기 위해 API Gateway에 리소스 기반 정책을 설정합니다.",
            "2": "Lambda 함수가 제한 없이 다른 AWS 서비스를 호출할 수 있도록 허용하는 IAM 정책을 사용합니다.",
            "3": "AWS Lambda에 대한 권한이 있는 역할을 생성하고 이를 함수에 연결합니다.",
            "4": "API Gateway 접근을 위한 Lambda 함수와 특정 IAM 역할 간의 신뢰 관계를 구현합니다.",
            "5": "Lambda 실행 역할에 대해 API Gateway에 대한 전체 접근 권한을 부여합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "AWS Lambda에 대한 권한이 있는 역할을 생성하고 이를 함수에 연결합니다.",
            "Lambda 함수의 역할 접근을 허용하기 위해 API Gateway에 리소스 기반 정책을 설정합니다."
        ],
        "Explanation": "AWS Lambda에 대한 특정 권한이 있는 역할을 생성하고 이를 함수에 연결하면 함수가 과도한 권한을 부여받지 않고 실행하는 데 필요한 권한을 보장합니다. 또한, API Gateway에 리소스 기반 정책을 설정하면 Lambda 함수의 역할이 최소 권한 원칙에 따라 API를 안전하게 호출할 수 있습니다.",
        "Other Options": [
            "Lambda 실행 역할에 대해 API Gateway에 대한 전체 접근 권한을 부여하는 것은 필요 이상으로 많은 접근 권한을 제공하여 최소 권한 원칙을 위반하므로 잘못된 것입니다.",
            "Lambda 함수가 제한 없이 다른 AWS 서비스를 호출할 수 있도록 허용하는 IAM 정책을 사용하는 것은 최소 권한 원칙을 준수하지 않아 보안 취약점을 초래할 수 있으므로 잘못된 것입니다.",
            "API Gateway 접근을 위한 Lambda 함수와 특정 IAM 역할 간의 신뢰 관계를 구현하는 것은 신뢰 관계가 교차 계정 접근과 더 관련이 있으며 Lambda가 API Gateway를 직접 호출하는 것과는 관련이 없기 때문에 잘못된 것입니다."
        ]
    },
    {
        "Question Number": "39",
        "Situation": "데이터 엔지니어링 팀은 고트래픽 애플리케이션을 지원하는 Amazon RDS 인스턴스의 성능을 최적화하는 임무를 맡고 있습니다. 이 애플리케이션은 간헐적으로 읽기 및 쓰기 요청의 폭주를 경험하여 지연 문제를 일으킬 수 있습니다. 팀은 일관된 애플리케이션 성능을 보장하기 위해 성능 조정을 위한 모범 사례를 구현해야 합니다.",
        "Question": "팀이 Amazon RDS 인스턴스의 성능을 개선하기 위해 어떤 접근 방식을 취해야 합니까?",
        "Options": {
            "1": "Amazon RDS Performance Insights를 사용하여 병목 현상을 식별하고 해결합니다.",
            "2": "읽기 요청을 기본 인스턴스에서 오프로드하기 위해 읽기 복제본을 활성화합니다.",
            "3": "인스턴스 클래스 변경 없이 인스턴스의 저장소 크기를 증가시킵니다.",
            "4": "더 나은 성능을 위해 데이터베이스 엔진을 Amazon Aurora로 전환합니다."
        },
        "Correct Answer": "읽기 요청을 기본 인스턴스에서 오프로드하기 위해 읽기 복제본을 활성화합니다.",
        "Explanation": "읽기 복제본을 활성화하면 여러 인스턴스에 읽기 작업 부하를 분산시켜 고트래픽 기간 동안 성능을 개선하고 읽기 요청의 지연을 줄이는 데 도움이 됩니다.",
        "Other Options": [
            "저장소 크기를 단독으로 증가시키는 것은 인스턴스 클래스 조정과 같은 다른 최적화와 결합되지 않는 한 성능에 큰 영향을 미치지 않습니다. 주로 저장 용량에 영향을 미치며 즉각적인 성능에는 영향을 미치지 않습니다.",
            "Amazon RDS Performance Insights를 사용하는 것은 성능 문제를 진단하는 데 유용하지만 성능을 직접적으로 개선하지는 않습니다. 병목 현상에 대한 가시성을 제공하지만 얻은 통찰력을 기반으로 추가 조치가 필요합니다.",
            "Amazon Aurora로 전환하는 것은 성능 이점을 제공할 수 있지만 복잡하고 시간이 많이 걸리는 마이그레이션 프로세스를 포함합니다. 읽기 복제본을 활성화하는 것에 비해 성능 조정 접근 방식이 간단하지 않습니다."
        ]
    },
    {
        "Question Number": "40",
        "Situation": "데이터 엔지니어는 여러 데이터 처리 애플리케이션을 호스팅하는 Amazon VPC에 대한 안전한 접근을 보장하는 임무를 맡고 있습니다. 엔지니어는 네트워크의 전반적인 보안 태세를 유지하면서 특정 IP 범위에서의 트래픽을 허용하도록 VPC 보안 그룹을 업데이트해야 합니다.",
        "Question": "권한이 있는 트래픽만 허용하면서 VPC 보안 그룹을 업데이트하기 위한 최선의 접근 방식은 무엇입니까?",
        "Options": {
            "1": "기존 보안 그룹을 수정하여 모든 IP 주소에서의 트래픽을 허용합니다.",
            "2": "인터넷에서 모든 수신 트래픽을 허용하는 네트워크 ACL을 설정합니다.",
            "3": "필요한 IP 범위에 대한 수신 규칙을 추가하고 과도하게 허용된 규칙을 제거합니다.",
            "4": "광범위한 권한을 가진 새 보안 그룹을 생성하고 이를 인스턴스에 연결합니다."
        },
        "Correct Answer": "필요한 IP 범위에 대한 수신 규칙을 추가하고 과도하게 허용된 규칙을 제거합니다.",
        "Explanation": "이 접근 방식은 지정된 IP 범위에서의 트래픽만 허용되도록 하여 불필요한 열린 규칙을 제거함으로써 보안을 강화합니다. 이는 안전한 환경을 유지하고 잠재적인 위협에 대한 노출을 최소화합니다.",
        "Other Options": [
            "인터넷에서 모든 수신 트래픽을 허용하는 네트워크 ACL을 설정하면 모든 외부 트래픽에 리소스를 노출시켜 상당한 보안 위험을 초래하므로 바람직하지 않습니다.",
            "광범위한 권한을 가진 새 보안 그룹을 생성하고 이를 인스턴스에 연결하면 보안이 약화되고 민감한 데이터가 무단 접근에 노출될 수 있습니다.",
            "기존 보안 그룹을 수정하여 모든 IP 주소에서의 트래픽을 허용하면 VPC의 보안이 완전히 손상되어 공격에 취약해집니다."
        ]
    },
    {
        "Question Number": "41",
        "Situation": "한 회사는 Amazon Kinesis를 사용하여 IoT 장치에서 대량의 스트리밍 데이터를 처리합니다. 데이터 엔지니어링 팀은 데이터 처리 파이프라인이 신뢰할 수 있고 유지 관리가 용이하며, 문제를 신속하게 식별하고 해결하여 지속적인 데이터 전달을 지원할 수 있도록 하기를 원합니다.",
        "Question": "스트리밍 데이터 처리 파이프라인의 신뢰성과 유지 관리를 보장하기 위한 최선의 접근 방식은 무엇입니까?",
        "Options": {
            "1": "Amazon Kinesis Data Firehose를 사용하여 데이터를 Amazon S3에 직접 로드하고, 주기적인 배치 작업을 설정하여 데이터를 정리하고 처리합니다.",
            "2": "모니터링 및 알림을 위해 Amazon CloudWatch를 구현하고, AWS Lambda를 사용하여 실패한 처리 작업을 자동으로 재시작합니다.",
            "3": "AWS Step Functions를 사용하여 데이터 처리 워크플로를 조정하고, 워크플로의 일환으로 오류 처리 및 재시도를 가능하게 합니다.",
            "4": "데이터 처리를 위해 Amazon EMR을 배포하고, 수동 개입 없이 가변 작업 부하를 처리하기 위해 자동 확장 클러스터를 구성합니다."
        },
        "Correct Answer": "AWS Step Functions를 사용하여 데이터 처리 워크플로를 조정하고, 워크플로의 일환으로 오류 처리 및 재시도를 가능하게 합니다.",
        "Explanation": "AWS Step Functions는 오류 처리 및 재시도를 포함하여 복잡한 처리 작업을 정의할 수 있는 시각적 워크플로를 제공합니다. 이는 데이터 처리 파이프라인의 신뢰성과 유지 관리를 향상시켜 문제 해결 및 관리가 용이해집니다.",
        "Other Options": [
            "모니터링 및 알림을 위해 Amazon CloudWatch를 구현하는 것은 유용하지만, AWS Lambda에만 의존하여 작업을 재시작하는 것은 실패를 놓치고 구조화된 오류 처리 프로세스 없이 추가적인 복잡성을 초래할 수 있습니다.",
            "Amazon EMR을 자동 확장과 함께 배포하면 작업 부하를 관리하는 데 도움이 되지만, AWS Step Functions가 제공하는 구조화된 오류 처리 및 워크플로 조정 기능을 본질적으로 제공하지 않습니다.",
            "Amazon Kinesis Data Firehose를 활용하는 것은 S3에 데이터를 로드하는 데 효과적이지만, 신뢰할 수 있고 유지 관리가 용이한 데이터 처리 파이프라인에 필수적인 조정된 워크플로 및 오류 처리가 부족합니다."
        ]
    },
    {
        "Question Number": "42",
        "Situation": "한 금융 서비스 회사는 Amazon Kinesis Data Streams를 사용하여 다양한 결제 처리 시스템에서 실시간 거래 데이터를 수집하고 있습니다. 거래량이 많아 회사는 Amazon DynamoDB에 데이터를 수집하는 과정에서 스로틀링 및 속도 제한 문제를 겪고 있습니다. 데이터 엔지니어링 팀은 이러한 제한에 대해 수집 프로세스가 효율적이고 탄력적으로 유지되도록 해야 합니다.",
        "Question": "Kinesis Data Streams에서 DynamoDB로 데이터를 수집하는 동안 스로틀링을 구현하고 속도 제한을 극복하기 위한 최선의 접근 방식은 무엇입니까?",
        "Options": {
            "1": "Kinesis Data Firehose 배달 스트림을 사용하여 레코드를 버퍼링하고 배치한 후 DynamoDB에 기록합니다.",
            "2": "Kinesis 레코드를 큐에 넣기 위해 SQS를 사용하고, DynamoDB 쓰기를 위한 별도의 Lambda 함수로 배치 처리합니다.",
            "3": "AWS Lambda를 사용하여 Kinesis에서 DynamoDB로 데이터를 직접 스트리밍하며 높은 동시성 설정을 사용합니다.",
            "4": "Kinesis 스트림에서 소비하는 Lambda 함수에 지수 백오프를 구현하여 스로틀링 문제를 처리합니다."
        },
        "Correct Answer": "Kinesis Data Firehose 배달 스트림을 사용하여 레코드를 버퍼링하고 배치한 후 DynamoDB에 기록합니다.",
        "Explanation": "Kinesis Data Firehose를 사용하면 수신 레코드를 자동으로 배치하고 버퍼링할 수 있어 DynamoDB에 대한 쓰기 속도를 제어하여 스로틀링 문제를 완화하는 데 도움이 됩니다. 이 솔루션은 수집 프로세스를 단순화하고 효율성을 향상시킵니다.",
        "Other Options": [
            "높은 동시성 설정으로 데이터를 직접 스트리밍하면 과도한 쓰기 작업이 발생하여 DynamoDB의 쓰기 용량을 초과하고 스로틀링 오류가 발생할 수 있습니다.",
            "Lambda 함수에 지수 백오프를 구현하면 스로틀링 후 재시도를 관리하는 데 도움이 되지만, DynamoDB에 대한 초기 요청 과부하를 방지하지 못해 데이터 손실이나 지연이 발생할 수 있습니다.",
            "SQS를 사용하는 것은 데이터 수집 프로세스에 불필요한 복잡성과 지연을 추가하며, DynamoDB에 기록하기 전에 레코드를 큐에 넣고 처리하는 추가 단계를 요구합니다."
        ]
    },
    {
        "Question Number": "43",
        "Situation": "데이터 엔지니어링 팀은 IoT 장치에서 스트리밍 데이터를 수집하고 변환하기 위해 AWS에서 서버리스 데이터 파이프라인을 구현하는 임무를 맡았습니다. 그들은 워크플로우가 자동화되고 들어오는 데이터 양에 따라 확장 가능하도록 하기를 원합니다. 이 사용 사례에 가장 적합한 AWS 서비스 조합은 무엇인가요?",
        "Question": "데이터 수집 및 변환을 위한 서버리스 워크플로우를 구현하기 위해 팀이 사용해야 할 AWS 서비스 조합은 무엇인가요?",
        "Options": {
            "1": "AWS Glue와 Amazon S3",
            "2": "Amazon EMR과 Amazon DynamoDB",
            "3": "AWS Lambda와 Amazon RDS",
            "4": "AWS Lambda와 Amazon Kinesis Data Firehose"
        },
        "Correct Answer": "AWS Lambda와 Amazon Kinesis Data Firehose",
        "Explanation": "AWS Lambda는 Amazon Kinesis Data Firehose와 결합하여 IoT 장치에서 오는 다양한 데이터 양을 자동으로 수용할 수 있는 완전한 서버리스 아키텍처를 제공합니다. Kinesis Data Firehose는 스트리밍 데이터를 수집하고 이를 다양한 저장 및 분석 서비스로 전송할 수 있으며, Lambda는 데이터 수집 과정 전후에 실시간 데이터 변환에 사용될 수 있습니다.",
        "Other Options": [
            "AWS Glue와 Amazon S3는 실시간 스트리밍 데이터 수집에 최적화되어 있지 않습니다. Glue는 주로 배치 처리 및 ETL 작업에 사용되며, S3는 저장 솔루션이지 실시간 수집 서비스가 아닙니다.",
            "AWS Lambda와 Amazon RDS는 이 시나리오에 적합하지 않습니다. RDS는 실시간 스트리밍 데이터 수집을 지원하지 않는 관리형 관계형 데이터베이스 서비스입니다. Lambda는 데이터를 처리할 수 있지만 효과적인 수집을 위해 추가 구성 요소가 필요합니다.",
            "Amazon EMR과 Amazon DynamoDB는 서버리스 워크플로우에 적합하지 않습니다. EMR은 일반적으로 클러스터에서 실행되는 빅데이터 처리 서비스이며 더 많은 관리가 필요하고, DynamoDB는 추가 서비스 없이 스트리밍 수집을 본질적으로 처리하지 않는 NoSQL 데이터베이스입니다."
        ]
    },
    {
        "Question Number": "44",
        "Situation": "한 소매 회사는 온프레미스 SQL 데이터베이스, Amazon RDS 인스턴스 및 소셜 미디어 플랫폼을 포함한 다양한 출처에서 고객 데이터를 통합하려고 합니다. 그들은 데이터가 변환되어 Amazon Redshift에 통합된 형식으로 저장되어 분석 목적으로 사용되기를 원합니다. 이 데이터 통합 및 변환 프로세스를 가장 잘 촉진할 수 있는 접근 방식은 무엇인가요?",
        "Question": "여러 출처의 데이터를 Amazon Redshift로 통합하고 변환하는 가장 효과적인 방법은 무엇인가요?",
        "Options": {
            "1": "Amazon Kinesis Data Firehose를 활용하여 SQL 데이터베이스와 소셜 미디어 플랫폼에서 Amazon Redshift로 지속적으로 데이터를 스트리밍합니다.",
            "2": "AWS Data Pipeline을 예약하여 SQL 데이터베이스와 RDS에서 Amazon S3로 데이터를 이동한 후, Amazon Redshift Spectrum을 사용하여 쿼리합니다.",
            "3": "각 출처에서 데이터를 수동으로 내보내고, Python 스크립트를 사용하여 변환한 후 Amazon Redshift에 업로드합니다.",
            "4": "AWS Glue를 사용하여 SQL 데이터베이스와 RDS에서 데이터를 추출하고 변환하여 Amazon Redshift에 로드하는 ETL 작업을 생성합니다."
        },
        "Correct Answer": "AWS Glue를 사용하여 SQL 데이터베이스와 RDS에서 데이터를 추출하고 변환하여 Amazon Redshift에 로드하는 ETL 작업을 생성합니다.",
        "Explanation": "AWS Glue는 다양한 출처에서 Amazon Redshift로 데이터를 효율적으로 통합하고 변환할 수 있는 완전 관리형 ETL(추출, 변환, 로드) 서비스로, 이 시나리오에 가장 적합한 선택입니다.",
        "Other Options": [
            "데이터를 수동으로 내보내고 Python 스크립트를 사용하여 변환하는 것은 오류가 발생하기 쉽고 AWS Glue와 같은 관리형 ETL 솔루션에 비해 확장성이 부족합니다.",
            "Amazon Kinesis Data Firehose는 스트리밍 데이터에 적합하지만, 여러 출처에서 데이터를 배치 처리하고 변환한 후 Redshift에 로드하는 데는 최선의 선택이 아닙니다.",
            "AWS Data Pipeline을 사용하면 데이터 이동을 촉진할 수 있지만, 이 시나리오에서 AWS Glue만큼 효과적으로 변환 기능을 제공하지는 않습니다."
        ]
    },
    {
        "Question Number": "45",
        "Situation": "한 금융 서비스 회사는 민감한 고객 데이터를 Amazon S3로 마이그레이션하고 있습니다. 규제 요구 사항을 준수하고 민감한 데이터의 보호를 보장하기 위해 회사는 적절한 보안 조치를 구현해야 합니다.",
        "Question": "Amazon S3에 저장된 민감한 데이터를 보호하기 위해 어떤 조치를 취해야 하나요? (두 가지 선택)",
        "Options": {
            "1": "저장 비용을 절감하기 위해 S3 버킷에서 버전 관리를 비활성화합니다.",
            "2": "S3 버킷에 대한 액세스를 제한하기 위해 AWS Identity and Access Management (IAM) 정책을 구현합니다.",
            "3": "S3 Object Lock을 사용하여 지정된 보존 기간 동안 민감한 데이터의 삭제를 방지합니다.",
            "4": "데이터 공유를 용이하게 하기 위해 S3 버킷 정책을 설정하여 버킷에 대한 공개 액세스를 허용합니다.",
            "5": "S3 객체에 대해 AWS Key Management Service (KMS)를 사용하여 서버 측 암호화를 활성화합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "S3 객체에 대해 AWS Key Management Service (KMS)를 사용하여 서버 측 암호화를 활성화합니다.",
            "S3 버킷에 대한 액세스를 제한하기 위해 AWS Identity and Access Management (IAM) 정책을 구현합니다."
        ],
        "Explanation": "AWS KMS를 사용하여 서버 측 암호화를 활성화하면 민감한 데이터가 저장 중에 암호화되어 추가적인 보안 계층을 제공합니다. IAM 정책을 구현하면 권한이 있는 사용자만 민감한 데이터에 접근할 수 있도록 제한합니다.",
        "Other Options": [
            "S3 버킷에 대한 공개 액세스를 허용하면 민감한 데이터 노출의 위험이 크게 증가하며, 이는 민감한 정보를 보호하는 목표에 반합니다.",
            "S3 Object Lock을 사용하는 것은 우발적인 삭제를 방지하는 좋은 방법이지만, 민감한 데이터를 보호하는 데 중요한 암호화나 액세스 제어를 직접적으로 해결하지는 않습니다.",
            "S3 버킷에서 버전 관리를 비활성화하면 역사적 데이터 손실로 이어질 수 있으며, 이는 민감한 데이터 보존 및 규정 준수 목적에 권장되지 않습니다."
        ]
    },
    {
        "Question Number": "46",
        "Situation": "데이터 엔지니어가 AWS Lambda를 사용하여 실시간 스트리밍 데이터를 처리하는 애플리케이션을 개발하고 있습니다. 이 애플리케이션은 Lambda 함수가 낮은 대기 시간과 높은 처리량을 유지하면서 들어오는 데이터의 급증을 처리해야 합니다. 엔지니어는 피크 부하 동안 동시성과 성능을 효율적으로 관리할 수 있도록 Lambda 함수를 구성해야 합니다.",
        "Question": "다음 중 애플리케이션의 동시성과 성능 요구를 가장 잘 충족하는 구성은 무엇입니까?",
        "Options": {
            "1": "Lambda 함수의 예약된 동시성 한도를 설정하여 피크 트래픽을 수용하고 예측 가능한 성능을 보장합니다.",
            "2": "Lambda 함수에 프로비저닝된 동시성을 활성화하여 인스턴스를 미리 워밍업하고 높은 트래픽 동안 콜드 스타트 대기 시간을 줄입니다.",
            "3": "들어오는 이벤트 비율에 따라 동적으로 동시성 한도를 조정하기 위해 사용자 지정 CloudWatch 메트릭을 구성합니다.",
            "4": "Amazon SQS 큐를 사용하여 들어오는 요청을 버퍼링하고 큐 깊이에 따라 Lambda 함수를 트리거합니다."
        },
        "Correct Answer": "Lambda 함수에 프로비저닝된 동시성을 활성화하여 인스턴스를 미리 워밍업하고 높은 트래픽 동안 콜드 스타트 대기 시간을 줄입니다.",
        "Explanation": "프로비저닝된 동시성을 활성화하면 Lambda 함수가 들어오는 요청을 처리할 수 있도록 미리 워밍업된 인스턴스의 지정된 수를 유지할 수 있어, 콜드 스타트 대기 시간을 크게 줄이고 피크 트래픽 동안 높은 성능을 보장합니다.",
        "Other Options": [
            "예약된 동시성 한도를 설정하면 동시성을 관리하는 데 도움이 될 수 있지만, 트래픽 급증 동안 성능에 중요한 콜드 스타트 대기 시간을 구체적으로 해결하지는 않습니다.",
            "사용자 지정 CloudWatch 메트릭을 사용하여 동적으로 조정하면 유연성을 제공할 수 있지만, 확장 시 잠재적인 지연을 초래하므로 즉각적인 성능 요구에 대한 최적의 솔루션은 아닙니다.",
            "Amazon SQS 큐를 사용하여 요청을 버퍼링하면 복잡성이 증가하고 대기 시간이 늘어날 수 있으며, 요청이 처리되기 전에 큐에서 대기해야 하므로 실시간 스트리밍 데이터에는 이상적이지 않습니다."
        ]
    },
    {
        "Question Number": "47",
        "Situation": "금융 서비스 회사가 다양한 출처에서 대규모 데이터 세트를 정기적으로 Amazon S3로 가져옵니다. 데이터 세트는 다양한 형식으로 되어 있으며, 진화하는 스키마를 가질 수 있습니다. 회사는 이러한 데이터 세트에서 스키마 발견을 자동화하고 효율적인 쿼리 처리 및 분석을 위해 포괄적인 데이터 카탈로그를 유지해야 합니다. 이 요구 사항을 달성하는 데 도움이 되는 AWS 서비스는 무엇입니까?",
        "Question": "Amazon S3에 저장된 데이터 세트의 스키마를 발견하고 데이터 카탈로그를 채우는 데 가장 적합한 AWS 서비스는 무엇입니까?",
        "Options": {
            "1": "AWS DataSync",
            "2": "Amazon Redshift Spectrum",
            "3": "Amazon Athena",
            "4": "AWS Glue Crawlers"
        },
        "Correct Answer": "AWS Glue Crawlers",
        "Explanation": "AWS Glue Crawlers는 Amazon S3의 데이터를 자동으로 스캔하고 스키마를 유추하여 AWS Glue 데이터 카탈로그를 채워, 데이터를 효율적으로 관리하고 쿼리할 수 있게 합니다. 이는 진화하는 스키마를 가진 데이터 세트에 이상적입니다.",
        "Other Options": [
            "Amazon Athena는 S3의 데이터를 SQL을 사용하여 분석할 수 있는 쿼리 서비스이지만, 스키마를 자동으로 발견하거나 데이터 카탈로그를 채우지는 않습니다.",
            "Amazon Redshift Spectrum은 S3의 데이터에 대해 쿼리를 실행할 수 있게 해주지만, 스키마를 발견하거나 데이터 카탈로그를 관리하는 기능은 제공하지 않습니다.",
            "AWS DataSync는 온프레미스 스토리지와 AWS 스토리지 서비스 간의 데이터 전송을 위한 서비스이지만, 스키마 발견이나 카탈로그화에 중점을 두지 않습니다."
        ]
    },
    {
        "Question Number": "48",
        "Situation": "데이터 엔지니어가 AWS Glue를 사용하여 대규모 데이터 수집 및 변환 작업을 수행하고 있습니다. 현재 작업은 비효율적인 코드로 인해 완료하는 데 몇 시간이 걸립니다. 엔지니어는 데이터 무결성을 손상시키지 않으면서 실행 시간을 줄이기 위해 코드를 최적화해야 합니다.",
        "Question": "데이터 엔지니어가 AWS Glue에서 더 빠른 데이터 수집 및 변환을 위해 코드를 최적화하기 위해 구현해야 할 전략은 무엇입니까?",
        "Options": {
            "1": "Spark DataFrames 대신 AWS Glue DynamicFrame을 사용하여 스키마 진화를 처리합니다.",
            "2": "사용자 지정 변환을 작성하는 대신 AWS Glue의 내장 변환을 적용합니다.",
            "3": "AWS Glue 작업 구성에서 작업자 노드 수를 늘려 병렬 처리를 개선합니다.",
            "4": "모든 데이터 변환을 한 단계에서 처리하기 위해 단일 모놀리식 스크립트를 사용합니다."
        },
        "Correct Answer": "AWS Glue 작업 구성에서 작업자 노드 수를 늘려 병렬 처리를 개선합니다.",
        "Explanation": "작업자 노드 수를 늘리면 더 나은 병렬 처리가 가능해져 데이터 수집 및 변환 작업에 소요되는 시간을 크게 줄일 수 있습니다. 이 방법은 AWS Glue의 확장성을 활용하여 성능을 향상시킵니다.",
        "Other Options": [
            "DynamicFrames를 사용하여 스키마 진화를 처리하는 것은 유익하지만, 전체 작업의 실행 시간 효율성을 직접적으로 해결하지는 않습니다.",
            "단일 모놀리식 스크립트는 선형 처리 특성으로 인해 실행 시간이 길어질 수 있어, 여러 노드를 사용하는 분산 접근 방식보다 덜 효율적입니다.",
            "내장 변환을 사용하면 코드를 단순화할 수 있지만, 복잡한 작업의 경우 잘 최적화된 사용자 지정 변환보다 항상 더 나은 성능을 보장하지는 않습니다."
        ]
    },
    {
        "Question Number": "49",
        "Situation": "회사는 특정 기간 후 고객 데이터를 삭제해야 하는 데이터 보존 정책 및 법적 요구 사항을 준수해야 합니다. 데이터 엔지니어는 이러한 규정에 따라 Amazon S3 버킷에서 민감한 고객 데이터가 삭제되도록 해야 합니다.",
        "Question": "데이터 엔지니어가 데이터 삭제 요구 사항을 충족하기 위해 구현해야 할 가장 효과적인 전략은 무엇입니까?",
        "Options": {
            "1": "지정된 기간 후 객체를 자동으로 삭제하도록 S3 버킷 생애 주기 정책을 구성합니다.",
            "2": "데이터 보존 정책을 준수하기 위해 정기적으로 S3 버킷에서 객체를 수동으로 삭제합니다.",
            "3": "사용자 지정 비즈니스 규칙에 따라 S3 버킷에서 객체를 삭제하기 위해 일정에 따라 트리거되는 AWS Lambda 함수를 설정합니다.",
            "4": "Amazon S3 인벤토리 보고서를 사용하여 객체의 나이를 추적하고 보존 기간을 초과하는 객체를 수동으로 삭제합니다."
        },
        "Correct Answer": "지정된 기간 후 객체를 자동으로 삭제하도록 S3 버킷 생애 주기 정책을 구성합니다.",
        "Explanation": "S3 버킷 생애 주기 정책을 구성하는 것은 지정된 기간 후 객체가 삭제되도록 보장하는 가장 효율적이고 자동화된 방법으로, 수동 개입 없이 데이터 보존 정책을 준수할 수 있습니다.",
        "Other Options": [
            "객체를 수동으로 삭제하는 것은 인적 오류에 취약하며, 간과로 인해 비준수로 이어질 수 있어 자동화된 솔루션보다 신뢰성이 떨어집니다.",
            "AWS Lambda 함수를 사용하여 객체를 삭제하는 것은 추가적인 복잡성을 도입하며, 생애 주기 정책이 자동으로 삭제를 처리할 수 있을 때 필요하지 않을 수 있습니다.",
            "Amazon S3 인벤토리 보고서를 사용하여 객체의 나이를 추적할 수 있지만, 여전히 객체를 삭제하기 위해 수동 개입이 필요하므로 비효율적이며 비준수의 위험이 있습니다."
        ]
    },
    {
        "Question Number": "50",
        "Situation": "소매 회사는 고객 주문과 관련된 다양한 데이터 파일을 저장하기 위해 Amazon S3를 사용합니다. 그들은 S3 버킷에 새 파일이 업로드될 때마다 데이터 엔지니어링 팀에 알림을 보내는 시스템을 구현하고자 합니다. 팀은 최소한의 지연으로 알림이 전송되고 추가 작업을 위해 비동기적으로 처리될 수 있도록 해야 합니다.",
        "Question": "회사가 S3 버킷에 새 파일이 업로드될 때 최소한의 지연으로 데이터 엔지니어링 팀에 알림을 보내기 위해 사용해야 할 AWS 서비스는 무엇입니까?",
        "Options": {
            "1": "S3 활동을 모니터링하기 위한 Amazon CloudWatch Events.",
            "2": "구독자에게 알림을 게시하기 위한 Amazon SNS.",
            "3": "알림을 직접 트리거하기 위한 AWS Lambda.",
            "4": "새 업로드에 대한 메시지를 큐에 저장하기 위한 Amazon SQS."
        },
        "Correct Answer": "구독자에게 알림을 게시하기 위한 Amazon SNS.",
        "Explanation": "Amazon SNS는 여러 구독자에게 효율적으로 알림을 보내도록 설계되었으며, S3에 새 파일이 업로드될 때 실시간 알림을 제공할 수 있습니다. 다양한 엔드포인트와의 통합이 용이하여 설명된 시나리오에 이상적입니다.",
        "Other Options": [
            "Amazon SQS는 비동기 처리를 위한 메시지 큐잉에 주로 사용되며, 알림을 직접 보내기 위한 것이 아닙니다. 큐에서 메시지를 가져오기 위한 추가 단계가 필요하여 지연이 증가할 수 있습니다.",
            "AWS Lambda는 알림을 트리거하는 데 사용할 수 있지만, 알림을 관리하고 전송하기 위한 추가 설정이 필요합니다. 이벤트에 응답할 수 있지만, 주로 알림 서비스는 아닙니다.",
            "Amazon CloudWatch Events는 S3 활동을 모니터링할 수 있지만, 알림을 직접 전송하는 책임이 없습니다. 알림을 전달하기 위해 SNS 또는 다른 서비스와의 통합이 필요합니다."
        ]
    },
    {
        "Question Number": "51",
        "Situation": "데이터 엔지니어는 구조화된 데이터와 반구조화된 데이터를 모두 포함하는 대규모 데이터 세트의 저장을 최적화하는 임무를 맡고 있습니다. 이 데이터 세트는 분석 목적으로 사용되어야 하며, 엔지니어는 성능과 저장 효율성을 균형 있게 유지하면서 빠른 쿼리 기능을 가능하게 하는 저장 형식을 선택해야 합니다.",
        "Question": "이 사용 사례에 가장 적합한 데이터 저장 형식은 무엇입니까?",
        "Options": {
            "1": "CSV",
            "2": "JSON",
            "3": "XML",
            "4": "Parquet"
        },
        "Correct Answer": "Parquet",
        "Explanation": "Parquet는 효율적인 데이터 처리 및 분석을 위해 설계된 최적화된 열 저장 형식입니다. 복잡한 중첩 데이터 구조를 지원하며, CSV 및 XML과 같은 행 기반 형식에 비해 쿼리 성능과 저장 효율성에서 상당한 개선을 제공합니다. 따라서 이 상황에 가장 적합한 선택입니다.",
        "Other Options": [
            "JSON은 반구조화된 데이터에 적합한 유연한 형식이지만, 분석 쿼리에 대한 Parquet의 효율성과 성능 최적화가 부족합니다.",
            "CSV는 구조화된 데이터에 널리 사용되는 간단한 형식이지만, 복잡한 데이터 유형을 지원하지 않으며, 열 기반 형식에 비해 파일 크기가 커지고 쿼리 성능이 느려질 수 있습니다.",
            "XML은 복잡한 데이터 구조를 처리할 수 있는 또 다른 유연한 형식이지만, 일반적으로 Parquet보다 저장 효율성이 떨어지고 쿼리 성능이 느려 분석에 덜 적합합니다."
        ]
    },
    {
        "Question Number": "52",
        "Situation": "금융 서비스 회사의 데이터 엔지니어가 두 개의 Amazon Redshift 클러스터 간 데이터 공유를 설정해야 하며, 데이터 거버넌스 및 보안 정책이 유지되도록 해야 합니다. 엔지니어는 데이터 접근을 허용하면서 무단 접근을 방지하기 위해 적절하게 권한을 부여해야 합니다.",
        "Question": "엔지니어가 Amazon Redshift 클러스터 간 데이터 공유를 위한 권한을 부여하기 위해 어떤 접근 방식을 취해야 합니까?",
        "Options": {
            "1": "Redshift 클러스터에서 공용 액세스를 활성화하여 누구나 데이터에 접근할 수 있도록 합니다.",
            "2": "AWS Identity and Access Management (IAM) 정책을 사용하여 전체 클러스터에 대한 접근을 허용합니다.",
            "3": "Amazon Redshift 클러스터의 마스터 자격 증명을 데이터 공유 팀과 공유합니다.",
            "4": "대상 클러스터에 데이터베이스 사용자를 생성하고 특정 테이블에 대한 SELECT 권한을 부여합니다."
        },
        "Correct Answer": "대상 클러스터에 데이터베이스 사용자를 생성하고 특정 테이블에 대한 SELECT 권한을 부여합니다.",
        "Explanation": "대상 클러스터에 데이터베이스 사용자를 생성하고 특정 테이블에 대한 SELECT 권한을 부여하면 데이터에 대한 통제된 접근이 가능해지며, 보안 및 거버넌스 정책을 준수할 수 있습니다. 이 방법은 승인된 사용자만 필요한 데이터에 접근할 수 있도록 하여 전체 클러스터를 노출하지 않습니다.",
        "Other Options": [
            "IAM 정책을 사용하여 전체 클러스터에 대한 접근을 허용하는 것은 잘못된 접근 방식입니다. 이는 과도한 권한을 초래하고 클러스터 내 모든 데이터에 대한 접근을 허용하여 잠재적인 보안 위험을 초래할 수 있습니다.",
            "Amazon Redshift 클러스터의 마스터 자격 증명을 데이터 공유 팀과 공유하는 것은 잘못된 접근 방식입니다. 이는 클러스터의 보안을 위협하고 모든 리소스에 대한 무제한 접근을 허용합니다.",
            "Redshift 클러스터에서 공용 액세스를 활성화하는 것은 잘못된 접근 방식입니다. 이는 무단 사용자가 민감한 데이터에 접근할 수 있도록 하여 심각한 보안 위험을 초래합니다."
        ]
    },
    {
        "Question Number": "53",
        "Situation": "회사가 AWS로 마이그레이션하면서 데이터 보안 및 거버넌스 전략을 평가하고 있습니다. 팀은 데이터 자산에 대해 관리형 서비스와 비관리형 서비스 사용의 의미를 이해해야 합니다.",
        "Question": "데이터 보안 및 거버넌스의 맥락에서 관리형 서비스와 비관리형 서비스 간의 주요 차이를 가장 잘 설명하는 진술은 무엇입니까?",
        "Options": {
            "1": "비관리형 서비스는 보안 구성에 대한 관리 오버헤드가 필요하지 않습니다.",
            "2": "관리형 서비스는 내장된 보안 기능과 규정 준수 관리를 제공합니다.",
            "3": "비관리형 서비스는 모든 규제 요구 사항을 자동으로 준수합니다.",
            "4": "관리형 서비스는 보안 및 거버넌스 설정에 대한 완전한 사용자 제어를 허용합니다."
        },
        "Correct Answer": "관리형 서비스는 내장된 보안 기능과 규정 준수 관리를 제공합니다.",
        "Explanation": "AWS의 관리형 서비스인 Amazon RDS 또는 Amazon S3는 다양한 규제 기준에 대한 준수를 간소화하는 통합 보안 기능과 도구를 제공합니다. 이는 팀이 이러한 측면을 수동으로 관리하는 부담을 줄여 더 나은 거버넌스를 가능하게 합니다.",
        "Other Options": [
            "비관리형 서비스는 여전히 보안 설정을 구성하고 유지하는 데 상당한 관리 노력이 필요하며, 적절하게 관리되지 않을 경우 잠재적인 취약점을 초래할 수 있습니다.",
            "관리형 서비스는 일반적으로 더 높은 수준의 보안 관리를 제공하며, 완전한 사용자 제어를 허용하지 않습니다. 미리 정의된 보안 설정을 제공하므로 이를 준수해야 합니다.",
            "비관리형 서비스는 자동으로 규제 요구 사항을 준수하지 않으며, 사용자가 스스로 준수 조치를 구현하고 유지해야 합니다."
        ]
    },
    {
        "Question Number": "54",
        "Situation": "회사가 다양한 출처에서 스트리밍 데이터를 처리하기 위한 실시간 분석 플랫폼을 구축할 계획입니다. 그들은 데이터가 효율적으로 쿼리될 수 있도록 하면서 비용을 관리 가능하게 유지하고자 합니다. 이러한 요구 사항을 충족하기 위해 어떤 서비스 조합을 사용해야 합니까? (두 가지 선택)",
        "Question": "실시간 분석 및 비용 효율성에 가장 적합한 저장 서비스는 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "Amazon Kinesis Data Streams",
            "2": "Amazon RDS",
            "3": "Amazon EMR",
            "4": "AWS Lake Formation",
            "5": "Amazon Redshift"
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Amazon Kinesis Data Streams",
            "Amazon EMR"
        ],
        "Explanation": "Amazon Kinesis Data Streams는 스트리밍 데이터를 실시간으로 처리하는 데 매우 확장 가능하고 효율적인 방법을 제공하여 분석에 적합합니다. Amazon EMR은 빅 데이터 처리를 위해 설계되었으며 Apache Spark와 같은 프레임워크를 사용하여 분석 작업을 실행할 수 있어, 전통적인 데이터 웨어하우스와 관련된 높은 비용 없이 실시간 데이터 처리를 수행하는 데 적합합니다.",
        "Other Options": [
            "Amazon RDS는 주로 관계형 데이터베이스 관리에 사용되며 스트리밍 데이터에 대한 실시간 분석에 최적화되어 있지 않습니다.",
            "Amazon Redshift는 강력한 데이터 웨어하우스 솔루션이지만 실시간 데이터 수집 및 분석을 위해 설계되지 않아 더 높은 비용과 지연을 초래할 수 있습니다.",
            "AWS Lake Formation은 데이터 레이크 관리를 위한 서비스이며, 실시간 분석을 특별히 목표로 하지 않으므로 즉각적인 처리 요구에 덜 적합합니다."
        ]
    },
    {
        "Question Number": "55",
        "Situation": "데이터 분석가는 Amazon RDS에 저장된 고객 데이터베이스에서 특정 정보를 검색해야 합니다. 이 데이터베이스에는 'customer_id', 'first_name', 'last_name', 'email', 'signup_date' 열이 있는 'customers'라는 테이블이 포함되어 있습니다. 분석가는 2022년 1월 1일 이후에 가입한 고객의 이메일 주소를 찾고 싶으며, 성이 'S'로 시작하는 고객을 대상으로 합니다. 쿼리는 또한 고유한 이메일 주소만 반환되도록 해야 합니다.",
        "Question": "필요한 이메일 주소를 MOST 효과적으로 검색할 수 있는 SQL 쿼리는 무엇입니까? (두 개 선택)",
        "Options": {
            "1": "SELECT email FROM customers WHERE signup_date > '2022-01-01' AND last_name LIKE 'S%' GROUP BY email;",
            "2": "SELECT DISTINCT email FROM customers WHERE last_name LIKE 'S%' AND signup_date > '2022-01-01';",
            "3": "SELECT email FROM customers GROUP BY email HAVING signup_date > '2022-01-01' AND last_name LIKE 'S%';",
            "4": "SELECT DISTINCT email FROM customers WHERE signup_date > '2022-01-01' AND last_name LIKE 'S%';",
            "5": "SELECT email FROM customers WHERE last_name LIKE 'S%' AND signup_date > '2022-01-01';"
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "SELECT DISTINCT email FROM customers WHERE signup_date > '2022-01-01' AND last_name LIKE 'S%';",
            "SELECT DISTINCT email FROM customers WHERE last_name LIKE 'S%' AND signup_date > '2022-01-01';"
        ],
        "Explanation": "두 쿼리는 모두 DISTINCT 키워드를 올바르게 사용하여 'customers' 테이블에서 고유한 이메일 주소를 반환하며, 가입 날짜와 성에 대한 지정된 조건에 따라 결과를 필터링합니다. 두 쿼리 모두 관련 이메일 주소만 출력에 포함되도록 보장합니다.",
        "Other Options": [
            "이 쿼리는 DISTINCT 키워드가 없으므로, 여러 고객이 기준을 충족할 경우 중복 이메일 주소가 반환될 수 있습니다. DISTINCT 없이 GROUP BY를 사용하는 것은 이 경우 고유성을 보장하지 않습니다.",
            "이 쿼리는 이메일을 반환하지만 집계 함수 없이 GROUP BY를 사용하는 것은 적절하지 않으며, 사용되는 SQL 방언에 따라 예기치 않은 결과나 오류를 초래할 수 있습니다.",
            "이 쿼리는 올바른 쿼리와 유사하지만 DISTINCT를 사용하지 않습니다. 따라서 여러 고객이 조건을 충족할 경우 중복 이메일 주소가 반환될 위험이 있습니다."
        ]
    },
    {
        "Question Number": "56",
        "Situation": "데이터 분석가는 Amazon S3에 저장된 대규모 데이터 세트에 대해 즉석 쿼리를 수행해야 합니다. 분석가는 서버 관리가 필요 없고 다양한 데이터 형식을 지원하는 비용 효율적인 솔루션을 찾고 있습니다. 또한 비즈니스 인텔리전스 도구를 사용하여 결과를 시각화하고 싶어합니다.",
        "Question": "이러한 요구 사항을 충족하기 위해 데이터 분석가는 어떤 AWS 서비스를 사용해야 합니까?",
        "Options": {
            "1": "AWS Glue",
            "2": "Amazon Athena",
            "3": "Amazon Redshift",
            "4": "Amazon EMR"
        },
        "Correct Answer": "Amazon Athena",
        "Explanation": "Amazon Athena는 서버리스 대화형 쿼리 서비스로, 사용자가 인프라 관리 없이 SQL을 사용하여 Amazon S3에 저장된 데이터를 분석할 수 있게 해줍니다. 다양한 데이터 형식을 지원하며 즉석 쿼리에 적합합니다. 또한 데이터 시각화를 위해 Amazon QuickSight와 원활하게 통합됩니다.",
        "Other Options": [
            "Amazon Redshift는 완전 관리형 데이터 웨어하우스 서비스로, 리소스 프로비저닝이 필요하며 Athena에 비해 즉석 쿼리에 대해 더 높은 비용이 발생할 수 있습니다.",
            "AWS Glue는 주로 ETL(추출, 변환, 적재) 프로세스 및 데이터 카탈로그를 위한 데이터 통합 서비스로, 데이터의 직접 쿼리를 위한 것이 아닙니다.",
            "Amazon EMR은 Apache Spark 또는 Hadoop과 같은 프레임워크를 사용하여 대규모 데이터 세트를 처리할 수 있는 관리형 빅데이터 프레임워크이지만, 서버 관리가 필요하며 Athena만큼 즉석 쿼리에 비용 효율적이지 않습니다."
        ]
    },
    {
        "Question Number": "57",
        "Situation": "데이터 엔지니어는 Amazon RDS 데이터베이스에서 예상보다 느리게 실행되는 SQL 쿼리를 최적화하는 작업을 맡고 있습니다. 이 쿼리는 여러 테이블에서 데이터를 검색하며 여러 조인 및 필터를 포함합니다. 엔지니어는 쿼리의 논리를 변경하지 않고 성능을 개선할 방법을 찾고 있습니다.",
        "Question": "데이터 엔지니어는 SQL 쿼리 성능을 최적화하기 위해 어떤 접근 방식을 취해야 합니까?",
        "Options": {
            "1": "Amazon Redshift를 사용하여 데이터를 마이그레이션하고 최적화된 쿼리 엔진의 이점을 누립니다.",
            "2": "Amazon RDS 데이터베이스의 인스턴스 크기를 늘려 쿼리 실행을 위한 처리 능력을 향상시킵니다.",
            "3": "더 나은 성능을 위해 조인 대신 서브쿼리를 사용하도록 SQL 쿼리를 다시 작성합니다.",
            "4": "쿼리 실행 속도를 높이기 위해 조인 및 필터링되는 열에 적절한 인덱스를 추가합니다."
        },
        "Correct Answer": "쿼리 실행 속도를 높이기 위해 조인 및 필터링되는 열에 적절한 인덱스를 추가합니다.",
        "Explanation": "조인 및 필터에 사용되는 열에 인덱스를 추가하면 데이터를 검색하는 데 걸리는 시간을 크게 줄일 수 있습니다. 데이터베이스 엔진이 전체 테이블을 스캔하는 대신 필요한 행을 신속하게 찾을 수 있기 때문입니다.",
        "Other Options": [
            "SQL 쿼리를 서브쿼리를 사용하도록 다시 작성하는 것은 성능을 반드시 개선하지 않으며, 더 복잡한 실행 계획을 초래하여 성능 저하를 초래할 수 있습니다.",
            "Amazon RDS 데이터베이스의 인스턴스 크기를 늘리면 더 많은 리소스를 제공할 수 있지만, 비효율적인 쿼리 설계의 근본적인 문제를 직접 해결하지 않으므로 여전히 느린 성능을 초래할 수 있습니다.",
            "Amazon Redshift를 사용하는 것은 쿼리 최적화에 적합한 솔루션이 아니며, 데이터 마이그레이션을 포함하고 RDS에서 기존 SQL 쿼리의 성능 문제를 직접 해결하지 않습니다."
        ]
    },
    {
        "Question Number": "58",
        "Situation": "데이터 엔지니어가 Amazon Athena를 사용하는 대규모 데이터 처리 애플리케이션의 성능을 최적화하는 임무를 맡았습니다. 이 애플리케이션은 비용을 통제하면서 파티션된 데이터 세트에 대한 효율적인 쿼리를 보장해야 합니다.",
        "Question": "이 시나리오에서 쿼리 성능을 가장 효과적으로 향상시키고 비용을 관리할 수 있는 전략은 무엇입니까?",
        "Options": {
            "1": "데이터를 압축하지 않은 텍스트 형식으로 유지하고 단일 비파티션 데이터 세트에 대해 쿼리하여 사용자 접근을 단순화합니다.",
            "2": "날짜 및 지역별 데이터 파티셔닝을 구현하고 데이터 세트를 Parquet 형식으로 변환하며, 데이터 스캔에 대한 쿼리당 제한이 있는 작업 그룹을 설정합니다.",
            "3": "데이터 저장을 위해 CSV 파일을 사용하고 복잡성을 줄이기 위해 파티션 수를 제한하며, 비용 통제가 없는 단일 작업 그룹을 강제합니다.",
            "4": "모든 데이터를 JSON 형식으로 저장하고 GZIP으로 압축하며, 쿼리 실패를 피하기 위해 모든 쿼리에 대해 무제한 데이터 스캔을 허용합니다."
        },
        "Correct Answer": "날짜 및 지역별 데이터 파티셔닝을 구현하고 데이터 세트를 Parquet 형식으로 변환하며, 데이터 스캔에 대한 쿼리당 제한이 있는 작업 그룹을 설정합니다.",
        "Explanation": "이 접근 방식은 데이터 파티셔닝을 효과적으로 활용하여 쿼리에서 스캔되는 데이터 양을 최소화하고, 효율적인 Parquet 형식으로 성능을 향상시키며, 작업 그룹과 쿼리당 제한을 통해 비용을 통제하여 쿼리가 예산 한도를 초과하지 않도록 보장합니다.",
        "Other Options": [
            "JSON 형식과 GZIP 압축을 사용하는 것은 Parquet만큼의 성능 이점을 제공하지 않으며, 무제한 데이터 스캔을 허용하면 예상치 못한 비용과 쿼리 실패로 이어질 수 있습니다.",
            "CSV 형식으로 데이터를 저장하고 파티션을 제한하는 것은 Parquet 또는 ORC와 같은 최적화된 데이터 저장 형식의 이점을 활용하지 못하며, 비용 통제가 부족하면 높은 비용으로 이어질 수 있습니다.",
            "압축하지 않은 텍스트 형식으로 데이터를 유지하고 비파티션 데이터 세트에 대해 쿼리하는 것은 비효율적이며, 이는 대량의 데이터를 스캔하게 되어 쿼리 성능 저하와 비용 증가를 초래합니다."
        ]
    },
    {
        "Question Number": "59",
        "Situation": "금융 서비스 조직이 민감한 고객 데이터를 처리하기 위해 AWS 분석 서비스를 사용할 계획입니다. 모든 데이터가 정지 상태와 전송 중에 암호화되어 규제 기준을 준수해야 합니다. 조직은 Amazon Redshift, Amazon EMR 및 AWS Glue와 같은 AWS 분석 서비스에서 사용할 수 있는 데이터 암호화 옵션을 평가하고 있습니다.",
        "Question": "조직이 선택한 AWS 분석 서비스 전반에 걸쳐 강력한 데이터 보안을 보장하기 위해 어떤 암호화 옵션을 구현해야 합니까?",
        "Options": {
            "1": "AWS Glue에서 암호화되지 않은 데이터에 대한 접근을 제한하기 위해 VPC 엔드포인트 정책을 활용합니다.",
            "2": "Amazon EMR 및 AWS Glue에 저장된 데이터에 대해 S3 관리 키를 사용하여 서버 측 암호화를 활성화합니다.",
            "3": "AWS Key Management Service (KMS)를 사용하여 암호화 키를 관리하고 Amazon Redshift에서 정지 상태의 데이터에 대한 암호화를 활성화합니다.",
            "4": "Amazon Redshift 및 Amazon EMR에 전송되기 전에 데이터에 대해 클라이언트 측 암호화만 구현합니다."
        },
        "Correct Answer": "AWS Key Management Service (KMS)를 사용하여 암호화 키를 관리하고 Amazon Redshift에서 정지 상태의 데이터에 대한 암호화를 활성화합니다.",
        "Explanation": "AWS Key Management Service (KMS)를 사용하는 것은 암호화 키를 관리하고 Amazon Redshift에서 정지 상태의 데이터가 암호화되도록 보장하는 권장 접근 방식입니다. 이 접근 방식은 중앙 집중식 키 관리를 가능하게 하고 보안 및 규제 기준을 준수합니다.",
        "Other Options": [
            "S3 관리 키를 사용하여 서버 측 암호화를 활성화하는 것은 유효한 옵션이지만, Amazon Redshift에서 정지 상태의 데이터에 대한 암호화를 직접 제공하지 않으며 KMS를 키 관리에 활용하지 않습니다.",
            "클라이언트 측 암호화는 전송 중인 데이터에 대한 암호화를 보장하지 않으며, 키 관리를 위한 추가 처리가 필요하여 AWS 서비스와의 통합을 복잡하게 만들 수 있습니다.",
            "VPC 엔드포인트 정책은 데이터 암호화에 직접적으로 대응하지 않습니다. 이들은 접근 제어에 중점을 두며 데이터가 암호화되도록 보장하지 않으므로 규정 준수 기준을 충족하는 데 필수적입니다."
        ]
    },
    {
        "Question Number": "60",
        "Situation": "데이터 엔지니어가 Amazon RDS 인스턴스에 저장된 대규모 데이터 세트를 분석에 적합한 형식으로 변환하는 임무를 맡았습니다. 변환 과정에서는 5년 이상 된 레코드를 필터링하고 나머지 데이터를 카테고리별로 집계해야 합니다. 팀은 데이터를 외부 도구로 내보내지 않고 RDS 인스턴스에서 직접 SQL 쿼리를 사용하여 이 변환을 실행하고자 합니다.",
        "Question": "데이터 엔지니어가 데이터를 올바르게 필터링하고 집계하기 위해 어떤 SQL 쿼리를 사용해야 합니까?",
        "Options": {
            "1": "SELECT category, COUNT(*) AS total_records FROM data_table WHERE record_date >= DATE_SUB(CURDATE(), INTERVAL 5 YEAR) GROUP BY category;",
            "2": "SELECT category, SUM(value) AS total_value FROM data_table WHERE record_date < DATE_SUB(CURDATE(), INTERVAL 5 YEAR) GROUP BY category;",
            "3": "SELECT category, COUNT(*) AS total_records FROM data_table WHERE record_date < DATE_ADD(CURDATE(), INTERVAL -5 YEAR) GROUP BY category;",
            "4": "SELECT category, AVG(value) AS average_value FROM data_table WHERE record_date >= DATE_ADD(CURDATE(), INTERVAL -5 YEAR) GROUP BY category;"
        },
        "Correct Answer": "SELECT category, COUNT(*) AS total_records FROM data_table WHERE record_date >= DATE_SUB(CURDATE(), INTERVAL 5 YEAR) GROUP BY category;",
        "Explanation": "올바른 SQL 쿼리는 레코드를 필터링하여 최근 5년 이내의 데이터만 포함하고, 이를 카테고리별로 그룹화하여 각 카테고리의 총 레코드 수를 계산합니다. 이는 데이터를 적절하게 필터링하고 집계하는 요구 사항을 충족합니다.",
        "Other Options": [
            "이 옵션은 5년 이상 된 레코드를 포함하는 대신 잘못 필터링하여 변환 요구 사항을 충족하지 않습니다.",
            "이 옵션은 레코드를 세는 대신 값을 집계하기 위해 SUM을 잘못 사용하고 5년 이상 된 레코드를 필터링하여 의도된 작업이 아닙니다.",
            "이 옵션은 COUNT 대신 AVG를 잘못 사용하고 최근 레코드만 포함하도록 필터링하여 요구 사항에서 지정한 내용과 일치하지 않습니다."
        ]
    },
    {
        "Question Number": "61",
        "Situation": "데이터 엔지니어링 팀은 분석을 위해 대규모 데이터 세트를 처리하는 임무를 맡고 있습니다. 그들은 확장 가능한 환경에서 복잡한 데이터 변환 스크립트를 실행할 수 있는 솔루션이 필요합니다. 이들은 이를 위해 다양한 AWS 서비스를 고려하고 있습니다.",
        "Question": "다음 중 복잡한 데이터 변환을 스크립팅하고 실행할 수 있는 AWS 서비스는 무엇입니까?",
        "Options": {
            "1": "AWS Glue",
            "2": "Amazon DynamoDB",
            "3": "Amazon EMR",
            "4": "Amazon RDS"
        },
        "Correct Answer": "Amazon EMR",
        "Explanation": "Amazon EMR은 대규모 데이터 처리를 위해 특별히 설계되었으며, Apache Spark 및 Apache Hive와 같은 프레임워크를 사용하여 복잡한 데이터 변환을 위한 스크립트를 실행하는 것을 지원합니다. 데이터 엔지니어링 작업에 적합한 확장 가능하고 유연한 환경을 제공합니다.",
        "Other Options": [
            "Amazon RDS는 주로 관계형 데이터베이스 서비스이며, 복잡한 데이터 변환을 위한 스크립팅을 본질적으로 지원하지 않습니다.",
            "Amazon DynamoDB는 전통적인 데이터 변환 스크립팅을 지원하지 않는 NoSQL 데이터베이스로, 복잡한 변환보다는 고성능 데이터 접근 및 저장을 위해 설계되었습니다.",
            "AWS Glue는 데이터 변환을 허용하는 완전 관리형 ETL 서비스이지만, 임의의 스크립트 실행 및 복잡한 처리 작업에 있어 Amazon EMR만큼 유연하지 않습니다."
        ]
    },
    {
        "Question Number": "62",
        "Situation": "데이터 엔지니어링 팀은 다양한 출처에서 들어오는 데이터 스트림을 처리하는 임무를 맡고 있습니다. 그들은 서버나 인프라를 관리하지 않고 데이터 처리 워크플로우를 자동화하고 싶어합니다. 팀은 이 목표를 달성하기 위해 AWS Lambda를 사용하려고 고려하고 있습니다.",
        "Question": "팀이 AWS Lambda를 사용하여 데이터 스트림을 효율적으로 처리하면서 확장성과 신뢰성을 보장하기 위해 어떤 접근 방식을 구현해야 합니까?",
        "Options": {
            "1": "정기적으로 새로운 데이터를 확인하고 처리하는 예약된 AWS Lambda 함수를 생성합니다.",
            "2": "새로운 데이터가 업로드될 때 Lambda 함수를 트리거하는 Amazon S3 이벤트 알림을 설정합니다.",
            "3": "데이터를 수집하고 처리를 위해 Lambda 함수를 트리거하는 Amazon Kinesis Data Streams를 구현합니다.",
            "4": "데이터 처리를 위해 일련의 Lambda 함수를 조정하는 AWS Step Functions를 사용합니다."
        },
        "Correct Answer": "데이터를 수집하고 처리를 위해 Lambda 함수를 트리거하는 Amazon Kinesis Data Streams를 구현합니다.",
        "Explanation": "Amazon Kinesis Data Streams를 사용하면 팀이 실시간 데이터 수집 및 처리를 효율적으로 처리할 수 있습니다. 이는 다양한 데이터 부하에 맞춰 자동으로 확장할 수 있으며, 각 레코드는 처리를 위해 Lambda 함수를 트리거할 수 있어 시스템이 반응적이고 신뢰성을 유지할 수 있습니다.",
        "Other Options": [
            "Amazon S3 이벤트 알림을 설정하는 것은 유효한 접근 방식이지만, Kinesis를 사용하는 것만큼 연속 데이터 처리에 효율적이지 않습니다. S3 이벤트 알림은 스트리밍 데이터보다는 배치 처리에 더 적합합니다.",
            "AWS Step Functions를 사용하여 Lambda 함수를 조정하는 것은 추가적인 복잡성과 지연을 초래합니다. 워크플로우 관리를 위해 유용할 수 있지만, Kinesis와 같은 실시간 데이터 처리에 특별히 설계된 것은 아닙니다.",
            "예약된 AWS Lambda 함수를 생성하는 것은 데이터 스트림의 실시간 처리를 제공하지 않기 때문에 데이터 처리에 지연을 초래할 수 있습니다. 이 방법은 불필요한 함수 호출로 인해 운영 비용이 증가할 수도 있습니다."
        ]
    },
    {
        "Question Number": "63",
        "Situation": "한 스타트업이 AWS에서 데이터 인프라를 구축하고 있으며, 관계형 데이터에 Amazon RDS를 사용하고 비구조적 데이터 저장을 위해 Amazon S3를 사용하고 있습니다. 그들은 확장하면서 데이터 거버넌스 기준을 준수하고 관리 오버헤드를 최소화해야 합니다. 이들은 관리형 서비스인 Amazon RDS와 EC2에서 자체 호스팅된 데이터베이스와 같은 비관리형 서비스의 차이를 평가하고 있습니다.",
        "Question": "데이터 보안 및 거버넌스 측면에서 관리형 서비스를 비관리형 서비스보다 사용하는 주요 이점을 가장 잘 설명하는 진술은 무엇입니까?",
        "Options": {
            "1": "관리형 서비스는 초기 설정이 적게 필요하고 데이터 접근 정책에 대한 더 큰 제어를 제공합니다.",
            "2": "비관리형 서비스는 맞춤형 보안 구성을 허용하여 더 많은 사용자 정의를 제공합니다.",
            "3": "비관리형 서비스는 전용 리소스와 격리로 인해 본질적으로 더 나은 성능을 제공합니다.",
            "4": "관리형 서비스는 자동 업데이트 및 패치 관리를 제공하여 취약성의 위험을 줄입니다."
        },
        "Correct Answer": "관리형 서비스는 자동 업데이트 및 패치 관리를 제공하여 취약성의 위험을 줄입니다.",
        "Explanation": "Amazon RDS와 같은 관리형 서비스는 백업, 패치 및 업데이트와 같은 일상적인 유지 관리 작업을 자동으로 처리하여 보안 취약성이 신속하게 해결되도록 합니다. 이는 데이터 엔지니어링 팀의 운영 부담을 줄이고 전반적인 데이터 보안 및 거버넌스 기준 준수를 향상시킵니다.",
        "Other Options": [
            "비관리형 서비스는 사용자 정의를 허용할 수 있지만, 적절히 관리되지 않으면 복잡성과 위험이 증가할 수 있습니다. 유연성은 종종 추가적인 보안 책임을 수반합니다.",
            "관리형 서비스는 초기 설정을 단순화하지만, 특정 요구 사항에 따라 비관리형 서비스만큼 보안 구성에 대한 동일한 수준의 제어를 허용하지 않을 수 있습니다.",
            "비관리형 서비스는 특정 시나리오에서 성능 이점을 제공할 수 있지만, 리소스 및 구성을 관리해야 하므로 전반적인 보안 및 거버넌스에 부정적인 영향을 미칠 수 있습니다."
        ]
    },
    {
        "Question Number": "64",
        "Situation": "한 금융 서비스 회사가 ETL 프로세스를 위해 AWS Glue를 사용하여 데이터 수집 파이프라인을 구현했습니다. 이 파이프라인은 트랜잭션 데이터베이스와 제3자 API를 포함한 다양한 소스에서 데이터를 수집합니다. 데이터 무결성을 보장하고 오류 발생 시 데이터를 재처리할 수 있도록 팀은 데이터 손실 없이 데이터 수집 작업을 재실행할 수 있는 전략을 구현해야 합니다.",
        "Question": "이 시나리오에서 데이터 수집 프로세스의 재실행 가능성을 가장 잘 지원하는 접근 방식은 무엇입니까?",
        "Options": {
            "1": "Amazon SQS 큐를 구현하여 들어오는 데이터 요청을 수신하고 AWS Glue 작업을 트리거하여 각 메시지의 상태를 추적하고 관리할 수 있는 작업의 재처리를 가능하게 합니다.",
            "2": "원시 데이터를 Amazon S3에 저장하고 데이터를 처리하기 위해 예약된 AWS Glue 작업을 생성하여 팀이 필요할 때마다 작업을 온디맨드로 다시 실행할 수 있도록 하며 처리된 데이터를 추적합니다.",
            "3": "Amazon Kinesis Data Streams를 활용하여 들어오는 데이터를 버퍼링하고 Lambda 함수를 구성하여 거의 실시간으로 데이터를 처리하며 보존 기간 내에 데이터를 재생할 수 있는 기능을 유지합니다.",
            "4": "Amazon EventBridge를 활용하여 들어오는 데이터 이벤트를 캡처하고 AWS Glue 작업을 트리거하여 각 이벤트가 잠재적인 재생을 위해 기록되도록 하며 상태 관리를 가능하게 합니다."
        },
        "Correct Answer": "Amazon Kinesis Data Streams를 활용하여 들어오는 데이터를 버퍼링하고 Lambda 함수를 구성하여 거의 실시간으로 데이터를 처리하며 보존 기간 내에 데이터를 재생할 수 있는 기능을 유지합니다.",
        "Explanation": "Amazon Kinesis Data Streams를 사용하면 들어오는 데이터를 버퍼링하기 위한 신뢰할 수 있고 확장 가능한 솔루션을 제공합니다. 이는 실시간 처리를 가능하게 하며, 특히 정의된 보존 기간 내에 데이터를 재생할 수 있도록 하여 데이터 수집 오류를 데이터 손실 없이 해결할 수 있도록 합니다.",
        "Other Options": [
            "Amazon SQS 큐를 구현하는 것은 Kinesis가 제공하는 내장 재생 기능이 부족합니다. 메시지 상태를 추적할 수 있지만, SQS는 Kinesis만큼 메시지를 오래 보존하지 않으므로 수집 작업을 재생하는 데 효과적이지 않습니다.",
            "원시 데이터를 Amazon S3에 저장하고 예약된 AWS Glue 작업을 생성하는 것은 실시간 처리 기능을 제공하지 않습니다. 온디맨드 재실행을 허용하지만 긴급한 재처리 요구에 비효율적일 수 있으며 데이터 수집 오류를 해결하는 데 지연이 발생할 수 있습니다.",
            "Amazon EventBridge를 활용하는 것은 이벤트 기반 아키텍처에 더 중점을 두고 있으며, 데이터 재생 기능이 활성화되어 있지 않습니다. 잠재적인 재생을 위해 이벤트를 기록할 수 있지만, Kinesis가 데이터 수집을 위해 제공하는 버퍼링 및 상태 관리 기능을 본질적으로 제공하지 않습니다."
        ]
    },
    {
        "Question Number": "65",
        "Situation": "데이터 엔지니어는 모든 AWS 계정 활동이 준수 및 감사 목적으로 기록되도록 하는 책임이 있습니다. 엔지니어는 여러 계정에서 중앙 집중식 로그 쿼리를 가능하게 하여 로그를 쉽게 분석하고 시각화할 수 있는 솔루션을 구현해야 합니다.",
        "Question": "데이터 엔지니어가 여러 계정에서 로그 쿼리를 효과적으로 중앙 집중화하면서 로그를 분석 및 준수를 위해 쉽게 접근할 수 있도록 하려면 어떤 AWS 서비스를 사용해야 합니까?",
        "Options": {
            "1": "AWS CloudTrail Lake를 사용하여 여러 계정의 로그를 집계하고 분석을 위해 SQL 기반 쿼리를 실행합니다.",
            "2": "Amazon CloudWatch Logs를 활용하여 다양한 AWS 서비스에서 로그를 수집하고 시각화를 위한 사용자 정의 대시보드를 생성합니다.",
            "3": "AWS Config를 구현하여 구성 변경 사항을 추적하고 준수 감사 보고서를 생성합니다.",
            "4": "AWS Lambda를 활용하여 로그를 실시간으로 처리하고 Amazon S3에 저장하여 이후 쿼리를 수행합니다."
        },
        "Correct Answer": "AWS CloudTrail Lake를 사용하여 여러 계정의 로그를 집계하고 분석을 위해 SQL 기반 쿼리를 실행합니다.",
        "Explanation": "AWS CloudTrail Lake는 중앙 집중식 로그를 위해 특별히 설계되었으며, 여러 계정에서 SQL 기반 쿼리를 가능하게 하여 준수 및 감사 목적으로 이상적인 선택입니다.",
        "Other Options": [
            "Amazon CloudWatch Logs는 다양한 서비스에서 로그를 수집할 수 있지만, CloudTrail Lake와 같이 여러 계정에서 중앙 집중식 로그 쿼리를 위해 특별히 설계되지 않았습니다.",
            "AWS Config는 중앙 집중식 로그가 아닌 구성 변경 사항 추적에 중점을 두고 있어 로그 집계 및 쿼리 목적에 적합하지 않습니다.",
            "AWS Lambda는 로그를 처리할 수 있는 서버리스 컴퓨팅 서비스이지만, 중앙 집중식 로그 집계나 SQL 기반 쿼리를 위한 내장 솔루션을 제공하지 않습니다."
        ]
    }
]