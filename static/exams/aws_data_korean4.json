[
    {
        "Question Number": "1",
        "Situation": "데이터 엔지니어가 AWS 서비스를 사용하여 대규모 데이터 세트를 효율적으로 처리하는 임무를 맡았습니다. 그들은 데이터를 분할, 변환 및 데이터 웨어하우스에 로드하기 위해 다양한 서비스를 고려하고 있습니다.",
        "Question": "이 작업에 효과적으로 활용할 수 있는 서비스 조합은 무엇입니까? (두 개 선택)",
        "Options": {
            "1": "분산 데이터 처리를 위한 Amazon EMR",
            "2": "실시간 데이터 처리를 위한 AWS Lambda",
            "3": "데이터 웨어하우징을 위한 Amazon Redshift",
            "4": "ETL 작업을 위한 AWS Glue",
            "5": "데이터 저장을 위한 Amazon S3"
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "분산 데이터 처리를 위한 Amazon EMR",
            "ETL 작업을 위한 AWS Glue"
        ],
        "Explanation": "Amazon EMR은 분산 데이터 처리를 위해 설계되었으며 Apache Spark, Hadoop 및 기타 프레임워크를 사용하여 대규모 데이터 작업을 처리할 수 있습니다. AWS Glue는 분석을 위해 데이터를 준비하고 변환하는 것을 쉽게 해주는 완전 관리형 ETL 서비스입니다. 이 두 서비스는 함께 대규모 데이터 세트를 효율적으로 처리하고 변환한 후 Amazon Redshift와 같은 데이터 웨어하우스에 로드할 수 있습니다.",
        "Other Options": [
            "Amazon S3는 주로 저장 서비스이며 데이터를 직접 처리하거나 변환하지 않으므로 주어진 작업에 불충분합니다.",
            "Amazon Redshift는 데이터를 저장하는 데이터 웨어하우스이지만 처리 서비스가 아니며, 데이터를 로드해야 하므로 처리 요구 사항에 맞지 않습니다.",
            "AWS Lambda는 이벤트에 대한 응답으로 실시간 데이터 처리에 적합하지만 복잡한 변환이 필요한 대규모 데이터 처리 또는 ETL 작업에는 이상적이지 않습니다."
        ]
    },
    {
        "Question Number": "2",
        "Situation": "금융 서비스 회사가 온프레미스 관계형 데이터베이스를 AWS로 마이그레이션할 계획입니다. 그들은 기존 데이터베이스의 스키마가 새로운 AWS 환경에 맞게 정확하게 변환되기를 원합니다. 팀은 이 작업을 수행하기 위해 AWS 도구를 사용하는 것을 고려하고 있습니다.",
        "Question": "최소한의 노력으로 스키마 변환을 수행하는 가장 효과적인 방법은 무엇입니까? (두 개 선택)",
        "Options": {
            "1": "AWS Schema Conversion Tool (AWS SCT)를 사용하여 기존 데이터베이스 스키마를 Amazon Aurora와 호환되는 형식으로 변환합니다.",
            "2": "AWS Database Migration Service (AWS DMS)를 사용하여 기존 데이터베이스 스키마와 데이터를 Amazon RDS로 직접 복제합니다.",
            "3": "AWS Glue를 사용하여 AWS로 마이그레이션하기 전에 기존 데이터베이스에서 ETL 프로세스를 수행합니다.",
            "4": "도구를 사용하지 않고 새로운 AWS 환경에서 데이터베이스 스키마를 수동으로 다시 작성합니다.",
            "5": "AWS Schema Conversion Tool (AWS SCT)를 활용하여 스키마를 분석하고 변환한 다음 Amazon RDS에 변경 사항을 적용합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "AWS Schema Conversion Tool (AWS SCT)를 사용하여 기존 데이터베이스 스키마를 Amazon Aurora와 호환되는 형식으로 변환합니다.",
            "AWS Schema Conversion Tool (AWS SCT)를 활용하여 스키마를 분석하고 변환한 다음 Amazon RDS에 변경 사항을 적용합니다."
        ],
        "Explanation": "AWS Schema Conversion Tool (AWS SCT)를 활용하면 기존 데이터베이스 스키마의 자동 분석 및 변환이 가능하여 Amazon Aurora 또는 Amazon RDS와 같은 AWS 서비스와 호환되도록 하여 수동 노력을 줄이고 잠재적인 오류를 최소화할 수 있습니다.",
        "Other Options": [
            "데이터베이스 스키마를 수동으로 다시 작성하는 것은 시간 소모적이며 인적 오류가 발생할 수 있어 스키마 변환을 위해 설계된 자동화 도구를 사용하는 목적에 반합니다.",
            "AWS DMS는 주로 데이터 마이그레이션을 목표로 하며 스키마 변환보다는 데이터 복제에 중점을 두고 있어 스키마를 효과적으로 변환하는 데 필요한 도구를 제공하지 않습니다.",
            "AWS Glue는 ETL(추출, 변환, 로드) 프로세스에 중점을 두고 있으며 스키마 변환을 위해 특별히 설계되지 않아 AWS SCT에 비해 이 작업에 덜 적합합니다."
        ]
    },
    {
        "Question Number": "3",
        "Situation": "데이터 엔지니어가 낮은 대기 시간의 읽기 및 쓰기 작업이 필요한 고트래픽 웹 애플리케이션을 위한 데이터 저장 솔루션을 설계하고 있습니다. 이 솔루션은 또한 사용자 수요의 갑작스러운 급증을 처리할 수 있는 확장성을 제공하면서 비용 효율적이어야 합니다.",
        "Question": "이 요구 사항을 가장 잘 충족하기 위해 엔지니어가 선택해야 할 데이터 저장 서비스는 무엇입니까?",
        "Options": {
            "1": "프로비저닝된 IOPS가 있는 Amazon RDS",
            "2": "S3 Select가 있는 Amazon S3",
            "3": "온디맨드 용량 모드가 있는 Amazon DynamoDB",
            "4": "동시성 확장이 있는 Amazon Redshift"
        },
        "Correct Answer": "온디맨드 용량 모드가 있는 Amazon DynamoDB",
        "Explanation": "온디맨드 용량 모드가 있는 Amazon DynamoDB는 고트래픽 애플리케이션을 위해 설계되어 낮은 대기 시간의 읽기 및 쓰기 작업을 제공하며, 변동하는 작업 부하를 처리하기 위해 자동으로 확장됩니다. 이는 예측할 수 없는 트래픽 패턴을 가진 애플리케이션에 비용 효율적인 솔루션이 됩니다.",
        "Other Options": [
            "S3 Select가 있는 Amazon S3는 주로 데이터 레이크 저장 솔루션이며 분석에 최적화되어 있어 즉각적인 데이터 접근이 필요한 고트래픽 애플리케이션에는 적합하지 않습니다.",
            "프로비저닝된 IOPS가 있는 Amazon RDS는 낮은 대기 시간 성능을 제공할 수 있지만 DynamoDB의 온디맨드 용량 모드에 비해 갑작스러운 수요 급증에 대해 효율적으로 또는 비용 효율적으로 확장하지 못할 수 있습니다.",
            "동시성 확장이 있는 Amazon Redshift는 복잡한 분석 쿼리 및 데이터 웨어하우징에 최적화되어 있어 낮은 대기 시간의 읽기 및 쓰기 작업에는 덜 적합합니다."
        ]
    },
    {
        "Question Number": "4",
        "Situation": "데이터 엔지니어가 Amazon RDS에서 관계형 데이터베이스를 관리하는 임무를 맡았습니다. 엔지니어는 사용자 프로필 정보를 저장하기 위해 새로운 테이블을 생성하고, 데이터를 삽입, 업데이트, 삭제 및 쿼리하는 다양한 작업을 수행해야 합니다. 이러한 작업을 위해 SQL 명령어를 사용해야 합니다.",
        "Question": "'user_id', 'username', 'email' 열이 있는 'users'라는 테이블을 올바르게 생성하는 SQL 명령어는 무엇입니까?",
        "Options": {
            "1": "CREATE TABLE users ( user_id INT PRIMARY KEY, username STRING(100), email STRING(100) );",
            "2": "CREATE TABLE users ( user_id INT PRIMARY KEY, username VARCHAR(100), email VARCHAR(100) );",
            "3": "CREATE TABLE users ( user_id INTEGER PRIMARY KEY, username VARCHAR(100), email VARCHAR(150) );",
            "4": "CREATE TABLE users ( user_id SERIAL PRIMARY KEY, username VARCHAR(100), email VARCHAR(100) );"
        },
        "Correct Answer": "CREATE TABLE users ( user_id INT PRIMARY KEY, username VARCHAR(100), email VARCHAR(100) );",
        "Explanation": "올바른 SQL 명령어는 지정된 열과 데이터 유형으로 'users'라는 테이블을 생성합니다. 'user_id'는 정수형이며 기본 키 역할을 하고, 'username'과 'email'은 최대 100자 길이의 가변 문자 필드로 정의됩니다.",
        "Other Options": [
            "이 옵션은 PostgreSQL에 특정한 'SERIAL' 데이터 유형을 사용하고 있어 일반 SQL 사용에 적합하지 않으므로 잘못된 것입니다.",
            "이 옵션은 데이터 유형으로 'STRING'을 잘못 사용하고 있으며, 이는 유효한 SQL 데이터 유형이 아닙니다. 올바른 유형은 'VARCHAR'여야 합니다.",
            "이 옵션은 'email' 필드의 길이를 150자로 잘못 지정하고 있으며, 이는 'username' 필드 길이가 100자라는 요구 사항과 일치하지 않습니다."
        ]
    },
    {
        "Question Number": "5",
        "Situation": "데이터 엔지니어링 팀은 Amazon RDS 데이터베이스의 쿼리 성능이 느려지고 있음을 발견했습니다. 그들은 성능 문제의 원인을 파악하고 데이터베이스의 전반적인 응답성을 개선하고자 합니다.",
        "Question": "팀이 Amazon RDS 데이터베이스의 성능 문제를 해결하기 위해 가장 먼저 취해야 할 접근 방식은 무엇입니까?",
        "Options": {
            "1": "성능 병목 현상을 분석하기 위해 느린 쿼리 로그를 분석합니다.",
            "2": "부하를 분산하기 위해 읽기 복제본을 구현합니다.",
            "3": "메트릭 수집을 위해 Enhanced Monitoring을 활성화합니다.",
            "4": "RDS 데이터베이스의 인스턴스 크기를 늘립니다."
        },
        "Correct Answer": "성능 병목 현상을 분석하기 위해 느린 쿼리 로그를 분석합니다.",
        "Explanation": "느린 쿼리 로그를 분석하면 팀이 실행 시간이 긴 특정 쿼리를 식별할 수 있습니다. 이는 데이터베이스 작업의 어떤 측면을 최적화해야 하는지를 이해하는 데 중요하며, 성능 문제를 해결하기 위한 가장 효과적인 첫 번째 단계입니다.",
        "Other Options": [
            "Enhanced Monitoring을 활성화하면 추가 메트릭을 제공하지만 느린 쿼리의 근본 원인을 직접적으로 해결하지 않으므로 초기 문제 해결 단계로서 효과적이지 않습니다.",
            "인스턴스 크기를 늘리면 성능 문제를 일시적으로 완화할 수 있지만 비효율적인 쿼리나 인덱싱과 같은 근본적인 문제를 해결하지 않으므로 먼저 식별해야 합니다.",
            "읽기 복제본을 구현하면 읽기 중심의 작업에 도움이 될 수 있지만 느린 쿼리의 근본적인 문제를 직접적으로 해결하지는 않습니다. 이는 문제 해결 단계라기보다는 확장 솔루션입니다."
        ]
    },
    {
        "Question Number": "6",
        "Situation": "데이터 엔지니어가 Amazon EMR 클러스터에서 생성된 모든 로그가 안전하게 저장되고 감사 및 규정 준수를 위해 쉽게 접근할 수 있도록 해야 합니다. 엔지니어는 효과적인 로그 관리를 위해 다양한 AWS 서비스와 통합되는 솔루션을 설계해야 합니다.",
        "Question": "Amazon EMR 클러스터에서 처리된 데이터의 안전한 로그 기록을 촉진하면서 로그가 내구성이 있고 쿼리 가능하도록 하는 AWS 서비스의 조합은 무엇입니까?",
        "Options": {
            "1": "Amazon S3와 AWS Glue 및 Amazon Redshift",
            "2": "AWS CloudWatch Logs와 Amazon RDS 및 Amazon Athena",
            "3": "Amazon DynamoDB와 AWS CloudTrail 및 Amazon EMR",
            "4": "Amazon S3와 AWS CloudTrail 및 Amazon Athena"
        },
        "Correct Answer": "Amazon S3와 AWS CloudTrail 및 Amazon Athena",
        "Explanation": "로그 저장 솔루션으로 Amazon S3를 사용하면 내구성과 확장성을 제공합니다. AWS CloudTrail은 EMR 클러스터에 대한 API 호출을 캡처하여 모든 작업이 규정 준수를 위해 기록되도록 합니다. Amazon Athena는 표준 SQL을 사용하여 S3에서 로그를 직접 쿼리할 수 있게 하여 로그 관리에 강력한 솔루션이 됩니다.",
        "Other Options": [
            "Amazon DynamoDB는 비용 및 성능 특성으로 인해 로그 기록 용도로 적합하지 않습니다. 로그를 저장할 수는 있지만 S3의 내구성과 쿼리 기능이 부족하며, AWS CloudTrail은 DynamoDB 로그를 모니터링하도록 설계되지 않았습니다.",
            "AWS CloudWatch Logs는 로그 관리에 유용하지만 Amazon RDS 및 Amazon Athena와 결합할 경우 Amazon EMR 로그에 대한 포괄적인 솔루션을 제공하지 않으며, RDS는 이 맥락에서 로그 저장 및 쿼리에 일반적으로 사용되지 않습니다.",
            "Amazon S3와 AWS Glue는 데이터 처리에 사용될 수 있지만, Amazon Redshift는 로그 저장에 적합하지 않습니다. Redshift는 주로 데이터 웨어하우스이며 Athena와 같은 로그 파일에 대한 직접 쿼리 기능을 제공하지 않습니다."
        ]
    },
    {
        "Question Number": "7",
        "Situation": "한 미디어 회사가 Amazon S3 버킷에 비디오 파일을 저장합니다. 이들은 새로운 비디오 콘텐츠를 자주 업로드하고 오래된 비디오를 삭제합니다. 회사는 사용자가 최신 콘텐츠에 문제 없이 접근할 수 있도록 S3가 데이터 일관성을 어떻게 처리하는지 이해할 필요가 있습니다.",
        "Question": "다음 중 이들의 사용 사례에 대한 S3 데이터 일관성 모델을 정확하게 설명하는 문장은 무엇입니까? (두 개 선택)",
        "Options": {
            "1": "S3에 업로드된 새로운 객체는 모든 지역에서 읽기-쓰기 일관성을 가지고 즉시 사용 가능합니다.",
            "2": "S3는 덮어쓰기 PUT 및 DELETE 요청에 대해 강력한 일관성을 제공하여 변경 사항이 즉시 가시화되도록 보장합니다.",
            "3": "사용자는 덮어쓰기 PUT 후 객체의 최신 버전을 보지만, 객체 목록은 일시적으로 이전 버전을 표시할 수 있습니다.",
            "4": "버킷에서 버전 관리를 처음 활성화할 때 S3는 모든 기존 객체에 대해 즉각적인 일관성을 보장합니다.",
            "5": "객체를 삭제한 후에는 최종 일관성으로 인해 다시 나열될 수 있기까지 지연이 있을 수 있습니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "S3에 업로드된 새로운 객체는 모든 지역에서 읽기-쓰기 일관성을 가지고 즉시 사용 가능합니다.",
            "S3는 덮어쓰기 PUT 및 DELETE 요청에 대해 강력한 일관성을 제공하여 변경 사항이 즉시 가시화되도록 보장합니다."
        ],
        "Explanation": "Amazon S3는 모든 새로운 객체 업로드에 대해 읽기-쓰기 일관성을 보장합니다. 즉, 객체가 업로드되면 즉시 읽을 수 있습니다. 또한, 덮어쓰기 PUT 및 DELETE 요청에 대해 강력한 일관성을 제공하여 이러한 작업이 모든 후속 읽기 요청에 즉시 가시화되도록 보장합니다.",
        "Other Options": [
            "이 문장은 잘못되었습니다. 객체를 삭제한 후 S3는 객체 목록에 대해 최종 일관성으로 작동합니다. 이는 삭제가 이후 목록 작업에 반영되기까지 지연이 있을 수 있음을 의미합니다.",
            "이 문장은 오해의 소지가 있습니다. S3는 버전 관리가 활성화될 때 모든 기존 객체에 대해 즉각적인 일관성을 보장하지 않으며, 향후 PUT 작업만 일관성을 보장합니다.",
            "이 문장은 잘못되었습니다. 객체 목록이 일시적으로 이전 버전을 표시할 수 있다고 제안하지만, 이는 덮어쓰기 PUT 작업에 대한 S3의 일관성 모델의 특성이 아닙니다. 대신 최신 버전이 즉시 가시화됩니다."
        ]
    },
    {
        "Question Number": "8",
        "Situation": "한 회사가 주기적으로 처리되는 대규모 데이터 세트를 저장하기 위해 Amazon S3를 사용하고 있습니다. 저장 비용을 최적화하고 데이터가 필요 이상으로 보존되지 않도록 하기 위해 데이터 엔지니어링 팀은 특정 기간이 지나면 객체를 자동으로 삭제하는 전략을 구현해야 합니다.",
        "Question": "Amazon S3에서 데이터가 특정 기간에 도달했을 때 만료시키기 위해 사용할 수 있는 방법은 무엇입니까? (두 개 선택)",
        "Options": {
            "1": "Amazon S3 인벤토리 보고서를 사용하여 특정 기간을 초과한 객체를 식별하고 수동으로 삭제합니다.",
            "2": "S3 버킷의 객체 나이를 확인하고 특정 임계값보다 오래된 객체를 삭제하는 AWS Lambda 함수를 매일 실행하도록 구현합니다.",
            "3": "AWS CloudTrail을 활용하여 객체 접근을 모니터링하고 특정 기간 동안 접근되지 않은 객체를 삭제합니다.",
            "4": "S3 수명 주기 정책을 설정하여 객체가 90일 동안 존재한 후 직접 삭제합니다.",
            "5": "S3 수명 주기 정책을 생성하여 객체를 30일 후 GLACIER 스토리지 클래스로 전환하고 추가 60일 후 삭제합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "S3 수명 주기 정책을 생성하여 객체를 30일 후 GLACIER 스토리지 클래스로 전환하고 추가 60일 후 삭제합니다.",
            "S3 수명 주기 정책을 설정하여 객체가 90일 동안 존재한 후 직접 삭제합니다."
        ],
        "Explanation": "두 가지 정답 옵션 모두 S3의 객체 수명 주기를 관리하기 위해 특별히 설계된 S3 수명 주기 정책을 사용합니다. 첫 번째 옵션은 삭제 전에 객체를 저비용 스토리지 클래스로 전환하고, 두 번째 옵션은 객체가 특정 기간에 도달한 후 직접 삭제합니다. 두 방법 모두 수동 개입 없이 데이터 만료를 효율적으로 자동화합니다.",
        "Other Options": [
            "AWS CloudTrail은 AWS 서비스에서 API 호출을 기록하고 모니터링하는 데 주로 사용되며, 객체 수명 주기를 관리하거나 만료 정책을 구현하는 데 사용되지 않습니다.",
            "AWS Lambda 함수를 사용하여 객체를 삭제할 수 있지만, 수동 설정이 필요하고 S3 수명 주기 정책의 내장 기능을 활용하지 않으므로 정답보다 효율성이 떨어집니다.",
            "Amazon S3 인벤토리 보고서는 객체 저장에 대한 통찰력을 제공하지만 삭제 프로세스를 자동화하지 않으며, 나이에 따라 객체를 삭제하기 위해 수동 노력이 필요합니다."
        ]
    },
    {
        "Question Number": "9",
        "Situation": "한 소매 회사가 온라인 거래 및 매장 구매를 포함한 다양한 출처에서 고객 데이터를 수집하고 있습니다. 데이터는 종종 서로 다른 형식, 누락된 값 및 중복으로 인해 일관성이 없습니다. 분석 및 보고를 위한 고품질 데이터를 보장하기 위해 회사는 효과적인 데이터 정제 기술을 구현해야 합니다. 이들은 이러한 기술을 적용할 가장 적절한 시기와 올바른 방법을 식별하고자 합니다.",
        "Question": "회사가 고품질 데이터를 보장하기 위해 데이터 정제 기술을 적용해야 할 시기는 언제입니까? (두 개 선택)",
        "Options": {
            "1": "데이터가 아카이브된 후 역사적 정확성을 보장하기 위해.",
            "2": "데이터 분석 후 분석 중 발견된 불일치를 수정하기 위해.",
            "3": "데이터 웨어하우스에 데이터를 로드하기 전에 나중에 문제를 피하기 위해.",
            "4": "데이터 무결성을 유지하기 위해 제3자 공급업체와 데이터를 공유하기 전에.",
            "5": "ETL 프로세스 중에 데이터를 변환하는 동안 정제합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "데이터 웨어하우스에 데이터를 로드하기 전에 나중에 문제를 피하기 위해.",
            "ETL 프로세스 중에 데이터를 변환하는 동안 정제합니다."
        ],
        "Explanation": "데이터 정제는 데이터 웨어하우스에 데이터를 로드하기 전에 적용해야 하며, 이는 다운스트림 분석에 영향을 줄 수 있는 문제를 방지하기 위함입니다. 또한 ETL 프로세스 중에 정제 기술을 구현하면 데이터가 변환되는 동안 정확하고 일관되게 유지되어 데이터 무결성을 유지하는 데 중요합니다.",
        "Other Options": [
            "데이터 분석 후 데이터 정제를 적용하는 것은 너무 늦으며, 이 단계에서 식별된 불일치는 결함 있는 데이터를 기반으로 잘못된 결론이나 결정을 초래할 수 있습니다.",
            "데이터가 아카이브된 후 정제하는 것은 현재 분석 및 보고 요구에 대한 데이터 품질을 유지하는 데 도움이 되지 않으므로 비효율적인 접근 방식입니다.",
            "제3자 공급업체와 공유할 때 데이터 무결성을 유지하는 것이 중요하지만, 이는 정제의 주요 시점이 되어서는 안 됩니다. 처리의 초기 단계에서 데이터를 정제하는 것이 더 효과적입니다."
        ]
    },
    {
        "Question Number": "10",
        "Situation": "데이터 엔지니어링 팀은 특정 간격으로 실행되어야 하고 이전 작업의 성공적인 완료에 의존하는 일련의 ETL 작업을 자동화하는 임무를 맡았습니다. 그들은 이러한 작업을 효율적으로 예약하고 적절한 모니터링 및 재시도 메커니즘이 마련되도록 관리형 서비스를 활용하고자 합니다.",
        "Question": "이 ETL 작업의 실행을 자동화하면서 워크플로우에 대한 가시성과 제어를 제공하는 최상의 솔루션은 무엇입니까?",
        "Options": {
            "1": "Apache Airflow를 구현하여 ETL 작업을 조정하고 복잡한 예약 및 종속성 관리를 가능하게 합니다.",
            "2": "EC2 인스턴스에서 사용자 지정 크론 작업을 생성하여 지정된 간격으로 ETL 스크립트를 실행합니다.",
            "3": "정의된 일정에 따라 각 ETL 작업에 대해 AWS Lambda 함수를 트리거하기 위해 Amazon EventBridge를 사용합니다.",
            "4": "AWS Glue 크롤러를 주기적으로 실행하도록 예약하고 크롤러의 완료에 따라 ETL 작업을 트리거합니다."
        },
        "Correct Answer": "Apache Airflow를 구현하여 ETL 작업을 조정하고 복잡한 예약 및 종속성 관리를 가능하게 합니다.",
        "Explanation": "Apache Airflow는 복잡한 워크플로우를 조정하기 위해 설계되었으며, 작업 간의 종속성을 처리하고 모니터링 및 예약을 위한 내장 기능을 제공합니다. 이는 특정 실행 순서와 전체 워크플로우에 대한 가시성이 필요한 ETL 작업을 관리하는 데 이상적입니다.",
        "Other Options": [
            "Amazon EventBridge를 사용하여 일정에 따라 AWS Lambda 함수를 트리거할 수 있지만, Apache Airflow가 제공하는 작업 종속성 및 워크플로우 관리에 대한 동일한 수준의 제어와 가시성을 제공하지 않습니다.",
            "EC2 인스턴스에서 사용자 지정 크론 작업을 생성하는 것은 기본적인 예약에는 효과적일 수 있지만, Apache Airflow와 같은 전용 조정 도구가 제공하는 모니터링, 오류 처리 및 작업 종속성 관리의 고급 기능이 부족합니다.",
            "AWS Glue 크롤러를 주기적으로 실행하도록 예약하는 것은 ETL 작업을 직접 실행하는 데 적합하지 않습니다. 크롤러는 스키마 발견에 사용되며 ETL 워크플로우의 실행을 관리하거나 작업 종속성을 효과적으로 처리하지 않습니다."
        ]
    },
    {
        "Question Number": "11",
        "Situation": "한 회사가 AWS를 사용하여 여러 데이터 소스를 호스팅하고 있으며, 이 데이터 소스에 대한 안전한 연결이 필요합니다. 데이터 엔지니어는 승인된 IP 주소만이 이러한 데이터 소스에 연결할 수 있도록 하는 솔루션을 구현하는 임무를 맡았습니다. 이는 민감한 데이터의 보안을 유지하는 데 중요합니다.",
        "Question": "데이터 엔지니어가 데이터 소스에 대한 안전한 연결을 가능하게 하는 IP 주소 허용 목록을 만들기 위해 어떤 접근 방식을 취해야 합니까?",
        "Options": {
            "1": "IP 주소가 아닌 지리적 위치를 기반으로 수신 트래픽을 필터링하기 위해 AWS WAF를 구현합니다.",
            "2": "승인된 IP 주소를 기반으로 데이터 소스에 대한 트래픽을 제한하기 위해 네트워크 ACL(NACL)을 설정합니다.",
            "3": "허용된 IP 주소에서만 수신 트래픽을 허용하도록 AWS 보안 그룹을 구성합니다.",
            "4": "IP 주소를 기반으로 액세스를 제한하기 위해 AWS Identity and Access Management(IAM) 정책을 사용합니다."
        },
        "Correct Answer": "허용된 IP 주소에서만 수신 트래픽을 허용하도록 AWS 보안 그룹을 구성합니다.",
        "Explanation": "AWS 보안 그룹을 사용하면 특정 리소스에 연결할 수 있는 IP 주소에 대한 세분화된 제어가 가능하므로 데이터 소스에 대한 안전한 연결을 위한 허용 목록을 만드는 효과적인 방법입니다.",
        "Other Options": [
            "IAM 정책은 주로 AWS 서비스 및 리소스에 대한 액세스를 제어하는 데 사용되며, IP 주소를 기반으로 한 네트워크 수준의 액세스에는 적합하지 않으므로 이 요구 사항에 부적합합니다.",
            "AWS WAF는 웹 애플리케이션 보안을 위해 설계되었으며, 애플리케이션 계층에서 작동하여 HTTP 요청에 집중하므로 데이터 소스에 대한 IP 주소 허용 목록을 특별히 다루지 않습니다.",
            "네트워크 ACL(NACL)은 서브넷 내에서 수신 및 송신 트래픽을 제어하는 데 사용할 수 있지만, 특정 리소스와의 연관성 측면에서 보안 그룹보다 유연성이 떨어지며 인스턴스 수준의 허용 목록을 위한 모범 사례가 아닙니다."
        ]
    },
    {
        "Question Number": "12",
        "Situation": "한 금융 서비스 회사가 데이터 웨어하우스를 Amazon Redshift로 마이그레이션하고 있습니다. 그들은 데이터 전송이 안전하고 특정 IP 주소만이 Redshift 클러스터에 접근할 수 있도록 보장하고자 합니다. 이 회사는 엄격한 보안 요구 사항을 가지고 있으며 적절한 SSL 설정과 VPC 보안 그룹을 구성해야 합니다.",
        "Question": "회사가 Amazon Redshift 클러스터를 안전하게 보호하기 위해 어떤 단계를 취해야 합니까? (두 가지 선택)",
        "Options": {
            "1": "CIDR 범위 0.0.0.0/0을 허용하여 보안 그룹에 대한 액세스를 승인합니다.",
            "2": "sslmode=require를 사용하여 psql로 SSL을 구성합니다.",
            "3": "클러스터의 기본 보안 그룹을 설정하여 모든 수신 트래픽을 허용합니다.",
            "4": "IP 필터링을 위해 aws redshift create-cluster-security-group 명령을 사용합니다.",
            "5": "Amazon Redshift에 저장된 데이터에 대해 휴면 상태에서 암호화를 구현합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "sslmode=require를 사용하여 psql로 SSL을 구성합니다.",
            "IP 필터링을 위해 aws redshift create-cluster-security-group 명령을 사용합니다."
        ],
        "Explanation": "데이터 전송의 보안을 보장하기 위해 회사는 sslmode=require 명령을 사용하여 SSL을 구성해야 하며, 이는 전송 중인 데이터를 암호화합니다. 또한 클러스터 보안 그룹을 생성하면 어떤 IP 주소가 클러스터에 접근할 수 있는지를 지정할 수 있어 보안을 강화합니다.",
        "Other Options": [
            "이 옵션은 모든 수신 트래픽을 허용하면 클러스터가 잠재적인 보안 취약점에 노출되므로 잘못된 것입니다. 이는 회사의 엄격한 보안 요구 사항에 부합하지 않습니다.",
            "이 옵션은 CIDR 범위 0.0.0.0/0을 승인하면 모든 IP 주소에서 접근할 수 있게 되어 IP 필터링의 목적을 무효화하고 보안을 위협하므로 잘못된 것입니다.",
            "휴면 상태에서의 암호화는 저장된 데이터를 보호하기 위한 좋은 관행이지만, 데이터 전송의 보안을 확보하거나 VPC 보안 그룹을 통해 접근을 관리하는 특정 요구 사항을 다루지 않습니다."
        ]
    },
    {
        "Question Number": "13",
        "Situation": "소매 회사가 SQL 데이터베이스와 CSV 파일을 포함한 다양한 출처의 판매 데이터를 분석하기 위해 Amazon Redshift를 사용하고 있습니다. 분석가는 이 데이터를 집계하고 변환하여 월별 판매 보고서를 생성해야 합니다. 그들은 총 판매 금액이 정확하게 계산되고 결과가 제품 카테고리와 월별로 그룹화되도록 하기를 원합니다. 분석가는 이를 달성하기 위해 SQL 쿼리를 작성하고 있습니다.",
        "Question": "다음 SQL 쿼리 중 판매 테이블에서 제품 카테고리와 월별로 총 판매 금액을 올바르게 집계하는 것은 무엇입니까?",
        "Options": {
            "1": "SELECT product_category, MONTH(sale_date) AS sale_month, AVG(sales_amount) FROM sales GROUP BY product_category, sale_month",
            "2": "SELECT product_category, sale_date, COUNT(sales_amount) FROM sales GROUP BY product_category, sale_date",
            "3": "SELECT product_category, EXTRACT(MONTH FROM sale_date) AS sale_month, SUM(sales_amount) FROM sales GROUP BY product_category, sale_month",
            "4": "SELECT product_category, YEAR(sale_date) AS sale_year, SUM(sales_amount) FROM sales GROUP BY product_category, sale_year"
        },
        "Correct Answer": "SELECT product_category, EXTRACT(MONTH FROM sale_date) AS sale_month, SUM(sales_amount) FROM sales GROUP BY product_category, sale_month",
        "Explanation": "이 쿼리는 EXTRACT 함수를 사용하여 sale_date에서 월을 가져오고, product_category와 추출된 월로 그룹화하며, sales_amount를 합산하여 카테고리별 월별 판매 총액을 제공합니다.",
        "Other Options": [
            "이 쿼리는 판매 금액의 수를 세는 대신 합산하고 있으며, 월별이 아닌 sale_date로 그룹화하여 월별 보고서에 필요한 올바른 집계를 제공하지 않습니다.",
            "이 쿼리는 판매 금액을 평균내는 대신 합산하고 있으며, 제품 카테고리와 월별로 그룹화하지만, 일부 SQL 방언에서 날짜에서 월을 추출하는 표준 SQL이 아닌 잘못된 함수(MONTH)를 사용합니다.",
            "이 쿼리는 월이 아닌 연도로 그룹화하여 월별 보고서를 생성하는 요구 사항을 충족하지 않습니다. 또한, 지정된 기간에 대한 판매 금액을 올바르게 집계하지 않습니다."
        ]
    },
    {
        "Question Number": "14",
        "Situation": "금융 기관이 데이터 분석 작업을 AWS로 마이그레이션하고 있습니다. 그들은 Amazon S3 버킷에 저장된 민감한 데이터에 접근할 수 있는 권한이 있는 인원만 접근할 수 있도록 해야 합니다. 이 기관은 역할 기반 접근 제어(RBAC)를 사용하여 권한을 관리하며, 데이터 분석가가 효과적으로 작업을 수행할 수 있도록 엄격한 거버넌스 정책을 준수하는 솔루션을 구현해야 합니다.",
        "Question": "데이터 엔지니어가 Amazon S3의 데이터 접근을 위한 역할 기반 접근 제어를 구현하기 위해 어떤 단계를 조합해야 합니까? (두 가지 선택)",
        "Options": {
            "1": "단일 정책 아래 여러 계정을 관리하기 위해 AWS Organizations를 활성화합니다.",
            "2": "그룹 접근을 위한 AWS Identity and Access Management (IAM) 정책을 구현합니다.",
            "3": "개별 사용자에 대한 접근 관리를 위해 Amazon S3 버킷 정책을 사용합니다.",
            "4": "특정 권한을 가진 다양한 직무 기능을 위한 IAM 역할을 생성합니다.",
            "5": "데이터 검색의 용이성을 위해 S3 버킷에 대한 공개 접근을 허용합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "특정 권한을 가진 다양한 직무 기능을 위한 IAM 역할을 생성합니다.",
            "그룹 접근을 위한 AWS Identity and Access Management (IAM) 정책을 구현합니다."
        ],
        "Explanation": "특정 권한을 가진 다양한 직무 기능을 위한 IAM 역할을 생성하면 기관이 최소 권한 원칙을 시행할 수 있어 직원들이 자신의 역할에 필요한 데이터에만 접근할 수 있도록 보장합니다. 그룹 접근을 위한 IAM 정책을 구현하면 권한 관리가 용이해져 특정 그룹의 모든 구성원이 개별 사용자 권한을 관리하지 않고도 동일한 접근 권한을 가질 수 있습니다.",
        "Other Options": [
            "개별 사용자에 대한 접근 관리를 위해 Amazon S3 버킷 정책을 사용하는 것은 IAM 역할 및 정책을 사용하는 것보다 효율성이 떨어지며, 특히 대규모 조직에서는 관리가 더 복잡해지고 RBAC 원칙과 일치하지 않습니다.",
            "S3 버킷에 대한 공개 접근을 허용하면 데이터 보안이 위협받으며, 이는 기관의 거버넌스 정책에 반합니다.",
            "단일 정책 아래 여러 계정을 관리하기 위해 AWS Organizations를 활성화하는 것은 S3의 데이터 접근을 위한 역할 기반 접근 제어와 직접적인 관련이 없습니다. 이는 계정을 관리하는 것에 더 가깝습니다."
        ]
    },
    {
        "Question Number": "15",
        "Situation": "금융 기관이 민감한 고객 데이터를 처리하고 있으며, 이 데이터가 저장 중 및 전송 중 모두 암호화되도록 해야 합니다. 이 기관은 암호화 키 관리를 위해 AWS Key Management Service (KMS)를 사용하는 것을 고려하고 있습니다. 그들은 Amazon S3에 데이터를 저장하기 전에 암호화하고 필요할 때 복호화할 수 있는 솔루션을 구현하고자 합니다.",
        "Question": "S3에 저장되기 전에 데이터가 암호화되고 AWS KMS를 사용하여 접근 시 복호화될 수 있도록 보장하는 가장 효율적인 방법은 무엇입니까?",
        "Options": {
            "1": "사용자가 AWS KMS를 사용하여 데이터를 암호화하고 암호화된 데이터를 S3에 업로드할 수 있도록 허용하는 IAM 정책을 생성합니다.",
            "2": "S3에 업로드하기 전에 클라이언트 측 암호화 라이브러리를 사용하여 데이터를 수동으로 암호화하고 AWS 외부에서 암호화 키를 관리합니다.",
            "3": "AWS KMS 관리 키(SSE-KMS)를 사용하여 S3 서버 측 암호화를 통해 추가 코드 없이 자동으로 암호화 및 복호화를 처리합니다.",
            "4": "AWS KMS를 사용하여 고객 관리 키를 생성한 다음, 이 키를 사용하여 들어오는 데이터를 자동으로 암호화하도록 S3 버킷을 구성합니다."
        },
        "Correct Answer": "AWS KMS 관리 키(SSE-KMS)를 사용하여 S3 서버 측 암호화를 통해 추가 코드 없이 자동으로 암호화 및 복호화를 처리합니다.",
        "Explanation": "AWS KMS 관리 키(SSE-KMS)를 사용하여 S3 서버 측 암호화를 사용하면 S3에 저장된 데이터의 암호화 및 복호화 프로세스가 간소화됩니다. AWS가 암호화 키를 관리하며, 데이터는 S3에 기록될 때 자동으로 암호화되고 접근할 때 복호화되어 추가 코드나 수동 암호화 프로세스가 필요하지 않습니다.",
        "Other Options": [
            "이 옵션은 AWS 외부에서 암호화 및 키 관리를 위한 추가 단계를 요구하여 운영 복잡성과 잠재적인 보안 위험을 증가시킵니다.",
            "이 옵션은 암호화 프로세스에 대한 제어를 허용하지만, 암호화 라이브러리와 키를 수동으로 처리해야 하므로 오류가 발생할 수 있고 효율성이 떨어집니다.",
            "IAM 정책만 생성하는 것은 암호화 또는 복호화를 자동으로 처리하지 않으며, 단순히 권한을 부여할 뿐 실제 암호화 프로세스를 다루지 않습니다."
        ]
    },
    {
        "Question Number": "16",
        "Situation": "금융 서비스 회사는 성능 문제와 시간에 따른 추세를 식별하기 위해 Amazon S3에 저장된 애플리케이션 로그를 분석해야 합니다. 복잡한 인프라를 설정하거나 서버를 관리할 필요 없이 이러한 로그를 효율적으로 쿼리하고자 합니다.",
        "Question": "Amazon S3에 저장된 로그 분석을 위한 요구 사항을 가장 잘 충족하는 AWS 서비스는 무엇입니까?",
        "Options": {
            "1": "AWS CloudWatch Logs Insights를 구현하여 로그 데이터를 분석하고 성능 추세를 시각화하는 대시보드를 생성합니다.",
            "2": "로그를 처리하고 결과를 Amazon Redshift로 내보내기 위해 Amazon EMR 클러스터를 설정합니다.",
            "3": "Amazon Athena를 사용하여 S3에 저장된 로그에 대해 SQL 쿼리를 직접 실행하고 Amazon QuickSight를 사용하여 결과를 시각화합니다.",
            "4": "Amazon OpenSearch Service를 활용하여 로그 데이터를 인덱싱하고 성능 문제를 찾기 위해 검색 쿼리를 실행합니다."
        },
        "Correct Answer": "Amazon Athena를 사용하여 S3에 저장된 로그에 대해 SQL 쿼리를 직접 실행하고 Amazon QuickSight를 사용하여 결과를 시각화합니다.",
        "Explanation": "Amazon Athena는 복잡한 인프라 없이 표준 SQL을 사용하여 Amazon S3에서 데이터를 직접 쿼리할 수 있게 해주며, 로그 분석을 위한 비용 효율적이고 효율적인 솔루션입니다. 시각화를 위해 Amazon QuickSight와 원활하게 통합됩니다.",
        "Other Options": [
            "Amazon EMR 클러스터를 설정하는 것은 Athena를 사용하는 것에 비해 더 많은 관리 오버헤드와 비용이 발생합니다. 특히 S3에 저장된 로그를 단순히 쿼리하는 경우에는 더욱 그렇습니다.",
            "Amazon OpenSearch Service는 전체 텍스트 검색 및 실시간 분석에 유용하지만, 로그 데이터를 서비스에 수집해야 하므로 Athena로 S3에서 직접 쿼리를 실행하는 것에 비해 복잡성이 증가합니다.",
            "AWS CloudWatch Logs Insights는 주로 CloudWatch Logs에 수집된 로그를 분석하기 위해 설계되었으며, S3에 저장된 로그에는 적합하지 않습니다. 따라서 이 특정 요구 사항에 적합하지 않습니다."
        ]
    },
    {
        "Question Number": "17",
        "Situation": "한 회사가 변동하는 작업 부하를 효율적으로 처리할 수 있는 새로운 데이터 처리 파이프라인을 설계하고 있습니다. 그들은 데이터 처리 작업을 위해 Amazon EC2의 프로비저닝된 인스턴스 또는 AWS Lambda를 사용하는 것을 고려하고 있습니다. 팀은 비용, 확장성 및 관리 오버헤드 측면에서 두 옵션 간의 장단점을 평가하고 있습니다.",
        "Question": "데이터 처리 파이프라인에서 변동하는 작업 부하를 처리하기 위한 최상의 확장성과 비용 효율성을 제공하는 옵션은 무엇입니까?",
        "Options": {
            "1": "Amazon EKS를 활용하여 데이터 처리 작업을 위한 Kubernetes 클러스터를 관리합니다.",
            "2": "Amazon EC2 인스턴스를 사용하고 Auto Scaling을 구성하여 변동하는 작업 부하를 관리합니다.",
            "3": "Amazon ECS를 Fargate와 함께 배포하여 컨테이너화된 데이터 처리 애플리케이션을 실행합니다.",
            "4": "AWS Lambda 함수를 구현하여 발생하는 데이터 이벤트를 처리합니다."
        },
        "Correct Answer": "AWS Lambda 함수를 구현하여 발생하는 데이터 이벤트를 처리합니다.",
        "Explanation": "AWS Lambda는 들어오는 요청을 처리하기 위해 자동으로 확장되는 서버리스 컴퓨팅 서비스를 제공하여 변동하는 작업 부하에 대해 매우 비용 효율적입니다. 사용한 컴퓨팅 시간에 대해서만 비용을 지불하므로, 기본 인프라를 관리할 필요 없이 간헐적이거나 예측할 수 없는 데이터 처리 작업에 이상적입니다.",
        "Other Options": [
            "Amazon EC2와 Auto Scaling을 사용하는 것은 인스턴스를 프로비저닝하고 유지 관리해야 하므로 관리 오버헤드가 발생하며, 인스턴스가 유휴 상태일 때 실행되는 경우 더 높은 비용이 발생할 수 있습니다.",
            "Amazon ECS를 Fargate와 함께 배포하면 좋은 확장성을 제공하지만, 실행 시간 및 컨테이너에 할당된 리소스 기반의 가격 모델로 인해 단기 작업에 대해 Lambda보다 더 비쌀 수 있습니다.",
            "Amazon EKS를 사용하여 Kubernetes 클러스터를 관리하는 것은 복잡성을 추가하고 Lambda에 비해 더 많은 관리 노력이 필요합니다. Lambda는 이벤트 기반 서버리스 작업을 위해 특별히 설계되었습니다."
        ]
    },
    {
        "Question Number": "18",
        "Situation": "소매 회사는 거래 데이터베이스 및 서버 로그를 포함한 다양한 출처의 데이터를 AWS에서 실시간 분석을 위해 효율적으로 수집하고 변환해야 합니다. 그들은 이 과정을 용이하게 하기 위해 다양한 서비스를 고려하고 있습니다.",
        "Question": "회사의 요구 사항을 충족하기 위해 어떤 솔루션을 사용할 수 있습니까? (두 가지 선택)",
        "Options": {
            "1": "Amazon Kinesis Data Firehose를 활용하여 데이터를 Amazon Redshift로 직접 스트리밍합니다.",
            "2": "AWS Data Pipeline을 구현하여 출처에서 Amazon S3로 주기적인 데이터 수집을 예약합니다.",
            "3": "AWS Glue를 사용하여 출처의 데이터 추출, 변환 및 적재(ETL)를 자동화합니다.",
            "4": "AWS Lambda 함수를 사용하여 데이터를 실시간으로 처리하고 변환한 후 Amazon S3에 저장합니다.",
            "5": "Amazon RDS를 활용하여 데이터베이스에서 Amazon Redshift로 실시간 데이터 수집을 수행합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "AWS Glue를 사용하여 출처의 데이터 추출, 변환 및 적재(ETL)를 자동화합니다.",
            "Amazon Kinesis Data Firehose를 활용하여 데이터를 Amazon Redshift로 직접 스트리밍합니다."
        ],
        "Explanation": "AWS Glue는 분석을 위해 데이터를 준비하고 로드하는 것을 쉽게 만들어주는 완전 관리형 ETL 서비스로, 데이터 수집 및 변환을 자동화하는 데 이상적입니다. Amazon Kinesis Data Firehose는 데이터를 Amazon Redshift로 실시간으로 스트리밍하여 들어오는 데이터에 대한 즉각적인 분석을 가능하게 합니다.",
        "Other Options": [
            "AWS Data Pipeline은 데이터 워크플로를 조정하는 데 유용하지만 실시간 처리에는 효율적이지 않으며 즉각적인 분석 요구에 적합하지 않을 수 있습니다.",
            "Amazon RDS는 관계형 데이터베이스 서비스로, 데이터베이스 호스팅에 주로 초점을 맞추고 있으며 분석을 위한 직접 데이터 수집을 용이하게 하는 데 적합하지 않습니다. 실시간 수집 요구 사항에는 적합하지 않습니다.",
            "AWS Lambda는 데이터를 처리하고 변환할 수 있지만, 대량 데이터 수집을 위해 주로 설계되지 않았으며 대량의 데이터를 처리할 때 복잡성과 관리 오버헤드가 증가할 수 있습니다."
        ]
    },
    {
        "Question Number": "19",
        "Situation": "데이터 엔지니어링 팀은 데이터 파이프라인 프로젝트를 위한 Git 저장소를 관리하는 임무를 맡았습니다. 그들은 저장소가 제대로 조직되어 있는지 확인해야 하며, 메인 브랜치에 영향을 주지 않고 기능 개발을 위한 새로운 브랜치를 만들고자 합니다.",
        "Question": "팀이 새로운 브랜치를 만들고 가장 효율적인 방법으로 전환하기 위해 어떤 Git 명령어 시퀀스를 사용해야 합니까?",
        "Options": {
            "1": "git checkout -b feature-branch",
            "2": "git branch -b feature-branch",
            "3": "git create branch feature-branch; git switch feature-branch",
            "4": "git branch feature-branch; git checkout feature-branch"
        },
        "Correct Answer": "git checkout -b feature-branch",
        "Explanation": "'git checkout -b feature-branch' 명령어는 'feature-branch'라는 새로운 브랜치를 효율적으로 생성하고 즉시 전환하는 단일 단계로 이루어져 있어, 필요한 작업에 가장 적합한 옵션입니다.",
        "Other Options": [
            "'git branch feature-branch; git checkout feature-branch' 명령어는 작동하지만, 올바른 답변과 동일한 결과를 얻기 위해 두 개의 별도 명령어가 필요하므로 덜 효율적입니다.",
            "'git create branch feature-branch; git switch feature-branch' 명령어는 'git create branch'가 유효한 Git 명령어가 아니기 때문에 잘못된 것입니다; 올바른 명령어는 'git branch'입니다.",
            "'git branch -b feature-branch' 명령어는 '-b' 옵션이 'git branch' 명령어에 존재하지 않기 때문에 잘못된 것입니다; 브랜치를 생성하고 전환하는 올바른 옵션은 'git checkout -b'입니다."
        ]
    },
    {
        "Question Number": "20",
        "Situation": "한 금융 서비스 회사는 AWS Glue 작업을 사용하여 여러 소스에서 데이터를 처리하고 Amazon Redshift 클러스터로 데이터를 변환 및 로드합니다. 최근 일부 Glue 작업이 간헐적으로 실패하여 데이터 가용성에 지연이 발생하고 있습니다. 데이터 엔지니어는 이러한 실패의 근본 원인을 파악하고 변환 작업의 성능을 향상시켜야 합니다.",
        "Question": "데이터 엔지니어가 AWS Glue에서 변환 실패를 해결하고 문제를 해결하기 위해 어떤 조치를 취해야 합니까?",
        "Options": {
            "1": "AWS Glue 작업에서 발생한 API 호출을 기록하기 위해 AWS CloudTrail을 활성화하여 무단 접근 또는 구성 변경을 식별합니다.",
            "2": "Glue 작업 스크립트를 수정하여 더 복잡한 변환을 사용하여 더 큰 데이터 볼륨을 효율적으로 처리합니다.",
            "3": "Glue 작업에 할당된 DPU(데이터 처리 단위)의 수를 늘려 성능을 향상시키고 타임아웃 가능성을 줄입니다.",
            "4": "데이터 변환을 위해 Amazon EMR로 전환하여 AWS Glue보다 더 나은 성능을 제공합니다."
        },
        "Correct Answer": "Glue 작업에 할당된 DPU(데이터 처리 단위)의 수를 늘려 성능을 향상시키고 타임아웃 가능성을 줄입니다.",
        "Explanation": "AWS Glue 작업에 할당된 DPU 수를 늘리면 처리 능력이 크게 향상되어 자원 제약으로 인한 타임아웃 및 실패 가능성을 줄일 수 있습니다. 이는 Glue 작업의 성능 문제를 해결하는 직접적이고 효과적인 방법입니다.",
        "Other Options": [
            "AWS CloudTrail을 활성화하는 것은 감사에 유용하지만 변환 작업 실패를 직접적으로 해결하거나 성능을 개선하지는 않습니다. 이는 API 호출 추적에 중점을 두고 있으며 작업 실행 문제를 해결하는 데는 도움이 되지 않습니다.",
            "더 복잡한 변환을 사용하는 것은 기본 인프라가 증가된 부하를 처리할 수 없는 경우 성능 문제를 악화시킬 수 있습니다. 자원 할당 문제를 먼저 해결하지 않고는 적절한 문제 해결 단계가 아닙니다.",
            "Amazon EMR은 특정 작업 부하에 대해 더 나은 성능을 제공할 수 있지만, 서비스 전환은 더 극단적인 조치이며 현재 문제를 직접적으로 해결하지 않을 수 있습니다. 추가적인 노력이 필요하며 새로운 복잡성을 초래할 수 있습니다."
        ]
    },
    {
        "Question Number": "21",
        "Situation": "한 회사는 IoT 장치에서 스트리밍 데이터를 처리하기 위한 실시간 분석 솔루션을 구현하고 있습니다. 데이터는 저장을 위해 Amazon S3로 전송되어야 하며, 압축 및 Parquet 형식으로의 변환과 같은 변환이 필요합니다. 이 솔루션은 다양한 데이터 처리량을 효율적으로 처리하고 운영 오버헤드를 최소화해야 합니다.",
        "Question": "압축 및 형식 변환 요구 사항을 충족하면서 이 스트리밍 데이터를 S3로 로드하고 변환하는 가장 효율적인 방법을 제공하는 AWS 서비스는 무엇입니까?",
        "Options": {
            "1": "Amazon Kinesis Data Firehose를 사용하여 스트리밍 데이터를 S3로 로드하고, 내장된 데이터 변환 및 압축 기능을 활용합니다.",
            "2": "Amazon Kinesis Data Analytics를 설정하여 데이터를 실시간으로 분석한 후 결과를 S3에 저장합니다.",
            "3": "AWS Lambda 함수를 구현하여 Kinesis Data Streams에서 데이터를 처리한 후 S3에 직접 기록합니다.",
            "4": "AWS Glue를 활용하여 스트리밍 데이터를 배치 처리하고 Parquet 형식으로 변환한 후 S3에 저장합니다."
        },
        "Correct Answer": "Amazon Kinesis Data Firehose를 사용하여 스트리밍 데이터를 S3로 로드하고, 내장된 데이터 변환 및 압축 기능을 활용합니다.",
        "Explanation": "Amazon Kinesis Data Firehose는 스트리밍 데이터를 처리하도록 특별히 설계되었으며, 데이터 변환, 압축 및 형식 변환을 위한 내장 기능을 제공하므로, 지정된 요구 사항을 충족하면서 S3로 데이터를 로드하는 가장 효율적인 선택입니다.",
        "Other Options": [
            "Kinesis Data Streams에서 데이터를 처리하기 위해 AWS Lambda를 사용하는 것은 Lambda 함수를 관리하고 데이터 변환을 처리하도록 보장하는 데 더 많은 운영 오버헤드가 필요합니다. 이는 Kinesis Data Firehose가 기본적으로 수행할 수 있는 작업입니다.",
            "AWS Glue는 주로 배치 처리를 위해 설계되었으므로 IoT 장치의 스트리밍 데이터에 대한 즉각적인 분석에는 덜 적합합니다.",
            "Amazon Kinesis Data Analytics는 스트리밍 데이터를 실시간으로 분석하는 데 중점을 두지만, 데이터 로드, 변환 또는 저장을 직접 처리하지 않으므로 결과를 S3에 저장하기 위해 추가 단계가 필요합니다."
        ]
    },
    {
        "Question Number": "22",
        "Situation": "한 의료 기관이 데이터 저장소를 AWS로 이전하고 있으며, 민감한 환자 정보가 데이터 프라이버시 규정을 준수하지 않는 AWS 리전으로 우연히 백업되거나 복제되지 않도록 해야 합니다. 이 기관은 이러한 요구 사항을 시행하기 위한 효과적인 전략을 찾고 있습니다.",
        "Question": "어떤 전략이 허용되지 않는 AWS 리전에서 민감한 데이터의 백업 또는 복제를 방지하는 데 가장 효과적일까요?",
        "Options": {
            "1": "AWS Identity and Access Management (IAM) 정책을 사용하여 특정 AWS 리전에 대한 액세스를 제한합니다.",
            "2": "AWS Config 규칙을 활성화하여 원하는 백업 및 복제 구성에 대한 준수를 평가합니다.",
            "3": "AWS CloudTrail을 구성하여 모든 AWS 리전에서 데이터 복제 활동을 모니터링합니다.",
            "4": "Amazon S3 버킷 정책을 구현하여 준수하는 AWS 리전으로만 데이터 복제를 허용합니다."
        },
        "Correct Answer": "Amazon S3 버킷 정책을 구현하여 준수하는 AWS 리전으로만 데이터 복제를 허용합니다.",
        "Explanation": "Amazon S3 버킷 정책을 구현하면 데이터가 복제될 수 있는 위치에 대한 특정 규칙을 정의하고 시행할 수 있어 민감한 데이터가 준수하는 리전을 벗어나지 않도록 보장합니다. 이 접근 방식은 무단 백업 또는 복제를 방지해야 한다는 요구 사항을 직접적으로 해결합니다.",
        "Other Options": [
            "IAM 정책을 사용하면 특정 AWS 서비스에 대한 액세스를 제한할 수 있지만, 허용되지 않는 리전으로 데이터가 백업되거나 복제되는 것을 구체적으로 방지하지는 않습니다.",
            "AWS CloudTrail을 구성하는 것은 AWS 계정 내에서 수행된 작업을 감사하고 모니터링하는 데 유용하지만, 비준수 리전으로의 데이터 복제를 적극적으로 방지하지는 않습니다.",
            "AWS Config 규칙을 활성화하면 준수를 평가하는 데 도움이 될 수 있지만, 데이터가 복제될 수 있는 위치에 대한 제한을 시행하지 않으므로 무단 백업을 방지하지는 않습니다."
        ]
    },
    {
        "Question Number": "23",
        "Situation": "한 소매 회사가 여러 출처에서 CSV 형식으로 대량의 판매 데이터를 수집하고 있습니다. 이 회사는 Amazon S3에 로드하여 Amazon Athena를 사용해 추가 분석을 수행하기 전에 이 데이터를 보다 효율적인 열 형식으로 변환하여 저장 및 쿼리 성능을 최적화하고자 합니다.",
        "Question": "CSV 파일을 Apache Parquet 형식으로 변환하는 데 가장 적합한 데이터 변환 접근 방식은 무엇인가요?",
        "Options": {
            "1": "AWS Glue를 사용하여 CSV 스키마를 식별하는 크롤러를 생성한 다음 ETL 작업을 사용하여 데이터를 Parquet 형식으로 변환합니다.",
            "2": "Amazon EMR과 Spark를 활용하여 CSV 파일을 읽고 단일 처리 작업에서 Parquet 형식으로 변환합니다.",
            "3": "AWS Lambda를 사용하여 CSV 파일을 읽고 Parquet 형식으로 변환한 후 S3에 다시 씁니다.",
            "4": "로컬 Python 스크립트를 사용하여 CSV 파일을 수동으로 Parquet 형식으로 변환하고 파일을 S3에 업로드합니다."
        },
        "Correct Answer": "AWS Glue를 사용하여 CSV 스키마를 식별하는 크롤러를 생성한 다음 ETL 작업을 사용하여 데이터를 Parquet 형식으로 변환합니다.",
        "Explanation": "AWS Glue를 사용하면 자동화된 스키마 추론과 최소한의 관리 오버헤드로 ETL 작업을 효율적으로 수행할 수 있습니다. 이러한 변환을 위해 설계되었으며 다른 AWS 서비스와 잘 통합됩니다.",
        "Other Options": [
            "AWS Lambda를 사용하는 것은 데이터 양이 많을 경우 최선의 선택이 아닐 수 있습니다. Lambda는 실행 시간과 메모리에 제한이 있어 Glue에 비해 대량 데이터 처리에 덜 적합합니다.",
            "로컬 Python 스크립트를 사용하여 파일을 수동으로 변환하는 것은 시간이 많이 소요될 수 있으며, 확장성이 떨어져 잠재적인 오류와 운영 오버헤드 증가로 이어질 수 있습니다.",
            "Amazon EMR과 Spark를 활용하는 것도 가능하지만, AWS Glue가 제공하는 보다 간단하고 서버리스 솔루션에 비해 불필요한 복잡성과 더 높은 비용을 초래할 수 있습니다."
        ]
    },
    {
        "Question Number": "24",
        "Situation": "한 회사가 Amazon Redshift를 사용하여 Amazon S3에 저장된 대규모 데이터 세트를 분석해야 합니다. 그들은 데이터 저장 비용을 최소화하면서 빠른 쿼리 성능을 보장하고자 합니다. 데이터 엔지니어링 팀은 데이터를 Redshift에 로드하지 않고 S3에서 직접 쿼리하기 위해 Amazon Redshift Spectrum의 사용을 고려하고 있습니다. 그러나 외부 스키마 관리에 대한 모범 사례를 이해해야 합니다.",
        "Question": "Amazon Redshift에서 Amazon Redshift Spectrum을 효과적으로 활용하기 위해 외부 스키마를 정의하는 권장 접근 방식은 무엇인가요?",
        "Options": {
            "1": "데이터가 포함된 S3 버킷에 액세스할 수 있는 권한을 가진 Redshift 전용 IAM 역할을 생성합니다.",
            "2": "Redshift 데이터베이스 사용자와 동일한 자격 증명을 사용하여 외부 스키마를 정의합니다.",
            "3": "S3 버킷에 직접 연결을 설정하고 외부 스키마의 필요성을 우회합니다.",
            "4": "CREATE EXTERNAL SCHEMA 명령을 사용하여 외부 스키마를 설정하고 S3 버킷에 연결합니다."
        },
        "Correct Answer": "CREATE EXTERNAL SCHEMA 명령을 사용하여 외부 스키마를 설정하고 S3 버킷에 연결합니다.",
        "Explanation": "CREATE EXTERNAL SCHEMA 명령을 사용하는 것은 Amazon Redshift가 S3에서 데이터를 효율적으로 쿼리할 수 있도록 외부 스키마를 정의하는 데 필수적입니다. 이 명령은 스키마를 S3 버킷에 연결하여 데이터를 Redshift에 로드하지 않고도 외부 테이블을 사용하여 쿼리할 수 있게 합니다.",
        "Other Options": [
            "별도의 IAM 역할을 생성하는 것은 권한에 중요하지만, S3에서 데이터를 쿼리하는 데 필요한 외부 스키마를 설정하지는 않습니다.",
            "Redshift 데이터베이스 사용자와 동일한 자격 증명으로 외부 스키마를 정의하는 것은 충분하지 않으며, S3 버킷에 연결하기 위해 CREATE EXTERNAL SCHEMA 명령이 필요합니다.",
            "외부 스키마의 필요성을 우회하는 것은 불가능합니다. 외부 스키마는 S3에 저장된 데이터에 접근하고 쿼리하는 방법을 정의하는 데 필요합니다."
        ]
    },
    {
        "Question Number": "25",
        "Situation": "금융 서비스 회사가 고객 거래 데이터를 Amazon S3에 저장하고 있으며, 데이터의 높은 가용성과 데이터 손실에 대한 복원력을 보장해야 합니다. 데이터 엔지니어링 팀은 내구성을 제공하면서도 비용 효율적인 솔루션을 구현하는 임무를 맡고 있습니다.",
        "Question": "데이터 엔지니어링 팀이 데이터의 복원력과 가용성을 보장하기 위해 구현해야 할 저장 솔루션은 무엇입니까?",
        "Options": {
            "1": "비용을 최적화하면서 가용성을 유지하기 위해 Amazon S3 Intelligent-Tiering을 활용합니다.",
            "2": "높은 가용성을 위해 Amazon S3 Standard 저장 클래스를 사용하여 데이터를 저장합니다.",
            "3": "S3 버킷에서 버전 관리를 활성화하여 각 객체의 여러 버전을 유지합니다.",
            "4": "내구성을 높이기 위해 S3 버킷에 대해 교차 지역 복제를 구현합니다."
        },
        "Correct Answer": "내구성을 높이기 위해 S3 버킷에 대해 교차 지역 복제를 구현합니다.",
        "Explanation": "S3 버킷에 대해 교차 지역 복제를 구현하면 다른 지역에 데이터 복사본이 생성되어 지역 장애 발생 시에도 데이터가 계속 사용 가능하게 되어 복원력과 가용성이 크게 향상됩니다.",
        "Other Options": [
            "버전 관리를 활성화하면 객체의 이전 버전을 복구하는 데 도움이 되지만 지역 장애에 대한 보호는 제공하지 않으므로 높은 가용성에는 덜 효과적입니다.",
            "Amazon S3 Intelligent-Tiering은 비용을 최적화하지만 지역 장애 발생 시 가용성 문제를 본질적으로 해결하지 않습니다.",
            "Amazon S3 Standard에 데이터를 저장하면 높은 가용성을 제공하지만 지역 장애로 인한 데이터 손실에 대한 추가적인 복원력을 제공하지 않습니다."
        ]
    },
    {
        "Question Number": "26",
        "Situation": "데이터 엔지니어가 온프레미스 데이터베이스에서 Amazon S3로 대량의 데이터를 배치하고 수집하는 작업을 맡고 있습니다. 그들은 높은 처리량을 보장하고 수집 프로세스를 구성하는 동안 비용을 최소화해야 합니다.",
        "Question": "Amazon S3로 데이터를 배치 수집하기 위한 가장 효율적인 접근 방식은 무엇입니까?",
        "Options": {
            "1": "Amazon Kinesis Data Firehose를 사용하여 데이터베이스에서 Amazon S3로 데이터를 지속적으로 스트리밍합니다.",
            "2": "AWS Glue ETL 작업을 구현하여 온프레미스 데이터베이스에서 데이터를 읽고 Amazon S3에 씁니다.",
            "3": "AWS Data Pipeline을 사용하여 데이터를 Amazon S3로 정기적으로 내보내도록 예약합니다.",
            "4": "크론 작업을 설정하여 데이터베이스에서 Amazon S3로 데이터를 전송하는 사용자 지정 스크립트를 실행합니다."
        },
        "Correct Answer": "AWS Glue ETL 작업을 구현하여 온프레미스 데이터베이스에서 데이터를 읽고 Amazon S3에 씁니다.",
        "Explanation": "AWS Glue ETL 작업은 배치 모드에서 데이터를 변환하고 로드하도록 특별히 설계되었습니다. 대량의 데이터 세트를 효율적으로 처리할 수 있으며, 비용과 관리 오버헤드를 최소화하는 서버리스 아키텍처를 제공합니다.",
        "Other Options": [
            "AWS Data Pipeline은 내보내기를 예약할 수 있지만 AWS Glue ETL만큼 변환에 효율적이지 않을 수 있어 복잡한 수집 작업에 덜 적합합니다.",
            "Amazon Kinesis Data Firehose는 주로 실시간 스트리밍 데이터 수집을 위한 것이므로 데이터베이스에서 대량의 데이터를 배치 처리하는 데 이상적이지 않습니다.",
            "사용자 지정 스크립트를 위한 크론 작업을 사용하는 것은 운영 오버헤드를 증가시킬 수 있으며, 효율성과 확장성을 위해 AWS의 관리 서비스를 활용하지 않습니다."
        ]
    },
    {
        "Question Number": "27",
        "Situation": "금융 서비스 회사가 데이터 웨어하우스를 Amazon Redshift로 마이그레이션하고 있습니다. 그들은 클러스터에 저장된 민감한 재무 데이터에 접근할 수 있는 사용자가 승인된 사용자만 되도록 보장하고자 합니다. 회사는 또한 데이터가 휴지 상태일 때 암호화를 요구하며, 효과적인 리소스 관리를 통해 비용 절감을 원합니다.",
        "Question": "회사가 Amazon Redshift 클러스터에 대한 필요한 접근 제어 및 보안 조치를 구현하기 위해 취해야 할 조치는 무엇입니까?",
        "Options": {
            "1": "각 사용자에 대해 IAM 역할을 생성하고 이를 Redshift 클러스터에 할당합니다.",
            "2": "Redshift 보안 그룹을 사용하여 접근을 제어하고 클러스터를 프로비저닝할 때 암호화를 활성화합니다.",
            "3": "데이터에 대한 더 쉬운 접근을 허용하기 위해 공용 서브넷에 Redshift 클러스터를 프로비저닝합니다.",
            "4": "AWS 계정의 모든 사용자에게 Redshift 클러스터에 대한 접근을 허용하고 SQL 명령을 통해 권한을 관리합니다."
        },
        "Correct Answer": "Redshift 보안 그룹을 사용하여 접근을 제어하고 클러스터를 프로비저닝할 때 암호화를 활성화합니다.",
        "Explanation": "Redshift 보안 그룹을 사용하면 클러스터에 대한 네트워크 접근을 관리하는 데 도움이 되며, 암호화를 활성화하면 민감한 데이터가 휴지 상태에서 보호되므로 회사의 보안 및 접근 제어 요구 사항을 충족합니다.",
        "Other Options": [
            "각 사용자에 대해 IAM 역할을 생성하는 것은 클러스터에 대한 접근을 제어하기 위한 충분한 조치가 아닙니다. IAM 역할은 권한에 도움이 되지만 보안 그룹이 제공하는 필요한 네트워크 접근 제어를 제공하지 않습니다.",
            "AWS 계정의 모든 사용자에게 Redshift 클러스터에 대한 접근을 허용하는 것은 상당한 보안 위험을 초래하며, 승인된 사용자만 접근할 수 있도록 제한하지 않으므로 회사의 제한된 접근 요구 사항을 위반합니다.",
            "민감한 데이터에 대해 공용 서브넷에 Redshift 클러스터를 프로비저닝하는 것은 바람직하지 않으며, 이는 클러스터를 인터넷에 노출시켜 무단 접근의 위험을 증가시키고 보안 모범 사례를 충족하지 못합니다."
        ]
    },
    {
        "Question Number": "28",
        "Situation": "한 금융 서비스 회사가 Amazon SageMaker를 사용하여 머신 러닝 모델을 개발하고 있습니다. 데이터 엔지니어링 팀은 규제 요구 사항을 준수하기 위해 데이터 변환, 모델 훈련 및 평가 프로세스의 계보를 추적할 수 있도록 해야 합니다.",
        "Question": "데이터 엔지니어링 팀이 머신 러닝 워크플로우의 포괄적인 데이터 계보를 설정하기 위해 어떤 AWS 도구를 사용해야 합니까?",
        "Options": {
            "1": "Amazon SageMaker ML Lineage Tracking을 활용하여 데이터 입력, 모델 훈련 및 평가 메트릭을 포함한 모델 생애 주기 전반에 걸쳐 데이터 계보를 캡처하고 시각화합니다.",
            "2": "Amazon QuickSight를 사용하여 머신 러닝 모델의 성능을 시간에 따라 시각화하는 대시보드를 생성합니다.",
            "3": "AWS Data Pipeline을 활용하여 모델 훈련 및 평가 프로세스를 포함한 데이터 워크플로우를 예약하고 관리합니다.",
            "4": "AWS CloudTrail을 구현하여 SageMaker가 모델 훈련 및 평가 중에 수행한 작업을 모니터링하기 위해 API 호출을 추적합니다."
        },
        "Correct Answer": "Amazon SageMaker ML Lineage Tracking을 활용하여 데이터 입력, 모델 훈련 및 평가 메트릭을 포함한 모델 생애 주기 전반에 걸쳐 데이터 계보를 캡처하고 시각화합니다.",
        "Explanation": "Amazon SageMaker ML Lineage Tracking은 머신 러닝 워크플로우 전반에 걸쳐 데이터와 모델의 계보를 추적하도록 특별히 설계되었습니다. 이는 데이터 소스와 적용된 변환, 모델 훈련 및 평가 방법에 대한 가시성을 제공합니다. 이는 데이터 관리의 준수 및 거버넌스에 매우 중요합니다.",
        "Other Options": [
            "AWS CloudTrail은 API 호출을 모니터링하지만 데이터 변환 및 모델 훈련 프로세스에 대한 자세한 계보 추적이나 통찰력을 제공하지 않으므로 포괄적인 데이터 계보를 설정하는 데는 불충분합니다.",
            "Amazon QuickSight는 주로 데이터 시각화 및 보고를 위한 BI 도구로, 머신 러닝 맥락에서 데이터 계보나 변환을 추적하는 데 사용되지 않습니다.",
            "AWS Data Pipeline은 데이터 워크플로우를 관리하기 위한 서비스이지만, 머신 러닝 모델이나 관련 데이터에 특정한 계보 추적 기능을 본질적으로 제공하지 않습니다."
        ]
    },
    {
        "Question Number": "29",
        "Situation": "데이터 엔지니어는 모든 AWS 서비스에 대한 접근이 규정 준수 및 보안 목적을 위해 기록되도록 해야 합니다. 팀은 조직 내에서 AWS 서비스에 대한 API 호출의 상세 로그를 캡처할 수 있는 솔루션이 필요합니다.",
        "Question": "데이터 엔지니어가 AWS 서비스의 API 호출을 기록하기 위해 활성화해야 할 AWS 서비스는 무엇입니까?",
        "Options": {
            "1": "Amazon CloudWatch Logs",
            "2": "AWS CloudTrail",
            "3": "AWS X-Ray",
            "4": "AWS Config"
        },
        "Correct Answer": "AWS CloudTrail",
        "Explanation": "AWS CloudTrail은 AWS 서비스에 대한 API 호출을 기록하도록 특별히 설계되었습니다. 이는 사용자, 역할 또는 AWS 서비스에 의해 수행된 작업의 포괄적인 기록을 제공하므로 규정 준수 및 보안 감사에 이상적입니다.",
        "Other Options": [
            "Amazon CloudWatch Logs는 주로 애플리케이션 및 서비스의 출력 모니터링 및 로그 기록에 사용되지만, AWS 서비스 API 호출 기록에 특별히 초점을 맞추지 않습니다.",
            "AWS Config는 AWS 리소스의 구성을 평가, 감사 및 평가하는 데 사용됩니다. 구성 변경을 추적하지만 서비스에 대한 API 호출을 기록하지는 않습니다.",
            "AWS X-Ray는 분산 애플리케이션을 분석하고 디버깅하기 위한 서비스입니다. 애플리케이션을 통한 요청 추적을 도와주지만 AWS 서비스에 대한 API 호출 기록에 초점을 맞추지 않습니다."
        ]
    },
    {
        "Question Number": "30",
        "Situation": "한 소매 회사가 Amazon DynamoDB를 사용하여 고객 거래 데이터를 저장하고 있습니다. 데이터 엔지니어링 팀은 쿼리 성능을 최적화하고 비용을 절감하는 것을 목표로 하고 있습니다. 그들은 이러한 목표를 달성하면서 데이터 검색이 효율적으로 유지되도록 효과적인 인덱싱 및 파티셔닝 전략을 결정해야 합니다.",
        "Question": "데이터 엔지니어링 팀이 DynamoDB 테이블을 쿼리 성능 및 비용 효율성을 개선하기 위해 최적화하기 위한 최선의 접근 방식은 무엇입니까?",
        "Options": {
            "1": "아이템을 파티션에 고르게 분배하는 파티션 키와 효율적인 쿼리를 위한 정렬 키를 가진 복합 기본 키를 사용합니다.",
            "2": "액세스 패턴을 고려하지 않고 글로벌 보조 인덱스(GSI)를 구현하여 더 유연한 쿼리 옵션을 허용합니다.",
            "3": "고객 인구 통계에 따라 데이터를 여러 테이블로 파티셔닝하여 읽기 성능을 향상시킵니다.",
            "4": "모든 아이템이 저장된 단일 파티션 키를 활용하여 데이터 접근을 단순화하고 비용을 최소화합니다."
        },
        "Correct Answer": "아이템을 파티션에 고르게 분배하는 파티션 키와 효율적인 쿼리를 위한 정렬 키를 가진 복합 기본 키를 사용합니다.",
        "Explanation": "복합 기본 키를 사용하면 데이터가 파티션에 더 잘 분배되어 읽기 성능을 최적화하고 핫 파티션 문제를 최소화할 수 있습니다. 정렬 키는 특정 속성을 기반으로 데이터를 효율적으로 검색할 수 있도록 쿼리 기능을 향상시킵니다.",
        "Other Options": [
            "모든 아이템이 저장된 단일 파티션 키를 사용하면 데이터의 고르지 않은 분배로 인해 핫 파티션과 성능 병목 현상이 발생할 수 있으며, 이는 비용을 증가시킬 수 있습니다.",
            "액세스 패턴을 고려하지 않고 글로벌 보조 인덱스(GSI)를 구현하면 인덱스가 데이터 접근 방식에 따라 효과적으로 활용되지 않을 경우 불필요한 비용과 성능 문제를 초래할 수 있습니다.",
            "고객 인구 통계에 따라 데이터를 여러 테이블로 파티셔닝하면 데이터 관리가 복잡해지고 중복 가능성 및 여러 테이블 관리의 오버헤드로 인해 비용이 증가할 수 있습니다."
        ]
    },
    {
        "Question Number": "31",
        "Situation": "소매 회사가 고객 거래 데이터를 저장하기 위해 Amazon S3를 사용하고 있습니다. 그들은 S3 버킷에 새 파일이 업로드될 때마다 데이터 처리 워크플로우를 트리거하고 싶어합니다. 팀은 워크플로우를 자동화하기 위해 이벤트 기반 아키텍처를 사용하는 것을 고려하고 있습니다.",
        "Question": "Amazon S3에서 데이터 수집을 위한 이벤트 트리거를 설정하는 데 사용할 수 있는 서비스는 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "Amazon EventBridge",
            "2": "Amazon SQS",
            "3": "Amazon SNS",
            "4": "Amazon CloudWatch",
            "5": "AWS Lambda"
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "AWS Lambda",
            "Amazon EventBridge"
        ],
        "Explanation": "AWS Lambda는 S3 버킷에 새 파일이 업로드될 때 자동으로 함수를 트리거하도록 구성할 수 있어 실시간 데이터 처리가 가능합니다. Amazon EventBridge는 S3 이벤트에 응답하는 규칙을 생성하는 데 사용할 수 있어 데이터 수집 워크플로우를 위한 복잡한 이벤트 기반 아키텍처를 가능하게 합니다.",
        "Other Options": [
            "Amazon SNS는 주로 알림 전송에 사용되며 S3 이벤트를 직접 처리하지 않습니다. 더 큰 워크플로우의 일부가 될 수 있지만, 자체적으로 데이터 처리를 트리거할 수는 없습니다.",
            "Amazon SQS는 메시지 전송을 용이하게 하는 큐잉 서비스입니다. 데이터 수집 파이프라인에서 사용할 수 있지만, S3 이벤트에 따라 작업을 트리거하지는 않습니다.",
            "Amazon CloudWatch는 AWS 서비스 모니터링 및 로깅에 사용됩니다. S3에서 데이터 수집을 위한 이벤트 기반 트리거를 직접 지원하지 않습니다."
        ]
    },
    {
        "Question Number": "32",
        "Situation": "데이터 엔지니어링 팀이 AWS에서 확장 가능한 데이터 처리 솔루션을 구현하기 위해 작업하고 있습니다. 그들은 데이터 변환 및 ETL 프로세스를 자동화하기 위해 스크립팅을 허용하는 서비스를 고려하고 있습니다. 그들은 워크플로우를 최적화하기 위해 어떤 AWS 서비스가 효과적으로 스크립팅을 수용할 수 있는지 평가해야 합니다.",
        "Question": "다음 AWS 서비스 중 데이터 처리 작업을 위한 스크립팅을 지원하는 서비스는 무엇입니까?",
        "Options": {
            "1": "Amazon EMR",
            "2": "Amazon QuickSight",
            "3": "Amazon Redshift",
            "4": "AWS Glue"
        },
        "Correct Answer": "Amazon EMR",
        "Explanation": "Amazon EMR (Elastic MapReduce)는 Apache Spark 및 Hadoop과 같은 프레임워크를 통해 스크립팅을 사용할 수 있게 하여 대량의 데이터를 처리하는 강력한 도구입니다. 사용자는 Python, Java 및 R과 같은 언어로 스크립트를 작성하여 데이터 변환 및 분석을 수행할 수 있습니다.",
        "Other Options": [
            "Amazon Redshift는 주로 데이터 웨어하우징 및 SQL 기반 쿼리에 중점을 두고 있으며, 데이터 처리 작업을 위한 스크립팅보다는 사용자 정의 함수 지원에 중점을 둡니다. ETL 프로세스를 위한 스크립팅 환경이 아닙니다.",
            "AWS Glue는 ETL 프로세스를 용이하게 하는 서버리스 데이터 통합 서비스입니다. 그러나 전통적인 스크립팅보다는 그래픽 인터페이스와 Glue 작업에 의존하므로 이 맥락에서 스크립팅 요구 사항과는 덜 일치합니다.",
            "Amazon QuickSight는 데이터 시각화 및 보고를 위한 비즈니스 인텔리전스 서비스입니다. 데이터 변환보다는 시각화 및 대시보드에 중점을 두므로 데이터 처리 작업을 위한 스크립팅을 지원하지 않습니다."
        ]
    },
    {
        "Question Number": "33",
        "Situation": "소매 회사는 Amazon S3에 저장된 대량의 거래 데이터를 분석해야 합니다. 데이터는 통찰력을 쿼리하기 전에 구조화된 형식으로 변환되어야 합니다. 회사는 비용을 최소화하면서 배치 처리를 효율적으로 처리할 수 있는 솔루션을 원합니다.",
        "Question": "Amazon S3에서 필요한 데이터 변환을 달성하기 위한 가장 비용 효율적이고 효율적인 방법은 무엇입니까?",
        "Options": {
            "1": "Amazon EMR 클러스터를 설정하여 S3의 데이터를 Apache Spark를 사용하여 처리하고 결과를 S3에 다시 저장합니다.",
            "2": "AWS Glue를 사용하여 S3에서 데이터를 읽고 변환한 후 출력을 S3에 다시 쓰는 ETL 작업을 생성합니다.",
            "3": "Amazon DMS를 활용하여 S3에서 Amazon Redshift로 데이터를 마이그레이션하여 변환 및 분석합니다.",
            "4": "AWS Lambda를 사용하여 S3에서 데이터를 읽고 변환한 후 결과를 다른 S3 버킷에 출력하는 함수를 트리거합니다."
        },
        "Correct Answer": "AWS Glue를 사용하여 S3에서 데이터를 읽고 변환한 후 출력을 S3에 다시 쓰는 ETL 작업을 생성합니다.",
        "Explanation": "AWS Glue는 S3에 저장된 데이터를 변환하는 과정을 단순화하는 완전 관리형 ETL 서비스입니다. 서버리스 환경을 제공하여 운영 오버헤드와 비용을 줄이면서 배치 변환을 효율적으로 처리합니다.",
        "Other Options": [
            "Amazon EMR 클러스터를 설정하면 추가 비용과 관리 오버헤드가 발생하며, 특히 작은 데이터 세트의 경우 클러스터를 프로비저닝하고 관리해야 하므로 비효율적입니다.",
            "이 작업에 AWS Lambda를 사용하는 것은 실행 시간 및 메모리 제한으로 인해 대량의 데이터에 적합하지 않을 수 있으며, 성능 문제를 초래할 수 있습니다.",
            "Amazon DMS는 주로 데이터베이스 마이그레이션을 위해 설계되었으며, AWS Glue와 같은 방식으로 변환 기능을 제공하지 않으므로 이 배치 처리 사용 사례에 덜 적합합니다."
        ]
    },
    {
        "Question Number": "34",
        "Situation": "한 금융 서비스 회사가 Amazon EMR에서 Apache Spark를 사용하여 매일 대량의 거래 데이터를 처리하고 있습니다. 최근에 그들은 데이터 품질 및 지연에 대한 새로운 규제 요구 사항을 수용하기 위해 데이터 변환 프로세스를 크게 개선해야 합니다.",
        "Question": "Apache Spark를 사용하여 데이터 변환 프로세스를 향상시키기 위해 어떤 조치를 취해야 합니까? (두 가지 선택)",
        "Options": {
            "1": "더 쉬운 데이터 조작을 위해 Spark SQL을 사용하여 뷰를 생성합니다.",
            "2": "더 나은 성능을 위해 EMR 클러스터의 인스턴스 유형을 증가시킵니다.",
            "3": "최적화된 데이터 변환 및 작업을 위해 DataFrame을 활용합니다.",
            "4": "배치 처리의 경우 Apache Hive를 활용합니다.",
            "5": "실시간 데이터 처리를 위해 Spark Structured Streaming을 구현합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "실시간 데이터 처리를 위해 Spark Structured Streaming을 구현합니다.",
            "최적화된 데이터 변환 및 작업을 위해 DataFrame을 활용합니다."
        ],
        "Explanation": "Spark Structured Streaming을 구현하면 회사가 데이터를 실시간으로 처리할 수 있어 새로운 규제 요구 사항을 충족하는 데 중요합니다. DataFrame을 활용하면 데이터 변환을 보다 효율적이고 관리하기 쉽게 만드는 고급 API를 제공하여 데이터 품질을 보장합니다.",
        "Other Options": [
            "Spark SQL을 사용하여 뷰를 생성하는 것은 Spark Structured Streaming 및 DataFrame의 사용에 비해 변환 프로세스를 직접적으로 향상시키는 데 덜 효과적입니다. 이는 본질적으로 데이터 처리 성능을 개선하지 않기 때문입니다.",
            "Apache Hive를 활용하면 배치 처리 기능을 제공할 수 있지만, 실시간 변환 요구 사항에 대해서는 Spark만큼 효율적이지 않으며 불필요한 지연을 초래할 수 있습니다.",
            "EMR 클러스터의 인스턴스 유형을 증가시키면 성능이 향상될 수 있지만 최적화된 데이터 변환 프로세스의 필요성을 구체적으로 해결하지는 않습니다."
        ]
    },
    {
        "Question Number": "35",
        "Situation": "한 온라인 소매 회사가 고객 거래를 실시간으로 자동 처리하고 성공적인 구매에 대한 알림을 트리거하고자 합니다. 그들은 데이터 수집 및 처리를 효율적으로 처리하기 위해 이벤트 기반 아키텍처를 사용하는 것을 고려하고 있습니다. 이 회사는 지연을 최소화하고 들어오는 거래의 양에 따라 자동으로 확장되는 솔루션이 필요합니다.",
        "Question": "거래를 거의 실시간으로 처리하기 위한 이벤트 기반 아키텍처를 구현하는 데 가장 적합한 솔루션은 무엇입니까?",
        "Options": {
            "1": "Amazon EventBridge를 활용하여 거래 이벤트를 처리하고 알림을 트리거하기 위해 Lambda 함수로 라우팅합니다.",
            "2": "Amazon Kinesis Data Stream을 설정하여 거래 이벤트를 수집하고 Firehose 배달 스트림으로 처리합니다.",
            "3": "Amazon SQS 큐를 구현하여 거래 데이터를 저장하고 EC2 인스턴스로 정기적으로 처리합니다.",
            "4": "AWS Lambda를 사용하여 거래 알림을 수신하는 Amazon SNS 주제에서 이벤트를 처리합니다."
        },
        "Correct Answer": "Amazon EventBridge를 활용하여 거래 이벤트를 처리하고 알림을 트리거하기 위해 Lambda 함수로 라우팅합니다.",
        "Explanation": "Amazon EventBridge를 사용하면 다양한 소스에서 AWS Lambda와 같은 대상으로 이벤트를 쉽게 라우팅할 수 있는 서버리스 이벤트 버스를 제공하여 고객 거래를 처리하고 실시간으로 알림을 트리거하는 데 매우 확장 가능하고 낮은 지연의 솔루션을 제공합니다.",
        "Other Options": [
            "AWS Lambda와 Amazon SNS를 사용하는 것도 가능하지만, SNS는 주로 퍼블리시/서브스크라이브 메시징을 위한 것이며 EventBridge와 같은 수준의 이벤트 라우팅 및 필터링 기능을 제공하지 않을 수 있습니다.",
            "SQS 큐를 구현하고 EC2 인스턴스로 정기적으로 처리하는 것은 지연을 초래하며, 실시간 처리 기능을 제공하지 않고 EC2 인스턴스를 관리하는 운영 오버헤드가 필요합니다.",
            "거래 이벤트를 위한 Kinesis Data Stream을 설정하는 것은 간단한 이벤트 처리를 위해 과도할 수 있으며, 이벤트 라우팅의 단순함에 비해 불필요한 복잡성과 비용을 초래할 수 있습니다."
        ]
    },
    {
        "Question Number": "36",
        "Situation": "한 회사가 온프레미스 데이터 웨어하우스를 AWS로 마이그레이션할 계획을 세우고 있습니다. 그들은 구조화된 데이터와 반구조화된 데이터를 효율적으로 처리할 수 있는 데이터 저장 솔루션이 필요하며, 마이그레이션 과정에서 확장성과 높은 가용성을 지원해야 합니다.",
        "Question": "다운타임을 최소화하면서 회사의 데이터 저장 및 마이그레이션 요구 사항에 가장 잘 부합하는 AWS 서비스 조합은 무엇입니까?",
        "Options": {
            "1": "모든 데이터 저장 요구 사항에 Amazon DynamoDB를 선택하고 대량 데이터 전송을 위해 AWS Snowball을 사용합니다.",
            "2": "데이터 저장을 위해 Amazon S3를 사용하고 마이그레이션 중 데이터 변환 및 카탈로깅을 위해 AWS Glue를 사용합니다.",
            "3": "구조화된 데이터를 위해 Amazon Redshift를 구현하고 마이그레이션 중 실시간 데이터 스트리밍을 위해 Amazon Kinesis를 사용합니다.",
            "4": "구조화된 데이터를 위해 Amazon RDS로 마이그레이션하고 데이터 수집 및 변환을 위해 AWS Data Pipeline을 사용합니다."
        },
        "Correct Answer": "데이터 저장을 위해 Amazon S3를 사용하고 마이그레이션 중 데이터 변환 및 카탈로깅을 위해 AWS Glue를 사용합니다.",
        "Explanation": "Amazon S3를 사용하면 마이그레이션 중 구조화된 데이터와 반구조화된 데이터를 모두 처리할 수 있는 매우 확장 가능하고 내구성이 뛰어난 저장 솔루션을 제공합니다. AWS Glue는 데이터 변환 및 카탈로깅을 용이하게 하여 다운타임을 최소화하면서 원활한 전환을 보장합니다.",
        "Other Options": [
            "Amazon RDS로 마이그레이션하면 대규모 데이터 세트에 대한 확장성이 제한되며 반구조화된 데이터를 효율적으로 처리하지 못할 수 있습니다. AWS Data Pipeline은 유용하지만 이 시나리오에 비해 불필요한 복잡성을 초래할 수 있습니다.",
            "Amazon DynamoDB는 NoSQL 데이터베이스로, 구조화된 데이터 요구 사항에 적합하지 않을 수 있으며, AWS Snowball은 대량 전송에 효과적이지만 마이그레이션 중 지속적인 데이터 처리 요구 사항을 해결하지 않습니다.",
            "Amazon Redshift는 구조화된 데이터에 최적화되어 있지만 반구조화된 데이터에는 적합하지 않습니다. 또한, 실시간 데이터 스트리밍을 위해 Amazon Kinesis를 사용하는 것은 핵심 저장 요구 사항을 해결하지 않고 전체 마이그레이션 전략을 복잡하게 만들 수 있습니다."
        ]
    },
    {
        "Question Number": "37",
        "Situation": "데이터 분석가는 다양한 출처에서 수집된 고객 정보가 포함된 데이터셋을 다루고 있습니다. 이 데이터셋에는 중복 항목, 누락된 값 및 잘못된 형식과 같은 불일치가 있습니다. 분석가는 분석을 수행하기 전에 데이터가 깨끗하고 신뢰할 수 있는지 확인해야 합니다. 그들은 적용할 다양한 데이터 정제 기술을 고려하고 있습니다.",
        "Question": "데이터 품질을 보장하기 위해 분석가가 적용해야 할 데이터 정제 기술은 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "모든 기록이 미리 정의된 유효성 검사 규칙을 충족하는지 확인하기 위해 데이터 무결성 검사를 수행합니다.",
            "2": "결측값을 평균 또는 중앙값으로 채우기 위해 대체 방법을 적용합니다.",
            "3": "정규 표현식을 사용하여 전화번호와 이메일 주소의 형식을 표준화합니다.",
            "4": "데이터셋에서 중복 항목을 제거하기 위해 중복 제거 알고리즘을 구현합니다.",
            "5": "소스 시스템에서 데이터셋을 새로 고치기 위해 전체 데이터 내보내기 및 가져오기 작업을 수행합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "데이터셋에서 중복 항목을 제거하기 위해 중복 제거 알고리즘을 구현합니다.",
            "정규 표현식을 사용하여 전화번호와 이메일 주소의 형식을 표준화합니다."
        ],
        "Explanation": "중복 제거 알고리즘을 구현하면 중복 항목 문제를 직접 해결하여 각 고객이 한 번만 나타나도록 보장합니다. 정규 표현식을 사용하면 데이터 형식의 표준화를 가능하게 하여 전화번호 및 이메일 주소와 같은 필드에서 데이터셋 전반에 걸쳐 일관성을 유지하는 데 중요합니다.",
        "Other Options": [
            "전체 데이터 내보내기 및 가져오기 작업을 수행하는 것은 데이터셋의 불일치를 구체적으로 해결하지 않으며, 기존 오류를 전파할 수도 있습니다.",
            "결측값을 채우기 위해 대체 방법을 적용하는 것은 유용할 수 있지만, 중복 또는 형식 불일치 문제를 해결하지 않으며, 이는 이 시나리오의 주요 우려 사항입니다.",
            "데이터 무결성 검사를 수행하는 것은 좋은 관행이지만 데이터셋을 직접 정제하지는 않습니다. 이는 데이터의 품질을 평가하는 것이지 정제 기술을 구현하는 것이 아닙니다."
        ]
    },
    {
        "Question Number": "38",
        "Situation": "데이터 엔지니어는 대량의 스트리밍 데이터를 처리하는 데이터 파이프라인의 성능을 최적화하는 임무를 맡고 있습니다. 현재 설정은 피크 로드 시간 동안 지연 및 병목 현상을 경험하고 있습니다. 엔지니어는 효율적인 데이터 처리를 보장하고 최소한의 지연을 위해 성능 조정을 위한 모범 사례를 구현하고자 합니다.",
        "Question": "데이터 엔지니어가 데이터 파이프라인의 성능 향상을 위해 우선적으로 고려해야 할 전략은 무엇입니까?",
        "Options": {
            "1": "데이터 파티셔닝 및 인덱싱 구현",
            "2": "데이터 업데이트 빈도 줄이기",
            "3": "컴퓨팅 리소스의 인스턴스 크기 증가",
            "4": "단일 스레드 처리 모델로 전환"
        },
        "Correct Answer": "데이터 파티셔닝 및 인덱싱 구현",
        "Explanation": "데이터 파티셔닝 및 인덱싱을 구현하면 쿼리 성능을 크게 향상시키고 처리 시간을 줄일 수 있습니다. 이는 시스템이 데이터의 관련 부분만 액세스할 수 있도록 하여 대규모 데이터셋을 보다 효과적으로 관리하고 피크 로드 동안 지연을 줄이는 데 도움이 됩니다.",
        "Other Options": [
            "인스턴스 크기를 늘리면 더 많은 리소스를 제공할 수 있지만, 데이터 접근 패턴의 근본적인 비효율성을 해결하지 않으며 성능 개선이 보장되지 않은 채 비용이 증가할 수 있습니다.",
            "데이터 업데이트 빈도를 줄이면 부하 관리에 도움이 될 수 있지만, 기존 데이터 처리 작업의 성능을 직접 최적화하지 않으며, 잠재적으로 오래된 데이터 문제를 초래할 수 있습니다.",
            "단일 스레드 처리 모델로 전환하면 병목 현상 문제를 악화시킬 가능성이 높으며, 이는 시스템이 데이터를 동시에 효율적으로 처리할 수 있는 능력을 제한합니다."
        ]
    },
    {
        "Question Number": "39",
        "Situation": "데이터 엔지니어링 팀은 Amazon S3에 데이터 레이크로 수집되는 데이터의 품질을 보장하는 임무를 맡고 있습니다. 주요 요구 사항 중 하나는 추가 처리 전에 빈 필드가 있는 레코드를 식별하기 위해 자동화된 데이터 품질 검사를 구현하는 것입니다. 이러한 데이터 품질 검사를 효율적으로 구현하기 위해 가장 적합한 AWS 서비스는 무엇입니까?",
        "Question": "Amazon S3에 수집되는 데이터에 대해 자동화된 데이터 품질 검사를 구현하는 데 사용할 수 있는 서비스는 무엇입니까?",
        "Options": {
            "1": "AWS Glue DataBrew",
            "2": "AWS Lambda",
            "3": "Amazon Kinesis Data Firehose",
            "4": "Amazon EMR"
        },
        "Correct Answer": "AWS Glue DataBrew",
        "Explanation": "AWS Glue DataBrew는 사용자가 코드를 작성하지 않고도 데이터를 정리하고 변환할 수 있는 시각적 데이터 준비 도구입니다. 이는 자동화된 빈 필드 및 기타 이상 징후에 대한 검사를 가능하게 하는 내장된 데이터 품질 규칙을 제공합니다.",
        "Other Options": [
            "Amazon Kinesis Data Firehose는 주로 스트리밍 데이터와 S3와 같은 목적지로의 전송에 사용되지만, 데이터 품질 검사를 위한 내장 도구를 제공하지 않습니다.",
            "Amazon EMR은 Apache Spark와 같은 프레임워크를 사용하여 대량의 데이터를 처리할 수 있는 클라우드 빅데이터 플랫폼이지만, Glue DataBrew에 비해 데이터 품질 검사를 구현하기 위한 복잡한 설정 및 관리가 필요합니다.",
            "AWS Lambda는 이벤트에 응답하여 코드를 실행할 수 있는 서버리스 컴퓨팅 서비스이지만, 수신 데이터에 대한 데이터 품질 검사를 위해 특별히 설계된 도구나 기능을 제공하지 않습니다."
        ]
    },
    {
        "Question Number": "40",
        "Situation": "데이터 엔지니어링 팀이 Git을 사용하여 버전 관리를 위한 데이터 파이프라인 구축 프로젝트에 협력하고 있습니다. 그들은 새로운 기능을 위한 브랜치를 생성하고, 기존 파일을 업데이트하며, 중앙 서버에서 리포지토리를 복제하여 코드 리포지토리를 효과적으로 관리해야 합니다. 이 과정을 원활하게 진행하기 위해 Git 명령어에 대한 지식이 필요합니다.",
        "Question": "다음 Git 명령어 중 이 시나리오와 가장 관련이 깊은 것은 무엇입니까? (두 개 선택)",
        "Options": {
            "1": "git branch new-feature-branch",
            "2": "git status",
            "3": "git merge new-feature-branch",
            "4": "git push origin master",
            "5": "git clone https://github.com/user/repo.git"
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "git branch new-feature-branch",
            "git clone https://github.com/user/repo.git"
        ],
        "Explanation": "'git branch new-feature-branch'는 기능 개발을 위한 새로운 브랜치를 생성하는 명령어이며, 'git clone https://github.com/user/repo.git'는 원격 서버에서 기존 리포지토리를 복제하는 명령어로, 협업 코딩 프로젝트를 관리하는 데 필수적인 작업입니다.",
        "Other Options": [
            "'git push origin master'는 로컬 변경 사항을 원격 리포지토리의 마스터 브랜치에 푸시하는 데 사용되지만, 브랜치를 생성하거나 리포지토리를 복제하는 것과는 직접적인 관련이 없습니다.",
            "'git merge new-feature-branch'는 한 브랜치의 변경 사항을 다른 브랜치에 병합하는 데 사용되지만, 리포지토리를 생성하거나 복제하는 초기 작업과는 직접적인 관련이 없습니다.",
            "'git status'는 작업 디렉토리와 스테이징 영역의 상태를 표시하는 명령어이지만, 리포지토리를 생성하거나 복제하는 것과 관련된 작업을 수행하지 않습니다."
        ]
    },
    {
        "Question Number": "41",
        "Situation": "데이터 엔지니어가 여러 소스(API 엔드포인트 및 데이터베이스 포함)에서 데이터를 가져오고 특정 비즈니스 로직에 따라 데이터를 변환하는 데이터 수집 파이프라인을 설계하는 임무를 맡았습니다. 이 파이프라인은 매시간 실행되도록 예약해야 합니다. 엔지니어는 이 솔루션을 구현하기 위해 다양한 AWS 서비스를 고려하고 있습니다.",
        "Question": "이 데이터 수집 및 변환 파이프라인에 가장 효율적인 스케줄링 및 종속성 관리를 제공하는 접근 방식은 무엇입니까?",
        "Options": {
            "1": "AWS Glue 워크플로를 설정하여 ETL 프로세스를 관리하고, 수집 일정에 따라 작업을 실행하기 위한 트리거를 활용합니다.",
            "2": "AWS Batch를 구성하여 데이터 처리 작업을 처리하고 Amazon CloudWatch Events를 사용하여 작업 실행을 예약합니다.",
            "3": "AWS Step Functions를 사용하여 워크플로를 조정하고 데이터 추출 및 변환을 위해 AWS Lambda 함수를 호출합니다.",
            "4": "Amazon EMR 클러스터를 구현하여 데이터 처리를 위한 Apache Spark 작업을 실행하고, 매시간 클러스터 시작을 트리거하는 크론 작업을 사용합니다."
        },
        "Correct Answer": "AWS Glue 워크플로를 설정하여 ETL 프로세스를 관리하고, 수집 일정에 따라 작업을 실행하기 위한 트리거를 활용합니다.",
        "Explanation": "AWS Glue 워크플로는 ETL 프로세스의 조정을 단순화하도록 설계되어 있으며, 종속성을 쉽게 관리하고, 일정에 따라 작업을 트리거하며, 데이터 변환을 효율적으로 처리할 수 있습니다. 이 서비스는 데이터 통합에 맞춰져 있으며 높은 수준의 자동화를 제공합니다.",
        "Other Options": [
            "AWS Step Functions는 워크플로를 조정할 수 있지만, 여러 데이터 소스와 변환을 관리할 때 AWS Glue보다 추가적인 복잡성이 발생할 수 있습니다.",
            "AWS Batch는 대량의 데이터를 처리하는 데 유용하지만, 반복적인 데이터 수집 작업을 AWS Glue 워크플로만큼 효과적으로 예약하는 데 최적화되어 있지 않습니다.",
            "Amazon EMR 클러스터를 사용하는 것은 클러스터 생명 주기 및 Spark 작업 실행과 관련된 비용을 관리해야 하므로 AWS Glue에 비해 매시간 예약된 수집에 덜 적합합니다."
        ]
    },
    {
        "Question Number": "42",
        "Situation": "소매 회사가 AWS Lambda를 사용하여 IoT 장치에서 들어오는 데이터를 처리하고 있습니다. 데이터는 Amazon S3에 장기 저장하기 전에 임시로 저장되고 처리되어야 합니다. 팀은 추가 비용 없이 이 데이터를 효과적으로 처리하기 위해 Lambda 함수 내에서 스토리지 볼륨을 활용하고자 합니다.",
        "Question": "다음 접근 방식 중 Lambda 함수가 임시 데이터 수집 및 변환을 위해 스토리지 볼륨을 마운트하는 데 가장 적합한 것은 무엇입니까?",
        "Options": {
            "1": "AWS Glue를 활용하여 Lambda 함수의 로컬 디스크에 데이터를 임시로 저장하는 ETL 작업을 생성합니다.",
            "2": "AWS Step Functions를 구현하여 데이터 흐름을 조정하고 DynamoDB를 임시 데이터 저장소로 사용합니다.",
            "3": "Amazon S3를 임시 저장 솔루션으로 활용하고 Lambda 함수 내에서 S3에서 데이터를 복사합니다.",
            "4": "Amazon EFS(Elastic File System)를 사용하고 이를 Lambda 함수에 마운트하여 실행 중 임시 데이터를 저장합니다."
        },
        "Correct Answer": "Amazon EFS(Elastic File System)를 사용하고 이를 Lambda 함수에 마운트하여 실행 중 임시 데이터를 저장합니다.",
        "Explanation": "Amazon EFS를 사용하면 Lambda 함수에서 파일 시스템을 마운트할 수 있어 함수 실행 중에 접근할 수 있는 지속적인 저장 옵션을 제공합니다. 이는 S3로 전송되기 전에 처리해야 하는 임시 데이터를 처리하는 데 이상적입니다.",
        "Other Options": [
            "Amazon S3를 임시 저장소로 활용하는 것은 최적이 아닙니다. S3는 내구성 있는 저장을 위해 설계되었으며, 마운트된 파일 시스템에 비해 읽기/쓰기 작업에 추가적인 지연이 발생합니다.",
            "AWS Glue를 ETL 작업에 활용하는 것은 Lambda의 실행 컨텍스트 외부에서 작동하므로 Lambda 실행 중 필요한 임시 저장소를 제공하지 않습니다.",
            "AWS Step Functions를 조정에 사용하는 것은 Lambda 함수 내에서 임시 저장소의 필요성을 해결하지 않으며, DynamoDB를 임시 데이터 저장소로 사용하는 것은 추가 비용과 지연을 초래할 것입니다."
        ]
    },
    {
        "Question Number": "43",
        "Situation": "데이터 엔지니어링 팀은 실시간으로 들어오는 스트리밍 데이터를 처리하는 서버리스 데이터 수집 파이프라인을 구축해야 합니다. 그들은 필요한 AWS Lambda 함수와 Step Functions를 패키징하고 배포하기 위해 AWS SAM을 사용하고자 합니다. 팀은 또한 처리된 데이터를 추가 분석을 위해 DynamoDB 테이블에 지속적으로 저장할 수 있는 솔루션이 필요합니다. 이 솔루션은 효율적이고 비용 효과적이어야 합니다.",
        "Question": "서버리스 데이터 파이프라인을 배포하기 위해 AWS SAM을 가장 잘 활용할 수 있는 단계의 조합은 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "배포를 위해 AWS CodePipeline을 사용하여 CI/CD 파이프라인을 생성합니다.",
            "2": "AWS CloudFormation을 사용하여 리소스의 배포를 관리합니다.",
            "3": "AWS SAM 템플릿에서 Lambda 함수와 Step Functions를 정의합니다.",
            "4": "AWS SAM CLI를 사용하여 Lambda 함수를 패키징합니다.",
            "5": "AWS Management Console을 사용하여 DynamoDB 테이블을 수동으로 구성합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "AWS SAM 템플릿에서 Lambda 함수와 Step Functions를 정의합니다.",
            "AWS SAM CLI를 사용하여 Lambda 함수를 패키징합니다."
        ],
        "Explanation": "AWS SAM 템플릿에서 Lambda 함수와 Step Functions를 정의하면 구성 및 배포 프로세스가 간소화되며, AWS SAM CLI를 사용하여 Lambda 함수를 패키징하면 배포 아티팩트가 올바르게 준비되고 AWS에 업로드됩니다. 두 단계 모두 서버리스 데이터 파이프라인을 위해 AWS SAM을 활용하는 데 필수적입니다.",
        "Other Options": [
            "AWS CloudFormation을 사용하는 것은 여기서 선호되는 접근 방식이 아닙니다. AWS SAM은 서버리스 애플리케이션의 배포를 간소화하도록 특별히 설계되었으며, 표준 CloudFormation에서는 제공되지 않는 추가 기능을 제공합니다.",
            "AWS CodePipeline을 사용하여 CI/CD 파이프라인을 생성하는 것은 배포에 유용할 수 있지만, 질문에서 요구하는 서버리스 아키텍처의 핵심 설정 및 초기 배포와는 직접적인 관련이 없습니다.",
            "AWS Management Console을 사용하여 DynamoDB 테이블을 수동으로 구성하는 것은 효율적이지 않거나 자동화되지 않습니다. 목표는 AWS SAM을 사용하여 DynamoDB 테이블을 포함한 모든 리소스를 정의하고 관리하여 재사용성과 유지 관리성을 보장하는 것입니다."
        ]
    },
    {
        "Question Number": "44",
        "Situation": "한 회사가 온프레미스 데이터베이스를 AWS로 마이그레이션하고 있으며, 기존 데이터베이스의 스키마가 Amazon Aurora와 호환되는지 확인해야 합니다. 데이터베이스에는 변환이 필요한 복잡한 유형과 사용자 정의 함수가 포함되어 있습니다. 데이터 엔지니어링 팀은 이 프로세스를 용이하게 하고 오류의 위험을 줄이기 위해 자동화 도구를 사용하는 것을 고려하고 있습니다.",
        "Question": "데이터 엔지니어링 팀이 스키마 변환을 수행하고 Amazon Aurora와의 호환성을 보장하기 위해 사용해야 할 AWS 서비스는 무엇입니까?",
        "Options": {
            "1": "Amazon RDS를 사용하여 온프레미스 데이터베이스를 Aurora로 직접 복제하되, 스키마 변환은 하지 않습니다.",
            "2": "AWS Glue를 사용하여 기존 스키마를 카탈로그화하고 Aurora로의 스키마 마이그레이션을 용이하게 합니다.",
            "3": "AWS Database Migration Service (AWS DMS)를 사용하여 데이터를 마이그레이션하고 Aurora에서 스키마를 수동으로 재구성합니다.",
            "4": "AWS Schema Conversion Tool (AWS SCT)을 사용하여 기존 스키마를 변환한 후 AWS DMS를 사용하여 데이터를 마이그레이션합니다."
        },
        "Correct Answer": "AWS Schema Conversion Tool (AWS SCT)을 사용하여 기존 스키마를 변환한 후 AWS DMS를 사용하여 데이터를 마이그레이션합니다.",
        "Explanation": "AWS SCT는 데이터베이스 스키마를 한 데이터베이스 엔진에서 다른 데이터베이스 엔진으로 변환하도록 특별히 설계되었으므로 Amazon Aurora와의 호환성을 보장하는 데 가장 적합한 선택입니다. 스키마가 변환된 후 AWS DMS를 사용하여 다운타임 없이 효율적으로 데이터를 마이그레이션할 수 있습니다.",
        "Other Options": [
            "이 옵션은 스키마 변환 없이 마이그레이션을 위해 AWS DMS를 사용하는 것을 제안하는데, 이는 기존 스키마에 복잡한 유형과 사용자 정의 함수가 포함된 경우 호환성을 보장하기에 적합하지 않습니다.",
            "Amazon RDS는 스키마 변환 도구가 아니라 관리형 데이터베이스 서비스입니다. 데이터베이스를 직접 복제하는 것은 스키마 호환성 문제를 해결하지 않습니다.",
            "AWS Glue는 주로 데이터 준비 및 변환에 사용되며, 스키마 변환을 위한 것이 아닙니다. 복잡한 데이터베이스 스키마를 변환하는 데 필요한 기능을 제공하지 않습니다."
        ]
    },
    {
        "Question Number": "45",
        "Situation": "한 소매 회사는 AWS Glue를 데이터 카탈로그로 사용하고 있으며, 여러 개의 Amazon S3 버킷에 파티셔닝된 데이터 세트를 보유하고 있습니다. 그들은 Glue 데이터 카탈로그가 S3 버킷의 최신 파티션 변경 사항과 동기화되어 Amazon Athena에서 효율적인 쿼리를 가능하게 하기를 원합니다.",
        "Question": "S3 데이터의 파티션을 AWS Glue 데이터 카탈로그와 동기화하는 가장 효율적인 방법은 무엇입니까?",
        "Options": {
            "1": "Amazon S3 이벤트 알림을 설정하여 AWS Lambda 함수를 트리거하고 AWS Glue API를 호출하여 새로운 파티션으로 데이터 카탈로그를 업데이트합니다.",
            "2": "새로운 데이터가 S3 버킷에 업로드될 때마다 AWS Glue API를 수동으로 호출하여 파티션을 데이터 카탈로그에 추가합니다.",
            "3": "S3 데이터에서 새로운 파티션을 감지하고 추가하도록 구성된 AWS Glue 크롤러를 생성합니다.",
            "4": "AWS Step Functions를 활용하여 S3 버킷에서 새로운 파티션을 주기적으로 확인하고 데이터 카탈로그를 업데이트하는 워크플로우를 생성합니다."
        },
        "Correct Answer": "S3 데이터에서 새로운 파티션을 감지하고 추가하도록 구성된 AWS Glue 크롤러를 생성합니다.",
        "Explanation": "AWS Glue 크롤러를 사용하는 것은 데이터 카탈로그에서 파티션을 자동으로 감지하고 동기화하는 가장 효율적인 방법입니다. 이는 사용자 정의 코딩이나 수동 개입 없이 자동 업데이트를 가능하게 합니다.",
        "Other Options": [
            "각 업로드마다 AWS Glue API를 수동으로 호출하는 것은 번거롭고 오류가 발생하기 쉬워 대량의 데이터에서는 자동화된 솔루션보다 효율적이지 않습니다.",
            "S3 이벤트 알림 및 Lambda 함수를 설정하는 것은 복잡성을 추가하고 동기화 프로세스에 지연을 초래할 수 있으며, 크롤러가 작업을 자동화할 수 있는 경우에는 필요하지 않습니다.",
            "AWS Step Functions를 사용하여 새로운 파티션을 주기적으로 확인하는 것은 불필요한 오버헤드와 복잡성을 추가합니다. Glue 크롤러는 이 목적을 위해 특별히 설계되었으며 주기적으로 실행될 수 있습니다."
        ]
    },
    {
        "Question Number": "46",
        "Situation": "데이터 엔지니어가 머신러닝 모델을 위한 대규모 데이터셋을 준비하는 임무를 맡았습니다. 데이터셋이 너무 커서 한 번에 처리할 수 없으며, 엔지니어는 모델이 데이터의 대표 샘플로 훈련되도록 해야 합니다. 그들은 편향을 최소화하고 원본 데이터셋의 무결성을 유지하는 샘플링 기법을 선택해야 합니다.",
        "Question": "데이터 엔지니어가 머신러닝 모델 훈련을 위한 데이터셋의 대표적인 하위 집합을 보장하기 위해 어떤 샘플링 기법을 사용해야 합니까?",
        "Options": {
            "1": "모든 하위 그룹이 비례적으로 대표되도록 계층화 샘플링을 사용합니다.",
            "2": "특성에 관계없이 데이터 포인트를 선택하기 위해 무작위 샘플링을 사용합니다.",
            "3": "데이터셋을 그룹으로 나누고 전체 클러스터를 샘플링하기 위해 클러스터 샘플링을 사용합니다.",
            "4": "데이터셋에서 매 n번째 데이터 포인트를 선택하기 위해 체계적 샘플링을 사용합니다."
        },
        "Correct Answer": "모든 하위 그룹이 비례적으로 대표되도록 계층화 샘플링을 사용합니다.",
        "Explanation": "계층화 샘플링은 데이터셋의 각 하위 그룹이 샘플에 대표되도록 보장하므로, 편향을 최소화하고 모델 훈련을 위한 데이터의 무결성을 유지하는 데 중요합니다. 이 기법은 각 하위 그룹 내의 변동성을 포착하는 데 도움을 주어 더 신뢰할 수 있는 모델 성능을 이끌어냅니다.",
        "Other Options": [
            "무작위 샘플링은 특정 하위 그룹의 과소 대표를 초래할 수 있어 샘플에 편향을 도입하고 모델 성능에 영향을 미칠 수 있습니다.",
            "체계적 샘플링은 데이터에 샘플링 간격과 일치하는 기본 패턴이 있을 경우 편향을 초래할 수 있으며, 이는 전체 데이터셋을 정확하게 대표하지 않을 수 있습니다.",
            "클러스터 샘플링은 클러스터가 동질적이지 않은 경우 적합하지 않을 수 있으며, 이는 전체 데이터셋의 다양성을 반영하지 않는 전체 그룹을 샘플링할 수 있습니다."
        ]
    },
    {
        "Question Number": "47",
        "Situation": "소매 회사가 민감한 고객 데이터를 적절하게 분류하고 관리하기 위해 새로운 데이터 거버넌스 전략을 구현하고 있습니다. 데이터 엔지니어링 팀은 회사의 규정 준수 및 보안 요구 사항에 따라 데이터를 분류하기 위한 최상의 도구를 식별해야 합니다.",
        "Question": "데이터 엔지니어링 팀이 데이터를 효과적으로 분류하는 데 도움이 되는 도구와 서비스는 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "AWS Config를 활용하여 데이터 분류의 변화를 모니터링하고 규정 준수 규칙을 시행합니다.",
            "2": "Amazon QuickSight를 구현하여 데이터 흐름을 시각화하고 민감한 데이터 위치를 식별합니다.",
            "3": "Amazon Macie를 사용하여 Amazon S3에 저장된 민감한 데이터를 자동으로 발견하고 분류합니다.",
            "4": "AWS Glue Data Catalog를 활용하여 메타데이터를 유지하고 다양한 AWS 데이터 저장소에서 데이터를 분류합니다.",
            "5": "AWS Lambda를 사용하여 Amazon RDS의 데이터에 대한 사용자 정의 데이터 분류 스크립트를 생성합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Amazon Macie를 사용하여 Amazon S3에 저장된 민감한 데이터를 자동으로 발견하고 분류합니다.",
            "AWS Glue Data Catalog를 활용하여 메타데이터를 유지하고 다양한 AWS 데이터 저장소에서 데이터를 분류합니다."
        ],
        "Explanation": "Amazon Macie는 Amazon S3에 저장된 민감한 데이터를 자동으로 발견하고 분류하며, 민감한 고객 정보를 식별하는 데 이상적인 선택입니다. AWS Glue Data Catalog는 다양한 AWS 서비스에서 데이터를 유지하고 분류하는 데 도움이 되는 통합 메타데이터 저장소를 제공합니다. 이는 적절한 데이터 거버넌스와 관리를 보장합니다.",
        "Other Options": [
            "Amazon QuickSight는 주로 데이터 시각화 도구이며 데이터 분류나 규정 준수에 중점을 두지 않으므로 이 요구 사항에 적합하지 않습니다.",
            "AWS Config는 규정 준수 및 구성 변경을 모니터링하는 데 사용되지만, 회사의 요구 사항에 필수적인 직접적인 데이터 분류 기능을 제공하지 않습니다.",
            "AWS Lambda는 사용자 정의 스크립트를 생성하는 데 사용할 수 있지만 수동 코딩이 필요하며 데이터 분류를 위한 즉시 사용 가능한 솔루션을 제공하지 않으므로 이 특정 작업에 덜 효율적입니다."
        ]
    },
    {
        "Question Number": "48",
        "Situation": "데이터 엔지니어링 팀이 대규모 Amazon RDS 인스턴스에서 데이터를 추출하는 데 사용되는 SQL 쿼리를 최적화하는 임무를 맡았습니다. 그들은 일부 쿼리가 특히 피크 시간대에 예상보다 더 오래 실행되는 것을 발견했습니다. 팀은 기본 데이터 구조를 변경하지 않고 쿼리 성능을 개선하기 위한 전략을 찾고 있습니다.",
        "Question": "팀이 SQL 쿼리 성능을 최적화하기 위해 구현해야 할 전략은 무엇입니까?",
        "Options": {
            "1": "데이터베이스 부하를 줄이기 위해 쿼리 캐싱을 구현합니다.",
            "2": "Amazon RDS 데이터베이스의 인스턴스 크기를 늘립니다.",
            "3": "SQL 쿼리에서 조인을 사용하지 않도록 합니다.",
            "4": "일반적으로 쿼리되는 필드의 인덱스 수를 줄입니다."
        },
        "Correct Answer": "데이터베이스 부하를 줄이기 위해 쿼리 캐싱을 구현합니다.",
        "Explanation": "쿼리 캐싱은 비용이 많이 드는 쿼리의 결과를 저장하여 동일한 데이터에 대한 후속 요청을 데이터베이스에 다시 접근하지 않고도 빠르게 제공할 수 있게 하여 성능을 크게 향상시킬 수 있습니다. 이는 데이터베이스에 대한 전체 부하를 줄이고 사용자에 대한 응답 시간을 단축합니다.",
        "Other Options": [
            "인스턴스 크기를 늘리면 성능이 향상될 수 있지만 느린 쿼리의 근본 원인을 해결하지 못할 수 있습니다. 또한 쿼리 자체를 최적화하는 것에 비해 더 비용이 많이 드는 솔루션입니다.",
            "인덱스 수를 줄이면 데이터 검색 속도를 높이는 데 도움이 되는 인덱스가 있기 때문에 읽기 작업에 대한 쿼리 성능이 실제로 저하될 수 있습니다. 적절하게 설계된 인덱스는 쿼리 성능 최적화에 필수적입니다.",
            "조인을 피하는 것은 비정규화된 데이터 구조로 이어질 수 있으며, 이는 실용적이지 않거나 효율적이지 않을 수 있습니다. 조인은 관련 데이터를 효율적으로 검색하는 데 종종 필요하며, 조인을 작성하는 방식을 최적화하는 것이 일반적으로 더 나은 접근 방식입니다."
        ]
    },
    {
        "Question Number": "49",
        "Situation": "데이터 엔지니어가 Amazon Redshift 데이터 웨어하우스의 성능 최적화 작업을 하고 있습니다. 그들은 쿼리가 예상보다 느리게 실행되고 있음을 발견했습니다. 쿼리 성능을 개선하기 위해, 그들은 테이블의 분포 스타일과 진공 프로세스를 고려하고 있습니다.",
        "Question": "느린 쿼리 시간을 겪고 있는 'mytable'이라는 Amazon Redshift 테이블에서 쿼리 성능을 가장 잘 개선할 수 있는 행동은 무엇입니까?",
        "Options": {
            "1": "mytable에서 VACUUM 명령을 실행하여 공간을 회수하고 쿼리 성능을 개선합니다.",
            "2": "mytable에 대해 DISTSTYLE ALL을 사용하여 모든 노드에서 모든 데이터가 사용 가능하도록 합니다.",
            "3": "mytable의 분포 스타일을 EVEN으로 변경하여 모든 노드에 걸쳐 행을 고르게 분배합니다.",
            "4": "mytable에서 ANALYZE 명령을 실행하여 쿼리 계획자를 위한 통계를 업데이트합니다."
        },
        "Correct Answer": "mytable에서 ANALYZE 명령을 실행하여 쿼리 계획자를 위한 통계를 업데이트합니다.",
        "Explanation": "mytable에서 ANALYZE 명령을 실행하면 쿼리 계획자가 최적화된 쿼리 실행 계획을 생성하는 데 사용하는 통계가 업데이트됩니다. 정확한 통계는 쿼리 실행 성능을 향상시킬 수 있습니다.",
        "Other Options": [
            "VACUUM 명령을 실행하면 공간을 회수하고 전반적인 성능을 개선하는 데 도움이 될 수 있지만, 이는 쿼리 계획 및 실행 효율성을 직접적으로 해결하지 않으며, 이는 정확한 통계에 의해 더 심각하게 영향을 받습니다.",
            "DISTSTYLE ALL을 사용하면 모든 노드에서 불필요한 데이터 중복이 발생할 수 있으며, 이는 저장 비용을 증가시키고 대규모 데이터 세트의 쿼리 성능을 효과적으로 개선하지 못할 수 있습니다.",
            "분포 스타일을 EVEN으로 변경하면 일부 경우에 도움이 될 수 있지만, 쿼리 패턴에 맞는 분포 키를 사용하는 이점이 부족하여 더 나은 쿼리 성능을 보장하지 않습니다."
        ]
    },
    {
        "Question Number": "50",
        "Situation": "소매 회사의 데이터 엔지니어링 팀이 Amazon Redshift 데이터베이스에 저장된 판매 거래 데이터를 처리하고 변환해야 합니다. 팀은 일일 판매를 집계하고 결과를 보고 목적으로 별도의 테이블에 저장하기 위해 변환 프로세스를 자동화하고자 합니다. 이 작업을 수행하기 위해 저장 프로시저를 사용하는 것을 고려하고 있습니다.",
        "Question": "팀이 저장 프로시저를 사용하여 Amazon Redshift에서 변환 논리를 구현하기 위해 어떤 접근 방식을 취해야 합니까?",
        "Options": {
            "1": "Amazon Athena를 사용하여 판매 데이터에 대한 SQL 쿼리를 실행하고 결과를 보고 테이블에 저장합니다.",
            "2": "저장 프로시저를 사용하지 않고 직접 보고 테이블에 데이터를 삽입하는 일일 ETL 작업을 예약합니다.",
            "3": "매시간 트리거되는 Lambda 함수를 구현하여 데이터 집계를 수행하고 결과를 보고 테이블에 저장합니다.",
            "4": "SQL 명령을 사용하여 판매 데이터를 집계하고 결과를 보고 테이블에 삽입하는 저장 프로시저를 생성합니다."
        },
        "Correct Answer": "SQL 명령을 사용하여 판매 데이터를 집계하고 결과를 보고 테이블에 삽입하는 저장 프로시저를 생성합니다.",
        "Explanation": "저장 프로시저는 SQL 논리를 재사용 가능한 방식으로 캡슐화할 수 있어, Redshift 환경 내에서 데이터를 집계하고 복잡한 변환을 직접 관리하는 데 적합합니다. 이 접근 방식은 성능을 최적화하고 변환 중 데이터 무결성을 유지합니다.",
        "Other Options": [
            "일일 ETL 작업을 예약하는 것은 유효한 접근 방식이지만, 복잡한 논리를 캡슐화하고 데이터 변환을 더 관리하기 쉽게 만드는 저장 프로시저의 이점을 활용하지 않습니다.",
            "Amazon Athena를 사용하는 것은 적절하지 않으며, 이는 S3의 데이터를 쿼리하기 위해 설계되었고 Redshift 데이터베이스 컨텍스트 내에서 직접 변환을 수행하는 데 적합하지 않습니다.",
            "Lambda 함수를 구현하면 저장 프로시저를 사용하여 Redshift 내에서 데이터 집계를 수행하는 것에 비해 불필요한 복잡성과 지연을 초래할 수 있습니다."
        ]
    },
    {
        "Question Number": "51",
        "Situation": "데이터 엔지니어링 팀이 여러 소스에서 스트리밍 데이터를 처리하는 실시간 분석 애플리케이션의 성능을 최적화하는 임무를 맡았습니다. 그들은 들어오는 데이터를 효율적으로 관리하고 분석하기 위해 다양한 데이터 구조를 고려하고 있습니다. 그들은 빠른 접근 및 업데이트를 허용하면서 데이터의 계층적 조직을 유지할 수 있는 데이터 구조를 선택해야 합니다.",
        "Question": "이 시나리오에서 빠른 접근, 업데이트 및 계층적 조직 요구 사항을 가장 잘 지원하는 데이터 구조는 무엇입니까?",
        "Options": {
            "1": "해시 테이블",
            "2": "그래프 구조",
            "3": "B-트리",
            "4": "이진 탐색 트리 (BST)"
        },
        "Correct Answer": "B-트리",
        "Explanation": "B-트리는 정렬된 데이터를 계층 구조로 유지하면서 빠른 접근 및 업데이트가 필요한 시나리오에 적합합니다. 빠른 삽입, 삭제 및 검색 작업을 허용하도록 설계되어 있으며, 데이터가 자주 수정되는 실시간 분석 애플리케이션에 이상적입니다.",
        "Other Options": [
            "이진 탐색 트리 (BST)는 빠른 접근 및 업데이트를 제공할 수 있지만, 균형을 효율적으로 유지하지 못할 수 있어 최악의 경우 성능 저하를 초래할 수 있습니다. 또한, B-트리와 같은 대량의 데이터를 관리하는 데 최적화되어 있지 않습니다.",
            "해시 테이블은 검색 및 업데이트 작업에 대해 평균 O(1) 시간 복잡성을 제공하지만, 요소 간의 순서를 유지하지 않으므로 이 시나리오에서 요구되는 계층적 데이터 조직에 적합하지 않습니다.",
            "그래프 구조는 복잡한 관계를 표현하는 데 유연하지만, 계층적 데이터에 대한 빠른 접근 및 업데이트 기능을 본질적으로 제공하지 않습니다. 이는 상호 연결된 데이터를 탐색해야 하는 시나리오에 더 적합합니다."
        ]
    },
    {
        "Question Number": "52",
        "Situation": "한 금융 서비스 회사가 AWS에 저장된 민감한 고객 데이터에 대한 접근을 관리하기 위해 새로운 데이터 거버넌스 프레임워크를 구현하고 있습니다. 팀은 역할과 속성에 따라 올바른 사용자만 접근할 수 있도록 다양한 권한 부여 방법을 평가하고 있습니다.",
        "Question": "회사가 사용자 특성과 조직 내 특정 역할에 따라 민감한 데이터에 대한 접근을 관리할 수 있도록 가장 적합한 권한 부여 방법은 무엇입니까?",
        "Options": {
            "1": "리소스와 사용자에 부착된 태그를 사용하여 접근을 제어하는 태그 기반 접근 제어(TBAC).",
            "2": "사용자 접근을 리소스에 대한 미리 정의된 규칙을 사용하여 결정하는 정책 기반 접근 제어(PBAC).",
            "3": "사용자 속성과 리소스 특성을 활용하여 접근 결정을 내리는 속성 기반 접근 제어(ABAC).",
            "4": "조직 내 사용자 역할에 따라 권한을 부여하는 역할 기반 접근 제어(RBAC)."
        },
        "Correct Answer": "속성 기반 접근 제어(ABAC)로, 사용자 속성과 리소스 특성을 활용하여 접근 결정을 내립니다.",
        "Explanation": "속성 기반 접근 제어(ABAC)는 사용자 속성(예: 부서, 직책)과 리소스 특성을 사용하여 세분화된 접근 제어를 가능하게 합니다. 이 방법은 유연성을 제공하며 동적 속성에 따라 민감한 데이터 접근을 관리하는 데 효과적입니다.",
        "Other Options": [
            "역할 기반 접근 제어(RBAC)는 미리 정의된 역할에만 국한되어 사용자 속성을 고려하지 않으므로 민감한 데이터 접근에 필요한 세분성을 제공하지 않을 수 있습니다.",
            "정책 기반 접근 제어(PBAC)는 규칙에 따라 접근을 제공하지만 사용자 속성에 따라 동적으로 조정되지 않아 복잡한 조직에서의 효과성이 제한될 수 있습니다.",
            "태그 기반 접근 제어(TBAC)는 리소스를 조직하는 데 유용하지만 태그가 일관되게 적용되어야 하며 효과적인 거버넌스를 위해 필요한 사용자 속성을 포함하지 않을 수 있습니다."
        ]
    },
    {
        "Question Number": "53",
        "Situation": "한 소매 회사의 데이터 엔지니어링 팀이 고객 거래 데이터를 처리하기 위한 ETL 파이프라인을 구축하고 있습니다. 그들은 파이프라인이 탄력적이고 관리하기 쉬우며 실패로부터 복구할 수 있도록 해야 합니다. AWS 서비스를 사용하여 워크플로를 조정하고 데이터 변환을 처리하는 것을 고려하고 있습니다.",
        "Question": "ETL 파이프라인의 워크플로를 조정하면서 탄력성과 내결함성을 보장하기 위해 효과적으로 활용할 수 있는 AWS 서비스는 무엇입니까? (두 개 선택)",
        "Options": {
            "1": "원시 데이터와 처리된 출력을 저장하기 위한 Amazon S3.",
            "2": "이벤트에 응답하여 코드를 실행하고 다른 AWS 서비스와 통합하기 위한 AWS Lambda.",
            "3": "자동화된 ETL 프로세스와 데이터 카탈로깅을 위한 AWS Glue.",
            "4": "상태 기계를 정의하고 워크플로 실행을 관리하기 위한 AWS Step Functions.",
            "5": "이벤트 기반 아키텍처와 이벤트를 대상으로 라우팅하기 위한 Amazon EventBridge."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "상태 기계를 정의하고 워크플로 실행을 관리하기 위한 AWS Step Functions.",
            "자동화된 ETL 프로세스와 데이터 카탈로깅을 위한 AWS Glue."
        ],
        "Explanation": "AWS Step Functions는 여러 AWS 서비스를 서버리스 워크플로로 조정할 수 있게 해주어 내장된 오류 처리 및 재시도를 통해 ETL 프로세스를 관리하기 쉽게 만듭니다. AWS Glue는 데이터를 추출, 변환 및 로드하는 프로세스를 자동화하는 완전 관리형 ETL 서비스를 제공하여 파이프라인의 탄력성과 확장성을 향상시킵니다.",
        "Other Options": [
            "Amazon S3는 주로 저장 서비스이며 ETL 워크플로를 관리하는 데 필요한 조정 기능을 제공하지 않습니다.",
            "AWS Lambda는 개별 변환에 사용할 수 있지만 Step Functions처럼 전체 워크플로 실행을 관리하지 않습니다.",
            "Amazon EventBridge는 이벤트 라우팅에 탁월하지만 복잡한 ETL 워크플로에 필요한 조정 기능을 제공하지 않습니다."
        ]
    },
    {
        "Question Number": "54",
        "Situation": "한 금융 서비스 회사가 Amazon S3에 저장된 대규모 데이터 세트를 분석하면서 데이터 저장 비용과 처리 시간을 최소화하려고 합니다. 그들은 데이터를 Amazon Redshift 클러스터에 로드할 필요 없이 S3의 데이터에 직접 SQL 쿼리를 실행하고 싶어합니다. 데이터는 자주 업데이트되며, 회사는 분석이 가장 최신의 데이터를 반영하도록 해야 합니다.",
        "Question": "Amazon Redshift로 이동하지 않고 Amazon S3에 저장된 데이터를 쿼리하는 데 사용할 수 있는 방법은 무엇입니까? (두 개 선택)",
        "Options": {
            "1": "Amazon Redshift 연합 쿼리를 구현하여 S3의 데이터를 Redshift 테이블에 있는 것처럼 접근합니다.",
            "2": "분석을 위해 S3 데이터를 Amazon Redshift 클러스터에 로드합니다.",
            "3": "Amazon Athena를 사용하여 Redshift 클러스터 없이 S3에 저장된 데이터에 SQL 쿼리를 실행합니다.",
            "4": "S3의 데이터로 정기적으로 새로 고쳐지는 Amazon Redshift 물리적 뷰를 생성합니다.",
            "5": "원래 형식을 유지하면서 S3 데이터를 직접 쿼리하기 위해 Amazon Redshift Spectrum을 활용합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "원래 형식을 유지하면서 S3 데이터를 직접 쿼리하기 위해 Amazon Redshift Spectrum을 활용합니다.",
            "Amazon Athena를 사용하여 Redshift 클러스터 없이 S3에 저장된 데이터에 SQL 쿼리를 실행합니다."
        ],
        "Explanation": "Amazon Redshift Spectrum은 데이터를 Redshift에 로드할 필요 없이 Amazon S3에 저장된 데이터에 대해 직접 쿼리를 실행할 수 있게 해주어 대규모 데이터 세트를 분석하는 비용 효율적인 솔루션입니다. 마찬가지로, Amazon Athena는 추가 인프라 없이 표준 SQL을 사용하여 S3 데이터를 쿼리할 수 있게 해주어 유연성을 제공하고 비용을 절감합니다.",
        "Other Options": [
            "Amazon Redshift 물리적 뷰를 생성하는 것은 S3에서 데이터를 쿼리하는 직접적인 방법이 아니며, 데이터가 먼저 Redshift에 로드되어야 하므로 데이터를 이동하지 않으려는 요구 사항에 위배됩니다.",
            "Amazon Redshift 연합 쿼리를 사용하면 외부 데이터 소스에 접근할 수 있지만, 일반적으로 데이터가 지원되는 데이터베이스 시스템을 통해 접근 가능해야 하며 S3 데이터를 Redshift에 로드하지 않고 쿼리하는 데 직접 적용되지 않습니다.",
            "S3 데이터를 Amazon Redshift 클러스터에 로드하는 것은 데이터를 이동하지 않으려는 요구 사항에 위배되며 S3에서 직접 데이터를 쿼리하는 비용 절감 혜택을 활용하지 않습니다."
        ]
    },
    {
        "Question Number": "55",
        "Situation": "데이터 엔지니어링 팀이 AWS Lake Formation을 사용하여 Amazon S3의 데이터 레이크에 대한 데이터 보안 및 거버넌스를 구현하고 있습니다. 그들은 데이터 접근 정책이 적절하게 관리되도록 해야 하며, 다양한 형식으로 저장된 데이터에 접근하는 사용자와 그룹에 대해 세분화된 권한을 제공하고자 합니다. 팀은 특히 Amazon Redshift와 Amazon Athena에서 실행되는 분석 작업에 대한 권한 관리에 대해 우려하고 있습니다.",
        "Question": "AWS Lake Formation의 어떤 기능이 데이터 엔지니어링 팀이 Amazon S3에 저장된 데이터에 대한 세분화된 접근 권한을 관리할 수 있도록 합니까?",
        "Options": {
            "1": "Resource Link",
            "2": "Data Catalog",
            "3": "Permissions Management",
            "4": "Data Lake Policy"
        },
        "Correct Answer": "Permissions Management",
        "Explanation": "AWS Lake Formation의 Permissions Management는 S3에 저장된 데이터에 대한 세분화된 접근 제어를 처리하도록 특별히 설계되어, 사용자가 누구에게 어떤 데이터에 접근할 수 있는지 및 어떻게 사용할 수 있는지를 정의할 수 있게 합니다. 이 기능은 데이터 거버넌스 정책이 효과적으로 시행되도록 보장하는 데 필수적입니다.",
        "Other Options": [
            "Data Catalog는 Lake Formation에서 메타데이터를 조직하고 관리하는 데 사용되지만, 권한을 직접 처리하지는 않습니다.",
            "Data Lake Policy는 포괄적인 거버넌스 프레임워크를 의미하지만, 세분화된 권한 관리를 위한 특정 도구를 제공하지는 않습니다.",
            "Resource Link는 서로 다른 데이터 레이크 간에 리소스를 연결할 수 있는 기능이지만, 접근 권한 관리에 주로 초점을 맞추고 있지는 않습니다."
        ]
    },
    {
        "Question Number": "56",
        "Situation": "데이터 엔지니어는 AWS 환경에서 애플리케이션 및 데이터베이스 자격 증명의 안전한 저장 및 검색을 보장하는 임무를 맡고 있습니다. 솔루션은 보안 및 사용 용이성에 대한 모범 사례를 준수해야 하며, 민감한 정보의 중앙 집중식 관리도 제공해야 합니다.",
        "Question": "어떤 AWS 서비스를 사용하여 애플리케이션 및 데이터베이스 자격 증명을 안전하게 저장하고 관리하며, 하드코딩 없이 애플리케이션에서 쉽게 접근할 수 있도록 해야 합니까?",
        "Options": {
            "1": "AWS Secrets Manager를 사용하여 데이터베이스 자격 증명을 안전하게 저장하고 자동 회전을 관리합니다.",
            "2": "Amazon S3를 활용하여 암호화된 파일에 자격 증명을 저장하고 IAM 정책을 통해 접근을 관리합니다.",
            "3": "AWS Lambda 환경 변수를 사용하여 애플리케이션 자격 증명을 저장하고 함수 실행 중 쉽게 접근합니다.",
            "4": "AWS Systems Manager Parameter Store를 활용하여 자격 증명을 보안 문자열로 저장하고 접근 제어를 적용합니다."
        },
        "Correct Answer": "AWS Secrets Manager를 사용하여 데이터베이스 자격 증명을 안전하게 저장하고 자동 회전을 관리합니다.",
        "Explanation": "AWS Secrets Manager는 데이터베이스 자격 증명과 같은 비밀을 관리하기 위한 전용 서비스를 제공하며, 자동 회전, 통합 접근 관리 및 내장 암호화와 같은 기능을 제공하여 민감한 정보를 안전하게 저장하는 데 가장 적합한 선택입니다.",
        "Other Options": [
            "Amazon S3에 자격 증명을 저장하는 것은 민감한 정보에 대해 권장되지 않으며, 자동 회전 및 세분화된 접근 제어와 같은 비밀 관리에 필요한 전문 기능이 부족합니다.",
            "AWS Lambda 환경 변수를 사용하는 경우, 신중하게 관리하지 않으면 민감한 정보가 노출될 수 있으며, 중앙 집중식 관리 및 자동 회전 기능이 부족합니다.",
            "AWS Systems Manager Parameter Store는 매개변수를 안전하게 저장할 수 있지만, 자동 회전이나 전용 비밀 관리와 같은 동일한 수준의 비밀 관리 기능을 제공하지 않으므로 Secrets Manager가 더 적합한 선택입니다."
        ]
    },
    {
        "Question Number": "57",
        "Situation": "데이터 엔지니어는 Amazon RDS 데이터베이스에서 데이터를 수집하고 변환하는 데이터 파이프라인을 생성하는 임무를 맡고 있습니다. 이 파이프라인은 여러 테이블에서 집계 및 조인을 수행한 후 결과를 Amazon Redshift에 로드하여 분석해야 합니다. 솔루션은 효율적이고 확장 가능하며 최소한의 관리가 필요해야 합니다. 다음 중 어떤 접근 방식이 이러한 요구 사항을 가장 잘 충족할 수 있습니까?",
        "Question": "데이터 엔지니어가 운영 오버헤드를 최소화하면서 데이터를 효율적으로 수집하고 변환하기 위해 어떤 솔루션을 구현해야 합니까?",
        "Options": {
            "1": "Amazon Kinesis Data Stream을 설정하여 RDS에서 데이터를 지속적으로 읽고 Lambda 함수를 사용하여 변환한 후 Redshift로 전송합니다.",
            "2": "예약된 AWS Lambda 함수를 생성하여 RDS를 쿼리하고 데이터를 처리한 후 결과를 Redshift로 푸시합니다.",
            "3": "Amazon EMR 클러스터를 구현하여 RDS에서 읽고 변환을 수행한 후 결과를 Redshift에 기록하는 Spark 작업을 실행합니다.",
            "4": "AWS Glue를 사용하여 RDS에서 데이터를 추출하고 변환을 수행한 후 결과를 Redshift에 로드하는 ETL 작업을 생성합니다."
        },
        "Correct Answer": "AWS Glue를 사용하여 RDS에서 데이터를 추출하고 변환을 수행한 후 결과를 Redshift에 로드하는 ETL 작업을 생성합니다.",
        "Explanation": "AWS Glue는 ETL 작업을 위해 특별히 설계되어 있으며, 광범위한 관리 없이 데이터를 추출, 변환 및 로드하는 데 가장 효율적인 선택입니다. 스키마 변경, 확장 및 작업 일정을 자동으로 처리합니다.",
        "Other Options": [
            "Amazon Kinesis Data Stream을 사용하는 것은 일회성 배치 프로세스에 불필요한 복잡성을 추가하며, 배치 ETL보다는 실시간 데이터 수집에 더 적합합니다.",
            "예약된 AWS Lambda 함수는 작동할 수 있지만, AWS Glue와 같은 전용 ETL 도구에 비해 더 큰 데이터 세트와 복잡한 변환을 효율적으로 처리하지 못할 수 있습니다.",
            "Amazon EMR 클러스터는 대규모 데이터 세트를 효과적으로 처리할 수 있지만, Glue로 처리할 수 있는 배치 작업에 대해 더 많은 운영 오버헤드와 비용을 초래합니다."
        ]
    },
    {
        "Question Number": "58",
        "Situation": "소매 회사가 AWS에서 클라우드 기반 솔루션으로 데이터 아키텍처를 전환하고 있습니다. 이 회사는 분석을 위해 통합된 데이터 모델로 통합해야 하는 여러 데이터 소스를 보유하고 있습니다. 데이터 엔지니어는 성능과 확장성을 유지하면서 구조화된 데이터와 반구조화된 데이터를 모두 수용할 수 있는 최적의 스키마를 설계하는 임무를 맡고 있습니다.",
        "Question": "데이터 엔지니어가 다양한 데이터 유형을 처리하는 데 유연성과 효율성을 보장하기 위해 선택해야 할 데이터 모델링 접근 방식은 무엇입니까?",
        "Options": {
            "1": "구조화된 데이터 분석을 위한 쿼리 성능을 최적화하기 위해 스타 스키마를 채택합니다.",
            "2": "구조화된 데이터와 반구조화된 데이터 모두에 대해 유연성과 확장성을 제공하기 위해 데이터 볼트 모델을 활용합니다.",
            "3": "데이터를 정규화하고 데이터 세트 간의 중복성을 줄이기 위해 스노우플레이크 스키마를 구현합니다.",
            "4": "데이터 관계의 포괄적인 문서를 보장하기 위해 개체-관계 모델을 선택합니다."
        },
        "Correct Answer": "구조화된 데이터와 반구조화된 데이터 모두에 대해 유연성과 확장성을 제공하기 위해 데이터 볼트 모델을 활용합니다.",
        "Explanation": "데이터 볼트 모델은 구조화된 데이터와 반구조화된 데이터를 포함한 다양한 데이터 유형을 처리하도록 특별히 설계되었으며, 진화하는 비즈니스 요구 사항에 대한 확장성과 유연성을 제공합니다. 기존 모델을 방해하지 않고 새로운 데이터 소스를 쉽게 통합할 수 있어 동적인 데이터 환경에 적합합니다.",
        "Other Options": [
            "스타 스키마는 주로 쿼리 성능을 최적화하도록 설계되었으며, 구조화된 데이터에 가장 적합하여 반구조화된 데이터 소스를 수용하는 데 효과적이지 않습니다.",
            "스노우플레이크 스키마는 데이터를 정규화하여 더 복잡한 쿼리를 초래할 수 있으며, 다양한 데이터 유형을 처리할 때 다른 접근 방식보다 성능이 덜 효율적일 수 있습니다.",
            "개체-관계 모델은 데이터 관계를 문서화하는 데 중점을 두지만, 클라우드 환경에서 다양한 데이터 유형을 처리하는 데 필요한 유연성과 확장성을 제공하지 않습니다."
        ]
    },
    {
        "Question Number": "59",
        "Situation": "금융 서비스 회사가 온프레미스 데이터베이스를 AWS로 마이그레이션하고 있습니다. 그들은 데이터가 안전할 뿐만 아니라 고가용성 및 장애에 대한 복원력이 뛰어나도록 해야 합니다. 비용 효율성을 유지하면서 이러한 목표를 달성하는 데 도움이 될 수 있는 다양한 AWS 서비스를 고려하고 있습니다.",
        "Question": "다음 옵션 중 적절한 복원력과 가용성으로 데이터를 가장 잘 보호할 수 있는 것은 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "고가용성을 위해 Amazon RDS를 Multi-AZ 배포로 사용합니다.",
            "2": "매시간 스냅샷을 찍은 Amazon Redshift를 배포합니다.",
            "3": "교차 지역 복제를 위해 글로벌 테이블이 있는 Amazon DynamoDB를 구현합니다.",
            "4": "버전 관리가 활성화된 Amazon S3에 데이터 백업을 저장합니다.",
            "5": "읽기 확장을 개선하기 위해 읽기 복제본이 있는 Amazon Aurora를 활용합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "고가용성을 위해 Amazon RDS를 Multi-AZ 배포로 사용합니다.",
            "교차 지역 복제를 위해 글로벌 테이블이 있는 Amazon DynamoDB를 구현합니다."
        ],
        "Explanation": "Amazon RDS를 Multi-AZ 배포로 사용하면 데이터베이스가 여러 가용 영역에 실시간으로 복제되어 자동 장애 조치 및 고가용성을 제공합니다. 글로벌 테이블이 있는 Amazon DynamoDB를 구현하면 데이터가 여러 AWS 리전 간에 복제되어 지역적 중단에 대한 가용성과 복원력이 향상됩니다.",
        "Other Options": [
            "버전 관리가 활성화된 Amazon S3에 백업을 저장하는 것은 데이터 복구에 유익하지만, 활성 장애 중에 필요한 즉각적인 가용성과 복원력을 제공하지 않습니다.",
            "읽기 복제본이 있는 Amazon Aurora를 활용하면 읽기 확장이 개선되지만, Multi-AZ 구성 없이는 본질적으로 고가용성이나 복원력을 제공하지 않습니다.",
            "매시간 스냅샷을 찍은 Amazon Redshift를 배포하면 데이터 복구에 도움이 될 수 있지만, 실시간 가용성과 중단에 대한 복원력을 위한 솔루션은 아닙니다."
        ]
    },
    {
        "Question Number": "60",
        "Situation": "소매 회사가 실시간 분석을 위한 데이터 처리를 간소화할 계획입니다. 이 회사는 판매 시점 시스템, 온라인 거래 및 고객 피드백과 같은 다양한 출처에서 데이터를 수집합니다. 그들은 데이터 웨어하우스로 변환 및 로딩하기 전에 이 데이터를 임시로 저장할 중간 데이터 스테이징 위치를 설정하고자 합니다. 이 솔루션은 비용 효율적이고 확장 가능해야 하며, 관리 오버헤드 없이 다양한 데이터 볼륨을 처리할 수 있어야 합니다.",
        "Question": "변동하는 데이터 볼륨을 처리하고 데이터 변환을 용이하게 할 수 있는 중간 데이터 스테이징 위치를 설정하는 데 가장 적합한 AWS 서비스는 무엇입니까?",
        "Options": {
            "1": "처리 전에 원시 데이터를 저장하기 위한 Amazon S3.",
            "2": "실시간 데이터 검색을 위한 Amazon DynamoDB.",
            "3": "분석 데이터 저장을 위한 Amazon Redshift.",
            "4": "거래 데이터 저장을 위한 Amazon RDS."
        },
        "Correct Answer": "처리 전에 원시 데이터를 저장하기 위한 Amazon S3.",
        "Explanation": "Amazon S3는 높은 확장성과 내구성을 위해 설계되어 원시 데이터를 임시 스테이징 위치로 저장하는 데 이상적인 선택입니다. 대량의 데이터를 처리할 수 있으며, 데이터 변환을 위한 다른 AWS 서비스와의 통합이 용이합니다.",
        "Other Options": [
            "Amazon RDS는 주로 관계형 데이터베이스 관리에 사용되며, 원시 데이터를 스테이징 영역으로 처리하기 위해 대량의 데이터를 처리하도록 설계되지 않았습니다.",
            "Amazon DynamoDB는 키-값 및 문서 데이터 모델에 최적화된 NoSQL 데이터베이스 서비스이지만, 대규모 데이터 스테이징이나 배치 처리를 위한 것이 아닙니다.",
            "Amazon Redshift는 분석 쿼리에 최적화된 데이터 웨어하우스 솔루션으로, 처리 전에 원시 데이터를 임시로 스테이징하는 데 적합하지 않습니다."
        ]
    },
    {
        "Question Number": "61",
        "Situation": "소매 회사는 온라인 스토어의 일일 판매 데이터를 처리하기 위한 데이터 파이프라인을 구축하고자 합니다. 판매 데이터는 JSON 데이터를 반환하는 REST API를 통해 제공되며, 회사는 이를 Amazon DynamoDB에 저장하여 실시간 분석을 수행해야 합니다. 데이터 엔지니어는 데이터를 효율적으로 수집하고 최소한의 관리 오버헤드로 다른 시스템에서 사용할 수 있도록 하는 솔루션을 설계해야 합니다.",
        "Question": "가장 적은 운영 오버헤드로 이를 달성하기 위해 사용할 수 있는 AWS 서비스의 조합은 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "Amazon API Gateway를 사용하여 REST API 엔드포인트를 생성합니다.",
            "2": "Amazon EC2 인스턴스에서 cron 작업을 예약하여 데이터를 가져옵니다.",
            "3": "AWS Glue를 활용하여 데이터를 변환하고 로드하는 ETL 작업을 실행합니다.",
            "4": "AWS Lambda 함수를 구현하여 API를 호출하고 데이터를 저장합니다.",
            "5": "AWS Step Functions를 사용하여 데이터 수집 프로세스를 조정합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Amazon API Gateway를 사용하여 REST API 엔드포인트를 생성합니다.",
            "AWS Lambda 함수를 구현하여 API를 호출하고 데이터를 저장합니다."
        ],
        "Explanation": "Amazon API Gateway를 사용하면 쉽게 트리거할 수 있는 서버리스 API 엔드포인트를 생성할 수 있습니다. AWS Lambda와 결합하면 전용 서버 없이 데이터 수집 로직을 실행할 수 있어, 이 솔루션은 API에서 DynamoDB로 데이터를 수집하는 매우 확장 가능하고 유지 관리가 적은 방법을 제공합니다.",
        "Other Options": [
            "Amazon EC2 인스턴스에서 cron 작업을 예약하면 EC2 인스턴스를 관리하고 가용성을 보장해야 하므로 운영 오버헤드가 증가합니다.",
            "이 작업에 AWS Step Functions를 사용하는 것은 불필요한 복잡성을 초래할 수 있으며, 간단한 Lambda 함수가 API 호출 및 데이터 수집을 조정 없이 처리할 수 있습니다.",
            "AWS Glue는 배치 처리 및 ETL 작업에 더 적합하며, 일일 판매 데이터의 실시간 수집에는 필요하지 않아 이 사용 사례에 대해 비효율적입니다."
        ]
    },
    {
        "Question Number": "62",
        "Situation": "금융 서비스 회사는 AWS 환경에서 민감한 데이터 접근을 모니터링하고 준수를 보장해야 합니다. 회사는 AWS 리소스의 변경 사항을 추적하고 Amazon S3에 저장된 민감한 데이터에 대한 무단 접근을 감지할 수 있는 보안 조치를 구현하고자 합니다. 또한, 회사는 접근 로그에서 패턴을 식별하여 추가 분석을 수행해야 합니다.",
        "Question": "다음 솔루션 중 회사가 준수 및 모니터링 요구 사항을 달성하는 데 도움이 되는 것은 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "Amazon Macie를 구현하여 S3에 저장된 민감한 데이터를 자동으로 발견, 분류 및 보호합니다.",
            "2": "AWS Config를 구성하여 AWS 리소스의 변경 사항을 추적하고 준수 위반에 대한 알림을 보냅니다.",
            "3": "AWS Lambda를 사용하여 S3 접근 로그를 분석하고 비정상적인 행동에 대한 경고를 트리거합니다.",
            "4": "Amazon CloudWatch를 설정하여 S3 버킷 크기 및 객체 수에 대한 메트릭을 수신합니다.",
            "5": "AWS CloudTrail을 활성화하여 AWS 계정 전반의 모든 API 호출을 기록하고 S3 접근 이벤트를 모니터링합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "AWS CloudTrail을 활성화하여 AWS 계정 전반의 모든 API 호출을 기록하고 S3 접근 이벤트를 모니터링합니다.",
            "Amazon Macie를 구현하여 S3에 저장된 민감한 데이터를 자동으로 발견, 분류 및 보호합니다."
        ],
        "Explanation": "AWS CloudTrail을 활성화하면 회사는 모든 API 호출을 기록할 수 있어 AWS 환경에서 수행된 작업의 포괄적인 기록을 제공하며, 이는 준수 모니터링에 필수적입니다. Amazon Macie를 구현하면 회사는 S3에서 민감한 데이터를 자동으로 발견하고 분류할 수 있어, 민감한 정보가 저장된 위치를 인식하고 이를 보호하기 위한 적절한 조치를 취할 수 있습니다.",
        "Other Options": [
            "S3 버킷 메트릭을 위한 Amazon CloudWatch 설정은 상세한 접근 로그나 준수 추적을 제공하지 않으며, 성능 메트릭만 제공할 뿐 보안 통찰력을 제공하지 않습니다.",
            "AWS Config를 구성하면 리소스 변경 사항을 추적하는 데 도움이 될 수 있지만, 민감한 데이터 접근을 구체적으로 모니터링하거나 S3와 관련된 API 호출 로그를 제공하지 않습니다.",
            "S3 접근 로그를 분석하기 위해 AWS Lambda를 사용하는 것은 유용할 수 있지만, 사용자 정의 구현이 필요하며 Amazon Macie와 같은 자동 발견 및 분류 기능을 제공하지 않습니다."
        ]
    },
    {
        "Question Number": "63",
        "Situation": "데이터 엔지니어는 조직 구조 및 파일 시스템과 같은 계층적 정보의 대규모 데이터 세트를 효율적으로 처리하고 쿼리하는 시스템을 설계하는 임무를 맡고 있습니다. 엔지니어는 이 계층적 데이터를 효율적으로 탐색하고 조작할 수 있는 데이터 구조를 선택해야 합니다.",
        "Question": "데이터 엔지니어가 데이터 세트의 계층적 관계를 가장 잘 표현하고 관리하기 위해 사용해야 할 데이터 구조는 무엇입니까?",
        "Options": {
            "1": "배열, 요소의 순차적 저장과 인덱스에 의한 쉬운 접근을 허용합니다.",
            "2": "해시 테이블, 키-값 쌍을 통한 빠른 데이터 검색을 제공합니다.",
            "3": "그래프 데이터 구조, 복잡한 관계를 가진 상호 연결된 데이터를 효율적으로 표현할 수 있습니다.",
            "4": "트리 데이터 구조, 자연스럽게 계층적 관계를 표현하고 효율적인 탐색을 허용합니다."
        },
        "Correct Answer": "트리 데이터 구조, 자연스럽게 계층적 관계를 표현하고 효율적인 탐색을 허용합니다.",
        "Explanation": "트리 데이터 구조는 노드가 엣지로 연결되어 있으며 각 노드는 여러 자식을 가질 수 있지만 단 하나의 부모만 가질 수 있기 때문에 계층적 데이터를 표현하는 데 이상적입니다. 이 구조는 깊이 우선 또는 너비 우선 탐색과 같은 효율적인 탐색 작업을 가능하게 하여 계층적 관계를 쿼리하는 데 적합합니다.",
        "Other Options": [
            "그래프 데이터 구조는 엄격한 계층보다는 상호 연결된 데이터의 네트워크를 표현하는 데 더 적합하여 이 특정 사용 사례에 대해 비효율적입니다.",
            "해시 테이블은 고유한 키를 기반으로 빠른 조회를 최적화하지만, 본질적으로 계층적 관계를 지원하지 않아 이 시나리오에 적합하지 않습니다.",
            "배열은 인덱스를 통해 요소에 쉽게 접근할 수 있지만, 계층적 관계를 효과적으로 표현하는 메커니즘을 제공하지 않아 이 작업에 대한 유용성이 제한됩니다."
        ]
    },
    {
        "Question Number": "64",
        "Situation": "소매 회사는 고객 거래 데이터를 대량으로 저장해야 하며, 이 데이터는 분석 및 보고를 위해 자주 접근됩니다. 또한, 그들은 규정 준수를 위해 보존해야 하는 역사적 데이터도 가지고 있지만, 이 데이터는 드물게 접근됩니다. 그들은 비용을 최소화하면서 핫 데이터와 콜드 데이터를 효율적으로 관리할 수 있는 솔루션이 필요합니다.",
        "Question": "핫 데이터와 콜드 데이터를 효율적으로 관리하기 위한 요구 사항을 가장 잘 충족하는 저장 솔루션은 무엇입니까?",
        "Options": {
            "1": "핫 데이터와 콜드 데이터 관리를 위해 Amazon S3의 Intelligent-Tiering을 활용합니다.",
            "2": "핫 데이터에 대해 읽기 복제본을 사용하고 콜드 데이터에 대해 수동 아카이빙을 위해 Amazon RDS를 활용합니다.",
            "3": "핫 데이터에 대해 Amazon EFS를 사용하고 콜드 데이터 저장을 위해 Glacier를 활용합니다.",
            "4": "모든 데이터 유형에 대해 온디맨드 용량 모드로 Amazon DynamoDB를 활용합니다."
        },
        "Correct Answer": "핫 데이터와 콜드 데이터 관리를 위해 Amazon S3의 Intelligent-Tiering을 활용합니다.",
        "Explanation": "Amazon S3의 Intelligent-Tiering은 접근 패턴이 변경될 때 데이터가 두 개의 접근 계층 간에 자동으로 이동되므로 핫 데이터와 콜드 데이터를 관리하는 데 비용 효율적인 솔루션입니다. 이를 통해 회사는 자주 접근되는 데이터가 즉시 사용 가능하도록 하면서 비용을 최적화할 수 있습니다.",
        "Other Options": [
            "Amazon RDS의 읽기 복제본을 활용하는 것은 주로 트랜잭션 데이터에 설계되었기 때문에 콜드 데이터 저장에는 이상적이지 않으며, 드물게 접근되는 데이터에 대한 비용 효율적인 솔루션을 제공하지 않습니다.",
            "온디맨드 용량 모드의 Amazon DynamoDB는 고성능 워크로드를 위해 설계되었지만, 콜드 데이터 저장에 대해서는 비용이 많이 들 수 있으며, S3와 같은 드물게 접근되는 데이터에 대한 계층화된 가격을 제공하지 않습니다.",
            "핫 데이터에 대해 Amazon EFS를 사용하는 것은 자주 접근되는 파일에 잘 작동하지만, Glacier는 아카이브 저장에 더 적합하며 검색 시간이 더 길어지므로 빠르게 접근해야 하는 콜드 데이터에는 덜 효율적입니다."
        ]
    },
    {
        "Question Number": "65",
        "Situation": "데이터 분석 팀은 Amazon S3에 저장된 판매 데이터를 시각화하기 위한 인터랙티브 대시보드를 생성해야 합니다. 그들은 비즈니스 사용자가 깊은 기술적 스킬 없이 다양한 필터와 시각화를 통해 데이터를 탐색할 수 있도록 하기를 원합니다.",
        "Question": "이 인터랙티브 대시보드를 구축하기 위한 팀의 요구 사항을 가장 잘 충족하는 AWS 서비스는 무엇입니까?",
        "Options": {
            "1": "AWS Data Pipeline",
            "2": "Amazon Athena",
            "3": "AWS Glue DataBrew",
            "4": "Amazon QuickSight"
        },
        "Correct Answer": "Amazon QuickSight",
        "Explanation": "Amazon QuickSight는 사용자가 Amazon S3를 포함한 다양한 소스에 저장된 데이터에서 직접 인터랙티브 대시보드와 시각화를 생성할 수 있도록 하는 비즈니스 분석 서비스입니다. 이는 비즈니스 사용자를 위해 설계되었으며, 깊은 기술적 스킬 없이 데이터를 탐색하고 인사이트를 생성할 수 있는 직관적인 인터페이스를 제공합니다.",
        "Other Options": [
            "AWS Glue DataBrew는 주로 데이터 준비 및 변환에 중점을 두고 있어 사용자가 분석 전에 데이터를 정리하고 정규화할 수 있도록 합니다. 데이터를 준비하지만 인터랙티브 시각화를 위한 대시보드 기능은 제공하지 않습니다.",
            "Amazon Athena는 SQL을 사용하여 Amazon S3의 데이터를 분석할 수 있는 서버리스 쿼리 서비스입니다. 시각화 도구와 함께 사용할 수 있지만, QuickSight와 같은 내장 대시보드 솔루션은 제공하지 않습니다.",
            "AWS Data Pipeline은 데이터의 이동 및 변환을 자동화하는 데 도움이 되는 서비스입니다. 그러나 인터랙티브 대시보드나 시각화를 생성하기 위해 설계되지 않았으므로 팀의 요구 사항에 적합하지 않습니다."
        ]
    }
]