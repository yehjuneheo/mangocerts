[
    {
        "Question Number": "1",
        "Situation": "데이터 엔지니어링 팀이 Amazon S3를 사용하여 애플리케이션의 사용자 생성 콘텐츠를 저장하고 있습니다. 그들은 저장된 데이터가 안전하고 권한이 있는 사용자만 접근할 수 있도록 보장하고 싶어합니다. 또한, 감사 목적으로 S3 버킷에 대한 접근을 기록하는 메커니즘을 구현해야 합니다. 팀은 버킷 관리 및 보안에 대한 모범 사례를 준수하고자 합니다.",
        "Question": "팀이 Amazon S3 버킷의 보안 및 로깅을 강화하기 위해 구현해야 할 전략은 무엇입니까?",
        "Options": {
            "1": "접근 제어를 구현하지 않고 Amazon S3 Transfer Acceleration을 사용하여 더 빠른 업로드 및 다운로드를 수행합니다.",
            "2": "특정 IAM 역할에 대한 접근을 제한하는 버킷 정책을 설정하고, 버킷에 대한 요청을 캡처하기 위해 S3 서버 접근 로깅을 활성화합니다.",
            "3": "버킷 버전 관리를 활성화하고 AWS CloudTrail을 구성하여 모든 S3 데이터 접근 이벤트를 감사용으로 기록합니다.",
            "4": "공용 버킷을 생성하고 Amazon CloudFront를 사용하여 콘텐츠를 캐시하며, 버킷에 접근 로깅을 활성화하지 않습니다."
        },
        "Correct Answer": "특정 IAM 역할에 대한 접근을 제한하는 버킷 정책을 설정하고, 버킷에 대한 요청을 캡처하기 위해 S3 서버 접근 로깅을 활성화합니다.",
        "Explanation": "버킷 정책을 통해 접근을 제한하면 권한이 있는 IAM 역할만 버킷에 접근할 수 있어 보안이 강화됩니다. S3 서버 접근 로깅을 활성화하면 버킷에 대한 요청의 포괄적인 감사 추적이 제공되어 준수 및 모니터링에 중요합니다.",
        "Other Options": [
            "버킷 버전 관리를 활성화하는 것은 데이터 복구에 유익하지만, 접근 보안을 직접적으로 강화하지는 않습니다. CloudTrail은 API 호출 추적에 유용하지만, 적절한 구성 없이는 S3에 특정한 모든 접근 이벤트를 기록하지 않을 수 있습니다.",
            "버킷을 공용으로 만드는 것은 보안 요구 사항과 모순되며, 누구나 버킷의 콘텐츠에 접근할 수 있게 합니다. 또한, 접근 로깅을 활성화하지 않으면 감사 추적을 제공하지 못합니다.",
            "S3 Transfer Acceleration을 사용하면 전송 속도가 향상되지만, 보안 문제나 감사와 관련된 사항을 해결하지 않습니다. 적절한 접근 제어 없이 민감한 데이터가 노출될 수 있습니다."
        ]
    },
    {
        "Question Number": "2",
        "Situation": "금융 서비스 회사가 개인 식별 정보(PII)와 관련된 규정을 준수하기 위해 데이터 거버넌스 프로토콜을 구현하고 있습니다. 이 회사는 AWS 서비스를 활용하여 AWS Lake Formation에서 관리되는 데이터 레이크 내의 PII를 식별하고 관리할 계획입니다. 그들은 PII 데이터를 자동으로 식별하고 적절한 보안 조치를 적용하고자 합니다.",
        "Question": "회사가 데이터 레이크 내에서 PII 데이터를 자동으로 식별하고 분류할 수 있도록 가장 잘 지원하는 접근 방식은 무엇입니까?",
        "Options": {
            "1": "PII와 관련된 데이터 접근 패턴에 대한 경고를 위해 Amazon CloudWatch 알람을 설정합니다.",
            "2": "데이터 레이크 내의 PII 데이터를 모니터링하기 위해 AWS Config 규칙을 구현합니다.",
            "3": "자동화된 PII 데이터 발견 및 분류를 위해 Amazon Macie와 AWS Lake Formation을 통합합니다.",
            "4": "PII 식별을 위한 데이터 스캔을 위해 AWS Glue 작업을 사용하여 사용자 정의 스크립트를 생성합니다."
        },
        "Correct Answer": "자동화된 PII 데이터 발견 및 분류를 위해 Amazon Macie와 AWS Lake Formation을 통합합니다.",
        "Explanation": "Amazon Macie와 AWS Lake Formation을 통합하면 PII 데이터의 자동 발견 및 분류가 가능해져 준수 요구 사항을 관리하기가 더 쉬워집니다. Macie는 기계 학습을 사용하여 민감한 데이터를 식별하고 데이터 거버넌스 관행을 효과적으로 유지하는 데 도움을 줄 수 있습니다.",
        "Other Options": [
            "AWS Glue 작업을 사용하여 사용자 정의 스크립트를 작성하는 것은 수동 개입이 필요하며, Macie의 기계 학습 기능과 같은 수준의 자동화 및 정확성을 제공하지 않습니다.",
            "AWS Config 규칙을 구현하는 것은 주로 구성 및 리소스의 준수 모니터링에 중점을 두며, PII 데이터의 식별이나 분류에 직접적으로 초점을 맞추지 않습니다.",
            "데이터 접근 패턴에 대한 Amazon CloudWatch 알람을 설정하는 것은 PII 데이터의 초기 식별 및 분류를 다루지 않으며, 접근 활동에 대한 모니터링만 제공합니다."
        ]
    },
    {
        "Question Number": "3",
        "Situation": "데이터 엔지니어가 AWS에서 실행되는 애플리케이션의 민감한 자격 증명을 관리하는 임무를 맡고 있습니다. 그들은 이러한 자격 증명이 안전하게 저장되고 자동으로 회전되어 무단 접근의 위험을 줄일 수 있도록 해야 합니다.",
        "Question": "이러한 자격 증명을 안전하게 저장하고 자동으로 회전하는 데 사용할 수 있는 AWS 서비스는 무엇입니까?",
        "Options": {
            "1": "AWS Secrets Manager",
            "2": "AWS Identity and Access Management",
            "3": "AWS Key Management Service",
            "4": "AWS Systems Manager Parameter Store"
        },
        "Correct Answer": "AWS Secrets Manager",
        "Explanation": "AWS Secrets Manager는 비밀을 안전하게 저장하고 관리하기 위해 특별히 설계되었으며, 자격 증명을 자동으로 회전할 수 있는 기능을 제공하여 이 요구 사항에 가장 적합한 선택입니다.",
        "Other Options": [
            "AWS Systems Manager Parameter Store는 매개변수와 비밀을 저장할 수 있지만, 비밀에 대한 내장 자동 회전 기능을 제공하지 않아 자격 증명 관리 기능이 제한됩니다.",
            "AWS Identity and Access Management (IAM)는 사용자 권한 및 접근 제어를 관리하는 데 사용되며, 자격 증명을 저장하거나 회전하는 데 적합하지 않습니다.",
            "AWS Key Management Service (KMS)는 데이터 보호를 위한 암호화 키 관리에 주로 초점을 맞추고 있어 자격 증명 저장 및 회전에는 적합하지 않습니다."
        ]
    },
    {
        "Question Number": "4",
        "Situation": "소매 회사의 데이터 엔지니어링 팀은 고객 거래 데이터를 분석하여 구매 행동에 대한 통찰력을 도출하는 임무를 맡고 있습니다. 그들은 Amazon S3에 저장된 대량의 거래 기록을 효율적으로 처리하기 위해 다양한 데이터 샘플링 기법을 고려하고 있습니다.",
        "Question": "팀이 최소한의 자원 사용으로 통찰력을 도출하기 위해 가장 효과적인 데이터 샘플링 기법의 조합은 무엇인가요? (두 가지 선택)",
        "Options": {
            "1": "가장 쉽게 접근할 수 있는 거래 기록을 선택하는 편의 샘플링.",
            "2": "대표적인 거래 기록의 하위 집합을 선택하는 단순 무작위 샘플링.",
            "3": "데이터셋에서 매 n번째 거래 기록을 선택하는 체계적 샘플링.",
            "4": "트렌드를 강조하기 위해 가장 빈번한 거래 기록을 과샘플링.",
            "5": "모든 고객 세그먼트가 샘플에 포함되도록 하는 층화 샘플링."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "대표적인 거래 기록의 하위 집합을 선택하는 단순 무작위 샘플링.",
            "모든 고객 세그먼트가 샘플에 포함되도록 하는 층화 샘플링."
        ],
        "Explanation": "단순 무작위 샘플링은 팀이 거래 기록의 대표적인 하위 집합을 얻을 수 있게 하여 편향과 자원 사용을 최소화합니다. 층화 샘플링은 다양한 고객 세그먼트가 분석에 포함되도록 하여 더 정확한 통찰력을 제공하며, 처리 자원 측면에서도 효율적입니다.",
        "Other Options": [
            "체계적 샘플링은 거래 기록에 패턴이 있을 경우 편향을 초래할 수 있어 이 분석에 신뢰성이 떨어질 수 있습니다.",
            "가장 빈번한 거래 기록을 과샘플링하면 전체 데이터셋을 효과적으로 대표하지 못하므로 왜곡된 결과를 초래할 수 있습니다.",
            "편의 샘플링은 편향에 취약하며 대표 샘플을 보장하지 않으므로 부정확한 통찰력을 초래할 수 있습니다."
        ]
    },
    {
        "Question Number": "5",
        "Situation": "데이터 엔지니어링 팀은 AWS Glue 작업을 사용하여 다양한 출처의 데이터를 분석을 위해 통합된 형식으로 변환하고 있습니다. 최근 그들은 ETL 작업의 성능 문제를 경험하고 있으며, 예상보다 완료하는 데 더 오랜 시간이 걸리고 있습니다. 팀은 비효율적인 변환이 이러한 지연의 원인일 수 있다고 의심하고 있습니다. 성능 문제를 해결하기 위해 그들이 취해야 할 가장 효과적인 첫 번째 단계는 무엇인가요?",
        "Question": "데이터 엔지니어링 팀이 AWS Glue 작업의 성능 문제를 해결하기 위해 가장 먼저 해야 할 일은 무엇인가요?",
        "Options": {
            "1": "최적화 기회를 위해 변환 스크립트를 분석합니다.",
            "2": "더 복잡한 데이터 파티셔닝 전략을 구현합니다.",
            "3": "오류 및 경고를 위해 작업 실행 로그를 검토합니다.",
            "4": "Glue 작업에 할당된 자원을 늘립니다."
        },
        "Correct Answer": "최적화 기회를 위해 변환 스크립트를 분석합니다.",
        "Explanation": "변환 스크립트를 최적화 기회를 위해 분석하면 팀이 지연을 초래할 수 있는 비효율적인 코드나 변환을 식별할 수 있습니다. 이 단계는 자원 할당이나 다른 변경을 하기 전에 성능 문제의 근본 원인을 직접적으로 해결하는 데 중요합니다.",
        "Other Options": [
            "작업 실행 로그를 검토하여 오류 및 경고를 확인하는 것은 문제에 대한 통찰력을 제공할 수 있지만, 오류가 없을 경우 성능 비효율성을 직접적으로 해결하지는 않습니다.",
            "Glue 작업에 할당된 자원을 늘리면 성능이 향상될 수 있지만, 기본 변환이 비효율적이라면 임시방편적인 해결책에 불과할 수 있습니다.",
            "더 복잡한 데이터 파티셔닝 전략을 구현하는 것은 성능에 도움이 될 수 있지만, 현재 변환이 최적화되지 않았다면 필요하지 않을 수 있어 간접적인 접근 방식이 될 수 있습니다."
        ]
    },
    {
        "Question Number": "6",
        "Situation": "금융 서비스 회사는 Amazon S3를 사용하여 민감한 고객 데이터를 저장하는 데이터 레이크를 구현하고 있습니다. 엄격한 접근 제어와 데이터 거버넌스 정책 준수를 보장하기 위해 회사는 S3 버킷에 적절한 IAM 정책을 설정해야 하며, S3 Access Points의 사용도 포함됩니다. 아키텍처는 또한 AWS PrivateLink를 사용하여 다양한 내부 서비스에서 안전하게 접근할 수 있도록 해야 합니다.",
        "Question": "회사가 AWS PrivateLink 및 S3 Access Points를 통해 S3 버킷에 대한 접근을 허용하면서 데이터 보안 및 거버넌스를 강화하기 위해 어떤 전략을 사용해야 하나요?",
        "Options": {
            "1": "태그를 기반으로 접근을 제한하는 특정 IAM 정책을 가진 S3 Access Points를 생성하고 필요한 엔드포인트에 연결합니다.",
            "2": "애플리케이션이 S3에 접근할 수 있도록 IAM 역할을 사용하고 VPC 엔드포인트를 구성하여 모든 S3 리소스에 대한 제한 없는 접근을 허용합니다.",
            "3": "IAM 정책을 사용하지 않고 특정 IP 범위에서의 접근을 허용하는 버킷 정책을 설정합니다.",
            "4": "S3 리소스에 대한 모든 작업을 허용하는 IAM 정책을 생성하고 이를 S3 버킷에 직접 연결합니다."
        },
        "Correct Answer": "태그를 기반으로 접근을 제한하는 특정 IAM 정책을 가진 S3 Access Points를 생성하고 필요한 엔드포인트에 연결합니다.",
        "Explanation": "특정 IAM 정책을 가진 S3 Access Points를 생성하면 태그를 기반으로 접근을 제한할 수 있는 세밀한 접근 제어가 가능합니다. 이는 민감한 데이터를 보호하는 모범 사례와 일치하며, AWS PrivateLink를 활용하여 내부 트래픽을 비공식적으로 유지하고 거버넌스 기준을 준수할 수 있도록 합니다.",
        "Other Options": [
            "IAM 정책 없이 버킷 정책을 설정하는 것은 민감한 데이터에 대한 접근 제어에 필요한 세분화 및 유연성을 제공하지 않습니다.",
            "제한 없는 접근을 가진 애플리케이션용 IAM 역할을 사용하는 것은 보안을 저해하며 AWS PrivateLink 및 S3 Access Points의 이점을 효과적으로 활용하지 못합니다.",
            "S3 버킷에 모든 작업을 허용하는 정책을 연결하는 것은 상당한 보안 취약점을 초래하며 데이터 거버넌스 모범 사례를 준수하지 않습니다."
        ]
    },
    {
        "Question Number": "7",
        "Situation": "데이터 분석가는 Amazon Redshift 클러스터에 저장된 데이터를 시각화하여 분석을 용이하게 해야 합니다. 분석가는 대규모 데이터 세트를 효과적으로 처리할 수 있는 인터랙티브 대시보드를 생성하고자 합니다. 팀은 시각화를 위해 Redshift와 통합할 다양한 AWS 서비스를 고려하고 있습니다.",
        "Question": "Amazon Redshift에서 데이터를 시각화하는 데 가장 적합한 두 가지 AWS 서비스는 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "비구조적 데이터를 저장하고 최종 사용자에게 직접 제공하기 위한 Amazon S3",
            "2": "분석을 위한 데이터를 준비하는 ETL 작업을 위한 AWS Glue",
            "3": "그래픽 분석을 위한 Amazon QuickSight와 머신 러닝 통찰력을 위한 Amazon SageMaker",
            "4": "인터랙티브 대시보드 및 시각화를 위한 Amazon QuickSight",
            "5": "복잡한 설정 없이 데이터를 쿼리하기 위한 Amazon Athena"
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "인터랙티브 대시보드 및 시각화를 위한 Amazon QuickSight",
            "그래픽 분석을 위한 Amazon QuickSight와 머신 러닝 통찰력을 위한 Amazon SageMaker"
        ],
        "Explanation": "Amazon QuickSight는 사용자가 Amazon Redshift에 저장된 데이터에서 직접 인터랙티브 대시보드와 시각화를 생성할 수 있게 해주는 강력한 BI 서비스입니다. 대규모 데이터 세트를 효율적으로 처리할 수 있으며 사용자 친화적인 다양한 시각화 옵션을 제공합니다. 두 번째 정답은 데이터 통찰력을 접근 가능하게 만드는 QuickSight의 그래픽 분석 사용을 강조하며, 시각화와 직접적으로 관련이 없지만 데이터 분석을 보완하는 SageMaker도 언급하고 있습니다.",
        "Other Options": [
            "Amazon S3는 시각화 기능을 제공하지 않으며, 주로 저장 솔루션입니다. 분석에 사용되는 데이터를 저장할 수 있지만 Redshift에서 데이터를 직접 시각화할 수는 없습니다.",
            "AWS Glue는 주로 ETL(추출, 변환, 적재) 서비스이며 시각화 기능을 제공하지 않습니다. 분석을 위한 데이터를 준비하는 데 도움을 주지만 시각화는 하지 않습니다.",
            "Amazon Athena는 SQL을 사용하여 S3에 저장된 데이터를 분석할 수 있는 쿼리 서비스입니다. 쿼리에 유용하지만 시각화 기능을 직접 제공하지는 않습니다."
        ]
    },
    {
        "Question Number": "8",
        "Situation": "데이터 엔지니어는 분석 데이터를 저장하는 중요한 Amazon Redshift 클러스터의 데이터 내구성을 보장하는 임무를 맡고 있습니다. 엔지니어는 데이터 손실을 방지하기 위해 클러스터의 정기적인 백업을 수행해야 합니다. 엔지니어는 스냅샷을 생성하고 나중에 해당 스냅샷에서 클러스터를 복원하고자 합니다.",
        "Question": "엔지니어가 Redshift 클러스터의 스냅샷을 생성하고 지정된 스냅샷을 사용하여 복원하기 위해 사용해야 하는 AWS CLI 명령은 무엇입니까?",
        "Options": {
            "1": "aws redshift snapshot-cluster --snapshot-identifier my-snapshot --cluster-identifier my-redshift-cluster; aws redshift restore-cluster-from-snapshot --snapshot-identifier my-snapshot --cluster-identifier my-new-cluster",
            "2": "aws redshift create-cluster-snapshot --snapshot-identifier my-snapshot --cluster-identifier my-redshift-cluster; aws redshift restore-from-cluster-snapshot --snapshot-identifier my-snapshot --cluster-identifier my-new-cluster",
            "3": "aws redshift create-snapshot --snapshot-identifier my-snapshot --cluster-identifier my-redshift-cluster; aws redshift restore-snapshot --snapshot-identifier my-snapshot --cluster-identifier my-new-cluster",
            "4": "aws redshift take-cluster-snapshot --snapshot-identifier my-snapshot --cluster-identifier my-redshift-cluster; aws redshift recover-cluster-from-snapshot --snapshot-identifier my-snapshot --cluster-identifier my-new-cluster"
        },
        "Correct Answer": "aws redshift create-cluster-snapshot --snapshot-identifier my-snapshot --cluster-identifier my-redshift-cluster; aws redshift restore-from-cluster-snapshot --snapshot-identifier my-snapshot --cluster-identifier my-new-cluster",
        "Explanation": "이 명령은 지정된 Redshift 클러스터의 스냅샷을 생성하고 해당 스냅샷 식별자를 사용하여 복원하는 데 올바르게 AWS CLI를 활용합니다. 이는 Redshift에서 스냅샷을 관리하는 적절한 구문입니다.",
        "Other Options": [
            "이 옵션은 잘못된 명령 이름을 사용합니다. 'snapshot-cluster'와 'restore-cluster-from-snapshot'는 AWS Redshift에 대한 유효한 명령이 아니므로 실행에 실패할 것입니다.",
            "이 옵션은 'create-snapshot'와 'restore-snapshot' 명령을 잘못 사용하고 있으며, 이는 Redshift에 대한 유효한 AWS CLI 명령이 아닙니다. 적절한 명령은 'create-cluster-snapshot'과 'restore-from-cluster-snapshot'을 포함해야 합니다.",
            "이 옵션은 'take-cluster-snapshot'과 'recover-cluster-from-snapshot'라는 잘못된 명령 이름을 사용하고 있으며, 이는 AWS CLI에서 Redshift에 존재하지 않으므로 이 옵션은 유효하지 않습니다."
        ]
    },
    {
        "Question Number": "9",
        "Situation": "금융 서비스 회사는 AWS에서 데이터 변환 작업의 배포를 자동화하기 위해 CI/CD 파이프라인을 구현합니다. 데이터 엔지니어는 ETL 스크립트의 변경 사항이 자동으로 테스트되고 원활하게 프로덕션에 배포되도록 해야 합니다. 이러한 요구 사항을 충족하기 위해 데이터 엔지니어는 어떤 접근 방식을 취해야 합니까?",
        "Question": "데이터 변환 작업에 대한 CI/CD를 구현하는 가장 효과적인 방법은 무엇입니까?",
        "Options": {
            "1": "ETL 스크립트를 일정에 따라 실행하기 위해 Jenkins 서버를 설정하고, AWS Lambda를 사용하여 실행 중 발생하는 오류를 모니터링하고 기록합니다.",
            "2": "AWS CodePipeline을 사용하여 ETL 스크립트의 배포를 자동화하고, 테스트를 위해 AWS CodeBuild를 통합하며, AWS CodeCommit의 소스 코드 변경 시 파이프라인을 트리거합니다.",
            "3": "ETL 스크립트를 로컬에서 테스트한 후 프로덕션 환경에 수동으로 배포하고, 모든 변경 사항을 감사용으로 공유 드라이브에 문서화합니다.",
            "4": "ETL 프로세스를 조정하기 위해 AWS Step Functions를 활용하고, 스크립트에 변경 사항이 있을 때마다 워크플로를 수동으로 트리거합니다."
        },
        "Correct Answer": "AWS CodePipeline을 사용하여 ETL 스크립트의 배포를 자동화하고, 테스트를 위해 AWS CodeBuild를 통합하며, AWS CodeCommit의 소스 코드 변경 시 파이프라인을 트리거합니다.",
        "Explanation": "AWS CodePipeline과 AWS CodeBuild를 함께 사용하면 완전 자동화된 CI/CD 프로세스를 구현할 수 있습니다. 이를 통해 ETL 스크립트의 변경 사항이 수동 개입 없이 테스트되고 배포되므로 인적 오류의 위험이 크게 줄어들고 배포 프로세스가 간소화됩니다.",
        "Other Options": [
            "이 옵션은 잘못된 것입니다. 스크립트를 수동으로 배포하고 변경 사항을 문서화하는 것은 CI/CD 파이프라인에 필요한 자동화나 효율성을 제공하지 않으며, 배포 중 오류의 위험을 증가시킵니다.",
            "이 옵션은 잘못된 것입니다. Jenkins 서버를 사용하여 일정에 따라 실행하는 것은 CI/CD의 지속적인 통합 및 배포 원칙과 일치하지 않으며, 자동화된 테스트 및 배포 기능이 부족합니다.",
            "이 옵션은 잘못된 것입니다. AWS Step Functions가 워크플로를 조정할 수 있지만, 이러한 워크플로를 수동으로 트리거하는 것은 진정한 CI/CD 접근 방식에 필요한 자동화를 제공하지 않습니다. CI/CD는 배포를 위한 자동 트리거에 의존합니다."
        ]
    },
    {
        "Question Number": "10",
        "Situation": "조직은 개인 식별 정보(PII) 및 재무 기록을 포함한 민감한 데이터를 처리합니다. 규정을 준수하고 데이터 보호를 강화하기 위해, 조직은 Amazon S3에 저장된 데이터의 암호화 방법을 평가하고 있습니다. 그들은 데이터 저장 전략을 위해 클라이언트 측 암호화와 서버 측 암호화 사용의 의미를 이해하는 데 특히 관심이 있습니다.",
        "Question": "데이터가 Amazon S3로 전송되기 전에 암호화되어 사용자가 암호화 키에 대한 완전한 제어를 할 수 있도록 보장하는 암호화 방법은 무엇입니까?",
        "Options": {
            "1": "클라이언트 측 암호화, 데이터가 S3에 업로드되기 전에 클라이언트에서 암호화됩니다.",
            "2": "고객 제공 키를 사용하는 서버 측 암호화(SSE-C), 사용자가 자신의 암호화 키를 관리할 수 있습니다.",
            "3": "Amazon S3 관리 키를 사용하는 서버 측 암호화(SSE-S3), S3가 암호화 및 복호화를 처리합니다.",
            "4": "AWS 키 관리 서비스를 사용하는 서버 측 암호화(SSE-KMS), AWS 관리 키를 사용하여 암호화합니다."
        },
        "Correct Answer": "클라이언트 측 암호화, 데이터가 S3에 업로드되기 전에 클라이언트에서 암호화됩니다.",
        "Explanation": "클라이언트 측 암호화는 데이터가 Amazon S3로 전송되기 전에 클라이언트 측에서 암호화되도록 보장합니다. 이 방법은 사용자가 자신의 암호화 키에 대한 완전한 제어를 유지할 수 있게 하여 민감한 데이터 처리와 관련된 규정을 준수하고 보안을 강화합니다.",
        "Other Options": [
            "Amazon S3 관리 키를 사용하는 서버 측 암호화(SSE-S3)는 AWS가 암호화 및 복호화에 사용되는 키를 관리하므로 사용자가 암호화 키를 제어할 수 없습니다.",
            "AWS 키 관리 서비스를 사용하는 서버 측 암호화(SSE-KMS)는 AWS 관리 키를 사용하므로 SSE-S3보다 더 많은 제어를 제공하지만 사용자가 암호화 키 자체에 대한 완전한 제어를 갖지 않습니다.",
            "고객 제공 키를 사용하는 서버 측 암호화(SSE-C)는 사용자가 자신의 키를 제공할 수 있지만 데이터는 여전히 서버 측에서 암호화 및 복호화되므로 사용자가 암호화 프로세스 자체를 제어할 수 없습니다."
        ]
    },
    {
        "Question Number": "11",
        "Situation": "데이터 엔지니어는 Amazon Redshift에 저장된 데이터 세트를 변환하는 작업을 맡고 있습니다. 목표는 들어오는 데이터를 처리하고 데이터 유효성을 검사한 후 유효성이 검사된 데이터를 최종 테이블에 삽입하는 저장 프로시저를 만드는 것입니다. 엔지니어는 프로시저가 효율적이고 오류를 우아하게 처리할 수 있도록 하기를 원합니다. 다음 옵션 중 어떤 것이 이러한 요구 사항을 가장 잘 충족합니까?",
        "Question": "데이터 엔지니어가 Amazon Redshift에서 저장 프로시저를 효과적으로 구현하기 위해 어떤 접근 방식을 취해야 합니까?",
        "Options": {
            "1": "오류 처리를 포함하고 데이터 유효성 검사 및 삽입을 위한 트랜잭션 제어를 사용하는 저장 프로시저를 생성합니다.",
            "2": "AWS Glue를 활용하여 데이터 유효성 검사를 수행하고 데이터를 Redshift에 삽입하는 ETL 작업을 생성합니다.",
            "3": "Amazon Lambda 함수를 조합하여 데이터를 처리하고 최종 삽입을 위해 저장 프로시저를 호출합니다.",
            "4": "오류 처리 없이 데이터를 유효성 검사하고 삽입하는 간단한 SQL 스크립트를 사용합니다."
        },
        "Correct Answer": "오류 처리를 포함하고 데이터 유효성 검사 및 삽입을 위한 트랜잭션 제어를 사용하는 저장 프로시저를 생성합니다.",
        "Explanation": "이 옵션은 저장 프로시저가 데이터를 처리할 뿐만 아니라 오류 처리 및 트랜잭션 제어를 포함하여 Amazon Redshift에서 데이터 변환 작업을 위해 강력하고 효율적이도록 보장합니다.",
        "Other Options": [
            "이 옵션은 간단한 SQL 스크립트가 오류 처리가 부족하여 적절한 유효성 검사 없이 데이터 불일치 또는 실패를 초래할 수 있기 때문에 잘못된 것입니다.",
            "이 옵션은 저장 프로시저가 Redshift 내에서 작업을 더 효율적으로 처리할 수 있는 경우 Lambda 함수를 사용하여 데이터 처리를 하는 것은 불필요한 복잡성을 도입하기 때문에 잘못된 것입니다.",
            "이 옵션은 AWS Glue가 ETL 프로세스에 유용하지만 Redshift 내에서 데이터 유효성 검사 및 삽입에 특정한 저장 프로시저에 가장 적합한 선택이 아니기 때문에 잘못된 것입니다."
        ]
    },
    {
        "Question Number": "12",
        "Situation": "소매 회사는 다양한 서비스가 고객 상호작용의 여러 측면을 처리하는 마이크로서비스 아키텍처로 전환하고 있습니다. 각 서비스는 Amazon S3에 저장되는 로그를 생성합니다. 회사가 확장됨에 따라, 그들은 다양한 팀에서 도입한 새로운 로깅 형식과 데이터 속성을 수용하면서 서비스 간 데이터 일관성과 사용성을 유지해야 합니다.",
        "Question": "이 분산 로깅 환경에서 스키마 변경을 관리하기 위한 최선의 전략은 무엇입니까?",
        "Options": {
            "1": "스키마 제약 조건을 시행하고 데이터베이스 마이그레이션을 통해 변경을 관리하기 위해 로그 데이터를 관계형 데이터베이스에 저장합니다.",
            "2": "AWS Glue 스키마 레지스트리를 활용하여 스키마 버전을 관리하고 모든 마이크로서비스에서 스키마 진화를 시행합니다.",
            "3": "Amazon CloudWatch Logs를 사용하여 로그를 집계하고 출처에 관계없이 모든 로그 항목에 대해 단일 스키마를 적용합니다.",
            "4": "Amazon S3 이벤트 알림을 구현하여 로그 도착 시 유효성을 검사하고 변환하는 AWS Lambda 함수를 트리거합니다."
        },
        "Correct Answer": "AWS Glue 스키마 레지스트리를 활용하여 스키마 버전을 관리하고 모든 마이크로서비스에서 스키마 진화를 시행합니다.",
        "Explanation": "AWS Glue 스키마 레지스트리를 사용하면 소매 회사가 여러 마이크로서비스에서 스키마를 효과적으로 정의, 관리 및 발전시킬 수 있습니다. 이는 스키마 버전 관리를 지원하고 모든 서비스가 확장됨에 따라 정의된 스키마를 준수하도록 도와 데이터 일관성과 사용성을 유지합니다.",
        "Other Options": [
            "Amazon S3 이벤트 알림을 구현하여 AWS Lambda 함수를 트리거하는 것은 더 반응적이며, 로그 변환을 처리할 수 있지만 서비스 간의 강력한 스키마 관리 솔루션을 제공하지 않습니다.",
            "로그 데이터를 관계형 데이터베이스에 저장하는 것은 로그 관리에 불필요한 복잡성을 도입하며, 이는 지속적인 스키마 마이그레이션을 요구하고 각 마이크로서비스에서 생성된 다양한 데이터 형식에 적합하지 않을 수 있습니다.",
            "Amazon CloudWatch Logs를 사용하여 단일 스키마로 로그를 집계하는 것은 각 서비스의 고유한 요구 사항을 무시하여 모든 로그를 균일한 구조로 강제함으로써 잠재적인 데이터 손실 및 불일치를 초래할 수 있습니다."
        ]
    },
    {
        "Question Number": "13",
        "Situation": "데이터 엔지니어링 팀은 Amazon Redshift 환경 내에서 직접 머신 러닝 모델을 구축하는 임무를 맡고 있습니다. 그들은 대규모 데이터 세트를 Redshift에서 이동하지 않고도 모델이 예측을 수행할 수 있도록 해야 합니다. 또한, Redshift 클러스터 내에서 가장 최신의 데이터를 활용하고자 합니다.",
        "Question": "어떤 Amazon Redshift 기능이 팀이 SQL 명령을 사용하여 머신 러닝 모델을 훈련하고 데이터를 이동하지 않고 데이터베이스 내에서 예측을 수행할 수 있게 해줄까요?",
        "Options": {
            "1": "Amazon SageMaker Integration",
            "2": "Amazon Redshift ML",
            "3": "Redshift Data Sharing",
            "4": "Cross-Database Query"
        },
        "Correct Answer": "Amazon Redshift ML",
        "Explanation": "Amazon Redshift ML은 SQL 명령을 사용하여 Redshift 내에서 직접 머신 러닝 모델을 훈련하고 배포할 수 있게 해줍니다. 이 기능은 데이터베이스 내 추론을 가능하게 하여, 데이터를 Redshift 환경 밖으로 이동하지 않고도 예측을 수행할 수 있습니다.",
        "Other Options": [
            "Redshift Data Sharing은 서로 다른 Redshift 클러스터 간에 실시간 데이터를 안전하게 공유하는 데 중점을 두지만, 머신 러닝 모델 훈련이나 데이터베이스 내 예측과는 관련이 없습니다.",
            "Cross-Database Query는 Redshift 클러스터 내의 서로 다른 데이터베이스를 쿼리할 수 있게 해주며, 데이터 접근에 유용하지만 머신 러닝 기능을 제공하지 않습니다.",
            "Amazon SageMaker Integration은 SageMaker의 기능을 사용할 수 있게 해주지만, 모델 훈련을 위해 데이터를 SageMaker로 이동해야 하며 Redshift 내에서 SQL 명령 기반의 모델 훈련을 직접 제공하지 않습니다."
        ]
    },
    {
        "Question Number": "14",
        "Situation": "소매 회사는 여러 출처에서 고객 거래 데이터를 처리하고 변환하여 분석을 위한 단일 형식으로 만들 필요가 있습니다. 그들은 AWS 서비스를 사용하여 Amazon S3에서 데이터를 추출하고, AWS Glue를 사용하여 변환한 후, Amazon Redshift에 로드하여 보고할 수 있는 ETL 파이프라인을 만들고자 합니다. 팀은 이 워크플로우를 구현하기 위해 다양한 AWS 서비스를 고려하고 있습니다.",
        "Question": "다음 중 이 데이터 처리 요구 사항을 위해 AWS 서비스를 가장 잘 활용하여 효율적인 ETL 파이프라인을 만드는 접근 방식은 무엇인가요?",
        "Options": {
            "1": "AWS Step Functions를 구현하여 Glue Job을 조정하고 ETL 프로세스를 자동화합니다.",
            "2": "EC2 인스턴스를 수동으로 실행하여 S3에서 데이터를 처리하고 Redshift에 로드합니다.",
            "3": "AWS Lambda를 사용하여 S3에 새 데이터가 도착할 때마다 Glue Job을 트리거합니다.",
            "4": "Amazon Kinesis를 활용하여 변환 없이 데이터를 Redshift로 직접 스트리밍합니다."
        },
        "Correct Answer": "AWS Step Functions를 구현하여 Glue Job을 조정하고 ETL 프로세스를 자동화합니다.",
        "Explanation": "AWS Step Functions를 사용하여 Glue Job을 조정하면 ETL 워크플로우를 더 잘 관리할 수 있으며, 자동화 및 오류 처리를 가능하게 합니다. 이 접근 방식은 데이터 파이프라인의 효율성을 높이고 AWS 서비스와 잘 통합되어 ETL 프로세스에 강력한 솔루션이 됩니다.",
        "Other Options": [
            "AWS Lambda를 사용하여 Glue Job을 트리거하는 것은 유효한 접근 방식이지만, 복잡한 워크플로우에 필수적인 Step Functions가 제공하는 포괄적인 조정 및 오류 처리를 제공하지 않을 수 있습니다.",
            "EC2 인스턴스를 수동으로 실행하는 것은 비효율적이며 AWS의 서버리스 기능을 활용하지 않아 운영 오버헤드가 증가하고 잠재적인 확장성 문제를 초래할 수 있습니다.",
            "Amazon Kinesis를 활용하여 데이터를 Redshift로 직접 스트리밍하면 변환 단계를 건너뛰게 되어 데이터 분석을 위한 준비가 필요하므로 데이터 처리에 대한 비즈니스 요구 사항을 충족하지 않습니다."
        ]
    },
    {
        "Question Number": "15",
        "Situation": "소매 회사는 중앙 집중식 데이터 카탈로그를 구현하여 데이터 발견 기능을 개선하고자 합니다. 그들은 데이터 분석가들이 관련 데이터 세트를 더 쉽게 찾을 수 있도록 하면서 데이터 거버넌스와 규정을 준수하고자 합니다. 데이터 엔지니어링 팀은 다양한 출처에서 데이터를 자동으로 카탈로그화할 수 있는 AWS에서 데이터 카탈로그 솔루션을 구현할 옵션을 평가하고 있습니다.",
        "Question": "팀이 여러 출처에서 데이터를 자동으로 발견하고 카탈로그화할 수 있는 데이터 카탈로그를 만들기 위해 어떤 서비스를 사용해야 할까요?",
        "Options": {
            "1": "Amazon Athena를 사용하여 S3에서 직접 데이터를 쿼리하고 AWS Glue에 카탈로그화합니다.",
            "2": "AWS Glue Data Catalog를 사용하여 AWS 서비스 전반에 걸쳐 데이터를 자동으로 발견하고 조직합니다.",
            "3": "Amazon QuickSight를 사용하여 데이터를 시각화하고 데이터 세트의 카탈로그를 제공합니다.",
            "4": "Amazon Redshift Spectrum을 사용하여 외부 데이터를 직접 쿼리하고 해당 데이터의 카탈로그를 유지합니다."
        },
        "Correct Answer": "AWS Glue Data Catalog를 사용하여 AWS 서비스 전반에 걸쳐 데이터를 자동으로 발견하고 조직합니다.",
        "Explanation": "AWS Glue Data Catalog는 AWS의 다양한 출처에서 데이터를 자동으로 발견하고 조직하도록 특별히 설계되었습니다. 이는 데이터 자산에 대한 메타데이터를 저장하는 중앙 저장소 역할을 하여 사용자가 데이터를 더 쉽게 찾고 사용할 수 있도록 하며, 규정 준수와 거버넌스를 보장합니다.",
        "Other Options": [
            "Amazon Athena는 주로 Amazon S3에 저장된 데이터를 쿼리하는 데 중점을 두며, 여러 데이터 출처에 대한 중앙 집중식 카탈로그 기능을 제공하지 않습니다.",
            "Amazon Redshift Spectrum은 외부 데이터를 쿼리할 수 있지만, 다양한 AWS 서비스 전반에 걸쳐 메타데이터를 관리하기 위한 포괄적인 카탈로그 솔루션을 제공하지 않습니다.",
            "Amazon QuickSight는 시각화 기능을 제공하는 비즈니스 인텔리전스 서비스이지만, 데이터 세트를 조직하고 발견하기 위한 데이터 카탈로그 역할을 하지 않습니다."
        ]
    },
    {
        "Question Number": "16",
        "Situation": "한 회사가 다양한 환경에 배포된 여러 애플리케이션에서 로그 데이터를 수집하고 있습니다. 이들은 이러한 로그에 대한 즉석 분석을 수행하여 운영 통찰력을 향상시키기 위해 트렌드와 이상 현상을 식별하고자 합니다. 로그는 S3에 저장되며, 회사는 추가 인프라를 설정하지 않고 이 데이터에 대한 쿼리를 실행할 수 있는 비용 효율적인 방법을 찾고 있습니다.",
        "Question": "광범위한 인프라 관리 없이 S3에 저장된 로그 데이터를 쿼리하는 데 가장 효율적인 솔루션을 제공하는 AWS 서비스는 무엇입니까?",
        "Options": {
            "1": "Amazon Redshift for data warehousing and analytics",
            "2": "AWS Glue to schedule ETL jobs for log transformation",
            "3": "Amazon Athena to run SQL queries directly on S3 data",
            "4": "Amazon EMR to set up a Hadoop cluster for log processing"
        },
        "Correct Answer": "Amazon Athena to run SQL queries directly on S3 data",
        "Explanation": "Amazon Athena는 복잡한 인프라 설정 없이 S3에 저장된 데이터에 대해 SQL 쿼리를 직접 실행할 수 있게 해줍니다. 서버리스 방식으로, 실행한 쿼리에 대해서만 비용을 지불하므로 로그 데이터의 즉석 분석을 위한 비용 효율적인 솔루션입니다.",
        "Other Options": [
            "Amazon Redshift는 데이터 웨어하우징에 더 적합하며 클러스터를 프로비저닝해야 하므로 즉석 쿼리에 대해 더 높은 비용과 관리 오버헤드가 발생할 수 있습니다.",
            "Amazon EMR은 빅데이터 처리를 위해 설계되었으며 클러스터 설정이 필요하므로 간단한 로그 분석에는 더 복잡하고 비용이 많이 듭니다.",
            "AWS Glue는 주로 ETL 서비스이며 데이터를 변환할 수 있지만 추가 설정 없이 S3에 저장된 로그를 직접 쿼리하는 데 이상적인 선택은 아닙니다."
        ]
    },
    {
        "Question Number": "17",
        "Situation": "한 금융 서비스 회사가 고객 거래 데이터를 저장하기 위해 Amazon DynamoDB를 사용하고 있습니다. 고객 활동의 갑작스러운 증가로 인해 데이터 읽기 및 쓰기 시 스로틀링 문제가 발생하고 있습니다. 데이터 엔지니어는 DynamoDB의 프로비저닝된 처리량 한도를 위반하지 않으면서 트래픽 급증을 처리할 수 있는 솔루션을 구현해야 합니다. 이 솔루션은 또한 피크 시간 동안 데이터 손실이 없도록 해야 합니다.",
        "Question": "데이터 손실을 최소화하면서 스로틀링 문제를 가장 잘 해결할 수 있는 솔루션은 무엇입니까?",
        "Options": {
            "1": "DynamoDB 테이블의 프로비저닝된 처리량을 증가시켜 피크 부하를 처리하고 사용량을 수동으로 모니터링합니다.",
            "2": "Amazon Kinesis Data Streams를 사용하여 들어오는 거래 요청을 버퍼링한 다음 이를 DynamoDB 쓰기 작업으로 배치합니다.",
            "3": "Amazon SQS를 사용하여 들어오는 거래를 큐에 넣고 이를 DynamoDB에 쓰는 전용 Lambda 함수로 비동기적으로 처리합니다.",
            "4": "DynamoDB Auto Scaling을 구현하여 시간에 따른 트래픽 패턴에 따라 프로비저닝된 처리량을 조정합니다."
        },
        "Correct Answer": "Use Amazon Kinesis Data Streams to buffer incoming transaction requests and then batch them into DynamoDB write operations.",
        "Explanation": "Amazon Kinesis Data Streams를 사용하면 금융 서비스 회사가 들어오는 거래 요청을 버퍼링하고 트래픽 급증을 효과적으로 관리할 수 있습니다. 이 접근 방식은 거래를 배치로 처리할 수 있도록 하여 스로틀링 위험을 줄이고 피크 부하 동안 데이터 손실이 없도록 보장합니다.",
        "Other Options": [
            "DynamoDB Auto Scaling을 구현하는 것은 유익하지만 갑작스러운 트래픽 급증에 신속하게 반응하지 못할 수 있어 스로틀링 문제와 피크 부하 동안 데이터 손실을 초래할 수 있습니다.",
            "Amazon SQS를 사용하여 거래를 큐에 넣는 것은 추가 지연과 복잡성을 초래하여 실시간 거래 처리에는 이상적이지 않으며 처리 시간 지연을 초래할 수 있습니다.",
            "프로비저닝된 처리량을 증가시키는 것은 단기적인 해결책이 될 수 있지만, 더 높은 비용을 초래할 수 있으며 갑작스러운 트래픽 급증을 처리하는 근본적인 문제를 효과적으로 해결하지 못합니다."
        ]
    },
    {
        "Question Number": "18",
        "Situation": "한 금융 서비스 회사가 높은 가용성과 낮은 대기 시간으로 거래 데이터에 접근할 수 있는 데이터 아키텍처를 설계하고 있습니다. 이들은 온라인 거래 처리를 위해 DynamoDB를 사용할 계획이며 읽기 및 쓰기 작업을 최적화하는 스키마를 설정해야 합니다. 또한 회사는 성능을 저하시키지 않으면서 다양한 속성을 기반으로 데이터를 쉽게 쿼리할 수 있도록 보장하고자 합니다. 이 시나리오에서 DynamoDB 스키마를 설계하는 최선의 접근 방식은 무엇입니까?",
        "Question": "회사가 읽기 및 쓰기 작업을 최적화하고 유연한 쿼리를 허용하기 위해 DynamoDB 스키마에 사용할 디자인 패턴은 무엇입니까?",
        "Options": {
            "1": "복잡한 복합 키를 피하고 단순성을 보장하기 위해 각 엔티티 유형에 대해 여러 테이블을 생성합니다.",
            "2": "관계형 모델을 사용하여 스키마를 설계하고 쿼리를 처리하기 위해 Amazon RDS를 구현합니다.",
            "3": "복합 기본 키가 있는 단일 테이블을 생성하고 추가 쿼리 패턴을 위해 글로벌 보조 인덱스(GSI)를 사용합니다.",
            "4": "단순 기본 키가 있는 단일 테이블을 사용하고 데이터 쿼리를 위해 스캔 작업에만 의존합니다."
        },
        "Correct Answer": "Create a single table with a composite primary key and use Global Secondary Indexes (GSIs) for additional query patterns.",
        "Explanation": "복합 기본 키가 있는 단일 테이블을 사용하면 효율적인 읽기 및 쓰기 작업이 가능하며, GSI를 활용하면 성능을 저하시키지 않으면서 추가 쿼리 기능을 제공합니다. 이 디자인 패턴은 DynamoDB의 기능에 최적화되어 있으며 높은 가용성을 보장합니다.",
        "Other Options": [
            "각 엔티티 유형에 대해 여러 테이블을 생성하면 데이터 중복과 데이터 간 관계 관리의 복잡성이 증가할 수 있습니다. 이는 여러 엔티티의 데이터를 요구하는 쿼리를 복잡하게 만들 수 있습니다.",
            "단순 기본 키가 있는 단일 테이블을 사용하고 스캔 작업에만 의존하는 것은 대규모 데이터 세트에 비효율적이며, 스캔은 성능 최적화가 되어 있지 않아 높은 대기 시간과 비용 증가를 초래할 수 있습니다.",
            "관계형 모델을 사용하여 스키마를 설계하고 Amazon RDS를 구현하는 것은 DynamoDB의 강점을 활용하지 못하며, 이는 확장성과 성능을 위해 설계된 NoSQL 데이터베이스입니다. 이 접근 방식은 낮은 대기 시간 접근 요구 사항을 충족하지 못할 것입니다."
        ]
    },
    {
        "Question Number": "19",
        "Situation": "한 회사가 Amazon S3에 민감한 고객 데이터를 저장하고 있으며 이를 적절히 보호하고자 합니다. 이들은 Amazon Macie를 활용하여 데이터를 발견하고 분류하고 있습니다. 또한, 무단 접근 시도를 모니터링하고 데이터 보호 규정을 준수하고자 합니다.",
        "Question": "데이터 엔지니어가 데이터 보안 및 준수를 강화하기 위해 어떤 서비스 조합을 구현해야 합니까? (두 가지 선택)",
        "Options": {
            "1": "AWS Config를 활성화하여 AWS 리소스의 구성을 모니터링합니다.",
            "2": "Amazon CloudWatch를 사용하여 S3 버킷 접근과 관련된 메트릭을 추적합니다.",
            "3": "Amazon Macie를 통합하여 민감한 데이터를 식별하고 그 사용을 추적합니다.",
            "4": "AWS CloudTrail을 활성화하여 API 호출 및 사용자 활동을 기록합니다.",
            "5": "AWS Shield를 배포하여 DDoS 공격으로부터 보호합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "AWS CloudTrail을 활성화하여 API 호출 및 사용자 활동을 기록합니다.",
            "Amazon Macie를 통합하여 민감한 데이터를 식별하고 그 사용을 추적합니다."
        ],
        "Explanation": "AWS CloudTrail을 활성화하면 API 호출 및 사용자 활동에 대한 포괄적인 로그가 제공되어 감사 및 준수에 필수적입니다. Amazon Macie를 통합하면 조직이 민감한 데이터를 발견하고 분류할 수 있어 데이터 보호 노력을 강화합니다.",
        "Other Options": [
            "Amazon CloudWatch를 사용하는 것은 주로 운영 메트릭과 로그를 추적하지만 민감한 데이터 발견이나 사용자 활동 로그를 구체적으로 다루지 않습니다.",
            "AWS Config를 활성화하면 리소스 구성을 모니터링하는 데 도움이 되지만 데이터 접근이나 민감한 데이터 분류에 대한 통찰력을 제공하지 않습니다.",
            "AWS Shield를 배포하면 DDoS 공격으로부터 보호하지만 민감한 데이터와 관련된 데이터 보안이나 준수를 직접적으로 강화하지는 않습니다."
        ]
    },
    {
        "Question Number": "20",
        "Situation": "데이터 엔지니어링 팀은 AWS 환경 내의 모든 데이터 작업이 감사 및 준수 목적을 위해 기록되도록 해야 합니다. 이들은 AWS 서비스 내에서 데이터 접근 및 수정에 대한 자세한 정보를 캡처하는 로깅 솔루션을 구현하고자 합니다.",
        "Question": "팀이 데이터 작업의 감사 및 추적성을 위한 로깅 및 모니터링 솔루션을 효과적으로 배포하기 위해 어떤 조치를 취해야 합니까?",
        "Options": {
            "1": "AWS Config를 사용하여 시간에 따른 리소스 구성 변경을 추적합니다.",
            "2": "AWS CloudTrail을 활성화하여 AWS 서비스에 대한 API 호출을 기록합니다.",
            "3": "Amazon CloudWatch를 배포하여 메트릭을 모니터링하고 데이터 접근에 대한 알람을 설정합니다.",
            "4": "Amazon GuardDuty를 구현하여 위협 탐지 및 모니터링을 수행합니다."
        },
        "Correct Answer": "AWS CloudTrail을 활성화하여 AWS 서비스에 대한 API 호출을 기록합니다.",
        "Explanation": "AWS CloudTrail은 AWS 계정 내에서 이루어진 모든 API 호출을 기록하도록 특별히 설계되어 있어 데이터 작업의 감사 및 추적성에 필수적입니다. 모든 행동에 대한 포괄적인 뷰를 제공하여 이 요구 사항에 가장 적합한 선택입니다.",
        "Other Options": [
            "Amazon CloudWatch는 메트릭을 모니터링하고 알람을 설정할 수 있지만, 데이터 작업 감사에 필수적인 API 호출에 대한 자세한 로그를 제공하지 않습니다.",
            "AWS Config는 리소스 구성의 변경을 추적하지만 데이터 접근이나 운영 이벤트를 기록하지 않아 효과적인 감사에 필요합니다.",
            "Amazon GuardDuty는 위협 탐지 및 보안 모니터링에 중점을 두고 있으며, 데이터 작업이나 접근에 대한 자세한 로그를 제공하지 않아 이 특정 감사 요구 사항에 적합하지 않습니다."
        ]
    },
    {
        "Question Number": "21",
        "Situation": "한 회사가 AWS를 활용하여 인프라를 유지 관리하고 있으며, AWS 리소스에 대한 구성 변경이 추적되고 내부 거버넌스 정책을 준수하도록 하고자 합니다. 이들은 리소스 구성에 대한 가시성을 제공하고 변경 사항에 대해 경고하는 솔루션이 필요합니다.",
        "Question": "구성 변경을 추적하고 거버넌스 정책 준수를 보장하기 위해 가장 효과적인 AWS 서비스는 무엇입니까?",
        "Options": {
            "1": "AWS Config를 사용하여 구성 변경을 모니터링하고 거버넌스 정책 준수를 평가합니다.",
            "2": "AWS CloudTrail을 구현하여 감사 목적으로 AWS 계정에서 이루어진 모든 API 호출을 기록합니다.",
            "3": "AWS Systems Manager를 배포하여 AWS 리소스 전반에 걸쳐 운영 작업을 관리하고 자동화합니다.",
            "4": "Amazon CloudWatch를 채택하여 AWS 서비스의 성능 및 리소스 활용도를 모니터링합니다."
        },
        "Correct Answer": "AWS Config를 사용하여 구성 변경을 모니터링하고 거버넌스 정책 준수를 평가합니다.",
        "Explanation": "AWS Config는 AWS 리소스의 구성 변경을 추적하도록 특별히 설계되었습니다. 리소스 구성을 지속적으로 모니터링하고 기록하며, 정의된 거버넌스 정책에 대한 준수를 평가할 수 있어 이 시나리오에 가장 적합한 선택입니다.",
        "Other Options": [
            "AWS Systems Manager는 주로 운영 관리 및 자동화에 중점을 두고 있어 구성 변경 추적에는 적합하지 않습니다.",
            "AWS CloudTrail은 API 호출을 기록하고 감사 기능을 제공하지만 리소스 구성 변경이나 준수를 추적하지 않습니다.",
            "Amazon CloudWatch는 성능 모니터링 및 리소스 활용에 사용되며, 구성 변경이나 거버넌스 준수를 추적하는 데는 특별히 사용되지 않습니다."
        ]
    },
    {
        "Question Number": "22",
        "Situation": "데이터 엔지니어는 여러 AWS 서비스에서 데이터 자산을 관리하고 발견하기 위한 데이터 카탈로그 솔루션을 구현하는 임무를 맡았습니다. 그들은 데이터 카탈로그가 데이터 스키마, 계보 및 거버넌스에 대한 자세한 정보를 제공하도록 하고 싶어합니다. 엔지니어는 이 목표를 달성하기 위해 다양한 도구를 평가하고 있습니다.",
        "Question": "다음 옵션 중 데이터 엔지니어가 포괄적인 데이터 카탈로그를 만드는 데 가장 잘 도움이 되는 것은 무엇입니까?",
        "Options": {
            "1": "데이터 자산 전반에 걸쳐 데이터 거버넌스를 시행하기 위해 Amazon S3 버킷 정책을 구현합니다.",
            "2": "Amazon RDS를 사용하여 데이터 자산의 메타데이터를 보유하는 관계형 데이터베이스를 생성합니다.",
            "3": "Amazon DynamoDB를 배포하여 메타데이터를 저장하고 이를 다양한 데이터 소스에 연결합니다.",
            "4": "AWS Glue Data Catalog를 활용하여 다양한 AWS 데이터 소스의 메타데이터를 저장하고 관리합니다."
        },
        "Correct Answer": "AWS Glue Data Catalog를 활용하여 다양한 AWS 데이터 소스의 메타데이터를 저장하고 관리합니다.",
        "Explanation": "AWS Glue Data Catalog는 메타데이터 관리를 위해 특별히 설계되었으며, 다양한 AWS 서비스 전반에 걸쳐 데이터 자산을 위한 중앙 집중식 저장소를 제공합니다. 데이터 발견, 스키마 관리 및 데이터 계보 추적을 지원하여 데이터 엔지니어의 요구 사항에 가장 적합한 선택입니다.",
        "Other Options": [
            "Amazon S3 버킷 정책을 구현하는 것은 데이터 거버넌스에 도움이 될 수 있지만, 메타데이터를 관리하거나 데이터 카탈로그를 생성하는 메커니즘을 제공하지는 않습니다.",
            "Amazon RDS는 주로 관계형 데이터베이스 서비스이며, 다양한 데이터 소스 전반에 걸쳐 메타데이터 관리를 위한 기능을 본질적으로 제공하지 않습니다.",
            "Amazon DynamoDB는 메타데이터를 저장할 수 있는 NoSQL 데이터베이스 서비스이지만, AWS Glue Data Catalog가 제공하는 데이터 카탈로그 및 거버넌스를 위한 전문 기능이 부족합니다."
        ]
    },
    {
        "Question Number": "23",
        "Situation": "컴플라이언스 팀은 민감한 데이터를 저장하는 Amazon S3 버킷의 접근 로그에 대한 정기적인 감사가 필요합니다. 데이터 엔지니어는 로그가 추출되어 감사 목적으로 안전하게 저장되도록 해야 합니다.",
        "Question": "데이터 엔지니어가 이러한 로그를 추출하고 저장하면서 감사에 접근할 수 있도록 보장하는 가장 효과적인 방법은 무엇입니까?",
        "Options": {
            "1": "S3 이벤트 알림을 활성화하여 AWS Lambda 함수를 트리거하여 로그를 Amazon DynamoDB 테이블에 기록합니다.",
            "2": "AWS CLI를 사용하여 S3에서 로그를 수동으로 다운로드하고 정기적으로 Amazon S3 Glacier 금고에 저장합니다.",
            "3": "S3 버킷 로깅을 설정하고 Amazon Kinesis Data Firehose를 구성하여 로그를 Amazon S3 버킷으로 스트리밍합니다.",
            "4": "S3 버킷 로깅을 구성하고 AWS Lambda 함수를 설정하여 로그를 Amazon RDS 인스턴스로 전송합니다."
        },
        "Correct Answer": "S3 버킷 로깅을 설정하고 Amazon Kinesis Data Firehose를 구성하여 로그를 Amazon S3 버킷으로 스트리밍합니다.",
        "Explanation": "S3 버킷 로깅을 설정하고 Amazon Kinesis Data Firehose를 구성하여 로그를 스트리밍하면 로그가 자동으로 수집되고 전용 S3 버킷에 안전하게 저장되어 감사에 쉽게 접근할 수 있습니다.",
        "Other Options": [
            "S3 버킷 로깅을 구성하고 Lambda 함수를 사용하여 로그를 RDS로 전송하는 것은 실행 가능한 옵션처럼 보일 수 있지만, 불필요한 복잡성을 초래하고 로그 저장 및 접근성에 최선의 선택이 아닐 수 있습니다.",
            "S3 이벤트 알림을 활성화하고 Lambda 함수를 사용하여 로그를 DynamoDB에 기록하는 것은 가능하지만, DynamoDB는 대용량 로그 저장에 최적이 아니며 높은 비용과 성능 문제를 초래할 수 있습니다.",
            "AWS CLI를 사용하여 로그를 수동으로 다운로드하고 S3 Glacier에 저장하는 것은 효율적이지 않으며 자동화되지 않아 정기적인 감사에 비해 스트리밍 솔루션보다 덜 적합합니다."
        ]
    },
    {
        "Question Number": "24",
        "Situation": "데이터 엔지니어링 팀은 여러 소스에서 데이터를 자동으로 수집하고 변환하는 새로운 데이터 파이프라인을 AWS에 배포하는 임무를 맡았습니다. 팀은 배포가 반복 가능하고 관리 가능하도록 인프라를 코드로(IaC) 사용하고 싶어합니다. 그들은 이를 위해 다양한 AWS 서비스를 고려하고 있습니다.",
        "Question": "어떤 솔루션이 팀이 IaC를 사용하여 반복 가능하고 관리 가능한 인프라로 데이터 파이프라인을 배포할 수 있도록 합니까?",
        "Options": {
            "1": "AWS Management Console을 사용하여 인프라를 수동으로 프로비저닝한 다음, 구성을 CloudFormation으로 내보내 스택을 생성합니다. 이렇게 하면 팀이 향후 설정을 복제할 수 있습니다.",
            "2": "AWS CLI 명령을 사용하여 데이터 파이프라인에 필요한 리소스를 프로비저닝하는 셸 스크립트를 생성합니다. 향후 사용을 위해 스크립트를 버전 관리 리포지토리에 저장합니다.",
            "3": "AWS CloudFormation을 사용하여 AWS Lambda 함수, Amazon S3 버킷 및 Amazon DynamoDB 테이블을 포함한 데이터 파이프라인의 전체 아키텍처를 정의합니다. CloudFormation 콘솔을 사용하여 스택을 배포합니다.",
            "4": "AWS CDK를 사용하여 지원되는 프로그래밍 언어로 데이터 파이프라인 구성 요소를 프로그래밍 방식으로 정의합니다. AWS CDK CLI를 사용하여 스택을 배포하면 업데이트 및 버전 관리가 용이합니다."
        },
        "Correct Answer": "AWS CDK를 사용하여 지원되는 프로그래밍 언어로 데이터 파이프라인 구성 요소를 프로그래밍 방식으로 정의합니다. AWS CDK CLI를 사용하여 스택을 배포하면 업데이트 및 버전 관리가 용이합니다.",
        "Explanation": "AWS CDK를 사용하면 AWS 리소스를 정의하고 배포하는 데 더 프로그래밍적인 접근 방식을 제공하여 업데이트 관리 및 버전 관리를 쉽게 할 수 있습니다. 유연성을 제공하며 기존 개발 워크플로와 잘 통합됩니다.",
        "Other Options": [
            "AWS CloudFormation을 사용하는 것은 유효한 접근 방식이지만, 더 많은 보일러플레이트 코드가 필요할 수 있으며, 프로그래밍 정의를 위해 AWS CDK보다 덜 유연할 수 있습니다.",
            "인프라를 수동으로 프로비저닝한 다음 CloudFormation으로 내보내는 것은 비효율적이며, 잠재적인 불일치를 초래하고 처음부터 IaC의 이점을 완전히 활용하지 못합니다.",
            "AWS CLI 명령을 사용하는 셸 스크립트를 사용하는 것은 프로비저닝을 자동화할 수 있지만, IaC 도구인 AWS CDK나 CloudFormation이 제공하는 구조, 버전 관리 및 관리 기능이 부족합니다."
        ]
    },
    {
        "Question Number": "25",
        "Situation": "한 회사가 Amazon S3 버킷, AWS Lambda 함수 및 Amazon RDS 인스턴스를 포함하는 데이터 파이프라인 인프라의 배포를 자동화해야 합니다. 팀은 다양한 환경에서 일관되게 배포를 복제할 수 있도록 Infrastructure as Code (IaC)를 사용하고자 합니다. 이 목표를 달성하기 위해 다양한 IaC 도구를 고려하고 있습니다.",
        "Question": "데이터 파이프라인을 위한 확장 가능하고 반복 가능한 인프라 배포를 생성하는 데 가장 적합한 도구는 무엇입니까?",
        "Options": {
            "1": "인스턴스의 수동 구성을 위한 Amazon EC2 사용자 데이터 스크립트.",
            "2": "리소스 및 구성을 정의하기 위한 YAML 템플릿을 사용하는 AWS CloudFormation.",
            "3": "애플리케이션 배포 및 자동 확장을 관리하기 위한 AWS Elastic Beanstalk.",
            "4": "템플릿 없이 요구에 따라 리소스 생성을 트리거하는 AWS Lambda."
        },
        "Correct Answer": "리소스 및 구성을 정의하기 위한 YAML 템플릿을 사용하는 AWS CloudFormation.",
        "Explanation": "AWS CloudFormation은 Infrastructure as Code를 위해 특별히 설계되어 사용자가 YAML 또는 JSON 템플릿을 사용하여 클라우드 리소스를 구조적으로 정의할 수 있게 합니다. 이는 다양한 환경에서 일관되고 반복 가능한 배포를 가능하게 하여 회사의 요구에 가장 적합한 선택입니다.",
        "Other Options": [
            "AWS Lambda는 이벤트에 응답하여 코드를 실행하는 데 주로 사용되며, 템플릿 없이 클라우드 리소스를 프로비저닝하기 위해 설계되지 않았으므로 자동화된 인프라 배포에 적합하지 않습니다.",
            "AWS Elastic Beanstalk는 S3 또는 RDS와 같은 인프라 구성 요소를 정의하고 관리하기보다는 웹 애플리케이션 및 서비스를 배포하는 데 맞춰져 있어 데이터 파이프라인 인프라에 대한 특정 요구를 충족하지 않습니다.",
            "Amazon EC2 사용자 데이터 스크립트는 인스턴스 시작 시 구성하는 데 사용되지만, Infrastructure as Code에 필수적인 여러 AWS 리소스를 반복 가능한 방식으로 관리하고 배포하기 위한 포괄적인 솔루션을 제공하지 않습니다."
        ]
    },
    {
        "Question Number": "26",
        "Situation": "데이터 엔지니어링 팀이 Amazon Redshift를 사용하여 대규모 데이터 웨어하우스를 관리하고 있습니다. 그들은 여러 쿼리가 동시에 동일한 데이터 세트를 수정하려고 할 때 데이터 접근 충돌 문제에 자주 직면합니다. 팀은 성능에 심각한 영향을 미치지 않으면서 이러한 충돌을 방지하기 위해 잠금을 효과적으로 구현할 전략이 필요합니다.",
        "Question": "최적의 쿼리 성능을 유지하면서 Amazon Redshift에서 데이터 접근 충돌을 방지하기 위해 잠금을 관리하는 가장 좋은 접근 방식은 무엇입니까?",
        "Options": {
            "1": "매시간 잠금을 해제하는 예약 작업을 설정합니다.",
            "2": "잠금을 관리하기 위해 Amazon Redshift의 자동 진공 기능을 활용합니다.",
            "3": "SELECT FOR UPDATE 절을 사용하여 행 수준 잠금을 구현합니다.",
            "4": "트랜잭션 격리 수준을 사용하여 잠금 동작을 제어합니다."
        },
        "Correct Answer": "트랜잭션 격리 수준을 사용하여 잠금 동작을 제어합니다.",
        "Explanation": "트랜잭션 격리 수준을 사용하면 트랜잭션 간의 상호 작용을 제어할 수 있어 데이터 접근 충돌을 방지하면서도 쿼리 실행에서 높은 성능을 유지할 수 있습니다. 이 접근 방식은 애플리케이션의 요구에 가장 적합한 방식으로 잠금 동작에 대한 세밀한 제어를 제공합니다.",
        "Other Options": [
            "매시간 잠금을 해제하는 예약 작업을 설정하는 것은 접근 충돌을 방지하지 않으며, 단지 기존 잠금을 제거할 뿐입니다.",
            "SELECT FOR UPDATE 절을 사용하여 행 수준 잠금을 구현하면 경쟁이 증가하고 전체 성능이 저하될 수 있으며, 필요 이상으로 행에 대한 잠금을 유지하게 됩니다.",
            "Amazon Redshift의 자동 진공 기능은 주로 공간을 회수하고 쿼리 성능을 최적화하는 데 사용되며, 잠금을 관리하는 데는 적합하지 않으므로 잠금 문제를 직접적으로 해결하지 않습니다."
        ]
    },
    {
        "Question Number": "27",
        "Situation": "한 금융 서비스 회사가 거래 처리를 위해 중요한 Amazon RDS 데이터베이스에서 성능 문제를 겪고 있습니다. 그들은 데이터 쿼리 시 지연 시간이 증가하는 것을 발견하고 있으며, 아키텍처에 큰 변화를 주지 않고 문제의 원인을 파악하고자 합니다.",
        "Question": "Amazon RDS 데이터베이스의 성능 문제를 해결하기 위한 가장 효과적인 초기 접근 방식은 무엇입니까?",
        "Options": {
            "1": "Amazon RDS 인스턴스에서 향상된 모니터링을 활성화하여 데이터베이스 성능에 대한 자세한 메트릭을 수집합니다.",
            "2": "Amazon RDS 데이터베이스의 인스턴스 크기를 늘려 성능을 개선하고 더 많은 쿼리를 처리합니다.",
            "3": "데이터베이스 파라미터 그룹을 검토하고 설정을 수정하여 쿼리 성능을 최적화합니다.",
            "4": "Amazon RDS 데이터베이스의 읽기 복제를 구현하여 읽기 트래픽을 분산시키고 쿼리 응답 시간을 개선합니다."
        },
        "Correct Answer": "Amazon RDS 인스턴스에서 향상된 모니터링을 활성화하여 데이터베이스 성능에 대한 자세한 메트릭을 수집합니다.",
        "Explanation": "향상된 모니터링을 활성화하면 아키텍처 변경 없이 병목 현상 및 성능 문제를 식별하는 데 도움이 되는 실시간 메트릭을 제공합니다. 이 데이터는 추가 문제 해결 단계를 효과적으로 안내할 수 있습니다.",
        "Other Options": [
            "인스턴스 크기를 늘리는 것은 성능 문제를 일시적으로 완화할 수 있지만, 근본 원인을 해결하거나 지연의 원인을 파악하는 데 도움이 되지 않습니다.",
            "읽기 복제를 구현하면 읽기 트래픽을 분산시킬 수 있지만, 이는 기본 데이터베이스의 구성이나 내부 비효율성에서 발생하는 성능 문제를 직접적으로 해결하지 않습니다.",
            "데이터베이스 파라미터 그룹을 검토하고 수정하는 것은 성능 최적화에 도움이 될 수 있지만, 모니터링을 통해 수집된 통찰을 기반으로 해야 합니다. 현재 성능 메트릭을 이해하지 않고 변경하면 추가 문제를 초래할 수 있습니다."
        ]
    },
    {
        "Question Number": "28",
        "Situation": "미디어 회사가 고객에게 처리 및 전달을 위해 자주 접근해야 하는 대용량 비디오 파일을 저장하고 있습니다. 또한, 규정 준수 이유로 보존해야 하지만 자주 접근하지 않는 아카이브 데이터를 저장하기 위한 비용 효율적인 솔루션이 필요합니다.",
        "Question": "다음 중 이러한 요구 사항을 가장 잘 충족하는 스토리지 솔루션은 무엇입니까? (두 개 선택)",
        "Options": {
            "1": "높은 IOPS 기능으로 인해 비디오 파일 저장을 위해 Amazon EBS 볼륨을 사용합니다.",
            "2": "자주 접근하는 비디오 파일을 위해 Amazon S3의 S3 Standard 스토리지를 사용합니다.",
            "3": "비용을 낮추면서 규정 준수를 보장하기 위해 아카이브 데이터에 Amazon S3 Glacier를 사용합니다.",
            "4": "비디오 파일과 아카이브 데이터 모두에 대해 Amazon S3의 S3 Intelligent-Tiering을 사용합니다.",
            "5": "더 쉬운 접근을 위해 비디오 파일을 저장하기 위해 Amazon FSx for Windows File Server를 사용합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "자주 접근하는 비디오 파일을 위해 Amazon S3의 S3 Standard 스토리지를 사용합니다.",
            "비용을 낮추면서 규정 준수를 보장하기 위해 아카이브 데이터에 Amazon S3 Glacier를 사용합니다."
        ],
        "Explanation": "Amazon S3의 S3 Standard 스토리지를 사용하면 높은 내구성과 가용성 덕분에 자주 접근하는 비디오 파일에 이상적입니다. 아카이브 데이터의 경우, Amazon S3 Glacier는 장기 데이터 보존을 가능하게 하면서 규정 준수 요구 사항을 충족하는 비용 효율적인 솔루션을 제공합니다.",
        "Other Options": [
            "Amazon EBS 볼륨은 EC2 인스턴스에 연결된 블록 스토리지 솔루션에 더 적합하며, 비디오 파일과 같은 대량의 데이터를 저장하는 데 비용 효율적이지 않습니다. 또한 S3와 같은 수준의 내구성을 제공하지 않습니다.",
            "Amazon S3의 S3 Intelligent-Tiering은 알 수 없는 접근 패턴을 가진 데이터에 맞춰 설계되었지만, 이 경우 접근 패턴이 알려져 있습니다. 비디오 파일에는 S3 Standard를, 아카이브 데이터에는 S3 Glacier를 사용하는 것이 더 적절합니다.",
            "Amazon FSx for Windows File Server는 완전 관리형 Windows 파일 시스템을 제공하는 데 초점을 맞추고 있습니다. S3에 비해 더 높은 비용과 복잡성으로 인해 대용량 비디오 파일을 저장하는 데 최선의 선택이 아닙니다."
        ]
    },
    {
        "Question Number": "29",
        "Situation": "개발 팀은 AWS에서 애플리케이션 로그가 안전하게 저장되고 모니터링되도록 해야 합니다. 데이터 보안 및 거버넌스에 대한 모범 사례를 준수하면서 효율적인 로그 저장, 관리 및 모니터링을 가능하게 하는 Amazon CloudWatch Logs를 사용하는 솔루션을 설정해야 합니다.",
        "Question": "CloudWatch Logs에서 애플리케이션 로그의 안전한 저장 및 모니터링을 가장 효과적으로 보장하는 솔루션은 무엇입니까? (두 개 선택)",
        "Options": {
            "1": "AWS Key Management Service (KMS)를 사용하여 데이터가 정지 상태일 때 CloudWatch Logs 암호화를 활성화합니다.",
            "2": "실시간 처리를 위해 로그를 Amazon Kinesis Data Stream으로 전송하는 CloudWatch Logs 구독 필터를 구현합니다.",
            "3": "감사를 위해 CloudWatch Logs에 대한 모든 API 호출을 기록하기 위해 CloudTrail을 설정합니다.",
            "4": "비용 관리를 위해 30일 이상 된 로그를 자동으로 만료하도록 CloudWatch Logs를 구성합니다.",
            "5": "사용자 역할에 따라 CloudWatch Logs에 대한 접근을 제한하기 위해 AWS Identity and Access Management (IAM) 정책을 적용합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "AWS Key Management Service (KMS)를 사용하여 데이터가 정지 상태일 때 CloudWatch Logs 암호화를 활성화합니다.",
            "사용자 역할에 따라 CloudWatch Logs에 대한 접근을 제한하기 위해 AWS Identity and Access Management (IAM) 정책을 적용합니다."
        ],
        "Explanation": "AWS KMS를 사용하여 CloudWatch Logs 암호화를 활성화하면 로그 데이터가 안전하게 저장되고 정지 상태에서 보호됩니다. IAM 정책을 적용하면 사용자 역할에 따라 접근을 제한하여 데이터 거버넌스를 강화하는 데 도움이 됩니다.",
        "Other Options": [
            "로그의 자동 만료를 구성하는 것은 비용 관리에 도움이 될 수 있지만, 로그 저장의 보안 및 거버넌스 측면을 직접적으로 해결하지는 않습니다.",
            "API 호출을 기록하기 위해 CloudTrail을 설정하는 것은 감사에 유용하지만, 로그 자체의 보안이나 거버넌스를 직접적으로 향상시키지는 않습니다.",
            "실시간 처리를 위한 CloudWatch Logs 구독 필터 구현은 분석에 유용하지만, 로그를 본질적으로 보호하거나 관리하지는 않습니다."
        ]
    },
    {
        "Question Number": "30",
        "Situation": "데이터 엔지니어링 팀은 다양한 출처에서 데이터를 추출하고 AWS에서 분석을 위해 변환하는 프로세스를 자동화하는 임무를 맡고 있습니다. 그들은 작업 간의 종속성을 처리하고 워크플로우의 모니터링 및 관리를 쉽게 할 수 있는 신뢰할 수 있는 스케줄링 메커니즘을 구현하고자 합니다.",
        "Question": "팀이 이러한 요구 사항을 달성하면서 강력한 스케줄링 및 종속성 관리를 제공할 수 있는 솔루션은 무엇입니까?",
        "Options": {
            "1": "Apache Airflow를 구현하여 작업 워크플로우를 위한 Directed Acyclic Graphs (DAGs)를 정의하고 이를 정기적으로 스케줄링합니다.",
            "2": "EC2 인스턴스에서 크론 작업을 설정하여 지정된 간격으로 데이터 추출 및 변환을 수행하는 스크립트를 실행합니다.",
            "3": "ETL 작업을 위해 AWS Glue를 활용하고 S3 이벤트를 기반으로 트리거를 구성하여 처리를 시작합니다.",
            "4": "Amazon EventBridge를 사용하여 시간 스케줄에 따라 데이터 처리를 위한 AWS Lambda 함수를 트리거하는 규칙을 생성합니다."
        },
        "Correct Answer": "Apache Airflow를 구현하여 작업 워크플로우를 위한 Directed Acyclic Graphs (DAGs)를 정의하고 이를 정기적으로 스케줄링합니다.",
        "Explanation": "Apache Airflow는 복잡한 워크플로우 관리를 위해 특별히 설계되었으며, 작업 간의 종속성을 명확하고 유지 관리하기 쉬운 방식으로 정의할 수 있습니다. 데이터 엔지니어링 작업에 적합한 강력한 스케줄링 기능과 모니터링 기능을 제공합니다.",
        "Other Options": [
            "AWS Glue는 주로 ETL 작업에 초점을 맞추고 있으며 S3 이벤트를 기반으로 작업을 트리거할 수 있지만, Apache Airflow와 같은 수준의 워크플로우 관리 및 종속성 처리를 제공하지 않습니다.",
            "EC2 인스턴스에서의 크론 작업은 종속성을 관리하거나 워크플로우의 시각적 표현을 제공하는 데 필요한 정교함이 부족하여 복잡한 데이터 엔지니어링 요구 사항에 적합하지 않습니다.",
            "Amazon EventBridge는 이벤트 기반 아키텍처에 적합하지만 Apache Airflow와 같은 방식으로 작업 종속성이나 워크플로우를 본질적으로 관리하지 않으므로 복잡한 데이터 처리 작업에 대한 효과가 제한됩니다."
        ]
    },
    {
        "Question Number": "31",
        "Situation": "한 금융 서비스 회사가 AWS에 저장된 데이터 세트에 대해 보다 구조화된 접근 방식을 활용하기 위해 데이터 관리 전략을 전환하고 있습니다. 이 회사는 다양한 분석 애플리케이션 전반에 걸쳐 데이터 발견 가능성과 거버넌스를 향상시키기 위해 포괄적인 데이터 카탈로그를 생성해야 합니다. 데이터 엔지니어링 팀은 이 데이터 카탈로그를 효과적으로 구축하고 관리하기 위해 AWS 서비스를 탐색하고 있습니다.",
        "Question": "데이터 엔지니어링 팀이 향상된 메타데이터 관리를 제공하고 다른 AWS 분석 서비스와 원활하게 통합되는 데이터 카탈로그를 생성하고 관리하기 위해 어떤 AWS 서비스를 사용해야 합니까?",
        "Options": {
            "1": "데이터 자산과 함께 메타데이터 파일을 유지하기 위한 사용자 지정 스크립트를 사용하는 Amazon S3",
            "2": "메타데이터 저장 및 Amazon Athena 및 Amazon Redshift와의 통합을 위한 AWS Glue Data Catalog",
            "3": "구조화된 데이터를 추가 메타데이터 속성과 함께 저장하기 위한 Amazon RDS",
            "4": "항목 카탈로그를 유지하고 메타데이터에 대한 빠른 쿼리를 수행하기 위한 Amazon DynamoDB"
        },
        "Correct Answer": "메타데이터 저장 및 Amazon Athena 및 Amazon Redshift와의 통합을 위한 AWS Glue Data Catalog",
        "Explanation": "AWS Glue Data Catalog는 메타데이터 관리를 위해 특별히 설계되었으며 Amazon Athena 및 Amazon Redshift와 같은 AWS 분석 서비스와 원활하게 통합됩니다. 이는 메타데이터에 대한 중앙 집중식 저장소를 제공하여 사용자가 데이터 세트를 효과적으로 발견하고 관리할 수 있도록 합니다.",
        "Other Options": [
            "Amazon RDS는 주로 관계형 데이터베이스 서비스이며 데이터 카탈로그를 유지하기 위한 용도로 설계되지 않았습니다. 구조화된 데이터를 저장할 수는 있지만 포괄적인 메타데이터 관리를 위한 전문 기능이 부족합니다.",
            "Amazon DynamoDB는 고속 데이터 접근 및 저장에 중점을 둔 NoSQL 데이터베이스 서비스입니다. 메타데이터 작업이나 분석 서비스와의 통합을 처리하도록 설계되지 않았기 때문에 데이터 카탈로그를 관리하는 데 필요한 기능을 제공하지 않습니다.",
            "메타데이터 파일을 유지하기 위해 사용자 지정 스크립트를 사용하는 Amazon S3를 사용하는 것은 수동적이고 비효율적인 데이터 카탈로그 접근 방식입니다. 이 방법은 AWS Glue Data Catalog가 제공하는 통합 및 자동화 기능이 부족하여 메타데이터 관리가 더 번거로워집니다."
        ]
    },
    {
        "Question Number": "32",
        "Situation": "한 의료 기관이 환자의 개인 식별 정보(PII)를 처리하는 데이터 처리 파이프라인을 개발하고 있습니다. 이 기관은 규제 요구 사항을 준수하기 위해 PII가 저장 중 및 전송 중 모두 암호화되도록 해야 합니다. 데이터 엔지니어는 데이터 보안 및 거버넌스를 위한 적절한 조치를 구현하는 임무를 맡고 있습니다.",
        "Question": "데이터 처리 및 저장 중 PII를 가장 잘 보호할 수 있는 솔루션은 무엇입니까?",
        "Options": {
            "1": "Amazon S3 서버 측 암호화를 AWS Key Management Service(AWS KMS)와 함께 사용하여 저장 중 데이터 암호화 및 전송 중 HTTPS를 활성화합니다.",
            "2": "암호화가 활성화된 Amazon DynamoDB에 PII를 저장하고 AWS Lambda를 사용하여 암호화 메커니즘 없이 데이터를 처리합니다.",
            "3": "암호화 없이 Amazon EFS에 데이터 저장을 구현하고 보안 그룹을 구성하여 데이터 접근을 제한합니다.",
            "4": "Amazon RDS를 사용하여 저장 중 암호화를 적용하고 네트워크를 통해 전송되기 전에 데이터 마스킹 기술을 적용합니다."
        },
        "Correct Answer": "Amazon S3 서버 측 암호화를 AWS Key Management Service(AWS KMS)와 함께 사용하여 저장 중 데이터 암호화 및 전송 중 HTTPS를 활성화합니다.",
        "Explanation": "Amazon S3 서버 측 암호화를 AWS KMS와 함께 사용하면 PII가 저장 중 암호화되며, HTTPS를 활성화하면 네트워크를 통한 데이터의 안전한 전송이 보장됩니다. 이 접근 방식은 민감한 정보를 보호하기 위한 모범 사례를 준수합니다.",
        "Other Options": [
            "암호화가 활성화된 DynamoDB에 PII를 저장하는 것은 좋은 시작이지만, 암호화 메커니즘 없이 데이터를 처리하는 것은 PII의 보안에 상당한 위험을 초래합니다.",
            "저장 중 암호화가 적용된 Amazon RDS를 사용하는 것은 유익하지만, 데이터 마스킹은 전송 중 데이터를 본질적으로 보호하지 않으므로 전송 중 취약하게 남아 있습니다.",
            "암호화 없이 Amazon EFS에 데이터 저장을 구현하는 것은 저장 중 PII를 보호하지 못하며, 보안 그룹을 단순히 구성하는 것만으로는 전송 중 데이터가 가로채이는 것을 방지할 수 없습니다."
        ]
    },
    {
        "Question Number": "33",
        "Situation": "데이터 엔지니어링 팀은 다양한 출처의 데이터를 처리하고 분석하기 위해 API 호출을 관리하는 책임이 있습니다. 그들은 데이터가 효율적으로 수집되고 변환되며 저장되도록 하면서 지연 시간과 비용을 최소화해야 합니다.",
        "Question": "팀이 데이터 처리를 위한 API 호출을 최적화하면서 최소한의 지연 시간과 비용을 보장할 수 있는 접근 방식은 무엇입니까?",
        "Options": {
            "1": "API 호출을 조정하고 상태 전환을 관리하기 위해 AWS Step Functions 워크플로우를 구현합니다.",
            "2": "API 호출 및 데이터 처리를 처리하는 사용자 지정 애플리케이션을 실행하기 위해 Amazon EC2 인스턴스를 배포합니다.",
            "3": "여러 데이터 소스와 효율적으로 상호 작용하는 GraphQL API를 구축하기 위해 AWS AppSync를 활용합니다.",
            "4": "Amazon API Gateway를 사용하여 AWS Lambda 함수를 호출하는 RESTful API를 생성합니다."
        },
        "Correct Answer": "Amazon API Gateway를 사용하여 AWS Lambda 함수를 호출하는 RESTful API를 생성합니다.",
        "Explanation": "Amazon API Gateway와 AWS Lambda를 사용하면 팀이 API 호출을 효율적으로 처리하기 위한 확장 가능하고 서버리스 아키텍처를 생성할 수 있습니다. 이 조합은 서버를 직접 관리할 필요가 없으므로 지연 시간과 운영 비용을 최소화할 수 있으며, 수요에 따라 자동으로 확장할 수 있습니다.",
        "Other Options": [
            "AWS Step Functions를 구현하면 복잡성이 추가되며, 단순히 API 호출을 처리하는 것보다는 워크플로우를 조정하는 데 더 적합하여 불필요한 지연을 초래할 수 있습니다.",
            "Amazon EC2 인스턴스를 배포하면 인프라를 관리해야 하므로 서버리스 솔루션에 비해 비용과 운영 오버헤드가 증가할 수 있습니다.",
            "AWS AppSync를 활용하는 것은 실시간 데이터 동기화 및 구독이 필요한 애플리케이션에 더 유리하지만, 단순한 API 데이터 처리 작업에는 필요하지 않을 수 있습니다."
        ]
    },
    {
        "Question Number": "34",
        "Situation": "금융 서비스 회사는 Amazon S3 버킷에 저장된 민감한 데이터가 암호화되어 있는지 확인해야 합니다. 데이터는 규정 준수를 위해 서로 다른 AWS 계정에서 접근할 때에도 암호화된 상태를 유지해야 합니다. 회사는 AWS에서 제공하는 다양한 암호화 옵션을 탐색하고 있습니다.",
        "Question": "회사가 암호화 키에 대한 제어를 유지하면서 AWS 계정 경계를 넘어 데이터가 암호화되도록 보장하기 위해 어떤 방법을 사용해야 합니까?",
        "Options": {
            "1": "Amazon S3 버킷 정책을 활용하여 데이터 접근을 제한합니다.",
            "2": "회사가 관리하는 AWS Key Management Service (KMS) 키를 사용하여 Amazon S3 서버 측 암호화를 사용합니다.",
            "3": "AWS 관리 키를 사용하여 Amazon S3 기본 암호화를 활성화합니다.",
            "4": "타사 암호화 라이브러리를 사용하여 Amazon S3 클라이언트 측 암호화를 구현합니다."
        },
        "Correct Answer": "회사가 관리하는 AWS Key Management Service (KMS) 키를 사용하여 Amazon S3 서버 측 암호화를 사용합니다.",
        "Explanation": "회사가 관리하는 AWS KMS 키를 사용하여 Amazon S3 서버 측 암호화를 사용하면 조직이 암호화 키에 대한 제어를 유지하면서 데이터가 서로 다른 AWS 계정에서 암호화된 상태를 유지할 수 있습니다. 이는 보안 및 규정 준수 요구 사항을 모두 충족합니다.",
        "Other Options": [
            "타사 라이브러리를 사용하여 클라이언트 측 암호화를 구현하는 것은 S3에서 데이터가 휴지 상태에서 암호화되도록 보장하지 않으며, 계정 간 키 관리가 복잡해질 수 있습니다.",
            "AWS 관리 키로 Amazon S3 기본 암호화를 활성화하는 것은 AWS가 키를 관리하므로 회사에 암호화 키에 대한 제어를 제공하지 않으며, 이는 규정 준수 요구 사항을 충족하지 못할 수 있습니다.",
            "S3 버킷 정책을 활용하면 데이터 접근을 제한할 수 있지만 암호화 기능을 제공하지 않아 데이터가 무단 접근에 취약해집니다."
        ]
    },
    {
        "Question Number": "35",
        "Situation": "데이터 엔지니어링 팀은 Amazon Kinesis Data Streams를 사용하여 실시간 분석 솔루션을 만드는 임무를 맡고 있습니다. 그들은 IoT 장치 및 웹 애플리케이션을 포함한 다양한 출처에서 데이터를 수집하고 있습니다. 팀은 수집 과정에서 일부 수신 데이터 레코드가 손실되고 있음을 발견했습니다. 그들은 이 데이터 손실의 가장 가능성이 높은 원인을 파악하려고 하고 있습니다.",
        "Question": "수집 과정에서 데이터 손실의 가장 가능성이 높은 이유는 무엇입니까?",
        "Options": {
            "1": "데이터 변환 논리에 오류가 있습니다.",
            "2": "데이터 레코드가 너무 느리게 처리되고 있습니다.",
            "3": "Kinesis 스트림이 샤드 한도에 도달했습니다.",
            "4": "Kinesis Data Streams API가 너무 자주 호출되고 있습니다."
        },
        "Correct Answer": "Kinesis 스트림이 샤드 한도에 도달했습니다.",
        "Explanation": "Kinesis 스트림이 샤드 한도에 도달하면 추가 데이터 레코드를 처리할 수 없게 되어 데이터 손실이 발생할 수 있습니다. 각 샤드는 고정 용량을 가지며, 이 한도를 초과하면 요청이 제한되고 레코드가 손실될 수 있습니다.",
        "Other Options": [
            "데이터 레코드가 너무 느리게 처리되면 백로그가 발생할 수 있지만, 레코드가 스트림에서 만료되지 않는 한 본질적으로 데이터 손실을 초래하지는 않습니다.",
            "데이터 변환 논리에 오류가 있으면 잘못된 레코드가 생성될 수 있지만, 수집 과정에서 수신 데이터 손실을 직접적으로 초래하지는 않습니다.",
            "Kinesis Data Streams API를 너무 자주 호출하면 일반적으로 제한이 발생하지만, 이 제한으로 인해 애플리케이션이 수신 레코드를 처리하지 못하지 않는 한 레코드가 손실되지는 않습니다."
        ]
    },
    {
        "Question Number": "36",
        "Situation": "데이터 엔지니어가 IoT 장치에서 수신 데이터를 처리하기 위해 AWS Serverless Application Model (AWS SAM)을 사용하여 서버리스 데이터 파이프라인을 개발하고 있습니다. 엔지니어는 운영 오버헤드를 최소화하면서 데이터 수집 및 변환을 효율적으로 처리할 수 있도록 솔루션을 보장해야 합니다.",
        "Question": "서버리스 데이터 파이프라인을 배포하기 위해 AWS SAM을 사용하는 이점은 무엇입니까?",
        "Options": {
            "1": "AWS SAM은 배포를 위한 내장된 모범 사례 및 패턴을 제공하여 서버리스 애플리케이션의 생성 및 관리를 간소화합니다.",
            "2": "AWS SAM은 그래픽 인터페이스를 사용하여 인프라 관리를 가능하게 하여 비기술 사용자도 애플리케이션을 쉽게 배포할 수 있도록 합니다.",
            "3": "AWS SAM은 Amazon RDS와 직접 통합되어 추가 구성 없이 관계형 데이터베이스에서 데이터 수집을 자동화합니다.",
            "4": "AWS SAM은 모든 AWS Lambda 함수가 CPU 사용량에 따라 자동으로 확장되도록 보장하여 성능을 크게 향상시킵니다."
        },
        "Correct Answer": "AWS SAM은 배포를 위한 내장된 모범 사례 및 패턴을 제공하여 서버리스 애플리케이션의 생성 및 관리를 간소화합니다.",
        "Explanation": "AWS SAM은 서버리스 애플리케이션을 정의하기 위한 프레임워크를 제공하고 패키징 및 배포 프로세스를 자동화하여 개발자가 인프라 관리가 아닌 애플리케이션 구축에 집중할 수 있도록 합니다.",
        "Other Options": [
            "AWS SAM은 CPU 사용량에 따라 AWS Lambda 함수를 자동으로 확장하지 않습니다. 대신 Lambda 함수는 수신 요청 수와 구성된 동시성 설정에 따라 확장됩니다.",
            "AWS SAM은 인프라 관리를 위한 그래픽 인터페이스를 제공하지 않으며, 주로 배포를 위한 구성 파일(템플릿)에서 리소스를 정의하는 명령줄 도구입니다.",
            "AWS SAM은 데이터 수집을 자동화하기 위해 Amazon RDS와 직접 통합되지 않습니다. 관계형 데이터베이스에서 데이터 수집은 일반적으로 AWS Database Migration Service (DMS)와 같은 추가 서비스가 필요합니다."
        ]
    },
    {
        "Question Number": "37",
        "Situation": "한 금융 서비스 회사가 다양한 제3자 API로부터 실시간 주식 시장 데이터를 분석하는 시스템을 개발하고 있습니다. 이들은 이러한 API로부터 데이터를 Amazon Kinesis Data Stream에 수집하여 추가 처리하고자 합니다. 회사는 모든 관련 데이터가 정확하게 캡처되면서 지연 시간과 운영 오버헤드를 최소화하는 솔루션이 필요합니다.",
        "Question": "다음 중 여러 API로부터 데이터를 Amazon Kinesis Data Stream에 수집하는 가장 효율적인 방법은 무엇입니까?",
        "Options": {
            "1": "Amazon EventBridge에 의해 트리거되는 AWS Lambda 함수를 활용하여 API를 폴링하고 Kinesis Data Stream에 데이터를 푸시합니다.",
            "2": "Amazon API Gateway를 구현하여 API를 노출하고 Kinesis Data Stream이 데이터를 직접 가져올 수 있도록 합니다.",
            "3": "Amazon EC2 인스턴스를 설정하여 API로부터 데이터를 지속적으로 가져오고 Kinesis Data Stream에 전송하는 사용자 지정 스크립트를 실행합니다.",
            "4": "AWS Step Functions를 사용하여 API의 폴링을 조정하고 Kinesis Data Stream에 데이터를 기록합니다."
        },
        "Correct Answer": "Amazon EventBridge에 의해 트리거되는 AWS Lambda 함수를 활용하여 API를 폴링하고 Kinesis Data Stream에 데이터를 푸시합니다.",
        "Explanation": "Amazon EventBridge에 의해 트리거되는 AWS Lambda 함수를 사용하면 운영 오버헤드를 최소화하는 서버리스 아키텍처를 구현할 수 있습니다. 이 접근 방식은 지정된 간격으로 API를 효율적으로 폴링하고 낮은 지연 시간과 확장성으로 Kinesis Data Stream에 데이터를 푸시할 수 있습니다.",
        "Other Options": [
            "Amazon EC2 인스턴스를 설정하는 것은 불필요한 운영 오버헤드와 복잡성을 초래합니다. EC2 인스턴스를 관리하고 가동 시간을 보장하며 수동으로 확장을 처리해야 하므로 서버리스 접근 방식보다 효율적이지 않습니다.",
            "API 호출을 조정하기 위해 AWS Step Functions를 사용하는 것은 실시간 데이터 수집에 대한 중요한 이점 없이 복잡성을 추가합니다. Step Functions는 지속적인 폴링 시나리오보다는 워크플로우에 더 적합하여 이 접근 방식이 덜 효율적입니다.",
            "Amazon API Gateway를 구현하여 API를 노출하는 것은 Kinesis Data Stream에 데이터를 수집하는 요구 사항을 직접적으로 해결하지 않습니다. Kinesis Data Streams는 API Gateway에서 직접 데이터를 가져올 수 없으며, 여전히 간극을 메우기 위해 Lambda 함수가 필요합니다."
        ]
    },
    {
        "Question Number": "38",
        "Situation": "한 금융 서비스 회사가 Amazon Redshift를 사용하여 대량의 거래 데이터를 분석하고자 합니다. 이들은 쿼리가 빠르게 결과를 반환하도록 보장하고 싶어하며, 특히 동일한 데이터에 대한 빈번한 분석을 위해서입니다. 또한 저장 비용 관리와 쿼리 성능 최적화에 대해 우려하고 있습니다.",
        "Question": "회사가 저장 비용을 효과적으로 관리하면서 반복 쿼리에 대한 성능을 향상시키기 위해 Amazon Redshift의 어떤 기능을 활용해야 합니까?",
        "Options": {
            "1": "결과 캐싱을 활용하여 이전에 계산된 결과를 신속하게 검색하고 쿼리 실행 시간을 줄입니다.",
            "2": "자동 진공 청소를 구현하여 삭제된 데이터로부터 저장 공간을 회수하여 전반적인 성능을 향상시킵니다.",
            "3": "쿼리 패턴을 최적화하지 않고 더 큰 쿼리 작업 부하를 처리하기 위해 클러스터의 노드 수를 증가시킵니다.",
            "4": "데이터 가용성과 쿼리 성능 향상을 위해 스냅샷의 교차 지역 복제를 활성화합니다."
        },
        "Correct Answer": "결과 캐싱을 활용하여 이전에 계산된 결과를 신속하게 검색하고 쿼리 실행 시간을 줄입니다.",
        "Explanation": "결과 캐싱을 사용하면 Amazon Redshift가 이전 쿼리의 결과를 메모리에 저장하여 기본 SQL 작업을 다시 실행할 필요 없이 반복 쿼리에 대해 서브 초 응답 시간을 가능하게 합니다. 이는 특히 빈번하고 유사한 쿼리가 있는 작업 부하에서 성능을 크게 향상시킵니다.",
        "Other Options": [
            "교차 지역 복제는 주로 재해 복구 및 데이터 가용성을 위해 사용되지만 반복 쿼리에 대한 쿼리 성능을 직접적으로 향상시키지는 않습니다.",
            "노드 수를 증가시키는 것은 더 큰 작업 부하를 처리하는 데 도움이 될 수 있지만 동일한 쿼리의 반복 실행에 대한 쿼리 성능을 최적화하지는 않습니다.",
            "자동 진공 청소는 저장 효율성을 유지하는 데 중요하지만 반복 쿼리의 성능에 직접적인 영향을 미치지 않습니다."
        ]
    },
    {
        "Question Number": "39",
        "Situation": "한 미디어 회사가 AWS Kinesis Video Streams를 사용하여 실시간 비디오 처리 시스템을 구현하고자 합니다. 이들은 여러 장치로부터 비디오를 수집하면서 보안 및 데이터 보존 정책 준수를 유지할 수 있는 솔루션을 보장하고자 합니다. 팀은 Kinesis Video Streams의 구성 요소와 기능을 더 잘 이해하여 정보에 기반한 결정을 내릴 필요가 있습니다.",
        "Question": "회사의 요구 사항을 가장 잘 지원하는 Kinesis Video Streams의 두 가지 기능은 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "스트리밍 효율성을 향상시키기 위한 비영속 메타데이터",
            "2": "비디오 저장 관리를 위한 사용자 지정 보존 기간",
            "3": "데이터 종속성 관리를 위한 단편화",
            "4": "여러 장치로부터 스트리밍을 위한 장치 연결성",
            "5": "강화된 보안을 위한 HLS를 사용한 비디오 재생"
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "여러 장치로부터 스트리밍을 위한 장치 연결성",
            "비디오 저장 관리를 위한 사용자 지정 보존 기간"
        ],
        "Explanation": "장치 연결성과 사용자 지정 보존 기간 기능은 회사의 요구에 매우 중요합니다. 장치 연결성은 수백만 개의 장치와의 연결 및 스트리밍을 가능하게 하여 실시간 수집에 필수적입니다. 사용자 지정 보존 기간은 회사가 준수 및 운영 요구 사항에 따라 비디오 스트림의 저장 기간을 구성할 수 있게 해줍니다.",
        "Other Options": [
            "HLS를 사용한 비디오 재생은 재생과 관련된 기능으로, 수집이나 저장 관리와 직접적으로 연결되지 않습니다.",
            "단편화는 비디오 데이터의 구조와 관련이 있으며, 회사의 수집 또는 보존 요구 사항을 해결하지 않습니다.",
            "비영속 메타데이터는 특정 단편에 유용하지만 장치 연결성이나 보존 관리의 광범위한 요구 사항에 기여하지 않습니다."
        ]
    },
    {
        "Question Number": "40",
        "Situation": "회사가 민감한 고객 데이터를 Amazon VPC에 저장할 새로운 애플리케이션을 배포하고 있습니다. 이 애플리케이션은 Amazon RDS 및 Amazon S3를 포함한 여러 서비스와 상호작용할 것입니다. 보안 팀은 애플리케이션이 무단 접근 및 데이터 유출로부터 안전하게 보호되며 데이터 거버넌스 정책을 준수하도록 요구합니다.",
        "Question": "팀이 강력한 보안 및 거버넌스를 보장하기 위해 구현해야 할 VPC 보안 네트워킹 개념은 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "VPC Flow Logs를 활성화하여 준수 감사용으로 VPC 내 모든 트래픽을 모니터링하고 기록합니다.",
            "2": "공용 서브넷에 배스천 호스트를 배포하여 개인 서브넷의 리소스에 대한 SSH 접근을 제공합니다.",
            "3": "특정 애플리케이션 요구 사항에 따라 인바운드 및 아웃바운드 트래픽을 제한하기 위해 보안 그룹을 구현합니다.",
            "4": "신뢰할 수 없는 IP 범위의 트래픽을 차단하면서 필요한 트래픽을 허용하기 위해 네트워크 ACL을 구성합니다.",
            "5": "모든 리소스에 대한 공용 접근을 가능하게 하여 더 나은 관리를 위해 NAT Gateway를 생성합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "신뢰할 수 없는 IP 범위의 트래픽을 차단하면서 필요한 트래픽을 허용하기 위해 네트워크 ACL을 구성합니다.",
            "특정 애플리케이션 요구 사항에 따라 인바운드 및 아웃바운드 트래픽을 제한하기 위해 보안 그룹을 구현합니다."
        ],
        "Explanation": "네트워크 ACL을 구성하고 보안 그룹을 구현하는 것은 VPC를 보호하기 위한 필수 조치입니다. 네트워크 ACL은 트래픽 흐름을 제어하여 서브넷 수준에서 보안 계층을 제공하며, 보안 그룹은 인스턴스에 대한 가상 방화벽 역할을 하여 규칙에 따라 트래픽을 세밀하게 제어합니다. 두 가지 모두 민감한 데이터를 보호하고 거버넌스 정책을 준수하는 데 중요합니다.",
        "Other Options": [
            "공용 접근을 위한 NAT Gateway를 생성하는 것은 리소스를 인터넷에 노출시켜 고객 데이터를 보호해야 하는 필요성과 모순되므로 민감한 애플리케이션에 적합한 보안 조치가 아닙니다.",
            "VPC Flow Logs를 활성화하는 것은 트래픽 모니터링을 위한 좋은 관행이지만 무단 접근을 직접적으로 방지하거나 트래픽 흐름을 제어하지 않으므로 주요 보안 조치가 아닙니다.",
            "배스천 호스트를 배포하면 개인 리소스에 대한 통제된 접근 지점을 제공할 수 있지만, 올바르게 관리되지 않으면 추가적인 복잡성과 잠재적인 보안 위험을 초래하므로 직접적인 네트워크 보안 조치에 비해 덜 바람직합니다."
        ]
    },
    {
        "Question Number": "41",
        "Situation": "데이터 엔지니어는 민감한 고객 데이터를 저장하는 Amazon S3 버킷에 대한 접근을 관리하는 임무를 맡고 있습니다. 현재 접근 방식은 S3 접근을 위한 AWS 관리 정책을 사용하고 있지만, 필요한 것보다 더 넓은 권한을 부여하고 있습니다. 엔지니어는 버킷 내 특정 접두사에 대해 'GetObject' 작업만을 제한하는 사용자 지정 IAM 정책을 생성해야 합니다.",
        "Question": "데이터 엔지니어가 보안 요구 사항을 충족하기 위해 이 사용자 지정 IAM 정책을 생성하는 가장 좋은 방법은 무엇입니까?",
        "Options": {
            "1": "모든 S3 버킷을 나열할 수 있는 권한을 가진 IAM 역할을 생성하고 이를 애플리케이션에 연결합니다.",
            "2": "버킷과 접두사를 지정하는 리소스 ARN과 함께 s3:GetObject를 허용하는 새로운 IAM 정책을 생성합니다.",
            "3": "기존 관리 정책을 연결하고 모든 다른 작업에 대한 거부 문을 추가합니다.",
            "4": "AWS CLI를 사용하여 관리 정책을 수정하여 권한을 제한합니다."
        },
        "Correct Answer": "버킷과 접두사를 지정하는 리소스 ARN과 함께 s3:GetObject를 허용하는 새로운 IAM 정책을 생성합니다.",
        "Explanation": "필요한 작업과 리소스에 대해 특별히 새로운 IAM 정책을 생성하는 것은 접근이 필요한 것만 부여되도록 보장하는 최선의 방법입니다. 이 접근 방식은 최소 권한 원칙을 준수합니다.",
        "Other Options": [
            "관리 정책을 연결하고 거부 문을 추가하는 것은 예상대로 작동하지 않을 수 있습니다. IAM 정책은 거부 문이 관리 정책의 허용 문을 무시하지 않는 방식으로 평가되기 때문입니다.",
            "관리 정책을 수정하는 것은 불가능합니다. 관리 정책은 AWS에 의해 관리되며, 이를 변경하면 해당 정책을 사용하는 모든 사용자와 역할에 영향을 미칠 수 있으므로 상황의 특정 요구를 충족하지 않습니다.",
            "모든 S3 버킷을 나열할 수 있는 권한을 가진 IAM 역할을 생성하는 것은 과도한 권한을 제공하며 특정 버킷 및 접두사 내에서 'GetObject' 작업에 대한 접근을 제한할 필요를 구체적으로 해결하지 않습니다."
        ]
    },
    {
        "Question Number": "42",
        "Situation": "데이터 엔지니어는 다양한 IoT 장치에서 스트리밍 데이터를 처리하기 위한 데이터 수집 시스템을 설계하는 임무를 맡고 있습니다. 이 솔루션은 높은 가용성과 내결함성을 보장하면서 실시간으로 레코드를 처리할 수 있는 능력을 유지해야 합니다. 엔지니어는 이를 위해 Amazon Kinesis Data Streams를 사용하는 것을 고려하고 있습니다.",
        "Question": "Kinesis Data Streams에서 파티션 키를 사용하는 주요 이점은 무엇입니까?",
        "Options": {
            "1": "파티션 키는 AWS CloudWatch를 통해 샤드 수준 메트릭을 모니터링하는 데 도움을 줍니다.",
            "2": "파티션 키는 동일한 키를 가진 레코드가 동일한 샤드로 전송되어 순서대로 처리되도록 보장합니다.",
            "3": "파티션 키는 들어오는 데이터의 양에 따라 샤드를 자동으로 확장할 수 있게 합니다.",
            "4": "파티션 키는 보안 목적으로 스트림의 모든 레코드를 자동으로 암호화하는 데 사용됩니다."
        },
        "Correct Answer": "파티션 키는 동일한 키를 가진 레코드가 동일한 샤드로 전송되어 순서대로 처리되도록 보장합니다.",
        "Explanation": "Kinesis Data Streams에서 파티션 키를 사용하면 동일한 키를 공유하는 레코드가 동일한 샤드로 라우팅될 수 있습니다. 이는 데이터의 순서를 유지하는 데 중요하며, 특히 관련되거나 순차적인 이벤트를 처리할 때 중요합니다.",
        "Other Options": [
            "파티션 키는 레코드를 자동으로 암호화하지 않습니다. 암호화는 AWS KMS에 의해 처리되며 특정 구성이 필요합니다.",
            "파티션 키는 샤드를 확장하지 않습니다. 샤드의 수는 예상 처리량에 따라 정의되어야 하며 수동으로 조정할 수 있습니다.",
            "파티션 키는 모니터링과 직접적으로 관련이 없습니다. 샤드 수준 메트릭은 AWS CloudWatch를 통해 독립적으로 모니터링됩니다."
        ]
    },
    {
        "Question Number": "43",
        "Situation": "금융 서비스 회사가 민감한 고객 데이터를 저장하기 위해 Amazon S3 데이터 레이크를 구현하고 있습니다. 이 조직은 특정 데이터 세트에 접근할 수 있는 권한이 있는 인원만 접근할 수 있도록 엄격한 보안 조치를 시행해야 합니다. 그들은 사용자 역할에 기반한 세분화된 접근 제어를 제공하고 데이터 거버넌스 정책 준수를 보장하는 솔루션이 필요합니다.",
        "Question": "AWS 리소스에 대한 역할 기반 접근 제어를 Amazon S3에 저장된 리소스에 효과적으로 구현하면서 거버넌스 정책을 준수하는 가장 효과적인 방법은 무엇입니까?",
        "Options": {
            "1": "AWS Resource Access Manager (RAM)를 활용하여 역할 기반 권한을 구현하지 않고 다른 AWS 계정과 S3 리소스를 공유합니다.",
            "2": "IP 주소 및 네트워크 위치에 따라 접근을 제한하기 위해 Amazon S3 버킷 정책을 활성화하되 사용자 역할은 고려하지 않습니다.",
            "3": "Amazon Macie를 구현하여 S3의 데이터를 분류하고 데이터 민감도에 따라 접근 제어를 관리하되 사용자 역할은 고려하지 않습니다.",
            "4": "AWS Identity and Access Management (IAM) 역할 및 정책을 사용하여 직무 기능에 따라 S3 버킷 접근을 위한 사용자 권한을 정의합니다."
        },
        "Correct Answer": "AWS Identity and Access Management (IAM) 역할 및 정책을 사용하여 직무 기능에 따라 S3 버킷 접근을 위한 사용자 권한을 정의합니다.",
        "Explanation": "AWS IAM 역할 및 정책을 사용하면 조직 내에서 특정 S3 리소스에 접근할 수 있는 사람을 정확하게 제어할 수 있습니다. 이 방법은 역할 기반 접근 제어의 필요성과 일치하며 데이터 거버넌스 정책 준수를 보장합니다.",
        "Other Options": [
            "S3 버킷 정책을 활성화하면 접근을 제한할 수 있지만, 이러한 정책은 사용자 역할이 아닌 네트워크 위치에 의존하므로 역할 기반 접근 제어에 필요한 세분화된 제어를 제공하지 않습니다.",
            "AWS Resource Access Manager (RAM)는 리소스 공유를 용이하게 하지만, 이 시나리오에서 사용자 역할에 따라 권한을 관리하는 데 필수적인 역할 기반 접근 제어를 본질적으로 제공하지 않습니다.",
            "Amazon Macie는 S3에서 민감한 데이터를 분류하는 데 효과적이지만, 역할 기반 접근 제어 메커니즘을 제공하지 않습니다. 이는 데이터 보안에 중점을 두고 있으며 사용자 접근 관리를 역할에 따라 수행하지 않습니다."
        ]
    },
    {
        "Question Number": "44",
        "Situation": "데이터 엔지니어링 팀이 Amazon S3에 저장된 대규모 데이터 세트를 분석하기 위해 Amazon Athena를 사용하고 있습니다. 그들은 Apache Spark가 포함된 Athena 노트북을 활용하여 복잡한 데이터 변환 및 탐색적 데이터 분석을 수행하고자 합니다. 팀은 특히 성능 및 비용 효율성을 위해 Spark 작업을 최적화하는 데 관심이 있습니다.",
        "Question": "데이터 엔지니어링 팀이 Athena 노트북에서 Spark 작업의 성능을 최적화하기 위해 어떤 전략을 구현해야 합니까?",
        "Options": {
            "1": "모든 Spark 작업에 대해 단일 대형 인스턴스 유형을 활용하여 리소스 관리를 단순화합니다.",
            "2": "데이터 세트의 파티션 수를 늘려 병렬 처리 능력을 향상시킵니다.",
            "3": "작은 데이터 세트에 대해 브로드캐스트 조인을 사용하여 셔플링을 줄이고 실행 속도를 향상시킵니다.",
            "4": "효율성을 위해 Spark에서 지연 평가를 활성화하여 필요할 때까지 계산을 연기합니다."
        },
        "Correct Answer": "작은 데이터 세트에 대해 브로드캐스트 조인을 사용하여 셔플링을 줄이고 실행 속도를 향상시킵니다.",
        "Explanation": "작은 데이터 세트에 대해 브로드캐스트 조인을 사용하면 Spark가 네트워크를 통한 데이터의 비싼 셔플링을 피할 수 있어 조인 프로세스가 크게 빨라지고 전체 작업 성능이 향상됩니다.",
        "Other Options": [
            "파티션 수를 늘리면 병렬 처리를 개선할 수 있지만, 신중하게 수행하지 않으면 오버헤드가 발생할 수 있습니다. 이는 브로드캐스트 조인을 사용하는 것만큼 직접적으로 Spark 작업 성능 최적화를 다루지 않습니다.",
            "지연 평가를 활성화하는 것은 Spark에서 좋은 관행이지만, 특정 작업 실행에 대한 성능을 본질적으로 최적화하지는 않습니다. 이는 계산이 수행되는 시점에 관한 것이지 얼마나 효율적으로 실행되는지에 관한 것이 아닙니다.",
            "단일 대형 인스턴스 유형을 활용하면 리소스 경합과 비효율성이 발생할 수 있습니다. 일반적으로 병렬 처리와 유연성을 활용하기 위해 더 작은 인스턴스 클러스터를 사용하는 것이 더 좋습니다."
        ]
    },
    {
        "Question Number": "45",
        "Situation": "의료 기관이 여러 AWS 서비스에서 환자 데이터를 관리하고 있습니다. 그들은 HIPAA 준수와 같은 규제 요구 사항에 따라 데이터를 분류해야 합니다. 이 조직은 민감한 환자 정보가 적절하게 저장되고 접근되도록 하면서 비용을 최소화하고 확장성을 보장하는 데 집중하고 있습니다.",
        "Question": "조직이 규정 준수 요구 사항을 충족하면서 민감한 환자 데이터의 저장을 효과적으로 분류하고 관리하기 위해 어떤 접근 방식을 취해야 합니까?",
        "Options": {
            "1": "클라이언트 측 암호화가 포함된 Amazon S3를 사용하고 접근 제어를 위해 AWS Identity and Access Management (IAM) 정책을 설정합니다.",
            "2": "투명 데이터 암호화(TDE)가 포함된 Amazon RDS에 데이터를 저장하고 보안 그룹을 사용하여 접근을 제한합니다.",
            "3": "Amazon DynamoDB를 활용하여 데이터가 저장될 때 암호화하고 IAM 역할을 구현하여 사용자 권한을 관리합니다.",
            "4": "서버 측 암호화가 포함된 Amazon S3를 활용하고 버킷 정책을 구성하여 민감한 데이터에 대한 접근을 제한합니다."
        },
        "Correct Answer": "서버 측 암호화가 포함된 Amazon S3를 활용하고 버킷 정책을 구성하여 민감한 데이터에 대한 접근을 제한합니다.",
        "Explanation": "서버 측 암호화가 포함된 Amazon S3를 사용하면 데이터가 저장될 때 암호화되어 민감한 정보를 보호하는 데 필수적입니다. 또한, 버킷 정책을 구성하면 세밀한 접근 제어가 가능하여 권한이 있는 사용자만 민감한 환자 데이터에 접근할 수 있도록 하여 규정 준수 요구 사항을 효과적으로 충족합니다.",
        "Other Options": [
            "클라이언트 측 암호화가 포함된 Amazon S3를 사용하는 것은 저장 수준에서 포괄적인 접근 제어를 제공하지 않으며, 이는 규정 준수에 중요합니다. 클라이언트 측 암호화는 클라이언트가 키와 권한을 관리해야 하므로 관리 오버헤드가 증가합니다.",
            "투명 데이터 암호화(TDE)가 포함된 Amazon RDS에 데이터를 저장하는 것은 관계형 데이터에 적합하지만, 의료 시나리오에서 일반적으로 발생하는 대량의 비구조적 데이터에 대해 가장 비용 효율적인 솔루션이 아닐 수 있습니다. 또한, 보안 그룹만으로는 접근 제어에 충분한 세분화를 제공하지 않을 수 있습니다.",
            "Amazon DynamoDB를 활용하여 저장 시 암호화하는 것은 실행 가능한 옵션이지만, 조직이 주로 대형 이진 파일이나 비구조적 데이터를 다룬다면 S3가 데이터 저장에 더 적합할 수 있습니다."
        ]
    },
    {
        "Question Number": "46",
        "Situation": "금융 서비스 회사의 데이터 엔지니어는 AWS 서비스에 대한 모든 API 호출을 추적하여 규정 준수 및 보안을 보장해야 합니다. 회사는 리소스에 대한 작업의 상세 로그를 요구하며, 이 데이터를 감사 목적으로 분석하고자 합니다.",
        "Question": "데이터 엔지니어가 API 호출을 추적하고 감사 목적으로 로그를 기록하기 위해 어떤 AWS 서비스를 사용해야 합니까?",
        "Options": {
            "1": "AWS Lambda",
            "2": "Amazon CloudWatch",
            "3": "AWS Config",
            "4": "AWS CloudTrail"
        },
        "Correct Answer": "AWS CloudTrail",
        "Explanation": "AWS CloudTrail은 AWS 서비스에 대한 API 호출을 기록하고 모니터링하도록 특별히 설계되어 있으며, 리소스에 대한 작업의 포괄적인 감사 추적을 제공합니다. 이는 규정 준수 및 감사 요구 사항에 가장 적합한 선택입니다.",
        "Other Options": [
            "AWS Config는 주로 AWS 리소스의 구성을 평가, 감사 및 평가하는 데 사용되며 API 호출에 대한 상세 로그를 제공하지 않습니다.",
            "Amazon CloudWatch는 주로 시스템 메트릭 및 애플리케이션 로그를 모니터링하고 기록하는 데 중점을 두지만, AWS 서비스에 대한 API 호출을 직접 추적하지는 않습니다.",
            "AWS Lambda는 이벤트에 응답하여 코드를 실행하는 컴퓨팅 서비스로, API 호출을 추적하거나 로그를 기록하도록 설계되지 않았습니다."
        ]
    },
    {
        "Question Number": "47",
        "Situation": "금융 서비스 회사는 역사적 거래 기록을 포함한 대규모 데이터 세트를 관리하고 있습니다. 회사는 오래된 데이터가 적절히 아카이브되면서도 규정 준수 목적으로 접근할 수 있도록 데이터 수명 주기 정책을 구현하여 저장 비용을 최적화하려고 합니다.",
        "Question": "회사의 요구 사항에 따라 데이터 수명 주기를 기반으로 저장 비용을 최적화하는 데 가장 적합한 전략은 무엇입니까?",
        "Options": {
            "1": "정해진 기간 후에 오래된 데이터를 Amazon S3 Glacier로 전송하는 수동 프로세스를 구현합니다.",
            "2": "Amazon S3 Intelligent-Tiering을 활용하여 변경되는 접근 패턴에 따라 데이터를 자동으로 접근 계층 간에 이동시킵니다.",
            "3": "모든 데이터를 Amazon S3 Standard 스토리지 클래스로 저장하여 항상 빠르게 접근할 수 있도록 합니다.",
            "4": "모든 데이터를 Amazon RDS에 아카이브하여 빠른 검색 시간을 유지하는 데 중점을 둡니다."
        },
        "Correct Answer": "Amazon S3 Intelligent-Tiering을 활용하여 변경되는 접근 패턴에 따라 데이터를 자동으로 접근 계층 간에 이동시킵니다.",
        "Explanation": "Amazon S3 Intelligent-Tiering은 접근 패턴이 변경될 때 데이터를 자동으로 두 개의 접근 계층 간에 이동시켜 비용 최적화를 위해 설계되었습니다. 이 기능은 회사가 저장 비용을 절감하면서 자주 사용되는 데이터에 대한 효율적인 접근을 보장할 수 있도록 합니다.",
        "Other Options": [
            "모든 데이터를 Amazon S3 Standard 스토리지 클래스로 저장하는 것은 비용을 최적화하지 않으며, 이는 드물게 접근되는 데이터에 비해 더 비쌉니다.",
            "모든 데이터를 Amazon RDS에 아카이브하는 것은 대규모 데이터 세트에 대해 비용 효율적이지 않으며, RDS는 주로 거래 데이터에 사용되며 S3보다 더 높은 비용이 발생합니다.",
            "오래된 데이터를 Amazon S3 Glacier로 전송하는 수동 프로세스를 구현하는 것은 비효율적일 수 있으며, 자동화된 솔루션에 비해 운영 오버헤드가 증가할 수 있습니다."
        ]
    },
    {
        "Question Number": "48",
        "Situation": "금융 서비스 회사는 온프레미스 데이터 수집 워크플로우를 클라우드로 마이그레이션하고 있습니다. 이 조직은 관계형 데이터베이스와 NoSQL 데이터베이스를 포함한 다양한 데이터베이스와 데이터 소스를 사용하고 있습니다. 데이터 엔지니어는 AWS 환경에서 컨테이너 사용을 최적화하여 효율적인 데이터 수집 및 변환을 보장해야 합니다. 엔지니어는 높은 성능과 확장성을 유지하면서 이러한 다양한 데이터 소스에 연결해야 합니다.",
        "Question": "어떤 솔루션이 엔지니어가 데이터 수집을 위한 컨테이너 사용을 최적화하고 AWS 클라우드에서 다양한 데이터 소스에 연결하는 데 가장 적합합니까?",
        "Options": {
            "1": "Amazon EKS를 사용하여 REST API를 사용하여 데이터 소스에만 연결하는 컨테이너를 조정합니다.",
            "2": "Amazon ECS와 Fargate를 배포하여 JDBC를 통해 데이터 소스에 연결하는 컨테이너화된 애플리케이션을 실행합니다.",
            "3": "네트워킹에 대한 제어를 높이기 위해 EC2 시작 유형으로 Amazon ECS를 구현하여 ODBC를 통해 직접 데이터베이스에 연결합니다.",
            "4": "AWS Lambda를 활용하여 여러 데이터 소스에 연결하는 컨테이너화된 기능으로 실시간으로 데이터를 처리합니다."
        },
        "Correct Answer": "Amazon ECS와 Fargate를 배포하여 JDBC를 통해 데이터 소스에 연결하는 컨테이너화된 애플리케이션을 실행합니다.",
        "Explanation": "Amazon ECS와 Fargate를 배포하면 데이터 엔지니어가 서버를 관리하지 않고도 컨테이너화된 애플리케이션을 실행할 수 있으며, JDBC를 사용하여 다양한 데이터베이스에 연결할 수 있는 기능을 제공합니다. 이 접근 방식은 자원 활용을 최적화하고 수요에 따라 자동으로 확장됩니다.",
        "Other Options": [
            "REST API를 사용하여 데이터 소스에만 연결하는 컨테이너를 조정하기 위해 Amazon EKS를 사용하는 것은 연결 옵션을 제한하고 데이터 수집에 필요한 다양한 데이터베이스 유형을 효과적으로 활용하지 못할 수 있으므로 덜 적합합니다.",
            "EC2 시작 유형으로 Amazon ECS를 구현하면 네트워킹에 대한 더 많은 제어를 제공하지만, 기본 인프라를 관리해야 하므로 확장을 복잡하게 하고 운영 오버헤드를 증가시킬 수 있습니다.",
            "여러 데이터 소스에 연결하기 위해 AWS Lambda를 활용하는 것은 이벤트 기반 아키텍처를 위해 설계되었기 때문에 높은 성능 요구에 이상적이지 않을 수 있으며, 콜드 스타트 시간과 실행 기간에 제한이 있을 수 있습니다."
        ]
    },
    {
        "Question Number": "49",
        "Situation": "한 금융 서비스 회사가 AWS를 활용하여 실시간 분석 및 보고를 위한 방대한 데이터 세트를 관리하고 있습니다. 이 회사는 데이터 스트리밍을 위해 Amazon Kinesis를 사용하고 처리된 데이터를 Amazon S3에 저장합니다. 통찰력을 얻기 위해 이 회사는 Amazon RDS 인스턴스와 Amazon Redshift 클러스터에 저장된 정적 데이터 세트와 함께 이 데이터를 분석해야 합니다. 목표는 데이터 무결성을 유지하고 지연 시간을 최소화하면서 결합된 데이터 세트에 대해 복잡한 분석 쿼리를 수행하는 것입니다.",
        "Question": "데이터 엔지니어링 팀이 실시간 분석을 위해 Amazon S3, Amazon RDS 및 Amazon Redshift의 데이터를 원활하게 분석하고 결합하기 위해 어떤 AWS 서비스를 사용해야 합니까?",
        "Options": {
            "1": "Amazon QuickSight를 구현하여 데이터 소스에 연결하고 시각화를 위한 대시보드를 생성합니다.",
            "2": "Amazon Athena를 활용하여 Amazon S3의 데이터에 대해 SQL 쿼리를 직접 실행하고 연합 쿼리를 통해 RDS 및 Redshift 데이터를 접근합니다.",
            "3": "AWS Glue를 사용하여 RDS 및 Redshift 데이터를 Amazon S3로 ETL한 후 Amazon Athena로 분석합니다.",
            "4": "Amazon EMR을 배포하여 Amazon S3 및 RDS의 데이터를 처리한 후 결과를 Amazon Redshift에 저장하여 쿼리합니다."
        },
        "Correct Answer": "Amazon Athena를 활용하여 Amazon S3의 데이터에 대해 SQL 쿼리를 직접 실행하고 연합 쿼리를 통해 RDS 및 Redshift 데이터를 접근합니다.",
        "Explanation": "Amazon Athena는 연합 쿼리를 지원하여 Amazon S3 및 Amazon RDS, Amazon Redshift와 같은 다른 데이터 소스의 데이터를 이동할 필요 없이 쿼리할 수 있습니다. 이 접근 방식은 데이터 이동과 지연 시간을 최소화하므로 실시간 분석에 효율적입니다.",
        "Other Options": [
            "Amazon QuickSight는 주로 시각화 도구이며 다양한 소스의 데이터 세트를 결합하는 데 필요한 실제 데이터 분석이나 복잡한 쿼리를 수행하지 않습니다.",
            "AWS Glue를 ETL에 사용하는 것은 데이터를 먼저 S3로 이동해야 하므로 중복이나 이동 없이 데이터를 분석하겠다는 요구 사항과 모순됩니다.",
            "Amazon EMR을 배포하면 데이터를 효과적으로 처리할 수 있지만, 결과를 Amazon Redshift에 준비하고 저장해야 하므로 복잡성이 추가되고 지연 시간이 발생할 수 있습니다."
        ]
    },
    {
        "Question Number": "50",
        "Situation": "한 소매 회사가 Amazon S3에 저장된 대규모 데이터 세트를 SQL 쿼리를 사용하여 분석해야 합니다. 이들은 Amazon Athena를 사용할 때 쿼리가 성능과 비용 효율성을 최적화되기를 원합니다. 또한 회사는 쿼리 결과가 조직 내 다양한 팀에서 보고 및 분석을 위해 쉽게 접근 가능해야 합니다.",
        "Question": "회사가 Amazon Athena 쿼리를 최적화하기 위해 구현해야 할 기술은 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "Amazon S3의 데이터를 공통 쿼리 패턴에 따라 파티셔닝합니다.",
            "2": "미래 쿼리를 빠르게 하기 위해 Amazon RDS에 쿼리 결과를 저장합니다.",
            "3": "모든 쿼리를 필터링 없이 실행하여 포괄적인 결과를 보장합니다.",
            "4": "AWS Glue Data Catalog를 활용하여 메타데이터와 스키마를 관리합니다.",
            "5": "데이터를 Parquet 또는 ORC와 같은 열 형식으로 변환합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Amazon S3의 데이터를 공통 쿼리 패턴에 따라 파티셔닝합니다.",
            "데이터를 Parquet 또는 ORC와 같은 열 형식으로 변환합니다."
        ],
        "Explanation": "Amazon S3의 데이터를 파티셔닝하면 Athena가 관련 데이터만 스캔할 수 있어 쿼리 성능이 크게 향상되고 비용이 절감됩니다. 데이터를 Parquet 또는 ORC와 같은 열 형식으로 변환하면 Athena가 전체 행이 아닌 필요한 열만 읽을 수 있어 성능과 효율성이 더욱 향상됩니다.",
        "Other Options": [
            "Amazon RDS에 쿼리 결과를 저장하는 것은 Athena 쿼리 자체를 최적화하지 않습니다. 대신 추가적인 복잡성과 비용이 발생하며 Athena의 성능을 직접적으로 개선하지 않습니다.",
            "AWS Glue Data Catalog를 사용하는 것은 메타데이터 관리에 도움이 될 수 있지만 쿼리 성능이나 비용에 직접적인 영향을 미치지 않습니다. Glue의 이점은 데이터 조직 및 스키마 관리와 관련이 있으며 쿼리 최적화와는 거리가 있습니다.",
            "모든 쿼리를 필터링 없이 실행하면 불필요한 데이터 스캔이 발생하여 비용이 증가하고 실행 시간이 길어집니다. 이는 성능과 비용을 최적화하려는 목표와 모순됩니다."
        ]
    },
    {
        "Question Number": "51",
        "Situation": "한 데이터 엔지니어가 분석 목적으로 Amazon S3에서 Amazon Redshift로 대규모 데이터 세트를 이동해야 합니다. 데이터 세트는 S3에 CSV 형식으로 저장되어 있으며, 데이터 엔지니어는 비용을 최소화하고 성능을 최적화하면서 효율적인 로딩 및 언로딩 프로세스를 보장하고자 합니다.",
        "Question": "Amazon Redshift에서 Amazon S3로 데이터를 언로드하는 가장 효율적인 방법은 무엇입니까?",
        "Options": {
            "1": "AWS Data Pipeline을 활용하여 Amazon Redshift에서 Amazon S3로 데이터를 정기적으로 전송하는 워크플로우를 생성합니다.",
            "2": "Amazon Redshift에서 SELECT 문을 수행하고 AWS SDK를 사용하여 결과를 S3 버킷에 수동으로 작성합니다.",
            "3": "Amazon Redshift의 COPY 명령을 사용하여 Amazon S3에서 데이터를 읽고 다른 리전의 S3 버킷으로 내보냅니다.",
            "4": "Amazon Redshift의 UNLOAD 명령을 사용하여 지정된 형식으로 데이터를 Amazon S3로 직접 내보내고 병렬 처리를 적용하여 작업 속도를 높입니다."
        },
        "Correct Answer": "Amazon Redshift의 UNLOAD 명령을 사용하여 지정된 형식으로 데이터를 Amazon S3로 직접 내보내고 병렬 처리를 적용하여 작업 속도를 높입니다.",
        "Explanation": "UNLOAD 명령은 Amazon Redshift에서 Amazon S3로 데이터를 효율적으로 내보내기 위해 특별히 설계되었습니다. 병렬 처리를 지원하여 성능을 크게 향상시키고 대규모 데이터 세트를 언로드하는 데 걸리는 시간을 줄입니다.",
        "Other Options": [
            "SELECT 문을 수행하고 결과를 S3에 수동으로 작성하는 것은 비효율적이고 시간이 많이 소요되며, 특히 대규모 데이터 세트의 경우 더욱 그렇습니다. 이 방법은 Redshift에서 데이터를 언로드하기 위한 최적화 기능을 활용하지 않습니다.",
            "COPY 명령은 S3에서 Amazon Redshift로 데이터를 로드하는 데 사용되며, S3로 데이터를 언로드하는 데는 사용되지 않습니다. 따라서 이 옵션은 주어진 요구 사항에 적용되지 않습니다.",
            "AWS Data Pipeline을 사용하여 정기적으로 전송하는 것은 불필요한 복잡성을 추가하며 UNLOAD 명령의 성능 이점을 제공하지 않을 수 있습니다. 또한 일회성 또는 임시 데이터 언로드에 대해 덜 효율적입니다."
        ]
    },
    {
        "Question Number": "52",
        "Situation": "데이터 엔지니어링 팀은 AWS 서비스에 대한 모든 접근이 준수 및 감사 목적으로 기록되도록 해야 합니다. 그들은 접근 로그를 효율적으로 모니터링하고 검토할 수 있는 솔루션을 구현하고자 합니다.",
        "Question": "팀이 AWS 서비스에 대한 접근을 기록하기 위해 사용할 수 있는 서비스 조합은 무엇입니까? (두 개 선택)",
        "Options": {
            "1": "AWS CloudTrail을 활성화하여 AWS 계정 내에서 이루어진 모든 API 호출을 기록합니다.",
            "2": "Amazon GuardDuty를 설정하여 계정 전반에 걸쳐 악의적인 활동을 분석하고 기록합니다.",
            "3": "Amazon CloudWatch를 구현하여 시스템 메트릭 및 이벤트를 모니터링하고 기록합니다.",
            "4": "AWS Config를 사용하여 AWS 리소스의 구성 변경 사항을 시간에 따라 추적합니다.",
            "5": "AWS Identity and Access Management (IAM)를 활용하여 사용자 로그인 이벤트를 기록합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "AWS CloudTrail을 활성화하여 AWS 계정 내에서 이루어진 모든 API 호출을 기록합니다.",
            "AWS Config를 사용하여 AWS 리소스의 구성 변경 사항을 시간에 따라 추적합니다."
        ],
        "Explanation": "AWS CloudTrail을 활성화하는 것은 모든 API 호출을 기록하는 데 필수적이며, 누가 어떤 서비스에 언제 접근했는지에 대한 포괄적인 뷰를 제공합니다. AWS Config를 사용하면 AWS 리소스의 구성 변경 사항을 추적하여 시간이 지남에 따라 리소스의 상태에 대한 통찰력을 제공하며, 이는 거버넌스 및 준수에도 중요합니다.",
        "Other Options": [
            "Amazon CloudWatch는 주로 시스템 메트릭 및 이벤트를 모니터링하고 기록하는 데 사용되지만, API 접근을 구체적으로 기록하지 않으므로 AWS 서비스 접근 기록이라는 특정 요구 사항에 덜 적합합니다.",
            "Amazon GuardDuty는 악의적인 활동 및 무단 행동을 모니터링하는 보안 서비스이지만, AWS 서비스에 대한 일반 접근을 위한 포괄적인 기록 솔루션을 제공하지 않습니다.",
            "AWS IAM은 사용자 로그인 이벤트를 기록하지 않으며, 접근 권한을 관리하는 데 사용됩니다. IAM은 기록 관행을 시행하도록 구성할 수 있지만, 자체적으로 접근을 직접 기록하지는 않습니다."
        ]
    },
    {
        "Question Number": "53",
        "Situation": "미디어 회사는 Amazon S3에 대용량 비디오 파일을 저장합니다. 비디오는 업로드 후 첫 달 동안 자주 접근되지만, 이후 접근이 크게 감소합니다. 회사는 비디오가 계속 접근 가능하도록 하면서 저장 비용을 최적화하려고 합니다. 그들은 수동 개입 없이 접근 패턴 변화에 자동으로 조정할 수 있는 솔루션을 원합니다.",
        "Question": "회사가 비디오 파일의 접근성을 유지하면서 비용을 최적화하기 위해 사용해야 할 S3 저장 클래스는 무엇입니까?",
        "Options": {
            "1": "S3 Glacier Flexible Retrieval",
            "2": "S3 One Zone-IA",
            "3": "S3 Standard-IA",
            "4": "S3 Intelligent-Tiering"
        },
        "Correct Answer": "S3 Intelligent-Tiering",
        "Explanation": "S3 Intelligent-Tiering은 접근 패턴이 변경될 때 데이터 간에 두 개의 접근 계층 간에 자동으로 이동하므로, 회사의 초기에는 자주 접근하고 이후에는 덜 접근하는 요구 사항에 이상적입니다. 성능에 영향을 주지 않으면서 비용을 최적화하므로 이 사용 사례에 가장 적합한 선택입니다.",
        "Other Options": [
            "S3 Standard-IA는 장기 보존되지만 드물게 접근되는 데이터에 적합하지만, 접근 패턴에 따라 객체를 자동으로 전환하지 않으므로 초기 높은 접근 기간 동안 더 높은 비용이 발생할 수 있습니다.",
            "S3 One Zone-IA는 더 저렴하지만 단일 가용 영역에만 데이터를 저장하며, 영역 손실에 대한 복원력이 없으므로 높은 내구성이 필요한 중요한 비디오 파일에는 적합하지 않을 수 있습니다.",
            "S3 Glacier Flexible Retrieval은 객체에 접근하기 전에 복원이 필요한 장기 아카이브를 위해 설계되었으므로, 비디오에 대한 빈번하고 즉각적인 접근이 필요한 시나리오에는 적합하지 않습니다."
        ]
    },
    {
        "Question Number": "54",
        "Situation": "금융 서비스 회사는 분석 및 보고를 지원하기 위해 대량의 거래 데이터를 실시간으로 처리하고 변환하는 업무를 맡고 있습니다. 회사는 증가하는 데이터 부하에 따라 확장할 수 있고 데이터 변환에 유연성을 제공하는 솔루션이 필요합니다.",
        "Question": "다음 서비스 중 실시간으로 거래 데이터를 처리하기 위한 확장성과 변환 기능의 최상의 조합을 제공하는 것은 무엇입니까?",
        "Options": {
            "1": "데이터 이벤트에 트리거되는 AWS Lambda 함수를 설정하고 필요에 따라 변환을 수행합니다.",
            "2": "AWS Glue를 사용하여 ETL 작업을 수행하고 데이터를 Amazon S3에 로드합니다.",
            "3": "Amazon Redshift Spectrum을 활용하여 변환 없이 S3에 저장된 데이터를 쿼리합니다.",
            "4": "Amazon EMR 클러스터를 Apache Spark와 함께 구현하여 데이터를 실시간으로 처리하고 변환합니다."
        },
        "Correct Answer": "Amazon EMR 클러스터를 Apache Spark와 함께 구현하여 데이터를 실시간으로 처리하고 변환합니다.",
        "Explanation": "Apache Spark와 함께하는 Amazon EMR은 대규모 데이터 처리를 위해 설계되었으며, 실시간 데이터 변환을 효율적으로 처리할 수 있습니다. 대량의 거래 데이터를 처리하기 위한 필요한 확장성과 유연성을 제공하므로 이 시나리오에 가장 적합합니다.",
        "Other Options": [
            "AWS Glue는 ETL 작업에 적합한 옵션이지만, 이 상황에서 Amazon EMR과 Apache Spark가 제공하는 실시간 처리 능력과 동일한 수준을 제공하지 않을 수 있습니다.",
            "AWS Lambda는 경량 변환에 적합하며 실시간 이벤트를 처리할 수 있지만, 대량의 거래 데이터에 대해 Amazon EMR만큼 효과적으로 확장되지 않을 수 있습니다.",
            "Amazon Redshift Spectrum은 변환 없이 S3에서 직접 데이터를 쿼리할 수 있지만, 데이터의 실시간 처리나 변환을 수행하지 않으므로 주어진 요구 사항에 덜 적합합니다."
        ]
    },
    {
        "Question Number": "55",
        "Situation": "소매 회사가 관계형 데이터베이스의 구조화된 데이터, JSON 파일의 반구조화된 데이터, 소셜 미디어 게시물의 비구조화된 데이터를 포함한 다양한 출처에서 수집한 고객 피드백을 분석하고 있습니다. 데이터 엔지니어는 최적의 쿼리 및 분석을 위해 이러한 다양한 유형의 데이터를 모델링하고 저장하는 가장 효과적인 방법을 선택해야 합니다.",
        "Question": "데이터 엔지니어가 구조화된 데이터, 반구조화된 데이터 및 비구조화된 데이터를 통합된 방식으로 효과적으로 관리하기 위해 어떤 데이터 저장 솔루션을 사용해야 합니까?",
        "Options": {
            "1": "구조화된 데이터에는 Amazon RDS를 구현하고 반구조화된 데이터와 비구조화된 데이터는 Amazon S3에 별도로 저장합니다.",
            "2": "구조화된 데이터, 반구조화된 데이터 및 비구조화된 데이터의 접근 및 조직 관리를 위해 Amazon S3와 AWS Lake Formation을 사용합니다.",
            "3": "모든 데이터 유형에 대해 Amazon DynamoDB를 채택하여 확장성과 저지연 액세스를 보장합니다.",
            "4": "모든 데이터 유형에 대해 Amazon Redshift를 활용하여 강력한 분석 기능을 활용합니다."
        },
        "Correct Answer": "구조화된 데이터, 반구조화된 데이터 및 비구조화된 데이터의 접근 및 조직 관리를 위해 Amazon S3와 AWS Lake Formation을 사용합니다.",
        "Explanation": "Amazon S3는 구조화된 데이터, 반구조화된 데이터 및 비구조화된 데이터를 저장하기 위한 비용 효율적이고 확장 가능한 솔루션을 제공합니다. AWS Lake Formation과 결합하면 효율적인 데이터 접근 관리, 데이터 거버넌스 및 조직을 가능하게 하여 데이터 엔지니어가 다양한 데이터 유형을 통합된 방식으로 효과적으로 처리할 수 있습니다.",
        "Other Options": [
            "Amazon RDS를 구현하면 구조화된 데이터만 처리할 수 있으며, 반구조화된 데이터와 비구조화된 데이터를 효과적으로 처리하지 못합니다. 이 접근 방식은 데이터 사일로를 초래하고 데이터 관리의 복잡성을 증가시킬 가능성이 높습니다.",
            "Amazon Redshift는 분석에 최적화되어 있지만 주로 구조화된 데이터에 맞춰 설계되었습니다. 반구조화된 데이터와 비구조화된 데이터를 Redshift에 저장하는 것은 실용적이지 않으며 성능 문제와 비용 증가를 초래할 수 있습니다.",
            "Amazon DynamoDB는 구조화된 데이터와 반구조화된 데이터에 적합한 NoSQL 데이터베이스 서비스이지만 비구조화된 데이터에는 이상적이지 않습니다. 모든 데이터 유형에 대해 사용하면 소셜 미디어 게시물과 같은 비구조화된 콘텐츠를 효과적으로 분석할 수 있는 능력이 제한될 수 있습니다."
        ]
    },
    {
        "Question Number": "56",
        "Situation": "데이터 엔지니어는 IoT 장치에서 들어오는 이벤트의 유입을 효율적으로 처리하기 위해 AWS Lambda를 사용하여 서버리스 데이터 처리 파이프라인을 설정하는 임무를 맡고 있습니다. 처리는 다양한 작업 부하를 수용하면서 비용을 최소화하고 높은 가용성을 보장해야 합니다.",
        "Question": "이 시나리오에 대해 Lambda 함수의 성능과 동시성을 최적화할 수 있는 구성은 무엇입니까?",
        "Options": {
            "1": "Lambda 함수의 예약 동시성을 구성하여 최대 동시 실행 수를 제한하고 비용 관리를 개선합니다.",
            "2": "AWS Lambda의 프로비저닝된 동시성을 활용하여 특정 수의 인스턴스가 항상 준비 상태로 이벤트에 응답하도록 합니다.",
            "3": "Lambda 함수의 타임아웃 설정을 늘려 실행 중에 타임아웃 없이 더 큰 작업 부하를 처리할 수 있도록 합니다.",
            "4": "Amazon SQS 큐를 설정하여 들어오는 이벤트를 버퍼링하고 Lambda 함수를 배치 크기 10으로 트리거하여 처리를 최적화합니다."
        },
        "Correct Answer": "AWS Lambda의 프로비저닝된 동시성을 활용하여 특정 수의 인스턴스가 항상 준비 상태로 이벤트에 응답하도록 합니다.",
        "Explanation": "프로비저닝된 동시성은 지정된 수의 Lambda 인스턴스를 따뜻하게 유지하여 성능 요구를 충족하도록 설계되어 있으며, 이는 간헐적인 이벤트 폭주 동안 발생할 수 있는 콜드 스타트 지연을 크게 줄여 고빈도 이벤트 처리를 최적화합니다.",
        "Other Options": [
            "Lambda 함수의 예약 동시성을 제한하면 비용 관리에 도움이 될 수 있지만, 이는 높은 볼륨의 이벤트 처리에서 스로틀링 및 지연을 초래할 수 있어 이 시나리오에는 적합하지 않습니다.",
            "Amazon SQS 큐를 사용하여 배치 크기 10으로 처리량을 개선할 수 있지만, 이는 Lambda 함수 자체의 성능을 직접적으로 해결하지 않으며, 특히 빠른 들어오는 이벤트 스트림 동안에는 더욱 그렇습니다.",
            "타임아웃 설정을 늘리면 더 긴 처리 시간을 허용하지만 동시성이나 성능을 향상시키지 않습니다. 함수가 더 빠른 처리를 위해 최적화되지 않은 경우 비효율성을 초래할 수 있습니다."
        ]
    },
    {
        "Question Number": "57",
        "Situation": "금융 서비스 회사는 효율성을 개선하고 수동 개입을 줄이기 위해 데이터 처리 워크플로우를 자동화하려고 합니다. 그들은 Amazon S3에 호스팅된 데이터 레이크로 데이터 수집, 변환 및 로딩을 조정하는 데이터 파이프라인을 구축하고자 합니다. 팀은 이 오케스트레이션을 구현하기 위해 다양한 AWS 서비스를 고려하고 있습니다.",
        "Question": "최소한의 관리 오버헤드로 이 데이터 파이프라인을 조정하기 위한 AWS 서비스의 조합은 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "Amazon Managed Workflows for Apache Airflow (MWAA)를 활용하여 데이터 처리 워크플로우를 예약하고 관리합니다.",
            "2": "Amazon Step Functions를 활용하여 데이터 파이프라인의 작업 순서를 조정합니다.",
            "3": "Amazon EventBridge를 사용하여 실시간 이벤트에 응답하고 데이터 처리 작업을 트리거합니다.",
            "4": "AWS Lambda를 사용하여 오케스트레이션 없이 데이터 변환을 직접 트리거합니다.",
            "5": "AWS Batch를 구현하여 파이프라인에서 데이터 처리를 위한 배치 작업을 실행합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Amazon Step Functions를 활용하여 데이터 파이프라인의 작업 순서를 조정합니다.",
            "Amazon Managed Workflows for Apache Airflow (MWAA)를 활용하여 데이터 처리 워크플로우를 예약하고 관리합니다."
        ],
        "Explanation": "Amazon Step Functions는 여러 AWS 서비스를 서버리스 워크플로우로 조정할 수 있게 해주며, 데이터 파이프라인의 작업을 조정하는 데 이상적입니다. Amazon MWAA는 Apache Airflow에 대한 관리형 서비스를 제공하여 복잡한 워크플로우를 쉽게 예약하고 관리할 수 있도록 하여 관리 노력을 최소화하는 목표와 일치합니다.",
        "Other Options": [
            "AWS Lambda는 데이터 변환에 사용할 수 있지만, 오케스트레이션 없이 단독으로 사용하면 복잡한 워크플로우를 효과적으로 관리할 수 있는 능력이 제한되어 데이터 파이프라인에 필수적입니다.",
            "AWS Batch는 배치 작업 실행에 적합하지만, 데이터 파이프라인의 여러 단계를 조정하는 데 필요한 오케스트레이션 기능을 제공하지 않습니다.",
            "Amazon EventBridge는 이벤트 기반 아키텍처에 유용하지만, 데이터 파이프라인에서 작업 순서를 조정하기 위해 특별히 설계되지 않았으며, 복잡한 워크플로우를 관리하는 기능이 부족합니다."
        ]
    },
    {
        "Question Number": "58",
        "Situation": "한 스타트업이 다양한 유형의 데이터를 서로 다른 접근 패턴으로 Amazon S3에 저장하고 있습니다. 그들은 자주 접근되는 데이터가 쉽게 이용 가능하도록 하면서, 덜 자주 접근되는 데이터는 비용 효율적으로 저장할 수 있도록 저장 비용을 최적화해야 합니다. 스타트업은 검색 비용을 발생시키지 않으면서 접근 패턴에 따라 자동으로 데이터를 관리하는 솔루션을 원합니다.",
        "Question": "스타트업이 데이터 접근 패턴을 효과적으로 관리하면서 비용을 자동으로 최적화하기 위해 선택해야 할 Amazon S3 저장 클래스는 무엇입니까?",
        "Options": {
            "1": "Amazon S3 Glacier는 드물게 접근되는 데이터의 장기 보관을 위해 사용됩니다.",
            "2": "Amazon S3 Standard-IA는 낮은 비용으로 덜 자주 접근되는 데이터를 저장하기 위해 사용됩니다.",
            "3": "Amazon S3 One Zone-IA는 덜 자주 접근되는 단일 AZ 데이터의 저비용 저장을 위해 사용됩니다.",
            "4": "Amazon S3 Intelligent-Tiering은 사용량에 따라 접근 계층 간에 데이터를 자동으로 이동합니다."
        },
        "Correct Answer": "Amazon S3 Intelligent-Tiering은 사용량에 따라 접근 계층 간에 데이터를 자동으로 이동합니다.",
        "Explanation": "Amazon S3 Intelligent-Tiering은 접근 패턴에 따라 자주 및 덜 자주 접근되는 계층 간에 데이터를 이동시켜 비용을 자동으로 최적화하도록 설계되었습니다. 이로 인해 예측할 수 없는 접근 패턴을 가진 데이터에 이상적인 선택이 되며, 성능을 희생하지 않고 비용 효율성을 제공합니다.",
        "Other Options": [
            "Amazon S3 Standard-IA는 장기적으로 존재하지만 덜 자주 접근되는 데이터에 적합하지만, 자동 계층화 기능을 제공하지 않으며 검색 비용이 발생하므로 이 시나리오에는 최적이 아닙니다.",
            "Amazon S3 One Zone-IA는 덜 자주 접근되는 데이터에 대해 낮은 비용을 제공하지만, 여러 가용 영역에 걸친 중복성을 제공하지 않으므로 AZ 실패 시 데이터 손실이 발생할 수 있습니다.",
            "Amazon S3 Glacier는 장기 보관을 위해 설계되었으며, 접근하기 전에 객체를 복원해야 하므로 쉽게 이용 가능한 데이터의 필요성과 일치하지 않습니다."
        ]
    },
    {
        "Question Number": "59",
        "Situation": "데이터 엔지니어링 팀이 제3자 API에서 데이터를 소비하는 마이크로서비스를 개발하고 있습니다. 이 서비스는 API 제공자가 부과하는 속도 제한을 처리하면서 데이터 무결성을 보장해야 합니다. 이를 달성하기 위한 최선의 접근 방식은 무엇입니까?",
        "Question": "데이터 엔지니어링 팀이 데이터 무결성을 유지하고 API의 속도 제한을 존중하기 위해 구현해야 할 전략은 무엇입니까?",
        "Options": {
            "1": "API 응답을 저장하고 API에 대한 요청 수를 줄이기 위해 캐싱 레이어를 사용합니다.",
            "2": "요청을 버퍼링하고 병렬로 처리하여 처리량을 극대화하기 위해 메시지 큐를 활용합니다.",
            "3": "정해진 간격으로 API에서 데이터를 가져오기 위해 주기적으로 배치 처리 작업을 예약합니다.",
            "4": "API 속도 제한 오류가 발생할 경우 재시도를 위한 지수 백오프를 구현하고 성공적인 요청을 기록합니다."
        },
        "Correct Answer": "API 속도 제한 오류가 발생할 경우 재시도를 위한 지수 백오프를 구현하고 성공적인 요청을 기록합니다.",
        "Explanation": "지수 백오프와 재시도를 구현하는 것은 API 속도 제한을 처리하는 가장 효과적인 전략입니다. 이 접근 방식은 서비스가 제한에 도달했을 때 일시 중지하고 재시도할 수 있도록 하여, 허용된 임계값 내에서 요청이 이루어지도록 하면서 데이터 무결성을 유지합니다.",
        "Other Options": [
            "요청을 버퍼링하기 위해 메시지 큐를 사용하는 것은 관리가 제대로 이루어지지 않으면 API를 압도할 수 있으며, 속도 제한을 본질적으로 존중하지 않기 때문에 제한에 도달할 경우 데이터 손실이 발생할 수 있습니다.",
            "캐싱 레이어를 구현하면 API 호출 수를 줄일 수 있지만, 캐시가 제대로 무효화되지 않으면 오래된 데이터가 포함될 수 있어 데이터 무결성을 저해할 수 있습니다.",
            "배치 처리 작업을 예약하면 데이터 가용성에 지연이 발생할 수 있으며, 속도 제한을 동적으로 처리하지 않으므로 API가 과부하 상태일 경우 요청 실패가 발생할 수 있습니다."
        ]
    },
    {
        "Question Number": "60",
        "Situation": "데이터 엔지니어링 팀이 AWS Glue DataBrew를 사용하여 분석을 위한 데이터 세트를 정리하고 준비하고 있습니다. 그들은 데이터 형식 차이 및 누락된 값 등 여러 이유로 인해 데이터의 불일치를 발견했습니다. 이러한 불일치를 해결하기 위해 팀은 분석 전에 데이터 품질과 일관성을 보장하기 위해 DataBrew 내에서 모범 사례를 구현하고자 합니다.",
        "Question": "데이터 엔지니어링 팀이 AWS Glue DataBrew에서 데이터 일관성을 개선하기 위해 취해야 할 조치는 무엇입니까?",
        "Options": {
            "1": "변경 사항을 분리하기 위해 각 데이터 세트에 대해 새로운 DataBrew 프로젝트를 생성합니다.",
            "2": "내장된 데이터 프로파일링 기능을 활용하여 데이터 품질 문제를 식별하고 해결합니다.",
            "3": "데이터 세트에 대한 원치 않는 변경을 방지하기 위해 자동 데이터 유형 감지를 비활성화합니다.",
            "4": "정리하기 전에 데이터 세트를 Amazon S3로 내보내 원본 버전을 유지합니다."
        },
        "Correct Answer": "내장된 데이터 프로파일링 기능을 활용하여 데이터 품질 문제를 식별하고 해결합니다.",
        "Explanation": "AWS Glue DataBrew의 내장된 데이터 프로파일링 기능을 사용하면 팀이 누락된 값 및 데이터 유형 불일치와 같은 불일치를 분석할 수 있어, 추가 분석 전에 이러한 문제를 효과적으로 해결할 수 있습니다.",
        "Other Options": [
            "각 데이터 세트에 대해 새로운 DataBrew 프로젝트를 생성하면 단편화가 발생하고 프로젝트 간 데이터 일관성을 관리하기 어려워질 수 있습니다.",
            "자동 데이터 유형 감지를 비활성화하면 DataBrew가 데이터를 올바르게 해석하고 형식을 지정하는 것을 방지할 수 있어, 추가적인 불일치를 초래할 수 있습니다.",
            "정리하기 전에 데이터 세트를 Amazon S3로 내보내는 것은 데이터 세트 내의 데이터 일관성 문제를 직접 해결하지 않으며, 원본 데이터를 보존할 뿐 품질을 개선하지 않습니다."
        ]
    },
    {
        "Question Number": "61",
        "Situation": "한 금융 서비스 회사가 Amazon S3에 저장된 고객 거래 데이터를 분석하고 있습니다. 팀은 추가 분석을 수행하기 전에 데이터 품질을 보장하고 데이터 세트의 특성을 이해해야 합니다. 그들은 데이터의 완전성, 정확성 및 분포를 평가할 수 있는 솔루션이 필요합니다.",
        "Question": "데이터 엔지니어가 데이터 세트에 대해 효과적인 데이터 프로파일링을 수행하기 위해 어떤 접근 방식을 취해야 합니까?",
        "Options": {
            "1": "AWS Glue DataBrew를 사용하여 S3 데이터 세트를 분석하고 요약 보고서를 생성하는 데이터 프로파일링 작업을 생성합니다.",
            "2": "AWS Glue ETL 작업을 예약하여 S3에서 Amazon Redshift로 데이터를 로드한 다음 Redshift에서 데이터 프로파일링 쿼리를 실행합니다.",
            "3": "S3 데이터를 스캔하고 프로파일링 결과를 Amazon DynamoDB에 기록하는 AWS Lambda 함수를 구현합니다.",
            "4": "S3 데이터에 대해 Amazon Athena 쿼리를 직접 실행하여 데이터 세트에 대한 기술 통계를 생성합니다."
        },
        "Correct Answer": "AWS Glue DataBrew를 사용하여 S3 데이터 세트를 분석하고 요약 보고서를 생성하는 데이터 프로파일링 작업을 생성합니다.",
        "Explanation": "AWS Glue DataBrew는 내장된 데이터 프로파일링 기능을 제공하는 데이터 준비 도구입니다. 데이터 세트를 자동으로 분석하여 데이터 품질 지표, 분포 등을 결정할 수 있어, 추가 분석 전에 데이터 세트의 특성을 이해하는 데 이상적입니다.",
        "Other Options": [
            "S3 데이터를 스캔하기 위해 AWS Lambda 함수를 구현하는 것은 프로파일링을 위한 사용자 정의 코드를 요구하며, DataBrew와 같은 전용 도구를 사용하는 것만큼 효율적이거나 포괄적이지 않을 수 있습니다. 또한 불필요한 복잡성을 추가합니다.",
            "Amazon Athena 쿼리를 실행하면 데이터에 대한 통찰력을 제공할 수 있지만, DataBrew가 제공하는 시각적 표현 및 자동 보고서와 같은 포괄적인 프로파일링 기능을 제공하지 않을 수 있습니다.",
            "AWS Glue ETL 작업을 예약하여 데이터를 Amazon Redshift로 로드하는 것은 프로파일링보다는 데이터 변환 및 저장에 더 적합합니다. 또한 데이터를 설정하고 처리하는 데 추가 비용과 시간이 발생합니다."
        ]
    },
    {
        "Question Number": "62",
        "Situation": "한 회사가 AWS에 저장된 데이터에 대한 안전한 접근을 보장하기 위해 데이터 거버넌스 전략을 구현하고 있습니다. 데이터 엔지니어는 민감한 데이터에 접근하는 다양한 사용자 역할에 대해 보안과 유연성을 모두 제공하는 적절한 인증 방법을 선택하는 임무를 맡고 있습니다. 엔지니어는 이 시나리오에 구현할 다양한 인증 방법을 고려하고 있습니다.",
        "Question": "데이터 엔지니어가 사용자 역할에 따라 효과적으로 사용자 접근을 관리하기 위해 우선시해야 할 인증 방법은 무엇입니까?",
        "Options": {
            "1": "권한을 동적으로 할당하기 위한 역할 기반 인증",
            "2": "모든 데이터 접근을 위한 인증서 기반 인증",
            "3": "모든 사용자를 위한 비밀번호 기반 인증",
            "4": "강화된 보안을 위한 다중 인증"
        },
        "Correct Answer": "권한을 동적으로 할당하기 위한 역할 기반 인증",
        "Explanation": "역할 기반 인증은 데이터 엔지니어가 사용자 역할에 따라 권한을 할당할 수 있게 하여 민감한 데이터에 대한 접근을 관리하는 유연하고 안전한 방법을 제공합니다. 역할을 정의함으로써 엔지니어는 사용자가 직무 기능에 필요한 데이터에만 접근할 수 있도록 보장하여 보안 및 규정 준수를 강화할 수 있습니다.",
        "Other Options": [
            "비밀번호 기반 인증은 사용자가 강력한 비밀번호를 생성해야 하므로 일반적으로 보안성이 떨어지며, 특히 대규모 조직에서는 이를 효과적으로 시행하고 관리하기 어려울 수 있습니다.",
            "인증서 기반 인증은 강력한 보안을 제공하지만, 관리가 복잡할 수 있으며 역할 기반 인증에 비해 동적 사용자 역할에 대한 유연성이 떨어집니다.",
            "다중 인증은 보안을 강화하지만 동적 역할 기반 권한의 필요성을 해결하지 않으며, 사용자 접근 관리를 위한 기본 방법으로 적합하지 않을 수 있습니다."
        ]
    },
    {
        "Question Number": "63",
        "Situation": "한 소매 회사가 판매 데이터를 분석하여 시간에 따른 추세를 파악하고 재고 관리를 개선하고자 합니다. 그들은 일일 판매에 대한 이동 평균을 계산해야 하며, 제품 카테고리별 판매 요약을 생성할 수 있어야 합니다. 기존 SQL 쿼리는 복잡하고 느려서 신속하게 실행 가능한 통찰력을 도출하기 어렵습니다.",
        "Question": "회사가 이동 평균을 효율적으로 계산하고 판매 데이터를 제품 카테고리별로 그룹화하기 위해 가장 잘 사용할 수 있는 접근 방식은 무엇입니까?",
        "Options": {
            "1": "Amazon Athena를 사용하여 S3에서 데이터를 직접 쿼리합니다.",
            "2": "SQL에서 윈도우 함수를 사용하여 이동 평균을 계산합니다.",
            "3": "각 제품 카테고리별로 별도의 데이터베이스를 생성합니다.",
            "4": "스프레드시트 애플리케이션에서 데이터 집계를 수행합니다."
        },
        "Correct Answer": "SQL에서 윈도우 함수를 사용하여 이동 평균을 계산합니다.",
        "Explanation": "SQL의 윈도우 함수는 집계를 수행하는 동안 이전 행의 상태를 유지하여 이동 평균을 효율적으로 계산할 수 있게 합니다. 이는 복잡한 조인이나 여러 쿼리의 오버헤드 없이 필요한 분석에 최적입니다.",
        "Other Options": [
            "각 제품 카테고리별로 별도의 데이터베이스를 생성하면 데이터 아키텍처가 복잡해지고 카테고리 간 쿼리가 어려워져 잠재적인 성능 문제를 초래할 수 있습니다.",
            "스프레드시트 애플리케이션에서 데이터 집계를 수행하는 것은 판매 데이터의 규모를 처리하기에 적합하지 않으며 성능 병목 현상과 수동 오류를 초래할 수 있습니다.",
            "Amazon Athena를 사용하여 S3에서 데이터를 직접 쿼리하는 것은 즉석 분석에 좋은 옵션이지만, 윈도우 함수처럼 이동 평균 계산을 효율적으로 지원하지는 않습니다."
        ]
    },
    {
        "Question Number": "64",
        "Situation": "데이터 엔지니어가 분석 플랫폼에 수집되는 데이터의 품질을 보장하는 임무를 맡았습니다. 데이터는 다양한 출처에서 수집되어 S3 버킷에 저장됩니다. 엔지니어는 AWS 서비스를 사용하여 데이터가 깨끗하고 분석할 준비가 되었는지 확인해야 합니다.",
        "Question": "엔지니어가 분석되기 전에 데이터를 정리하고 검증하는 프로세스를 자동화하기 위해 어떤 AWS 서비스를 사용해야 합니까?",
        "Options": {
            "1": "AWS Lambda",
            "2": "Amazon SageMaker Data Wrangler",
            "3": "Amazon QuickSight",
            "4": "AWS Glue DataBrew"
        },
        "Correct Answer": "AWS Glue DataBrew",
        "Explanation": "AWS Glue DataBrew는 데이터 준비를 위해 특별히 설계되어 사용자가 코드를 작성하지 않고도 시각적으로 데이터를 정리하고 변환할 수 있게 해줍니다. 데이터 정리 및 검증 프로세스를 자동화하는 다양한 기능을 제공하여 이 작업에 이상적입니다.",
        "Other Options": [
            "Amazon SageMaker Data Wrangler는 주로 머신 러닝 워크플로우에서 데이터 준비에 사용되지만, 분석 맥락에서 데이터 정리 및 검증 자동화를 위해 특별히 설계된 것은 아닙니다.",
            "AWS Lambda는 이벤트에 응답하여 코드를 실행하는 서버리스 컴퓨팅 서비스이지만, 데이터 정리 또는 검증을 위한 전용 도구를 제공하지 않습니다.",
            "Amazon QuickSight는 데이터 시각화 및 보고를 위한 비즈니스 인텔리전스 도구이지만, 데이터 정리 또는 검증 프로세스에 중점을 두지 않습니다."
        ]
    },
    {
        "Question Number": "65",
        "Situation": "데이터 엔지니어링 팀이 AWS Step Functions를 사용하여 데이터 처리를 위한 일련의 Lambda 함수를 조정하고 있습니다. 그들은 일부 실행이 간헐적으로 실패하는 것을 발견하고 이러한 실패의 잠재적 원인과 해결책을 식별하고자 합니다.",
        "Question": "AWS Step Functions 실행에서 간헐적 실패의 가장 가능성이 높은 이유는 무엇입니까?",
        "Options": {
            "1": "Step Functions는 동시 실행을 효율적으로 처리할 수 없습니다.",
            "2": "Lambda 함수가 타임아웃 한계를 초과했습니다.",
            "3": "Step Functions와 연결된 IAM 역할에 필요한 권한이 부족합니다.",
            "4": "처리 중인 데이터가 Lambda 함수의 메모리 한계를 초과합니다."
        },
        "Correct Answer": "Lambda 함수가 타임아웃 한계를 초과했습니다.",
        "Explanation": "Lambda 함수가 구성된 타임아웃 한계를 초과하면 Step Functions는 해당 실행을 실패로 표시합니다. 이는 데이터 양이나 복잡성으로 인해 처리 시간이 가끔 급증할 경우 간헐적 실패로 이어질 수 있는 일반적인 문제입니다.",
        "Other Options": [
            "Step Functions는 동시 실행을 처리할 수 있지만, 실행 속도가 AWS 서비스 한계를 초과하거나 리소스 경합 문제가 발생하면 제한이 걸릴 수 있지만, Lambda 타임아웃에 비해 간헐적 실패의 주요 원인일 가능성은 낮습니다.",
            "Step Functions와 연결된 IAM 역할에 필요한 권한이 부족하면 모든 실행이 실패하게 되며, 간헐적인 실패가 아닙니다. 이는 보다 시스템적인 문제입니다.",
            "Lambda 함수의 메모리 한계를 초과하면 실패를 초래할 수 있지만, 일반적으로 간헐적 실패보다는 일관된 실패를 초래합니다. 또한, 함수가 입력 데이터를 처리하는 방식에 따라 달라질 수 있습니다."
        ]
    }
]