[
    {
        "Question Number": "1",
        "Situation": "한 머신러닝 엔지니어가 다양한 스케일을 가진 여러 특성을 포함하는 시계열 데이터셋을 작업하고 있습니다. 이 데이터셋은 Amazon SageMaker에서 신경망 모델을 훈련하는 데 사용될 것입니다. 탐색적 데이터 분석 단계에서 엔지니어는 일부 특성이 서로 다른 스케일에 있어 모델의 성능에 영향을 미칠 수 있음을 깨닫습니다.",
        "Question": "모든 특성 값이 훈련 과정에 동등하게 기여하도록 보장하기 위해 엔지니어가 사용해야 할 전처리 기법은 무엇입니까?",
        "Options": {
            "1": "범주형 특성을 이진 벡터로 변환하기 위해 원-핫 인코딩을 사용합니다.",
            "2": "모든 특성의 왜곡을 줄이기 위해 로그 변환을 적용합니다.",
            "3": "모든 특성을 [0, 1] 범위로 스케일링하기 위해 최소-최대 정규화를 적용합니다.",
            "4": "특성을 중심화하고 단위 분산으로 스케일링하여 표준화합니다."
        },
        "Correct Answer": "모든 특성을 [0, 1] 범위로 스케일링하기 위해 최소-최대 정규화를 적용합니다.",
        "Explanation": "최소-최대 정규화는 특성을 균일한 범위로 스케일링하는 데 효과적이며, 어떤 특성도 스케일로 인해 지배하지 않도록 보장합니다. 이는 신경망에서 특히 중요하며, 서로 다른 스케일이 최적의 학습을 방해할 수 있습니다.",
        "Other Options": [
            "원-핫 인코딩은 범주형 변수를 머신러닝 모델에 적합한 형식으로 변환하는 데 사용되지만, 수치적 특성의 스케일링 문제를 해결하지는 않습니다.",
            "특성을 중심화하고 단위 분산으로 스케일링하여 표준화하는 것은 좋은 기법이지만, 모델이 데이터의 경계에 민감한 경우, 특히 신경망에서는 이상적이지 않을 수 있습니다.",
            "로그 변환은 왜곡을 줄이는 데 유용하지만, 모든 특성에 대해 균일한 스케일을 제공하지 않으므로 모델의 성능 문제를 여전히 초래할 수 있습니다."
        ]
    },
    {
        "Question Number": "2",
        "Situation": "한 회사가 전 세계에서 실시간 데이터 스트림을 처리하는 머신러닝 모델을 배포하고 있습니다. 이 모델은 여러 AWS 리전에서 사용 가능해야 하며, 여러 가용 영역을 활용하여 높은 가용성을 유지해야 합니다. 운영 팀은 배포 아키텍처를 설계하는 임무를 맡고 있습니다.",
        "Question": "머신러닝 모델이 여러 AWS 리전과 가용 영역에 효과적으로 배포되도록 보장하기 위한 최선의 접근 방식은 무엇입니까?",
        "Options": {
            "1": "각 리전에서 AWS Lambda 함수를 사용하여 머신러닝 모델을 호출합니다.",
            "2": "각 리전에서 로드 밸런서를 사용하여 Amazon SageMaker 다중 리전 엔드포인트를 활용합니다.",
            "3": "여러 EC2 인스턴스를 사용하여 단일 AWS 리전에서 모델을 배포합니다.",
            "4": "다중 리전 Amazon Elastic Kubernetes Service (EKS) 클러스터를 구현합니다."
        },
        "Correct Answer": "각 리전에서 로드 밸런서를 사용하여 Amazon SageMaker 다중 리전 엔드포인트를 활용합니다.",
        "Explanation": "Amazon SageMaker 다중 리전 엔드포인트를 사용하면 여러 리전에서 모델을 원활하게 배포할 수 있어 낮은 대기 시간과 높은 가용성을 보장합니다. 각 리전의 로드 밸런서는 들어오는 요청을 적절한 인스턴스로 분배하여 전체 성능과 신뢰성을 향상시킬 수 있습니다.",
        "Other Options": [
            "여러 EC2 인스턴스를 사용하여 단일 AWS 리전에서 배포하는 것은 다른 리전의 사용자에게 중복성이나 낮은 대기 시간을 제공하지 않으며, 이는 글로벌 애플리케이션에 필수적입니다.",
            "AWS Lambda 함수를 사용하는 것은 경량 처리에 적합할 수 있지만, 여러 리전에서 머신러닝 모델을 효율적으로 호스팅하는 것을 직접 지원하지 않습니다.",
            "다중 리전 Amazon EKS 클러스터를 구현하는 것은 복잡할 수 있으며, 머신러닝 모델 배포를 위한 SageMaker가 제공하는 최적화 및 사용 용이성 수준을 제공하지 않을 수 있습니다."
        ]
    },
    {
        "Question Number": "3",
        "Situation": "한 머신러닝 엔지니어가 이미지 분류를 위한 딥러닝 모델을 작업하고 있습니다. 이 모델은 복잡하며 과적합의 징후를 보이고, 훈련 세트에서 높은 정확도를 보이지만 검증 세트에서는 상당히 낮은 정확도를 보입니다. 엔지니어는 모델의 일반화 성능을 향상시키기 위한 기법을 탐색하고 있습니다.",
        "Question": "딥러닝 모델에서 과적합을 완화하기 위해 엔지니어가 구현해야 할 정규화 기법은 무엇입니까?",
        "Options": {
            "1": "은닉층 사이에 드롭아웃 레이어 추가",
            "2": "각 레이어 후 배치 정규화 적용",
            "3": "더 복잡한 활성화 함수 사용",
            "4": "각 레이어의 뉴런 수 증가"
        },
        "Correct Answer": "은닉층 사이에 드롭아웃 레이어 추가",
        "Explanation": "드롭아웃은 훈련 시간 동안 각 업데이트에서 입력 유닛의 일부를 무작위로 0으로 설정하는 정규화 기법으로, 모델이 특정 뉴런에 과도하게 의존하지 않도록 하여 과적합을 방지하는 데 도움을 줍니다.",
        "Other Options": [
            "배치 정규화는 주로 훈련을 안정화하고 가속화하는 데 사용되지만, 과적합을 구체적으로 해결하지 않으며 이 시나리오에서 도움이 되지 않을 수 있습니다.",
            "각 레이어의 뉴런 수를 증가시키면 모델이 보지 못한 데이터에 일반화되지 않는 더 복잡한 패턴을 학습할 수 있어 과적합을 악화시킬 수 있습니다.",
            "더 복잡한 활성화 함수를 사용하면 모델의 복잡성이 증가하여 과적합을 완화하기보다는 오히려 악화시킬 수 있습니다."
        ]
    },
    {
        "Question Number": "4",
        "Situation": "기술 스타트업의 데이터 과학 팀은 대규모 데이터 세트를 분석하기 위해 딥 러닝 모델을 훈련하는 임무를 맡았습니다. 그들은 훈련 작업의 효율성을 보장하면서 비용을 최소화하고자 합니다. 팀은 훈련 작업에 AWS Batch와 Spot Instances를 함께 사용하는 것을 고려하고 있습니다.",
        "Question": "팀이 딥 러닝 훈련을 위해 AWS Batch와 Spot Instances를 효과적으로 활용하기 위해 구현해야 할 두 가지 전략은 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "Amazon EC2 Auto Scaling을 활용하여 훈련 부하에 따라 Spot Instances의 수를 동적으로 조정합니다.",
            "2": "AWS Batch를 구성하여 자동으로 Spot Instances를 요청하는 작업을 제출합니다.",
            "3": "훈련 중 지속적인 가용성을 보장하기 위해 온디맨드 인스턴스만 사용합니다.",
            "4": "비용 절감을 위해 Spot Instances의 최대 가격을 온디맨드 가격보다 낮게 설정합니다.",
            "5": "Spot Instances가 사용 불가능할 경우 온디맨드 인스턴스로 전환하는 백업 메커니즘을 AWS Batch에 구현합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "AWS Batch를 구성하여 자동으로 Spot Instances를 요청하는 작업을 제출합니다.",
            "Spot Instances가 사용 불가능할 경우 온디맨드 인스턴스로 전환하는 백업 메커니즘을 AWS Batch에 구현합니다."
        ],
        "Explanation": "AWS Batch를 사용하여 자동으로 Spot Instances를 요청하면 팀이 Spot 가격과 관련된 낮은 비용을 활용할 수 있습니다. 또한, 백업 메커니즘을 갖추면 Spot Instances가 사용 불가능할 경우에도 훈련 프로세스가 중단 없이 계속될 수 있어 훈련 중 자원 활용을 최적화합니다.",
        "Other Options": [
            "온디맨드 인스턴스만 사용하는 것은 Spot Instances의 비용 절감 이점을 무효화하며, 이는 딥 러닝 모델 훈련 중 비용을 최소화하려는 팀의 목표에 반합니다.",
            "Spot Instances의 최대 가격을 온디맨드 가격보다 낮게 설정하면 Spot Instances를 자주 확보하지 못하거나 실패할 수 있으며, 이는 훈련을 지연시키고 장기적으로 비용을 증가시킬 수 있습니다.",
            "Amazon EC2 Auto Scaling을 활용하는 것은 유익할 수 있지만, Spot Instances와 함께 AWS Batch를 사용하는 데 필수적인 요구 사항은 아니며, 팀은 Auto Scaling을 구현하지 않고도 목표를 달성할 수 있습니다."
        ]
    },
    {
        "Question Number": "5",
        "Situation": "머신 러닝 전문가가 분류 모델을 구축할 준비를 하고 있지만, 사용 가능한 레이블이 있는 데이터 세트가 작고 문제 공간의 다양성을 충분히 나타내지 못할 수 있음을 발견했습니다. 이 문제를 해결하기 위해 전문가가 레이블이 있는 데이터의 양을 늘리기 위해 데이터 레이블링 도구를 사용하는 것을 고려하고 있습니다.",
        "Question": "분류 모델을 위한 충분한 레이블이 있는 데이터를 보장하기 위한 가장 효과적인 접근 방식은 무엇입니까?",
        "Options": {
            "1": "Amazon Mechanical Turk를 활용하여 다양한 기여자로부터 추가 레이블이 있는 데이터를 수집합니다.",
            "2": "무작위 샘플링 기법을 사용하여 합성 데이터를 생성하여 데이터 세트를 확장합니다.",
            "3": "온라인 저장소에서 사전 레이블이 있는 데이터 세트를 사용하되 그 관련성을 검증하지 않습니다.",
            "4": "기존 레이블이 있는 데이터의 하위 집합을 선택하고 이를 복제하여 데이터 세트 크기를 늘립니다."
        },
        "Correct Answer": "Amazon Mechanical Turk를 활용하여 다양한 기여자로부터 추가 레이블이 있는 데이터를 수집합니다.",
        "Explanation": "Amazon Mechanical Turk를 사용하면 전문가가 더 넓은 범위의 시나리오를 포함하는 레이블이 있는 데이터를 얻을 수 있어 모델의 성능과 일반화 가능성을 향상시킵니다.",
        "Other Options": [
            "사전 레이블이 있는 데이터 세트는 특정 문제에 적용 가능하거나 관련성이 없을 수 있어 모델 성능이 저하될 수 있습니다.",
            "무작위 샘플링을 통해 합성 데이터를 생성하는 것은 데이터가 실제 시나리오를 나타낼 것이라는 보장을 제공하지 않으며, 모델에 노이즈를 도입할 수 있습니다.",
            "기존 레이블이 있는 데이터를 복제하는 것은 새로운 정보를 도입하지 않으며, 모델이 반복된 예제에 편향될 수 있어 과적합을 초래할 수 있습니다."
        ]
    },
    {
        "Question Number": "6",
        "Situation": "소매 회사는 고객 구매 행동을 분석하여 마케팅 전략을 개선하고자 합니다. 그들은 고객 인구 통계, 구매 이력 및 제품 속성을 포함하는 대규모 데이터 세트를 수집했습니다. 데이터 과학 팀은 기존 데이터에서 더 많은 통찰력을 포착하여 모델 성능을 향상시킬 수 있는 새로운 특성을 생성해야 합니다. 그들은 파생된 특성이 관련성이 있고 고객 세분화를 효과적으로 예측하는 데 도움이 되는지 확인해야 합니다.",
        "Question": "고객 세분화를 위한 효과적인 특성 공학을 수행하기 위해 팀이 취해야 할 접근 방식은 무엇입니까?",
        "Options": {
            "1": "모든 수치적 특성을 평균이 0이고 표준 편차가 1이 되도록 정규화하고, 이러한 정규화된 특성을 추가 변환 없이 세분화 모델에 직접 사용합니다.",
            "2": "각 고객의 구매 이력을 요약하기 위해 평균 및 중앙값과 같은 간단한 통계적 측정을 사용하고, 이러한 요약을 세분화 모델의 특성으로 사용합니다.",
            "3": "특성 집합을 단순화하기 위해 데이터 세트에서 모든 범주형 변수를 제거하고 고객 세분화 모델에 수치적 특성만 의존합니다.",
            "4": "고객 인구 통계와 제품 속성 간의 상호 작용 항을 생성하여 구매 행동에 영향을 미칠 수 있는 관계를 포착하고, 이러한 상호 작용을 세분화 모델의 특성으로 사용합니다."
        },
        "Correct Answer": "고객 인구 통계와 제품 속성 간의 상호 작용 항을 생성하여 구매 행동에 영향을 미칠 수 있는 관계를 포착하고, 이러한 상호 작용을 세분화 모델의 특성으로 사용합니다.",
        "Explanation": "고객 인구 통계와 제품 속성 간의 상호 작용 항을 생성하면 모델이 구매 행동에 상당한 영향을 미칠 수 있는 복잡한 관계를 포착할 수 있습니다. 이 접근 방식은 다양한 특성이 서로 어떻게 상호 작용하는지를 기반으로 세그먼트를 구별하는 모델의 능력을 향상시켜 예측 성능을 개선합니다.",
        "Other Options": [
            "평균 및 중앙값과 같은 간단한 통계적 측정을 사용하는 것은 고객 구매 행동의 복잡성을 포착하기에 불충분하며, 효과적인 세분화를 위해 필요한 깊이 있는 통찰력을 제공하지 않습니다.",
            "수치적 특성을 정규화하는 것은 좋은 관행이지만, 특성 공학을 위한 충분한 조치가 아닙니다. 데이터에서 상호 작용과 관계를 포착할 수 있는 새로운 특성을 생성할 기회를 놓치고 있습니다.",
            "모든 범주형 변수를 제거하면 고객을 효과적으로 세분화하는 모델의 능력을 향상시킬 수 있는 귀중한 정보를 제거하게 됩니다. 범주형 변수는 종종 구매 행동에 대한 중요한 통찰력을 포함하고 있습니다."
        ]
    },
    {
        "Question Number": "7",
        "Situation": "기계 학습 전문가가 S3 데이터 레이크에 저장된 대규모 데이터셋을 사용하여 기계 학습 모델을 훈련하기 위해 데이터를 효율적으로 쿼리하고 전처리하려고 합니다. 전문가는 서버리스 아키텍처와 다양한 데이터 형식으로 작업할 수 있는 기능 덕분에 이 작업에 Amazon Athena를 사용하는 것을 고려하고 있습니다.",
        "Question": "전문가는 Amazon Athena에서 어떤 기능 조합을 활용해야 합니까? (두 가지 선택)",
        "Options": {
            "1": "SQL 쿼리에서 테이블 또는 뷰를 생성하기 위해 Amazon Athena를 사용합니다.",
            "2": "쿼리 결과를 자동 생성된 S3 버킷에 직접 저장합니다.",
            "3": "쿼리하기 전에 Amazon Glue DataBrew를 사용하여 데이터를 변환합니다.",
            "4": "CSV 파일만 작업할 수 있는 Athena의 기능을 활용합니다.",
            "5": "AWS Glue Catalog를 활용하여 메타데이터와 스키마를 관리합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "쿼리 결과를 자동 생성된 S3 버킷에 직접 저장합니다.",
            "AWS Glue Catalog를 활용하여 메타데이터와 스키마를 관리합니다."
        ],
        "Explanation": "Amazon Athena는 사용자가 쿼리 결과를 자동 생성된 S3 버킷에 저장할 수 있게 하여 처리된 데이터에 쉽게 접근할 수 있도록 합니다. 또한, AWS Glue Catalog와 원활하게 통합되어 메타데이터와 스키마를 관리하는 데 도움을 주어 쿼리 경험을 향상시킵니다.",
        "Other Options": [
            "데이터를 쿼리하기 전에 변환할 수 있지만, Amazon Glue DataBrew는 Amazon Athena의 기능이 아니며 쿼리 프로세스에 필요하지 않습니다.",
            "Athena는 JSON, Parquet, Avro와 같은 여러 형식으로 작업할 수 있으며, CSV 파일만 작업하는 것이 아니므로 이 옵션은 오해의 소지가 있습니다.",
            "SQL 쿼리에서 테이블과 뷰를 생성하는 것은 Athena의 기능이지만, 기계 학습을 위한 데이터 전처리를 직접 지원하는 두 가지 가장 중요한 기능 중 하나는 아닙니다."
        ]
    },
    {
        "Question Number": "8",
        "Situation": "기계 학습 엔지니어가 제곱 피트, 침실 수, 위치와 같은 다양한 기능을 기반으로 주택 가격을 예측하는 회귀 모델을 구축하는 임무를 맡고 있습니다. 엔지니어는 최적의 성능을 달성하기 위해 모델이 적절하게 초기화되도록 해야 합니다.",
        "Question": "엔지니어는 훈련 전에 회귀 모델 매개변수를 효과적으로 초기화하기 위해 어떤 전략을 활용해야 합니까?",
        "Options": {
            "1": "모든 매개변수에 대해 제로 초기화",
            "2": "데이터 통찰력을 기반으로 한 휴리스틱 초기화",
            "3": "균일 분포를 사용한 랜덤 초기화",
            "4": "정규 분포를 사용한 랜덤 초기화"
        },
        "Correct Answer": "정규 분포를 사용한 랜덤 초기화",
        "Explanation": "모델 매개변수를 정규 분포로 초기화하면 더 다양한 시작점을 제공하여 훈련 중 최적화 알고리즘이 더 효과적으로 수렴할 수 있도록 도와줍니다. 이는 특히 복잡한 모델에서 더욱 중요합니다.",
        "Other Options": [
            "균일 분포를 사용한 랜덤 초기화는 모든 매개변수가 유사한 값에서 시작하게 되어 수렴 속도를 늦추고 최적이 아닌 솔루션으로 이어질 수 있습니다.",
            "데이터 통찰력을 기반으로 한 휴리스틱 초기화는 유용할 수 있지만 모든 상황에 대해 일반적으로 권장되는 방법은 아닙니다. 통찰력이 정확하지 않으면 편향을 초래할 수 있습니다.",
            "제로 초기화는 모델에서 대칭을 초래하여 효과적으로 학습하지 못하게 할 수 있으며, 모든 매개변수가 훈련 중 동일한 방식으로 업데이트됩니다."
        ]
    },
    {
        "Question Number": "9",
        "Situation": "데이터 과학자가 이미지 분류를 위한 신경망을 최적화하고 있습니다. 학습률은 모델의 수렴과 훈련 시간에 영향을 미치는 중요한 하이퍼파라미터입니다. 데이터 과학자는 효율적인 훈련을 보장하기 위해 적절한 학습률을 선택해야 합니다.",
        "Question": "모델의 훈련 효율성을 개선할 가능성이 가장 높은 학습률 설정 조합은 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "훈련 내내 일정한 학습률을 사용합니다.",
            "2": "수렴 속도를 높이기 위해 너무 높은 학습률을 사용합니다.",
            "3": "훈련 과정에 적응하는 학습률을 사용합니다.",
            "4": "시간이 지남에 따라 감소하는 학습률 스케줄을 사용합니다.",
            "5": "안정을 보장하기 위해 너무 낮은 학습률을 사용합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "훈련 과정에 적응하는 학습률을 사용합니다.",
            "시간이 지남에 따라 감소하는 학습률 스케줄을 사용합니다."
        ],
        "Explanation": "적응형 학습률은 모델의 성능에 따라 조정되어 손실이 개선되는 영역에서는 더 빠르게 수렴하고 최소값에 가까워질 때는 느려지도록 합니다. 시간이 지남에 따라 감소하는 학습률 스케줄은 최소값을 초과하는 것을 방지하여 모델이 수렴할 때 가중치를 미세 조정할 수 있도록 합니다.",
        "Other Options": [
            "너무 높은 학습률은 불안정성과 최적 솔루션을 초과하는 결과를 초래하여 수렴이 아닌 발산을 초래할 수 있습니다.",
            "너무 낮은 학습률은 안정성을 보장하지만 훈련 시간을 극적으로 증가시켜 비효율적이게 만듭니다.",
            "일정한 학습률은 최적화 과정의 변화하는 동적에 적응하지 않으며 최적이 아닌 수렴으로 이어질 수 있습니다."
        ]
    },
    {
        "Question Number": "10",
        "Situation": "머신 러닝 엔지니어가 보안과 격리를 강화하기 위해 개인 VPC 내에서 Amazon SageMaker 훈련 작업을 설정하고 있습니다. 엔지니어는 훈련 데이터를 가져오기 위해 S3 버킷에 접근해야 하지만, VPC에 인터넷 접근이 없다는 것을 깨닫습니다.",
        "Question": "엔지니어가 개인 VPC에서 S3 버킷에 접근할 수 있도록 하려면 무엇을 구성해야 합니까?",
        "Options": {
            "1": "다른 VPC와 VPC 피어링 연결을 생성합니다.",
            "2": "NAT 게이트웨이를 사용하여 트래픽을 인터넷으로 라우팅합니다.",
            "3": "SageMaker 인스턴스에 공용 IP 주소를 활성화합니다.",
            "4": "개인 VPC에 S3 VPC 엔드포인트를 설정합니다."
        },
        "Correct Answer": "개인 VPC에 S3 VPC 엔드포인트를 설정합니다.",
        "Explanation": "인터넷 접근 없이 개인 VPC에서 S3에 접근하려면 엔지니어가 S3 VPC 엔드포인트를 설정해야 합니다. 이를 통해 VPC와 S3 간의 직접적인 통신이 가능해지며, 인터넷을 통해 트래픽을 라우팅할 필요가 없어 안전한 환경을 유지할 수 있습니다.",
        "Other Options": [
            "S3에 직접 접근하기 위해 VPC 피어링 연결을 생성할 필요는 없으며, 개인 VPC에서 필요한 접근을 가능하게 하지 않습니다.",
            "NAT 게이트웨이를 사용하면 인터넷 접근이 가능하지만, VPC 엔드포인트가 인터넷을 통해 라우팅하지 않고 S3에 안전한 연결을 제공할 수 있을 때는 필요하지 않습니다.",
            "SageMaker 인스턴스에 공용 IP 주소를 활성화하면 인터넷에 노출되어 개인 VPC를 사용하는 목적에 반하며 접근 문제를 해결하지 않습니다."
        ]
    },
    {
        "Question Number": "11",
        "Situation": "데이터 과학자가 머신 러닝 모델 훈련을 위한 데이터셋을 준비하고 있습니다. 데이터셋은 오랜 기간에 걸쳐 수집되었으며, 과학자는 잠재적인 편향에 대해 우려하고 있습니다. 모델의 성능이 견고하도록 하기 위해 데이터 준비에서 모범 사례를 적용해야 합니다.",
        "Question": "모델 훈련 전에 훈련 데이터셋에서 편향을 방지하기 위한 가장 효과적인 전략은 무엇입니까?",
        "Options": {
            "1": "전체 데이터셋을 훈련에 사용하고 동일한 데이터에서 모델을 평가합니다.",
            "2": "층화 샘플링을 사용하여 각 분할에서 모든 클래스가 고르게 대표되도록 합니다.",
            "3": "훈련 데이터셋을 훈련, 검증 및 테스트 세트로 분할하기 전에 무작위로 섞습니다.",
            "4": "시간 기간에 따라 데이터셋을 훈련, 검증 및 테스트 세트로 분리합니다."
        },
        "Correct Answer": "훈련 데이터셋을 훈련, 검증 및 테스트 세트로 분할하기 전에 무작위로 섞습니다.",
        "Explanation": "훈련 데이터셋을 무작위로 섞는 것은 데이터 수집 단계에서 도입된 잠재적인 편향을 제거하는 데 도움이 됩니다. 이 방법은 모델이 다양한 예제 세트에서 학습하도록 보장하며, 데이터 내 특정 패턴이나 순서에 과적합되지 않도록 합니다.",
        "Other Options": [
            "층화 샘플링은 대표성을 보장하는 데 유용할 수 있지만, 데이터 수집 순서에 의해 도입된 잠재적인 편향을 해결하지는 못하며, 이는 섞는 것으로 해결됩니다.",
            "시간 기간에 따라 데이터셋을 분리하면 시간적 편향이 도입될 수 있으며, 이는 모델 성능에 영향을 미칠 수 있습니다. 이러한 위험을 완화하기 위해 무작위화가 필요합니다.",
            "전체 데이터셋을 훈련에 사용하고 동일한 데이터에서 평가하는 것은 모델 성능에 대한 진정한 측정을 제공하지 않으며, 모델이 훈련 데이터를 단순히 암기하게 되어 과적합으로 이어질 수 있습니다."
        ]
    },
    {
        "Question Number": "12",
        "Situation": "머신 러닝 전문가가 성능을 개선하기 위해 선형 회귀 모델을 조정하고 있습니다. 전문가는 과적합을 줄이고 모델의 일반화 능력을 향상시키기 위해 정규화 기법을 적용하고자 합니다.",
        "Question": "이 목표를 달성하기 위해 적용할 수 있는 정규화 기법은 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "L1 정규화",
            "2": "드롭아웃 정규화",
            "3": "L2 정규화",
            "4": "배치 정규화",
            "5": "조기 중단"
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "L1 정규화",
            "L2 정규화"
        ],
        "Explanation": "L1 정규화는 계수의 크기의 절대값에 해당하는 패널티를 추가하여 희소 모델을 생성할 수 있습니다. L2 정규화는 계수의 크기의 제곱에 해당하는 패널티를 추가하여 모델 복잡성을 줄이고 과적합을 방지하는 데 도움이 됩니다. 두 기법 모두 모델의 일반화를 효과적으로 개선합니다.",
        "Other Options": [
            "드롭아웃 정규화는 주로 신경망에서 과적합을 방지하기 위해 사용되며 선형 회귀 모델에는 적용되지 않습니다.",
            "배치 정규화는 심층 네트워크의 훈련을 안정화하고 가속화하는 데 사용되는 기법이지만, 선형 회귀의 정규화에 직접 적용되지 않습니다.",
            "조기 중단은 검증 세트에서 성능이 저하되기 시작할 때 훈련을 중단하여 과적합을 방지하는 기법이지만, 모델 자체에 적용되는 정규화 형태는 아닙니다."
        ]
    },
    {
        "Question Number": "13",
        "Situation": "기계 학습 엔지니어가 Amazon SageMaker에서 대규모 데이터셋의 훈련 프로세스를 최적화하고 있습니다. 엔지니어는 훈련 단계 동안 데이터의 효율적인 스트리밍을 허용하는 최상의 데이터 형식을 활용하고자 합니다. 이는 훈련 처리량을 크게 향상시키면서 레코드가 개별적으로 제출되지 않도록 보장하는 데 도움이 됩니다.",
        "Question": "Amazon SageMaker에서 더 빠른 훈련 처리량을 달성하기 위해 엔지니어가 사용해야 할 데이터 형식은 무엇입니까?",
        "Options": {
            "1": "효율적인 데이터 처리와 빠른 훈련을 위해 Pipe 모드에서 RecordIO 형식을 사용하십시오.",
            "2": "데이터 표현을 위한 구조화된 접근 방식을 유지하기 위해 JSON Lines 형식을 사용하십시오.",
            "3": "데이터가 사람이 읽을 수 있고 쉽게 편집 가능하도록 CSV 형식을 사용하십시오.",
            "4": "최적화된 저장 및 쿼리 성능을 위해 Parquet 형식을 사용하십시오."
        },
        "Correct Answer": "효율적인 데이터 처리와 빠른 훈련을 위해 Pipe 모드에서 RecordIO 형식을 사용하십시오.",
        "Explanation": "Pipe 모드의 RecordIO 형식은 Amazon SageMaker에서 훈련 중 고처리량 데이터 스트리밍을 위해 특별히 설계되었습니다. 이는 대규모 데이터셋의 효율적인 입력을 허용하여 다른 형식에 비해 오버헤드를 줄이고 훈련 속도를 극대화하는 데 가장 좋은 선택입니다.",
        "Other Options": [
            "CSV 형식은 사용자 친화적이지만 스트리밍에서 동일한 효율성을 제공하지 않으며 각 줄을 개별적으로 구문 분석해야 하므로 훈련 시간이 느려질 수 있습니다.",
            "JSON Lines 형식은 구조화되어 있지만 대규모 데이터셋을 처리할 때 RecordIO보다 효율성이 떨어지는 경향이 있어 훈련 중 처리량을 저해할 수 있습니다.",
            "Parquet 형식은 저장 및 분석에 적합하지만 SageMaker에서 훈련 중 데이터 스트리밍에 최적화되어 있지 않아 가장 빠른 훈련 시간을 달성하는 데 중요합니다."
        ]
    },
    {
        "Question Number": "14",
        "Situation": "데이터 과학자가 Amazon SageMaker를 사용하여 이미지 분류 모델을 훈련할 준비를 하고 있습니다. 훈련 데이터는 S3 버킷에 저장되어 있으며, 과학자는 적절한 구성으로 훈련 작업을 설정해야 합니다. 모델은 이미지를 여러 카테고리로 분류할 것이며, 데이터 과학자는 작업을 시작하기 전에 하이퍼파라미터 및 데이터 위치를 포함한 모든 필요한 매개변수가 올바르게 구성되었는지 확인해야 합니다.",
        "Question": "데이터 과학자가 이미지 분류를 위해 Amazon SageMaker에서 훈련 작업이 올바르게 설정되도록 하기 위해 취해야 할 중요한 단계는 무엇입니까?",
        "Options": {
            "1": "설정 프로세스를 단순화하고 구성 오류를 피하기 위해 훈련 데이터와 검증 데이터 모두에 대해 단일 S3 경로를 지정하십시오.",
            "2": "클래스 수와 이미지 차원에 대한 기본 하이퍼파라미터를 사용하십시오. SageMaker가 자동으로 최적화합니다.",
            "3": "이미지 분류는 효율적인 훈련을 위해 GPU 인스턴스가 필요하지 않으므로 훈련 작업을 CPU 인스턴스만 사용하도록 구성하십시오.",
            "4": "SageMaker에서 제공하는 내장 알고리즘 중 적절한 알고리즘을 선택하고 훈련 및 검증 데이터에 대한 S3 경로를 포함하여 입력 데이터 구성을 지정하십시오."
        },
        "Correct Answer": "SageMaker에서 제공하는 내장 알고리즘 중 적절한 알고리즘을 선택하고 훈련 및 검증 데이터에 대한 S3 경로를 포함하여 입력 데이터 구성을 지정하십시오.",
        "Explanation": "데이터 과학자는 적절한 알고리즘을 선택하고 입력 데이터 경로를 올바르게 구성해야 훈련 작업이 훈련 및 검증에 필요한 데이터셋에 접근할 수 있습니다. 이는 효과적인 모델 학습에 매우 중요합니다.",
        "Other Options": [
            "CPU 인스턴스가 일부 작업에 충분할 수 있지만, 이미지 분류는 특히 대규모 데이터셋에 대해 GPU 인스턴스의 병렬 처리 기능의 이점을 누릴 수 있습니다.",
            "훈련 데이터와 검증 데이터 모두에 대해 단일 S3 경로를 사용하면 데이터 유출 문제가 발생할 수 있으며, 별도의 데이터셋이 필요한 모델 훈련의 모범 사례를 준수하지 않습니다.",
            "기본 하이퍼파라미터는 모든 시나리오에 적합하지 않을 수 있습니다. 데이터 과학자는 모델 성능을 최적화하기 위해 클래스 수와 이미지 차원을 포함한 하이퍼파라미터 조정을 고려해야 합니다."
        ]
    },
    {
        "Question Number": "15",
        "Situation": "데이터 과학자가 다양한 국가를 나타내는 범주형 특성을 포함한 분류 작업을 위한 데이터셋을 준비하고 있습니다. 이 데이터를 기계 학습 모델에 공급하기 위해 데이터 과학자는 범주형 값을 숫자 형식으로 변환해야 합니다.",
        "Question": "범주형 특성을 기계 학습 알고리즘에 적합한 형식으로 변환하는 가장 적절한 방법은 무엇입니까?",
        "Options": {
            "1": "국가 이름을 발생 횟수로 대체하기 위해 빈도 인코딩을 구현하십시오.",
            "2": "국가 이름을 고유한 정수로 변환하기 위해 레이블 인코딩을 적용하십시오.",
            "3": "국가 인구에 따라 정수 값을 할당하기 위해 서수 인코딩을 활용하십시오.",
            "4": "각 국가에 대해 이진 열을 생성하기 위해 원-핫 인코딩을 사용하십시오."
        },
        "Correct Answer": "각 국가에 대해 이진 열을 생성하기 위해 원-핫 인코딩을 사용하십시오.",
        "Explanation": "원-핫 인코딩은 범주형 특성을 기계 학습에 적합한 형식으로 변환하는 가장 좋은 접근 방식입니다. 이는 각 범주에 대해 새로운 이진 열을 생성하여 모델이 범주 간의 자연스러운 순서를 가정하지 않고 학습할 수 있도록 합니다. 이는 국가와 같은 범주형 변수에 필요합니다.",
        "Other Options": [
            "레이블 인코딩은 각 범주에 고유한 정수를 할당하지만 국가 이름과 같은 명목 데이터에 존재하지 않는 순위 순서를 암시하여 모델을 오도할 수 있습니다.",
            "서수 인코딩은 순위나 순서에 따라 정수 값을 할당하므로 국가에 적용할 수 없으므로 부적절합니다.",
            "빈도 인코딩은 범주형 값을 발생 횟수로 대체하지만 편향을 초래할 수 있으며 모델을 위한 범주 간의 명확한 분리를 생성하지 않습니다."
        ]
    },
    {
        "Question Number": "16",
        "Situation": "소매 회사가 온라인 스토어를 위한 새로운 추천 시스템을 출시하고 있습니다. 데이터 과학 팀은 제품 추천에 대한 최상의 전환율을 제공하는 머신 러닝 모델을 비교하고자 합니다. 그들은 실제 사용자와 함께 두 모델의 성능을 실시간으로 평가하기 위해 A/B 테스트 전략을 구현하기로 결정했습니다.",
        "Question": "데이터 과학 팀이 두 모델에 대해 A/B 테스트를 효과적으로 수행하기 위해 어떤 접근 방식을 취해야 합니까?",
        "Options": {
            "1": "하나의 모델을 일정 기간 실행한 후 사용자 데이터를 수집한 후 다른 모델로 전환하여 비교합니다.",
            "2": "두 모델을 동시에 서로 다른 사용자 세그먼트에 배포하고 성능 지표를 측정합니다.",
            "3": "두 모델을 별도로 훈련시키고 배포 전에 오프라인 검증 지표를 기반으로 하나를 선택합니다.",
            "4": "단일 모델을 사용하고 서로 다른 모델 버전 간에 사용자 트래픽을 회전시켜 성능을 측정합니다."
        },
        "Correct Answer": "두 모델을 동시에 서로 다른 사용자 세그먼트에 배포하고 성능 지표를 측정합니다.",
        "Explanation": "A/B 테스트를 효과적으로 수행하기 위해 두 모델을 동시에 서로 다른 사용자 세그먼트에 배포하면 유사한 조건에서 실시간으로 성능을 직접 비교할 수 있습니다. 이 접근 방식은 실제 사용자 상호작용을 기반으로 어떤 모델이 더 나은 전환율을 유도하는지에 대한 즉각적인 통찰력을 제공합니다.",
        "Other Options": [
            "두 모델을 별도로 훈련시키고 오프라인 검증 지표를 기반으로 하나를 선택하는 것은 실제 성능과 사용자 상호작용에 대한 통찰력을 제공하지 않으며, 이는 A/B 테스트에 중요합니다.",
            "단일 모델을 사용하고 사용자 트래픽을 회전시키는 것은 시간이나 외부 요인에 따라 편향을 초래할 수 있으며, 이는 모델 성능 평가를 왜곡합니다.",
            "하나의 모델을 일정 기간 실행한 후 다른 모델로 전환하는 것은 실시간 비교의 즉각성이 부족하며, 전환 과정에서 사용자 행동에 대한 외부 영향의 효과를 포착하지 못할 수 있습니다."
        ]
    },
    {
        "Question Number": "17",
        "Situation": "데이터 엔지니어가 Amazon S3에 저장된 데이터를 크롤링하고 분석을 위해 변환하는 데이터 처리 파이프라인을 설정하는 임무를 맡았습니다. 팀은 이 목적을 위해 AWS Glue를 사용하기로 계획하고 있으며, 메타데이터가 적절하게 관리되고 Amazon Athena에서 쿼리를 위해 접근 가능하도록 해야 합니다. 또한 서버 관리를 피하고 확장성을 보장하고자 합니다.",
        "Question": "Glue Crawler가 S3의 데이터 스키마를 성공적으로 발견하고 Glue Data Catalog를 채우기 위해 필수적인 단계는 무엇입니까?",
        "Options": {
            "1": "크롤러를 실행하기 전에 Glue Data Catalog에서 각 데이터 세트의 스키마를 수동으로 정의합니다.",
            "2": "새 데이터가 S3에 업로드될 때마다 Glue Crawler를 트리거하는 AWS Lambda 함수를 설정합니다.",
            "3": "크롤러를 사용하지 않고 Glue Data Catalog를 구성하여 데이터를 S3에 직접 저장합니다.",
            "4": "Glue Crawler가 S3 버킷 및 기타 필요한 리소스에 접근할 수 있도록 IAM 역할을 생성합니다."
        },
        "Correct Answer": "Glue Crawler가 S3 버킷 및 기타 필요한 리소스에 접근할 수 있도록 IAM 역할을 생성합니다.",
        "Explanation": "Glue Crawler는 데이터를 크롤링하고 스키마를 성공적으로 발견하기 위해 S3 버킷 및 기타 리소스에 접근할 수 있는 권한이 있는 IAM 역할이 필요합니다. 이는 크롤러가 올바르게 작동하고 Glue Data Catalog를 채우기 위해 중요한 단계입니다.",
        "Other Options": [
            "Glue Data Catalog는 메타데이터를 저장하도록 설계되었으며 크롤러를 사용하지 않고 S3에 직접 데이터를 저장할 수 없습니다. 따라서 이 옵션은 Glue가 S3와 상호작용하는 방식을 잘못 표현하고 있으므로 잘못된 것입니다.",
            "스키마를 수동으로 정의하는 것은 가능하지만, 이는 스키마를 자동으로 발견하도록 설계된 크롤러의 목적을 무색하게 합니다. 따라서 이 옵션은 잘못된 것입니다.",
            "AWS Lambda 함수를 설정하여 크롤러를 트리거하는 것은 자동화를 향상시킬 수 있지만, 크롤러의 스키마 발견 능력에 필수적이지 않습니다. 따라서 이 옵션은 주요 요구 사항을 다루지 않습니다."
        ]
    },
    {
        "Question Number": "18",
        "Situation": "데이터 과학자가 크기, 위치 및 침실 수와 같은 다양한 특성을 기반으로 주택 가격을 예측하기 위해 선형 회귀 모델을 훈련시키고 있습니다. 훈련 과정에서 그녀는 모델의 성능이 각 에포크마다 크게 변동하는 것을 관찰합니다.",
        "Question": "데이터 과학자가 훈련 중 모델의 성능을 안정화하기 위해 어떤 조정을 할 수 있습니까?",
        "Options": {
            "1": "다른 회귀 알고리즘을 사용합니다.",
            "2": "학습률을 감소시킵니다.",
            "3": "훈련 에포크 수를 증가시킵니다.",
            "4": "모델에 더 많은 특성을 추가합니다."
        },
        "Correct Answer": "학습률을 감소시킵니다.",
        "Explanation": "학습률을 감소시키면 모델이 점진적으로 수렴하도록 도와주어 최적의 가중치를 초과하는 위험을 줄이고 성능의 변동성을 최소화하여 모델의 훈련 과정을 안정화할 수 있습니다.",
        "Other Options": [
            "훈련 에포크 수를 증가시키면 높은 학습률로 인한 불안정성을 해결하지 못하고 과적합으로 이어질 수 있습니다.",
            "모델에 더 많은 특성을 추가하는 것은 성능 변동 문제를 반드시 해결하지 않으며, 오히려 모델을 더 복잡하게 만들 수 있습니다.",
            "다른 회귀 알고리즘을 사용하는 것은 유효한 접근 방식일 수 있지만, 학습률이 모델의 안정성에 미치는 즉각적인 문제를 직접적으로 해결하지는 않습니다."
        ]
    },
    {
        "Question Number": "19",
        "Situation": "기계 학습 전문가가 자연어 처리 모델의 성능을 개선하는 임무를 맡았습니다. 이 모델은 소셜 미디어, 이메일 및 설문조사를 포함한 다양한 출처의 고객 피드백을 분석해야 합니다. 전문가는 이 비구조적 텍스트 데이터에서 의미 있는 특징을 추출하여 모델 훈련을 향상시키는 것을 목표로 합니다.",
        "Question": "비구조적 텍스트 데이터를 기계 학습 모델에 적합한 구조적 숫자 특징으로 변환하는 데 가장 효과적인 기술은 무엇입니까?",
        "Options": {
            "1": "Feature Extraction",
            "2": "Data Imputation",
            "3": "Data Normalization",
            "4": "Tokenization"
        },
        "Correct Answer": "Feature Extraction",
        "Explanation": "특징 추출은 텍스트와 같은 비구조적 데이터를 관련 특징을 식별하고 정량화하여 구조적 데이터로 변환하는 과정입니다. 이는 기계 학습 모델이 데이터를 효과적으로 학습할 수 있도록 하는 자연어 처리에서 필수적입니다.",
        "Other Options": [
            "Tokenization은 텍스트를 개별 단어 또는 토큰으로 분해하는 과정으로, 이는 초기 단계이지만 텍스트를 구조적 숫자 특징으로 변환하지는 않습니다.",
            "Data Normalization은 데이터 세트의 값을 공통 스케일로 조정하는 것을 의미하며, 이는 비구조적 텍스트 데이터보다는 숫자 데이터에 더 관련이 있습니다.",
            "Data Imputation은 데이터 세트 내의 누락된 값을 채우는 데 사용되는 기술이지만, 비구조적 텍스트를 구조적 숫자 특징으로 변환하는 데는 적용되지 않습니다."
        ]
    },
    {
        "Question Number": "20",
        "Situation": "데이터 엔지니어가 S3에 저장된 데이터를 관리하는 임무를 맡았으며, 이 데이터를 분석을 위해 효율적으로 카탈로그하고 처리할 방법을 구현해야 합니다. 데이터 세트의 스키마를 쉽게 발견하고, 중복 레코드를 처리하며, Amazon Athena에서 쿼리를 위해 데이터를 준비할 수 있도록 해야 합니다.",
        "Question": "데이터 엔지니어가 어떤 조합의 작업을 수행해야 합니까? (두 가지 선택)",
        "Options": {
            "1": "AWS Data Pipeline을 사용하여 데이터의 ETL 작업을 조정합니다.",
            "2": "Glue Crawler에 IAM 역할을 할당하여 S3 버킷에 접근할 수 있도록 합니다.",
            "3": "Glue Crawler를 생성하여 데이터의 스키마와 파티션을 발견합니다.",
            "4": "Glue 내에서 데이터를 직접 접근하고 쿼리하여 분석합니다.",
            "5": "Glue ETL 작업을 구현하여 데이터를 변환하고 FindMatches 변환을 적용합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Glue Crawler를 생성하여 데이터의 스키마와 파티션을 발견합니다.",
            "Glue Crawler에 IAM 역할을 할당하여 S3 버킷에 접근할 수 있도록 합니다."
        ],
        "Explanation": "Glue Crawler를 생성하면 S3에 저장된 데이터 세트의 스키마와 파티셔닝을 자동으로 발견할 수 있어 효율적인 데이터 관리 및 분석에 필수적입니다. 또한, Glue Crawler에 IAM 역할을 할당하는 것은 AWS 환경에서 필요한 리소스에 접근할 수 있는 권한을 부여하는 데 필요하여 올바르게 작동할 수 있게 합니다.",
        "Other Options": [
            "AWS Data Pipeline은 필요한 ETL 기능을 제공하지 않으며, 주로 워크플로를 관리하는 조정 서비스일 뿐 실제 데이터 변환을 수행하지 않습니다.",
            "Glue는 데이터에 직접 접근할 수 없으며, 대신 Athena와 같은 서비스에서 쿼리를 위해 데이터를 준비합니다. 따라서 이 옵션은 부정확합니다.",
            "IAM 역할은 Glue Crawler에 필요하지만, 이 옵션만으로는 스키마와 파티션을 발견하는 작업을 수행하지 않으므로 독립적인 작업으로는 부정확합니다."
        ]
    },
    {
        "Question Number": "21",
        "Situation": "기계 학습 엔지니어가 컴퓨터 비전 프로젝트를 진행 중이며, 특정 클래스의 이미지가 제한된 데이터 세트로 60장만 있습니다. 목표는 기존 데이터 세트를 증강하여 모델의 성능을 향상시키는 것입니다. 증강된 이미지가 원래 클래스의 관련 특성을 유지하도록 해야 합니다.",
        "Question": "제한된 수의 이미지로 데이터 세트를 향상시키기 위한 최선의 접근 방식은 무엇입니까?",
        "Options": {
            "1": "공공 데이터 세트에서 추가 이미지를 수집하여 훈련 데이터를 늘립니다.",
            "2": "회전, 선명도 조정 및 색상 대비 조정과 같은 이미지 증강 기술을 사용하여 기존 데이터 세트에서 더 많은 훈련 이미지를 생성합니다.",
            "3": "기존 60장으로 모델을 훈련하고 증강 없이 사전 훈련된 모델에서 전이 학습에 의존합니다.",
            "4": "다른 클래스에 속하지 않는 대체 데이터 세트를 수동으로 레이블링하여 훈련 이미지의 다양성을 증가시킵니다."
        },
        "Correct Answer": "이미지 증강 기술을 사용하여 기존 데이터 세트에서 더 많은 훈련 이미지를 생성합니다.",
        "Explanation": "이미지 증강 기술을 사용하면 기존 60장의 이미지 변형을 효과적으로 생성하여 데이터 세트의 크기와 다양성을 증가시키고, 궁극적으로 모델 성능과 일반화를 향상시킵니다.",
        "Other Options": [
            "공공 데이터 세트에서 추가 이미지를 수집하는 것은 라이센스 문제나 가용성으로 인해 항상 가능하지 않을 수 있으며, 기존 이미지를 효과적으로 활용하지 않습니다.",
            "기존 60장으로 모델을 훈련하고 증강 없이 진행하면 제한된 데이터로 인해 과적합이 발생할 수 있으며, 이는 강력한 모델을 생성하지 못할 것입니다.",
            "다른 클래스에 속하지 않는 대체 데이터 세트를 수동으로 레이블링하면 노이즈와 관련 없는 데이터가 추가되어 모델을 혼란스럽게 하고 성능을 저하시킬 수 있습니다."
        ]
    },
    {
        "Question Number": "22",
        "Situation": "데이터 과학 팀이 Amazon SageMaker를 사용하여 예측 모델을 개발했으며, 모델이 다양한 부하를 효율적으로 처리할 수 있도록 보장하고자 합니다. 이 팀은 사용자에게 예측을 제공하기 위해 모델을 프로덕션 환경에 배포할 계획입니다. 팀은 배포 관리를 위해 Auto Scaling 그룹을 사용하는 것을 고려하고 있습니다.",
        "Question": "모델 배포를 위한 Auto Scaling을 구성하여 가용성과 성능을 보장하는 가장 효과적인 방법은 무엇입니까?",
        "Options": {
            "1": "부하에 관계없이 성능의 변동을 피하기 위해 Auto Scaling 그룹에 고정된 수의 인스턴스를 설정합니다.",
            "2": "CPU 활용도 메트릭에 따라 피크 부하 동안 인스턴스 수를 증가시키고 저사용 시 감소시키도록 Auto Scaling을 구성합니다.",
            "3": "실제 사용량을 모니터링하지 않고 특정 시간에 인스턴스 수를 증가시키는 예약 스케일링 정책을 사용합니다.",
            "4": "모델의 평균 응답 시간을 기반으로 인스턴스 수를 조정하는 목표 추적 스케일링 정책을 구현합니다."
        },
        "Correct Answer": "모델의 평균 응답 시간을 기반으로 인스턴스 수를 조정하는 목표 추적 스케일링 정책을 구현합니다.",
        "Explanation": "목표 추적 스케일링 정책을 구현하면 시스템이 응답 시간과 같은 실시간 성능 메트릭에 따라 인스턴스 수를 자동으로 조정할 수 있습니다. 이는 모델이 다양한 부하를 효율적으로 처리하면서 성능과 가용성을 유지할 수 있도록 보장합니다.",
        "Other Options": [
            "CPU 활용도를 기반으로 Auto Scaling을 구성하는 것은 모델의 성능 요구를 정확하게 반영하지 못할 수 있으며, 높은 CPU 사용량이 항상 응답 시간이나 요청 부하와 상관관계가 있는 것은 아닙니다.",
            "고정된 수의 인스턴스를 설정하면 변화하는 부하에 대한 유연성이 없어져, 피크 시간 동안 자원이 과도하게 할당되거나 용량이 부족해질 수 있습니다.",
            "예약 스케일링 정책을 사용하는 것은 실제 사용 패턴을 고려하지 않으며, 실시간 수요에 적응하지 않기 때문에 비효율성을 초래할 수 있습니다."
        ]
    },
    {
        "Question Number": "23",
        "Situation": "프로덕션에 배포된 머신 러닝 모델이 시간에 따라 성능 메트릭의 변동을 겪고 있습니다. 데이터 과학 팀은 이러한 변화의 근본 원인을 파악하고 모델이 의도된 목적에 대해 효과적으로 유지되도록 해야 합니다.",
        "Question": "시간에 따라 배포된 머신 러닝 모델의 성능을 모니터링하는 가장 좋은 접근 방식은 무엇입니까?",
        "Options": {
            "1": "성능이 떨어질 때마다 모델을 자동으로 재훈련하기 위해 AWS Lambda 함수를 사용합니다.",
            "2": "Amazon CloudWatch를 사용하여 주요 성능 메트릭을 추적하는 실시간 모니터링을 구현합니다.",
            "3": "모델의 예측을 검증 데이터셋과 비교하여 주간 배치 평가를 예약합니다.",
            "4": "Amazon QuickSight를 사용하여 모델의 역사적 성능 데이터를 시각화하는 대시보드를 생성합니다."
        },
        "Correct Answer": "Amazon CloudWatch를 사용하여 주요 성능 메트릭을 추적하는 실시간 모니터링을 구현합니다.",
        "Explanation": "Amazon CloudWatch를 사용한 실시간 모니터링은 팀이 모델의 성능을 지속적으로 추적할 수 있게 하여 문제가 발생할 때 즉시 식별할 수 있도록 합니다. 이러한 사전 예방적 접근 방식은 프로덕션 환경에서 모델의 효과성을 유지하는 데 중요합니다.",
        "Other Options": [
            "주간 배치 평가를 예약하는 것은 성능 문제에 대한 적시 통찰력을 제공하지 못할 수 있으며, 이는 최적이 아닌 모델 동작이 장기간 지속될 수 있습니다.",
            "Amazon QuickSight를 사용하여 대시보드를 생성하면 역사적 데이터의 시각적 표현을 제공하지만, 즉각적인 성능 문제에 대한 실시간 통찰력이나 경고를 제공하지 않습니다.",
            "AWS Lambda 함수를 사용하여 모델을 자동으로 재훈련하는 것은 적절한 평가 없이 빈번한 재훈련을 초래할 수 있으며, 현재 성능 문제를 해결하기보다는 새로운 문제를 도입할 수 있습니다."
        ]
    },
    {
        "Question Number": "24",
        "Situation": "소매 회사가 고객 거래의 실시간 분석을 위한 데이터 처리 파이프라인을 최적화하려고 합니다. 이들은 데이터가 효율적으로 정리되고 변환되어 데이터 웨어하우스에 로드되어 분석 및 보고를 지원하도록 보장하고자 합니다. 팀은 AWS에서 사용할 수 있는 다양한 데이터 변환 도구를 고려하고 있습니다.",
        "Question": "실시간 데이터 스트림을 처리할 수 있는 서버리스, 확장 가능한 데이터 변환 솔루션을 구현하는 데 가장 적합한 AWS 서비스는 무엇입니까?",
        "Options": {
            "1": "Amazon EMR",
            "2": "AWS Lambda",
            "3": "Amazon Kinesis Data Firehose",
            "4": "AWS Glue"
        },
        "Correct Answer": "Amazon Kinesis Data Firehose",
        "Explanation": "Amazon Kinesis Data Firehose는 실시간 데이터 스트리밍을 위해 특별히 설계되었으며, 데이터를 데이터 레이크와 데이터 저장소에 로드하기 전에 자동으로 변환할 수 있습니다. 이는 데이터 양에 따라 원활하게 확장되는 완전 관리형 솔루션을 제공합니다.",
        "Other Options": [
            "AWS Glue는 주로 배치 처리 및 ETL 작업에 중점을 두고 있어 Kinesis Data Firehose만큼 실시간 데이터 변환 요구에 효과적이지 않을 수 있습니다.",
            "Amazon EMR은 대규모 데이터 변환에 일반적으로 사용되는 빅 데이터 처리 서비스이지만, 상당한 관리가 필요하며 서버리스가 아니므로 실시간 애플리케이션에 덜 적합합니다.",
            "AWS Lambda는 서버리스 컴퓨팅에 유용하지만 주로 이벤트 기반 서비스이며 Kinesis Data Firehose만큼 지속적인 데이터 스트림을 효율적으로 처리하도록 설계되지 않았습니다."
        ]
    },
    {
        "Question Number": "25",
        "Situation": "데이터 과학자가 소매 회사의 미래 매출을 예측하기 위해 시계열 데이터셋을 분석하고 있습니다. 그들은 데이터에서 계절성과 추세를 구분해야 하며, 예측에서 노이즈를 고려해야 합니다. 그들은 가산적 및 승산적 패턴을 모두 포착할 수 있는 다양한 모델을 고려하고 있습니다.",
        "Question": "데이터 과학자가 시계열 데이터를 분석할 때 이해해야 할 특성은 무엇인가요? (두 가지 선택)",
        "Options": {
            "1": "승산적 모델은 계절 변동을 추세와 함께 조정합니다.",
            "2": "가산적 모델은 계절 변동이 시간이 지남에 따라 일정하다고 가정합니다.",
            "3": "추세는 예측할 수 없는 무작위 변동입니다.",
            "4": "계절성은 데이터에서 규칙적이고 예측 가능한 변화를 나타냅니다.",
            "5": "노이즈는 모델링할 수 있는 시계열의 일관된 부분입니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "계절성은 데이터에서 규칙적이고 예측 가능한 변화를 나타냅니다.",
            "승산적 모델은 계절 변동을 추세와 함께 조정합니다."
        ],
        "Explanation": "계절성은 주간 또는 월간 변화와 같이 정기적으로 반복되는 패턴을 의미하며, 추세는 데이터의 장기적인 증가 또는 감소를 나타냅니다. 승산적 모델은 계절 변동의 진폭이 추세의 수준에 따라 변할 때 적합합니다.",
        "Other Options": [
            "추세는 데이터의 장기적인 움직임을 의미하며, 무작위 변동이 아닙니다. 따라서 이 옵션은 추세의 정의를 잘못 설명하고 있어 잘못된 것입니다.",
            "가산적 모델은 계절 변동이 일정하다고 가정하지만, 시간이 지남에 따라 일정하다고 가정하지는 않습니다. 이 옵션은 가산적 모델의 본질을 부정확하게 설명하고 있습니다.",
            "노이즈는 계절성이나 추세에 기인할 수 없는 데이터의 무작위 변동을 나타내며, 시계열의 일관된 부분이 아닙니다. 따라서 이 옵션은 잘못된 것입니다."
        ]
    },
    {
        "Question Number": "26",
        "Situation": "금융 서비스 회사는 매일 대량의 거래 데이터를 처리해야 합니다. 그들은 데이터가 일관되게 변환되고 분석 데이터베이스에 로드되도록 데이터 처리 작업의 일정을 자동화하고자 합니다.",
        "Question": "운영 오버헤드를 최소화하면서 이러한 데이터 처리 작업을 예약하고 실행하는 완전 관리형 솔루션은 무엇인가요?",
        "Options": {
            "1": "AWS Step Functions 상태 기계를 생성하여 데이터 처리를 조정하고 Amazon EventBridge의 예약 규칙을 사용합니다.",
            "2": "Amazon EC2 인스턴스를 설정하여 데이터 처리 스크립트를 트리거하는 cron 작업을 실행합니다.",
            "3": "AWS Lambda를 사용하여 데이터 처리 스크립트를 실행하고 Amazon EventBridge를 구성하여 일정에 따라 트리거합니다.",
            "4": "Amazon CloudWatch Events를 사용하여 일정에 따라 데이터 처리 작업을 실행하는 Amazon ECS 클러스터를 배포합니다."
        },
        "Correct Answer": "AWS Lambda를 사용하여 데이터 처리 스크립트를 실행하고 Amazon EventBridge를 구성하여 일정에 따라 트리거합니다.",
        "Explanation": "AWS Lambda와 Amazon EventBridge를 사용하면 자동으로 확장되고 서버 관리가 필요 없는 완전 관리형 솔루션을 제공합니다. 이 접근 방식은 운영 오버헤드를 최소화하고 Lambda 실행 시간 제한 내에서 실행할 수 있는 작업에 비용 효율적입니다.",
        "Other Options": [
            "Amazon EC2 인스턴스를 설정하여 cron 작업을 실행하는 것은 EC2 인스턴스를 관리해야 하며, 패치, 확장 및 가용성을 보장해야 하므로 운영 오버헤드가 증가합니다.",
            "Amazon ECS 클러스터를 Fargate와 함께 배포하면 일부 관리 측면을 단순화할 수 있지만, AWS Lambda에 비해 더 많은 구성 및 모니터링이 필요하므로 간단한 예약 작업에는 덜 효율적입니다.",
            "AWS Step Functions 상태 기계를 생성하는 것은 간단한 데이터 처리 작업에 불필요한 복잡성을 도입합니다. 워크플로를 조정하는 데 강력하지만, 간단한 예약 작업 실행에는 가장 적합하지 않습니다."
        ]
    },
    {
        "Question Number": "27",
        "Situation": "소매 회사가 제품 이미지를 미리 정의된 클래스에 자동으로 분류하는 이미지 분류 시스템을 개발하고 있습니다. 그들은 다양한 방향과 조명 조건에서도 제품을 정확하게 식별할 수 있도록 고급 기술을 활용하고자 합니다.",
        "Question": "이 이미지 분류 작업에 대해 Convolutional Neural Networks (CNNs)의 어떤 기능이 특히 효과적인가요?",
        "Options": {
            "1": "CNN은 이미지 데이터를 처리하기 위해 여러 개의 완전 연결된 레이어를 사용합니다.",
            "2": "CNN은 컨볼루션 레이어를 통해 특징의 공간적 계층 구조를 학습할 수 있습니다.",
            "3": "CNN은 효과적으로 작동하기 위해 많은 양의 레이블이 있는 데이터가 필요합니다.",
            "4": "CNN은 특징 추출을 위해 전통적인 이미지 처리 기술에만 의존합니다."
        },
        "Correct Answer": "CNN은 컨볼루션 레이어를 통해 특징의 공간적 계층 구조를 학습할 수 있습니다.",
        "Explanation": "Convolutional Neural Networks (CNNs)는 컨볼루션 레이어를 사용하여 이미지에서 특징의 공간적 계층 구조를 자동으로 학습하도록 설계되었습니다. 이 레이어는 다양한 공간 해상도에서 여러 필터를 적용하여 다양한 특징을 추출합니다. 이 기능 덕분에 CNN은 다양한 조건에서도 객체를 효과적으로 식별하고 분류할 수 있습니다.",
        "Other Options": [
            "CNN에는 완전 연결된 레이어가 있지만, 그들의 주요 강점은 특징을 추출하는 컨볼루션 레이어에 있으며, 특징 추출 후 분류에 일반적으로 사용되는 완전 연결된 레이어에는 없습니다.",
            "CNN은 많은 양의 레이블이 있는 데이터에서 이점을 얻을 수 있지만, 그들의 독특한 구조 덕분에 전통적인 방법에 비해 적은 예제에서도 일반화할 수 있습니다. 따라서 이 진술은 그들의 효과성을 설명하는 주요 이유가 아닙니다.",
            "CNN은 전통적인 이미지 처리 기술에만 의존하지 않으며, 학습된 필터와 레이어를 사용하여 자동으로 특징을 추출하므로 CNN의 작동 방식에 대한 이 진술은 부정확합니다."
        ]
    },
    {
        "Question Number": "28",
        "Situation": "머신 러닝 엔지니어가 AWS 환경에서 API 활동 및 리소스 변경 사항을 추적하기 위한 모니터링 솔루션을 설정하는 임무를 맡았습니다. 그들은 AWS 서비스에서 수행된 작업을 효과적으로 기록하고 모니터링할 수 있도록 해야 합니다.",
        "Question": "엔지니어가 AWS 환경에서 변경 사항을 기록하고 모니터링하기 위해 어떤 AWS 서비스를 활용해야 합니까? (두 가지 선택)",
        "Options": {
            "1": "Amazon CloudWatch Logs",
            "2": "AWS Config",
            "3": "Amazon S3",
            "4": "Amazon RDS",
            "5": "AWS CloudTrail"
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "AWS CloudTrail",
            "AWS Config"
        ],
        "Explanation": "AWS CloudTrail은 사용자가 AWS 인프라 전반에 걸쳐 수행된 작업과 관련된 계정 활동을 기록하고 지속적으로 모니터링하며 보존할 수 있도록 해줍니다. 이는 API 활동을 추적하는 데 필수적입니다. AWS Config는 AWS 리소스의 구성에 대한 자세한 정보를 제공하고 시간에 따른 변경 사항을 추적하여 규정 준수 감사 및 보안 분석을 가능하게 합니다.",
        "Other Options": [
            "Amazon CloudWatch Logs는 AWS 서비스에서 로그 파일을 수집하고 모니터링하는 데 주로 사용되지만 API 활동이나 리소스 변경 사항에 대한 완전한 로깅 솔루션을 제공하지 않습니다.",
            "Amazon S3는 객체 저장 서비스로, AWS 환경의 변경 사항이나 API 활동을 직접적으로 기록하고 모니터링하는 데 관련이 없습니다.",
            "Amazon RDS는 관리형 데이터베이스 서비스로, AWS 환경의 변경 사항이나 API 활동에 대한 로깅 또는 모니터링 기능을 제공하지 않습니다."
        ]
    },
    {
        "Question Number": "29",
        "Situation": "데이터 과학자가 결측치가 있는 데이터셋을 사용하여 예측 모델을 작업하고 있으며, 목표 클래스 간의 불균형이 있습니다. 데이터셋에는 여러 특성이 포함되어 있으며, 그 중 일부는 모델의 성능에 영향을 미칠 수 있는 결측치를 가지고 있습니다. 데이터 과학자는 모델 훈련을 진행하기 전에 결측치와 데이터셋의 불균형을 처리하는 방법을 결정해야 합니다.",
        "Question": "데이터 과학자가 데이터셋의 결측치를 처리하면서 클래스 불균형도 해결하기 위해 어떤 전략을 사용해야 합니까?",
        "Options": {
            "1": "K 최근접 이웃을 사용하여 결측치를 보완하고 소수 클래스에 대한 추가 예제를 생성합니다.",
            "2": "결측치가 있는 특성을 삭제하고 데이터 증강을 적용하여 더 많은 샘플을 생성합니다.",
            "3": "결측치가 있는 모든 행을 제거하고 클래스 분포를 무시합니다.",
            "4": "평균으로 결측치를 보완하고 다수 클래스를 언더샘플링합니다."
        },
        "Correct Answer": "K 최근접 이웃을 사용하여 결측치를 보완하고 소수 클래스에 대한 추가 예제를 생성합니다.",
        "Explanation": "K 최근접 이웃을 사용하여 결측치를 보완하는 것은 인근 샘플의 값을 기반으로 결측치를 예측하기 때문에 효과적이며, 이는 더 나은 성능으로 이어질 수 있습니다. 또한 소수 클래스에 대한 새로운 예제를 생성하는 것은 데이터셋의 균형을 맞추는 데 도움이 되어 모델을 더 강력하게 만듭니다.",
        "Other Options": [
            "결측치가 있는 모든 행을 제거하면 많은 샘플에 결측치가 있는 경우 상당한 데이터 손실이 발생할 수 있으며, 클래스 분포를 무시하면 불균형으로 인해 모델 성능이 악화될 수 있습니다.",
            "평균으로 보완하는 것은 데이터를 지나치게 단순화할 수 있으며, 값의 실제 분포를 포착하지 못할 수 있습니다. 다수 클래스를 언더샘플링하면 잠재적으로 유용한 정보가 손실될 수 있습니다.",
            "결측치가 있는 특성을 삭제하면 데이터셋에서 잠재적으로 유용한 정보가 제거되며, 데이터 증강이 도움이 될 수 있지만 원본 데이터에 결측치가 상당한 경우에는 효과가 떨어질 수 있습니다."
        ]
    },
    {
        "Question Number": "30",
        "Situation": "소매 회사가 다양한 고객 및 제품 특성을 기반으로 판매를 예측하는 모델을 구축하고 있습니다. 모델의 효율성과 정확성을 개선하기 위해 데이터 과학 팀은 가장 관련성이 높은 특성을 식별하고 기존 데이터셋에서 새로운 특성을 생성할 가능성을 고려해야 합니다. 그들은 특성 선택 및 엔지니어링 프로세스를 향상시키기 위해 어떤 접근 방식을 우선시해야 합니까?",
        "Question": "데이터 과학 팀이 필수 정보를 유지하면서 특성 집합의 차원을 줄이기 위해 사용할 수 있는 기술은 무엇입니까?",
        "Options": {
            "1": "무관한 특성을 제거하기 위해 랜덤 포레스트를 활용하여 특성 중요도 순위를 매깁니다.",
            "2": "기존 특성 간의 상관관계를 식별하기 위해 선형 회귀 모델을 적용합니다.",
            "3": "주성분 분석(PCA)을 구현하여 특성 집합을 저차원 공간으로 변환합니다.",
            "4": "모델 성능에 따라 상위 특성을 선택하기 위해 재귀적 특성 제거(RFE)를 사용합니다."
        },
        "Correct Answer": "주성분 분석(PCA)을 구현하여 특성 집합을 저차원 공간으로 변환합니다.",
        "Explanation": "PCA는 데이터셋의 분산을 보존하면서 차원을 줄이는 효과적인 비지도 학습 기술입니다. 이는 가장 중요한 특성을 식별하고 이를 상관관계가 없는 더 작은 구성 요소 집합으로 변환하는 데 도움이 됩니다.",
        "Other Options": [
            "RFE는 유용한 기술이지만 주로 모델 성능에 의존하며 PCA만큼 효과적으로 차원을 줄이지는 않습니다.",
            "선형 회귀는 상관관계를 보여줄 수 있지만 차원 문제를 직접적으로 해결하지 못하며, 너무 많은 특성을 사용할 경우 과적합으로 이어질 수 있습니다.",
            "랜덤 포레스트는 특성 중요도를 순위 매길 수 있지만 PCA처럼 특성 집합의 차원을 본질적으로 줄이지는 않습니다."
        ]
    },
    {
        "Question Number": "31",
        "Situation": "소매 회사가 고객 구매 행동을 예측하기 위해 머신 러닝 모델을 개발했습니다. 이 모델을 AWS SageMaker를 사용하여 실시간으로 예측할 수 있도록 배포하고자 합니다. 모델은 이미 훈련이 완료되었으며, 애플리케이션에서 접근할 수 있도록 올바르게 배포되었는지 확인해야 합니다.",
        "Question": "실시간 예측을 위해 Amazon SageMaker에서 모델을 배포하는 올바른 단계 순서는 무엇입니까?",
        "Options": {
            "1": "훈련 작업에서 훈련 이미지를 전달하고, 엔드포인트를 생성한 후, 엔드포인트를 호출하여 예측을 얻습니다.",
            "2": "ECR에서 훈련 이미지를 선택하고, 모델 정의를 생성한 후, 엔드포인트 구성을 생성합니다.",
            "3": "모델 정의를 생성하고, IAM 역할을 선택하며, 모델 S3 위치를 지정한 후, 엔드포인트 구성을 사용하여 엔드포인트를 생성합니다.",
            "4": "엔드포인트 구성을 생성하고, IAM 역할을 선택한 후, 예측을 위해 엔드포인트를 생성합니다."
        },
        "Correct Answer": "모델 정의를 생성하고, IAM 역할을 선택하며, 모델 S3 위치를 지정한 후, 엔드포인트 구성을 사용하여 엔드포인트를 생성합니다.",
        "Explanation": "이 순서는 SageMaker에서 모델을 배포하는 데 필요한 단계를 정확하게 반영합니다. 먼저 훈련된 모델의 S3 위치와 ECR 이미지를 사용하여 모델 정의를 생성하고, 권한을 위한 IAM 역할을 지정한 후, 마지막으로 예측을 위한 엔드포인트 구성과 엔드포인트 자체를 생성합니다.",
        "Other Options": [
            "이 옵션은 모델 정의 전에 엔드포인트 구성을 생성하라고 잘못 제안하고 있으며, 엔드포인트 구성은 기존 모델이 필요하기 때문에 불가능합니다.",
            "이 옵션은 배포 단계가 발생하기 전에 모델 정의를 먼저 생성해야 한다고 언급하지 않고 있습니다.",
            "이 옵션은 엔드포인트를 호출하는 것을 포함하고 있지만, 모델 정의 및 엔드포인트 구성과 같은 중요한 초기 단계를 건너뛰어 배포에 불완전합니다."
        ]
    },
    {
        "Question Number": "32",
        "Situation": "데이터 과학자는 구독 기반 서비스의 고객 이탈을 예측하는 임무를 맡았습니다. 팀은 예측의 정확성을 높이기 위해 앙상블 학습 방법을 사용하기로 결정했습니다. 그들은 결정 트리, 로지스틱 회귀 및 랜덤 포레스트를 포함하여 선택할 수 있는 여러 모델이 있습니다. 데이터 과학자는 이러한 모델의 예측을 결합하여 강력한 최종 예측을 생성할 수 있는 앙상블 방법을 고려하고 있습니다.",
        "Question": "데이터 과학자가 여러 모델의 출력을 결합하여 더 나은 예측 성능을 달성하기 위해 어떤 앙상블 방법을 사용해야 합니까?",
        "Options": {
            "1": "Boosting",
            "2": "Stacking",
            "3": "Bagging",
            "4": "Voting"
        },
        "Correct Answer": "Stacking",
        "Explanation": "Stacking은 여러 모델을 결합하여 메타 모델을 훈련시켜 예측을 최적으로 결합하는 방법을 배우게 하는 앙상블 방법으로, 개별 모델보다 예측 성능이 향상되는 경우가 많습니다. 이 방법은 데이터 과학자가 다양한 알고리즘의 강점을 효과적으로 활용할 수 있게 합니다.",
        "Other Options": [
            "Bagging은 데이터의 서로 다른 하위 집합에서 여러 모델을 훈련하고 예측을 평균하여 분산을 줄이는 방법으로 효과적이지만, Stacking만큼 개별 모델의 강점을 효과적으로 활용하지 못할 수 있습니다.",
            "Boosting은 모델을 순차적으로 결합하여 각 새로운 모델이 이전 모델의 오류에 집중하게 하며, 이는 과적합으로 이어질 수 있고 데이터 과학자의 특정 작업 요구에 적합하지 않을 수 있습니다.",
            "Voting은 여러 모델의 출력을 다수결 또는 평균으로 결합하는 간단한 접근 방식으로, Stacking만큼 데이터의 복잡한 관계를 효과적으로 포착하지 못할 수 있습니다."
        ]
    },
    {
        "Question Number": "33",
        "Situation": "머신 러닝 엔지니어는 시간이 지남에 따라 변화하는 데이터 패턴에 적응할 수 있는 모델을 개발하는 임무를 맡았습니다. 모델은 예측의 정확성과 관련성을 유지하기 위해 정기적으로 재훈련되어야 합니다. 엔지니어는 이 프로세스를 자동화하는 강력한 재훈련 파이프라인을 구현해야 합니다.",
        "Question": "엔지니어가 모델을 위한 효과적인 재훈련 파이프라인을 만들기 위해 어떤 접근 방식을 구현해야 합니까?",
        "Options": {
            "1": "AWS Step Functions를 사용하여 재훈련 작업을 조정하고 AWS Lambda를 사용하여 파이프라인을 트리거합니다.",
            "2": "AWS Glue를 사용하여 데이터를 전처리한 후 Amazon EMR을 사용하여 모델을 재훈련합니다.",
            "3": "AWS Data Pipeline을 활용하여 재훈련 작업을 예약하고 모델 아티팩트를 Amazon S3에 저장합니다.",
            "4": "Amazon SageMaker Pipelines를 활용하여 전체 재훈련 워크플로를 정의, 자동화 및 관리합니다."
        },
        "Correct Answer": "Amazon SageMaker Pipelines를 활용하여 전체 재훈련 워크플로를 정의, 자동화 및 관리합니다.",
        "Explanation": "Amazon SageMaker Pipelines는 머신 러닝 워크플로를 생성, 자동화 및 관리하기 위한 완전 관리형 서비스를 제공하여 변화하는 데이터 패턴에 적응할 수 있는 효과적인 재훈련 파이프라인을 구축하는 데 이상적인 선택입니다.",
        "Other Options": [
            "AWS Step Functions와 AWS Lambda를 사용하는 것은 작업을 조정하는 데 유효한 옵션이지만, 효과적인 재훈련에 필요한 전체 머신 러닝 워크플로 관리 기능을 제공하지 않을 수 있습니다.",
            "AWS Data Pipeline은 데이터 워크플로를 예약하고 관리할 수 있지만, 원활한 모델 재훈련에 필수적인 특정 머신 러닝 파이프라인 기능이 부족합니다.",
            "AWS Glue는 주로 데이터 준비 및 ETL 프로세스를 위한 것이며, Amazon EMR은 빅 데이터 처리에 적합하지만, 둘 다 포괄적인 재훈련 파이프라인의 필요를 직접적으로 해결하지 않습니다."
        ]
    },
    {
        "Question Number": "34",
        "Situation": "금융 기관이 대출 채무 불이행 가능성을 더 잘 예측하기 위해 신용 점수 모델을 개선하려고 합니다. 그들은 소득, 신용 기록 및 기존 부채를 포함한 다양한 특성을 가진 다양한 데이터 세트를 보유하고 있습니다. 데이터 과학 팀은 최상의 예측 성능을 달성하기 위해 여러 모델링 기법을 고려하고 있으며, 특히 비선형 관계 및 특성 간의 상호작용을 처리할 수 있는 방법에 관심이 있습니다.",
        "Question": "이 작업에 가장 적합한 기계 학습 기법은 무엇이며 그 이유는 무엇입니까?",
        "Options": {
            "1": "로지스틱 회귀는 해석 가능한 결과를 제공하고 이진 분류 작업에 잘 작동하기 때문에.",
            "2": "선형 회귀는 선형 관계를 포착하고 구현이 간단하기 때문에.",
            "3": "K-평균은 유사한 고객 프로필을 클러스터링할 수 있기 때문에 데이터 이해에 도움이 될 수 있습니다.",
            "4": "랜덤 포레스트는 광범위한 특성 엔지니어링 없이 비선형 관계 및 상호작용을 효과적으로 모델링할 수 있기 때문에."
        },
        "Correct Answer": "랜덤 포레스트는 광범위한 특성 엔지니어링 없이 비선형 관계 및 상호작용을 효과적으로 모델링할 수 있기 때문에.",
        "Explanation": "랜덤 포레스트는 여러 결정 트리를 사용하여 강력한 예측을 제공하는 앙상블 방법입니다. 비선형 관계를 처리하고 특성 간의 상호작용을 포착하는 데 특히 효과적이며, 신용 점수와 같은 복잡한 데이터 세트에 적합합니다.",
        "Other Options": [
            "로지스틱 회귀는 선형 관계에 제한되어 있으며 데이터 세트의 상호작용의 복잡성을 포착하지 못하므로 이 시나리오에서 랜덤 포레스트보다 효과적이지 않습니다.",
            "K-평균은 주로 클러스터링 알고리즘이며 대출 채무 불이행 예측을 직접적으로 다루지 않으며, 이는 분류가 필요한 감독 학습 작업입니다.",
            "선형 회귀는 로지스틱 회귀와 마찬가지로 관계의 선형성을 가정하며 비선형 상호작용을 효과적으로 처리하지 못하므로 신용 점수의 복잡성에 적합하지 않습니다."
        ]
    },
    {
        "Question Number": "35",
        "Situation": "AI 엔지니어가 이미지를 분류하기 위한 딥 러닝 모델을 설계하고 있습니다. 그는 다양한 신경망 아키텍처를 고려하고 있으며 모델 성능을 최적화하기 위해 학습률 및 활성화 함수에 대한 결정을 내려야 합니다.",
        "Question": "모델 수렴 및 성능을 개선하기 위해 엔지니어가 우선적으로 고려해야 할 아키텍처 선택은 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "학습을 안정화하기 위해 배치 정규화를 구현합니다.",
            "2": "은닉층의 활성화 함수로 ReLU를 사용합니다.",
            "3": "더 빠른 수렴을 위해 매우 높은 학습률을 설정합니다.",
            "4": "출력층의 활성화 함수로 시그모이드 함수를 사용합니다.",
            "5": "과적합을 방지하기 위해 드롭아웃 정규화를 적용합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "은닉층의 활성화 함수로 ReLU를 사용합니다.",
            "학습을 안정화하기 위해 배치 정규화를 구현합니다."
        ],
        "Explanation": "은닉층의 활성화 함수로 ReLU를 사용하면 기울기 소실 문제를 완화하여 더 빠른 훈련과 딥 네트워크에서 더 나은 성능을 가능하게 합니다. 배치 정규화를 구현하면 각 층의 입력을 정규화하여 학습 과정을 상당히 안정화하고 수렴을 개선하여 훈련을 더 빠르고 신뢰할 수 있게 만듭니다.",
        "Other Options": [
            "매우 높은 학습률을 설정하면 훈련 중 불안정성과 발산을 초래하여 모델이 제대로 수렴하지 못하게 됩니다.",
            "출력층의 활성화 함수로 시그모이드 함수를 사용하는 것은 다중 클래스 분류 작업에 이상적이지 않으며, 일반적으로 소프트맥스가 클래스 간 확률 분포를 출력하는 데 선호됩니다.",
            "드롭아웃 정규화를 적용하는 것은 과적합을 방지하는 데 유익하지만, 다른 옵션들처럼 수렴 속도나 초기 모델 성능에 직접적인 영향을 미치지 않습니다."
        ]
    },
    {
        "Question Number": "36",
        "Situation": "데이터 과학자가 구독 기반 서비스의 고객 이탈을 예측하기 위한 신경망 모델을 작업하고 있습니다. 모델 아키텍처에는 입력, 은닉 및 출력 층이 포함되며, 적절한 활성화 함수와 최적화 기법을 통해 성능을 개선하는 것을 목표로 합니다.",
        "Question": "신경망의 은닉층에서 활성화 함수의 역할에 대한 다음 진술 중 어떤 것이 TRUE입니까?",
        "Options": {
            "1": "활성화 함수는 비선형성을 도입하여 네트워크가 복잡한 패턴을 학습할 수 있게 합니다.",
            "2": "활성화 함수는 신경망의 훈련 과정에 영향을 미치지 않습니다.",
            "3": "활성화 함수는 시그모이드 유형만 가능하며, 이는 모든 신경망에 가장 효과적입니다.",
            "4": "활성화 함수는 입력에 관계없이 모든 출력을 0과 1 사이로 보장합니다."
        },
        "Correct Answer": "활성화 함수는 비선형성을 도입하여 네트워크가 복잡한 패턴을 학습할 수 있게 합니다.",
        "Explanation": "ReLU 및 Tanh와 같은 활성화 함수는 네트워크에 비선형성을 도입하여 데이터의 복잡한 관계를 학습할 수 있게 합니다. 이는 모델이 일반화하고 보지 못한 데이터에서 잘 작동하는 데 중요합니다.",
        "Other Options": [
            "일부 활성화 함수인 시그모이드가 출력을 0과 1 사이로 제한할 수 있지만, 이는 ReLU와 같이 1보다 큰 값을 출력할 수 있는 모든 활성화 함수에 해당하지 않습니다.",
            "이 진술은 잘못되었습니다. 시그모이드는 하나의 활성화 함수 유형일 뿐이며, ReLU 및 Tanh와 같은 다른 함수도 일반적으로 사용되며 다양한 시나리오에서 시그모이드보다 더 나은 성능을 발휘할 수 있습니다.",
            "이 진술은 잘못되었습니다. 활성화 함수는 비선형성을 도입하여 모델이 데이터에서 학습할 수 있도록 하여 훈련 과정에서 중요한 역할을 합니다."
        ]
    },
    {
        "Question Number": "37",
        "Situation": "한 금융 서비스 회사가 실시간으로 사기 거래를 감지하기 위해 머신 러닝 모델을 구현하려고 합니다. 이 회사는 거래 금액, 위치 및 결제 방법과 같은 다양한 기능을 포함하는 과거 거래 데이터에 접근할 수 있습니다. 그들은 모델이 시간이 지남에 따라 새로운 사기 패턴에 적응할 수 있도록 하고, significant downtime 없이 새로운 데이터로 쉽게 업데이트될 수 있도록 해야 합니다.",
        "Question": "회사가 머신 러닝 모델이 효과적이고 시간이 지남에 따라 적응할 수 있도록 하려면 어떤 접근 방식을 취해야 합니까?",
        "Options": {
            "1": "새로운 거래 데이터가 들어올 때 실시간으로 업데이트되는 온라인 학습 모델을 구현합니다.",
            "2": "사전 훈련된 모델을 사용하고 마지막 레이어만 수정하여 거래를 사기 또는 정당으로 분류합니다.",
            "3": "과거 데이터를 한 번만 분석하고 추가 업데이트 없이 배포되는 모델을 개발합니다.",
            "4": "최신 과거 데이터를 사용하여 매달 모델을 재훈련하는 배치 처리 파이프라인을 구축합니다."
        },
        "Correct Answer": "새로운 거래 데이터가 들어올 때 실시간으로 업데이트되는 온라인 학습 모델을 구현합니다.",
        "Explanation": "온라인 학습 모델은 새로운 데이터가 수신될 때 시스템이 지속적으로 적응할 수 있게 하여, 실시간으로 진화하는 사기 패턴을 감지하는 데 이상적입니다. 이는 모델이 최신 상태를 유지하고 새로운 유형의 사기에 대해 효과적임을 보장합니다.",
        "Other Options": [
            "매달 모델을 재훈련하는 배치 처리 파이프라인은 새로운 사기 패턴을 충분히 빠르게 포착하지 못할 수 있어, 모델이 업데이트되기 전에 잠재적인 손실이 발생할 수 있습니다.",
            "사전 훈련된 모델을 사용하면 회사의 거래 데이터의 특정 뉘앙스를 포착하지 못할 수 있으며, 마지막 레이어만 수정하는 것으로는 정확한 예측이 충분하지 않을 수 있습니다.",
            "과거 데이터를 한 번만 분석하는 모델을 개발하면 빠르게 구식이 되어 시간이 지남에 따라 나타나는 새로운 사기 패턴에 적응하지 못하게 됩니다."
        ]
    },
    {
        "Question Number": "38",
        "Situation": "데이터 엔지니어가 실시간 분석 애플리케이션을 위한 데이터 수집 파이프라인을 설계하는 임무를 맡았습니다. 이 애플리케이션은 IoT 장치 및 애플리케이션 로그를 포함한 다양한 소스에서 거의 실시간으로 데이터를 수집해야 하며, 나중에 쉽게 쿼리할 수 있는 형식으로 이 데이터를 저장해야 합니다. 엔지니어는 이 솔루션을 구현하기 위해 다양한 AWS 서비스를 고려하고 있습니다.",
        "Question": "이 애플리케이션을 위한 거의 실시간 데이터 수집 및 저장을 제공하는 가장 효율적인 AWS 서비스 조합은 무엇입니까?",
        "Options": {
            "1": "처리 전에 들어오는 데이터를 임시로 저장하기 위해 Amazon ElastiCache를 사용합니다.",
            "2": "데이터 수집 전에 Amazon RDS에 저장된 데이터에 대해 ETL을 수행하기 위해 AWS Glue를 사용합니다.",
            "3": "IoT 장치에서 직접 데이터를 수집하기 위해 Amazon Redshift를 사용합니다.",
            "4": "데이터 수집을 위해 Kinesis Data Streams를 사용하고 Kinesis Data Firehose를 사용하여 Amazon S3에 데이터를 저장합니다."
        },
        "Correct Answer": "데이터 수집을 위해 Kinesis Data Streams를 사용하고 Kinesis Data Firehose를 사용하여 Amazon S3에 데이터를 저장합니다.",
        "Explanation": "Kinesis Data Streams는 실시간 데이터 수집을 가능하게 하며, Kinesis Data Firehose를 사용하여 Amazon S3에 데이터를 저장하면 나중에 쿼리 및 분석을 위해 거의 실시간으로 데이터에 접근할 수 있습니다. 이 조합은 실시간 분석 애플리케이션에서 일반적으로 요구되는 높은 처리량과 낮은 대기 시간 시나리오에 적합합니다.",
        "Other Options": [
            "RDS 데이터에 대해 ETL을 수행하기 위해 AWS Glue를 사용하는 것은 실시간 수집을 지원하지 않으므로 이 애플리케이션에 적합하지 않습니다.",
            "Amazon ElastiCache는 주로 캐싱 서비스이며 지속적인 데이터 저장 또는 실시간 데이터 수집을 위해 설계되지 않았으므로 이 사용 사례에 적합하지 않습니다.",
            "Amazon Redshift는 OLAP를 위한 데이터 웨어하우징 솔루션이며 IoT 장치에서 직접 수집하기에 최적화되어 있지 않아 요구되는 거의 실시간 처리에 비효율적입니다."
        ]
    },
    {
        "Question Number": "39",
        "Situation": "데이터 엔지니어가 소매 조직의 분석을 촉진하기 위해 AWS에서 데이터 레이크 아키텍처를 설계하고 있습니다. 이 아키텍처는 실시간 데이터 수집을 위해 Amazon Kinesis를 활용하고, 데이터를 Amazon S3에 저장하며, Amazon Athena를 사용하여 쿼리합니다. 또한, 엔지니어는 민감한 데이터가 암호화되고 접근이 안전하게 관리되도록 해야 합니다.",
        "Question": "데이터 엔지니어가 보안을 보장하고 쿼리의 용이성을 확보하면서 이 아키텍처를 구현하기 위해 어떤 AWS 서비스 조합을 활용해야 합니까?",
        "Options": {
            "1": "Amazon Redshift를 데이터 웨어하우스로 구성하고 S3를 백업 저장소로 사용합니다.",
            "2": "데이터 스트리밍을 위해 Amazon Kinesis를 구현하고, 원시 데이터를 S3에 저장하며, SQL 쿼리를 위해 Athena를 사용합니다.",
            "3": "S3의 데이터를 카탈로그하기 위해 AWS Glue를 사용하고 암호화를 위한 버킷 정책을 설정합니다.",
            "4": "데이터 저장을 위해 Amazon S3를 활용하고 데이터 변환을 트리거하기 위해 AWS Lambda를 사용합니다."
        },
        "Correct Answer": "데이터 스트리밍을 위해 Amazon Kinesis를 구현하고, 원시 데이터를 S3에 저장하며, SQL 쿼리를 위해 Athena를 사용합니다.",
        "Explanation": "이 옵션은 실시간 데이터 수집, 저장 및 쿼리를 위한 완전한 솔루션을 설명합니다. Amazon Kinesis를 사용하여 스트리밍 데이터를 수집하고, S3에 데이터를 저장하며, Athena로 쿼리함으로써 데이터 엔지니어는 효율적이고 비용 효과적인 데이터 레이크 아키텍처를 만들 수 있습니다.",
        "Other Options": [
            "이 옵션은 실시간 데이터 수집이나 쿼리를 언급하지 않고 AWS Glue를 카탈로그 용도로만 사용하는 것을 잘못 제안하고 있으며, 이는 아키텍처에 필수적입니다.",
            "이 옵션은 SQL을 사용하여 S3에서 직접 데이터를 쿼리하는 데 필요한 주요 요구 사항이 아닌 변환을 위해 AWS Lambda에 초점을 맞추고 있으며, 실시간 수집에 대한 언급이 없습니다.",
            "이 옵션은 데이터 웨어하우스 솔루션인 Amazon Redshift를 사용하는 것을 권장하며, 데이터 레이크 아키텍처가 아닙니다. 이는 실시간 수집 및 쿼리 요구 사항을 다루지 않습니다."
        ]
    },
    {
        "Question Number": "40",
        "Situation": "머신 러닝 엔지니어가 모델 훈련을 위한 레이블이 있는 데이터가 필요한 새로운 프로젝트를 진행하고 있습니다. 엔지니어는 사용 가능한 레이블이 있는 데이터가 충분한지 평가해야 하며, 데이터 레이블링 서비스를 사용하는 것을 고려하고 있습니다.",
        "Question": "엔지니어가 프로젝트에 충분한 레이블이 있는 데이터가 있는지 결정하기 위한 가장 효과적인 접근 방식은 무엇입니까?",
        "Options": {
            "1": "문서와 이전 프로젝트를 검토하여 현재 모델에 필요한 레이블이 있는 데이터의 양을 추정합니다.",
            "2": "Amazon Mechanical Turk를 활용하여 새로운 레이블이 있는 데이터를 수집하고 작업자 리뷰를 통해 품질을 평가합니다.",
            "3": "외부 검증 없이 마크되지 않은 데이터에 대한 레이블을 생성하기 위해 맞춤형 데이터 레이블링 도구를 구현합니다.",
            "4": "기존 데이터 세트를 분석하고 데이터 수집 결정을 내리기 전에 레이블을 수동으로 확인합니다."
        },
        "Correct Answer": "Amazon Mechanical Turk를 활용하여 새로운 레이블이 있는 데이터를 수집하고 작업자 리뷰를 통해 품질을 평가합니다.",
        "Explanation": "Amazon Mechanical Turk를 사용하면 레이블이 있는 데이터를 수집하는 확장 가능하고 효율적인 방법을 제공하며, 작업자 리뷰를 통해 품질 관리를 가능하게 하여 모델 훈련에 신뢰할 수 있는 데이터를 보장합니다.",
        "Other Options": [
            "기존 데이터 세트를 분석하는 것은 레이블이 있는 데이터의 충분성에 대한 포괄적인 이해를 제공하지 않을 수 있으며, 데이터의 격차와 편향을 간과할 수 있습니다.",
            "맞춤형 데이터 레이블링 도구를 구현하면 레이블링의 일관성이 떨어지고 외부 검증이 부족하여 훈련 데이터의 품질이 저하될 수 있습니다.",
            "문서와 이전 프로젝트를 검토하면 추정치를 제공하지만 현재 프로젝트의 레이블링 요구 사항에 대한 실시간 데이터나 통찰력을 제공하지 않습니다."
        ]
    },
    {
        "Question Number": "41",
        "Situation": "데이터 과학자가 머신 러닝 모델 훈련을 위한 데이터 세트를 준비하고 있으며, 모델 성능을 개선하고 훈련 시간을 줄이기 위해 특성 집합을 최적화하고자 합니다.",
        "Question": "데이터 과학자가 특성을 효과적으로 선택하고 엔지니어링하기 위해 고려해야 할 행동은 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "훈련 과정을 단순화하기 위해 범주형 특성만 유지합니다.",
            "2": "잠재적으로 가치 있는 정보를 잃지 않기 위해 데이터 세트에 모든 특성을 포함합니다.",
            "3": "PCA를 적용하여 대부분의 분산을 유지하면서 특성 집합의 차원을 줄입니다.",
            "4": "타겟 변수에 중요한 영향을 미치는 모든 선택된 특성을 보장하기 위해 분산이 낮은 특성을 제거합니다.",
            "5": "도메인 지식을 사용하여 기존 특성 간의 비율과 같은 새로운 특성을 생성합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "타겟 변수에 중요한 영향을 미치는 모든 선택된 특성을 보장하기 위해 분산이 낮은 특성을 제거합니다.",
            "PCA를 적용하여 대부분의 분산을 유지하면서 특성 집합의 차원을 줄입니다."
        ],
        "Explanation": "분산이 낮은 특성을 제거하면 데이터 세트에 모델에 의미 있는 정보를 제공하는 특성만 포함되도록 보장합니다. PCA를 적용하면 가장 중요한 정보를 유지하면서 특성의 수를 줄여 모델 훈련 과정을 더 효율적으로 만들고 정확성을 개선할 수 있습니다.",
        "Other Options": [
            "모든 특성을 포함하면 과적합과 훈련 시간이 증가할 수 있으며, 모든 특성이 관련성이 있는 것은 아닙니다.",
            "도메인 지식을 사용하여 새로운 특성을 생성하는 것은 가치가 있지만, 이 맥락에서 선택된 두 가지 옵션 중 하나는 아닙니다; 여기서는 특성을 제거하고 줄이는 데 초점을 맞추고 있습니다.",
            "범주형 특성만 유지하면 수치적 특성의 잠재적인 예측력을 무시하게 되어 중요한 정보를 잃고 모델 성능이 저하될 수 있습니다."
        ]
    },
    {
        "Question Number": "42",
        "Situation": "데이터 과학자가 고객이 프리미엄 서비스에 가입할지 여부를 역사적 행동을 기반으로 예측하는 모델을 구축하는 임무를 맡았습니다. 모델은 가입 가능성을 나타내는 확률 점수를 출력해야 하며, 이는 이진 출력(예 또는 아니오)으로 변환됩니다. 데이터 과학자는 이를 위해 로지스틱 회귀를 사용하기로 결정합니다.",
        "Question": "다음 중 이 이진 분류 작업에 로지스틱 회귀를 사용하는 주요 장점을 정확하게 설명하는 진술은 무엇입니까?",
        "Options": {
            "1": "로지스틱 회귀는 수정 없이 다중 클래스 분류 문제를 처리할 수 있습니다.",
            "2": "로지스틱 회귀는 특성 엔지니어링 없이 특성 간의 복잡한 관계를 모델링할 수 있습니다.",
            "3": "로지스틱 회귀는 0과 1 사이로 제한된 확률을 출력합니다.",
            "4": "로지스틱 회귀는 간단하고 해석 가능한 모델을 제공하며 이상치에 강합니다."
        },
        "Correct Answer": "로지스틱 회귀는 0과 1 사이로 제한된 확률을 출력합니다.",
        "Explanation": "로지스틱 회귀는 이진 결과를 모델링하도록 특별히 설계되었으며, 시그모이드 함수를 사용하여 0에서 1까지의 확률을 출력합니다. 이 특성은 확률 임계값에 따라 이진 예측을 하는 데 필수적입니다.",
        "Other Options": [
            "로지스틱 회귀는 해석 가능하지만, 데이터 세트의 극단값에 의해 상당한 영향을 받을 수 있어 이상치에 강하지 않습니다.",
            "로지스틱 회귀는 선형 모델이며, 특성이 적절하게 변환되거나 엔지니어링되지 않는 한 복잡한 관계를 자동으로 포착하지 않습니다.",
            "로지스틱 회귀는 본질적으로 이진 분류 알고리즘이며, 다중 클래스 분류 작업을 처리하기 위해 수정해야 합니다(예: 일대다 기법 사용)."
        ]
    },
    {
        "Question Number": "43",
        "Situation": "데이터 과학자는 소매 회사의 지난 5년간 월별 판매 수치를 포함하는 데이터 세트를 분석하는 임무를 맡았습니다. 트렌드와 계절적 패턴을 식별하기 위해 데이터 과학자는 데이터를 효과적으로 시각화할 필요가 있습니다. 목표는 데이터에서 주요 통찰력을 강조하여 경영진에게 결과를 발표하는 것입니다.",
        "Question": "데이터 과학자가 계절 변화를 포함하여 시간에 따른 판매 트렌드를 가장 잘 설명하기 위해 어떤 유형의 그래프를 사용해야 합니까?",
        "Options": {
            "1": "월별 판매 수치를 추적하는 시계열 선 그래프.",
            "2": "판매와 마케팅 지출 간의 상관관계를 보여주는 산점도.",
            "3": "중앙값과 사분위를 강조하여 판매 데이터를 요약하는 박스 플롯.",
            "4": "월별 판매 수치의 분포를 표시하는 히스토그램."
        },
        "Correct Answer": "월별 판매 수치를 추적하는 시계열 선 그래프.",
        "Explanation": "시계열 선 그래프는 시간에 따른 트렌드를 설명하는 데 가장 적합한 선택으로, 판매 수치가 월별로 어떻게 변화하는지를 효과적으로 보여주어 계절적 변동과 데이터의 전반적인 트렌드를 쉽게 식별할 수 있게 합니다.",
        "Other Options": [
            "히스토그램은 데이터 세트의 데이터 포인트 분포를 보여주는 데 사용됩니다. 판매 수치가 어떻게 분포되어 있는지에 대한 통찰력을 제공할 수 있지만, 시간에 따른 트렌드를 효과적으로 보여주지는 않습니다.",
            "박스 플롯은 중앙값과 사분위를 보여주어 데이터 요약에 유용할 수 있지만, 시간에 따른 변화를 효과적으로 설명하지는 않습니다.",
            "산점도는 일반적으로 판매와 마케팅 지출과 같은 두 변수 간의 관계를 평가하는 데 사용되며, 시간에 따른 판매 트렌드를 시각화할 필요를 다루지 않습니다."
        ]
    },
    {
        "Question Number": "44",
        "Situation": "데이터 엔지니어는 대량의 역사적 데이터와 실시간 스트리밍 데이터를 처리하는 머신 러닝 프로젝트를 위한 효율적인 데이터 수집 파이프라인을 개발하는 임무를 맡았습니다. 팀은 확장성과 관리 용이성을 보장하기 위해 이 목적을 위해 AWS Glue를 사용하는 것을 고려하고 있습니다.",
        "Question": "AWS Glue의 어떤 기능이 데이터 엔지니어가 배치 및 스트리밍 데이터 수집 워크플로를 조정하는 데 가장 잘 지원할 수 있습니까?",
        "Options": {
            "1": "AWS Glue 데이터 카탈로그",
            "2": "AWS Glue 크롤러",
            "3": "AWS Glue 스튜디오",
            "4": "AWS Glue 작업"
        },
        "Correct Answer": "AWS Glue 스튜디오",
        "Explanation": "AWS Glue 스튜디오는 사용자가 ETL 작업을 생성, 실행 및 모니터링할 수 있는 시각적 인터페이스를 제공하여 배치 및 스트리밍 데이터 수집 워크플로를 효과적으로 조정할 수 있도록 합니다. 복잡한 데이터 파이프라인 구축 과정을 단순화합니다.",
        "Other Options": [
            "AWS Glue 데이터 카탈로그는 주로 메타데이터 관리를 위해 사용되며 데이터 수집 워크플로의 조정을 직접적으로 지원하지 않습니다.",
            "AWS Glue 작업은 실제 변환이 발생하는 컴퓨팅 단위이지만, 배치 및 스트리밍 데이터 워크플로를 관리하는 데 필요한 조정 기능을 제공하지 않습니다.",
            "AWS Glue 크롤러는 데이터 소스를 스캔하고 메타데이터로 데이터 카탈로그를 채우기 위해 설계되었지만, 데이터 수집 파이프라인을 조정하지는 않습니다."
        ]
    },
    {
        "Question Number": "45",
        "Situation": "소매 회사는 제품에 대한 고객 수요를 예측하기 위해 머신 러닝 모델을 사용합니다. 회사는 계절적 트렌드와 프로모션 이벤트로 인해 수요의 변동을 경험합니다. 데이터 과학 팀은 프로모션 이벤트 후 모델을 배치로 업데이트할지, 아니면 새로운 데이터가 들어올 때마다 지속적으로 업데이트되는 실시간 모델을 구현할지를 결정하고 있습니다.",
        "Question": "팀이 프로모션 이벤트 동안 고객 행동의 갑작스러운 변화에 신속하게 모델을 적응시키고 싶다면 어떤 접근 방식을 고려해야 합니까?",
        "Options": {
            "1": "각 프로모션 이벤트 후 모델을 배치로 업데이트하기.",
            "2": "몇 주마다 모델을 주기적으로 재훈련하기.",
            "3": "배치 업데이트와 실시간 업데이트를 결합한 하이브리드 접근 방식.",
            "4": "새로운 데이터가 들어올 때 모델에 대한 실시간 온라인 업데이트."
        },
        "Correct Answer": "새로운 데이터가 들어올 때 모델에 대한 실시간 온라인 업데이트.",
        "Explanation": "실시간 온라인 업데이트는 모델이 고객 행동의 갑작스러운 변화에 신속하게 적응할 수 있게 하여, 수요가 극적으로 그리고 예측할 수 없이 변화할 수 있는 프로모션 이벤트 동안 필수적입니다.",
        "Other Options": [
            "각 프로모션 이벤트 후 모델을 배치로 업데이트하는 것은 갑작스러운 변화에 적응하기에 충분하지 않으며, 모델 업데이트와 반응성에 지연을 초래할 수 있습니다.",
            "배치 업데이트와 실시간 업데이트를 결합한 하이브리드 접근 방식은 불필요한 복잡성을 초래할 수 있으며, 프로모션 이벤트 동안 필요한 즉각적인 적응력을 제공하지 못할 수 있습니다.",
            "몇 주마다 모델을 주기적으로 재훈련하는 것은 고객 행동의 급격한 변화를 포착하기에 민첩하지 않아, 중요한 판매 기간 동안 예측이 구식이 될 수 있습니다."
        ]
    },
    {
        "Question Number": "46",
        "Situation": "데이터 엔지니어는 실시간 판매 데이터와 분석을 위한 역사적 판매 데이터를 처리해야 하는 소매 회사의 데이터 수집 파이프라인을 설계하는 임무를 맡았습니다. 회사는 보고 지연 없이 즉각적인 처리와 배치 처리를 모두 처리할 수 있는 유연성을 보장하고자 합니다.",
        "Question": "이 시나리오에서 실시간 데이터와 역사적 데이터를 효과적으로 처리하기 위해 가장 적합한 데이터 작업 스타일은 무엇인가요?",
        "Options": {
            "1": "역사적 데이터를 로드하기 위한 배치 처리 작업과 실시간 판매 데이터를 위한 별도의 스트리밍 작업을 구현합니다.",
            "2": "역사적 데이터를 수집하면서 실시간 업데이트도 지원하는 마이크로 배치 처리 작업을 설계합니다.",
            "3": "역사적 데이터를 집계하고 이를 주기적으로 분석 플랫폼으로 스트리밍하는 예약 작업을 생성합니다.",
            "4": "역사적 데이터와 실시간 데이터를 동시에 처리하기 위해 단일 스트리밍 작업을 사용합니다."
        },
        "Correct Answer": "역사적 데이터를 로드하기 위한 배치 처리 작업과 실시간 판매 데이터를 위한 별도의 스트리밍 작업을 구현합니다.",
        "Explanation": "이 접근 방식은 배치 처리를 통해 대량의 역사적 데이터를 효율적으로 처리하면서 동시에 스트리밍 작업을 통해 실시간 판매 데이터를 관리할 수 있게 해줍니다. 이러한 분리는 성능을 최적화하고 분석을 위한 적시 업데이트를 보장합니다.",
        "Other Options": [
            "두 가지 유형의 데이터를 모두 처리하기 위해 단일 스트리밍 작업을 사용하는 것은 비효율성을 초래할 수 있습니다. 역사적 데이터 로드는 방대할 수 있으며, 스트리밍 작업이 일반적으로 처리하는 것보다 더 많은 자원을 요구할 수 있어 지연을 초래할 수 있습니다.",
            "마이크로 배치 처리 작업을 설계하는 것은 실시간 데이터 요구에 적합하지 않은 지연을 초래할 수 있습니다. 마이크로 배칭은 실시간 판매 데이터에 필요한 즉각성을 제공하지 않을 수 있습니다.",
            "역사적 데이터를 집계하기 위한 예약 작업을 생성하는 것은 실시간 처리의 필요성을 해결하지 않으며, 최신 판매 데이터에서 적시 통찰력을 제공하지 않기 때문에 구식 분석을 초래할 수 있습니다."
        ]
    },
    {
        "Question Number": "47",
        "Situation": "데이터 과학자는 마케팅 전략을 개선하기 위해 고객 구매 행동을 분석하는 임무를 맡았습니다. 그녀는 사용 가능한 데이터 세트의 특성에 따라 데이터를 효과적으로 모델링하기 위한 올바른 접근 방식을 선택해야 합니다.",
        "Question": "사전 정의된 레이블 없이 뚜렷한 고객 세그먼트를 식별하고자 할 때 데이터 과학자가 사용해야 할 기계 학습 기술의 유형은 무엇인가요?",
        "Options": {
            "1": "레이블이 있는 데이터와 레이블이 없는 데이터를 결합한 반지도 학습 기술.",
            "2": "클러스터링 알고리즘과 같은 비지도 학습 기술.",
            "3": "최적의 의사 결정을 위한 강화 학습 기술.",
            "4": "회귀 분석과 같은 지도 학습 기술."
        },
        "Correct Answer": "클러스터링 알고리즘과 같은 비지도 학습 기술.",
        "Explanation": "비지도 학습 기술은 사전 정의된 레이블이 없는 상황을 위해 특별히 설계되었습니다. K-평균 또는 계층적 클러스터링과 같은 클러스터링 알고리즘은 고객의 행동 유사성에 따라 고객을 효과적으로 그룹화할 수 있으며, 카테고리에 대한 사전 지식이 필요하지 않습니다.",
        "Other Options": [
            "지도 학습 기술은 레이블이 있는 데이터를 기반으로 결과를 예측하는 데 초점을 맞추므로 레이블이 없는 고객 세그먼트를 분할하는 데 적합하지 않습니다.",
            "강화 학습은 행동에 대한 피드백을 기반으로 의사 결정의 순서를 만드는 모델을 훈련하는 데 사용되며, 레이블이 없는 데이터에서 세그먼트를 식별하는 데는 사용되지 않습니다.",
            "반지도 학습은 레이블이 있는 데이터와 레이블이 없는 데이터 모두를 필요로 하므로, 사전 정의된 레이블 없이 순수하게 세그먼트를 나누는 목표에는 적용되지 않습니다."
        ]
    },
    {
        "Question Number": "48",
        "Situation": "데이터 과학자는 고객 이탈을 예측하는 기계 학습 모델을 위한 데이터 세트를 준비하고 있습니다. 데이터 세트에는 많은 결측값, 이상치 및 인코딩이 필요한 범주형 특성이 포함되어 있습니다. 과학자는 데이터가 정리되고 효과적인 모델링을 위해 준비되도록 하려 합니다.",
        "Question": "모델링을 위한 데이터 세트를 정리하고 준비하는 가장 효과적인 접근 방식은 무엇인가요?",
        "Options": {
            "1": "결측값을 평균으로 채우고, 이상치는 그대로 두며, 범주형 특성에 대해 레이블 인코딩을 적용합니다.",
            "2": "결측값을 상수로 대체하고, 모든 수치적 특성을 제거하며, 범주형 특성에 대해 순서형 인코딩을 적용합니다.",
            "3": "결측값이 있는 모든 행을 삭제하고, 이상치를 유지하며, 범주형 특성에 대해 이진 인코딩을 사용합니다.",
            "4": "결측값에 대해 대체 방법을 사용하고, 이상치를 제거하며, 범주형 특성에 대해 원-핫 인코딩을 적용합니다."
        },
        "Correct Answer": "결측값에 대해 대체 방법을 사용하고, 이상치를 제거하며, 범주형 특성에 대해 원-핫 인코딩을 적용합니다.",
        "Explanation": "이 접근 방식은 결측값을 대체 방법으로 처리하여 데이터 무결성을 유지하고, 모델 성능을 왜곡할 수 있는 이상치를 제거하며, 범주형 특성을 적절하게 표현하기 위해 원-핫 인코딩을 활용합니다.",
        "Other Options": [
            "결측값을 평균으로 채우는 것은 데이터 분포를 왜곡할 수 있으며, 이상치를 유지하는 것은 모델 정확도에 부정적인 영향을 미칠 수 있습니다. 레이블 인코딩은 범주형 특성에서 의도하지 않은 순서 관계를 도입할 수 있습니다.",
            "결측값이 있는 행을 삭제하면 상당한 데이터 손실이 발생할 수 있으며, 이상치를 유지하는 것은 모델에 부정적인 영향을 미칠 수 있습니다. 이진 인코딩은 덜 일반적이며 사용되는 모델에 따라 적합하지 않을 수 있습니다.",
            "결측값을 상수로 대체하면 편향이 발생할 수 있으며, 모든 수치적 특성을 제거하면 귀중한 정보가 사라집니다. 순서형 인코딩은 범주형 데이터에 존재하지 않을 수 있는 순위를 부여합니다."
        ]
    },
    {
        "Question Number": "49",
        "Situation": "소매 회사가 고객 구매 데이터를 분석하여 추천 시스템을 개선하고 있습니다. 데이터는 구매 금액과 같은 수치적 특성과 제품 카테고리와 같은 범주적 특성으로 구성되어 있습니다. 머신 러닝 전문가가 이 데이터를 모델링을 위해 준비하는 임무를 맡고 있습니다.",
        "Question": "전문가가 추천 모델 훈련을 위해 데이터를 준비하기 위해 어떤 전처리 단계를 수행해야 합니까?",
        "Options": {
            "1": "모든 수치적 특성에 차원 축소 기법을 적용하고 모든 범주적 특성을 완전히 제거합니다.",
            "2": "수치적 특성에 중앙값 필터를 사용하고 범주적 특성에 레이블 인코딩을 적용하되 정규화는 하지 않습니다.",
            "3": "Min-Max 스케일링을 사용하여 수치적 특성을 정규화하고 원-핫 인코딩을 사용하여 범주적 특성을 인코딩합니다.",
            "4": "모든 특성을 평균이 0이 되도록 스케일링하고 범주적 특성을 인코딩하기 전에 이상치를 제거합니다."
        },
        "Correct Answer": "Min-Max 스케일링을 사용하여 수치적 특성을 정규화하고 원-핫 인코딩을 사용하여 범주적 특성을 인코딩합니다.",
        "Explanation": "수치적 특성을 정규화하면 특정 범위 내에 있도록 하여 모델이 더 빠르게 수렴하는 데 도움이 됩니다. 원-핫 인코딩은 범주적 특성에 필수적이며, 이는 모델이 존재하지 않는 서열 관계를 부여하는 것을 방지합니다.",
        "Other Options": [
            "차원 축소는 수치적 특성에서 귀중한 정보 손실을 초래할 수 있으며, 범주적 특성을 제거하면 데이터의 중요한 측면이 사라집니다.",
            "중앙값 필터를 사용하는 것은 모든 유형의 수치 데이터에 적합하지 않을 수 있으며, 정규화를 하지 않으면 모델 성능이 저하될 수 있습니다. 레이블 인코딩은 잘못된 서열 관계를 암시할 수 있습니다.",
            "특성을 평균이 0이 되도록 스케일링하는 것은 모든 데이터 유형에 적합하지 않을 수 있으며, 이상치 제거는 유익할 수 있지만 범주적 특성을 준비하기 위한 주요 전처리 단계는 아닙니다."
        ]
    },
    {
        "Question Number": "50",
        "Situation": "데이터 엔지니어가 기계 학습 목적으로 Amazon S3에 저장된 대규모 데이터 세트를 처리하는 임무를 맡고 있습니다. 그들은 데이터를 Amazon SageMaker에 전달하기 전에 효율적으로 변환하고 정규화해야 합니다. 엔지니어는 데이터 처리 작업을 관리하기 위해 Apache Spark와 함께 Amazon EMR을 사용하는 것을 고려하고 있습니다. 그들은 비핵심 작업에 대해 스팟 인스턴스를 사용하여 비용을 최적화하고 데이터를 비용 효율적인 방식으로 저장하고자 합니다.",
        "Question": "Amazon EMR과 Apache Spark를 사용하여 데이터를 처리하기 위한 가장 효율적이고 비용 효과적인 솔루션은 어떤 아키텍처입니까?",
        "Options": {
            "1": "마스터 노드, 데이터 저장을 위한 코어 노드, 스팟 인스턴스로서의 작업 노드를 가진 EMR 클러스터를 시작합니다. EMRFS를 사용하여 S3의 데이터에 접근하고 결과를 SageMaker를 위한 다른 S3 버킷에 기록합니다.",
            "2": "마스터 노드, 코어 노드 및 스팟 인스턴스를 사용하는 작업 노드를 가진 EMR 클러스터를 설정합니다. 데이터를 HDFS에 저장하고 변환된 데이터를 SageMaker 처리를 위해 S3 버킷에 기록합니다.",
            "3": "마스터 노드와 코어 노드로 EMR 클러스터를 생성하고, 온디맨드 인스턴스만 사용합니다. HDFS를 데이터 저장소로 사용하고 데이터를 SageMaker로 전송하기 전에 모든 변환을 메모리 내에서 수행합니다.",
            "4": "코어 노드만 있는 EMR 클러스터를 배포하고 모든 처리 작업에 대해 온디맨드 인스턴스만 사용하며, 중간 데이터를 HDFS에 저장한 후 SageMaker로 전송합니다."
        },
        "Correct Answer": "마스터 노드, 데이터 저장을 위한 코어 노드, 스팟 인스턴스로서의 작업 노드를 가진 EMR 클러스터를 시작합니다. EMRFS를 사용하여 S3의 데이터에 접근하고 결과를 SageMaker를 위한 다른 S3 버킷에 기록합니다.",
        "Explanation": "데이터 저장을 위한 코어 노드와 스팟 인스턴스로서의 작업 노드를 조합하여 EMR을 사용하면 대규모 데이터 세트를 비용 효율적으로 처리할 수 있습니다. EMRFS는 S3에 저장된 데이터에 직접 접근할 수 있게 하여 대규모 변환에 더 효율적이며, SageMaker와 잘 통합되어 후속 모델 훈련에 적합합니다.",
        "Other Options": [
            "마스터 노드, 코어 노드 및 스팟 인스턴스를 사용하는 작업 노드를 가진 EMR 클러스터를 설정하더라도 HDFS에 데이터를 저장하는 것은 추가 비용과 복잡성을 초래하므로 비효율적입니다. 특히 S3가 저비용 저장 솔루션으로 제공될 때 더욱 그렇습니다.",
            "온디맨드 인스턴스만 사용하는 EMR 클러스터를 생성하면 스팟 인스턴스를 사용하여 비용 절감을 할 수 없습니다. 또한, EMRFS가 S3에서 데이터를 직접 처리하는 효율적인 방법을 제공하므로 HDFS에 데이터를 저장할 필요가 없을 수 있습니다.",
            "코어 노드만 있는 EMR 클러스터를 배포하고 모든 작업에 대해 온디맨드 인스턴스를 사용하는 것은 비용 효율적이지 않습니다. 비핵심 작업에 대해 스팟 인스턴스를 활용하지 않으면 비용이 증가하며, 작업 노드가 없으면 클러스터가 대규모 처리 작업에 대해 효율적으로 확장되지 않을 수 있습니다."
        ]
    },
    {
        "Question Number": "51",
        "Situation": "머신 러닝 엔지니어가 새로운 추천 시스템을 위해 커스텀 모델을 구현할지 아니면 Amazon SageMaker에서 제공하는 내장 알고리즘을 활용할지를 평가하고 있습니다. 이 시스템은 사용자 상호작용에 기반하여 실시간 추천을 제공해야 하며, 높은 확장성 요구 사항이 있습니다.",
        "Question": "엔지니어가 Amazon SageMaker 내장 알고리즘 대신 커스텀 모델을 구축하는 것을 고려해야 하는 경우는 언제입니까? (두 가지 선택)",
        "Options": {
            "1": "성능을 향상시킬 수 있는 도메인 특정 데이터가 상당량 있을 때.",
            "2": "프로젝트가 내장 알고리즘이 수용할 수 없는 독특한 모델 아키텍처를 요구할 때.",
            "3": "문제 도메인이 내장 알고리즘으로 포착되지 않는 전문 지식을 요구할 때.",
            "4": "내장 알고리즘이 애플리케이션에 필요한 모델 평가 지표를 지원하지 않을 때.",
            "5": "엔지니어가 최소한의 설정과 구성으로 빠르게 시작해야 할 때."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "문제 도메인이 내장 알고리즘으로 포착되지 않는 전문 지식을 요구할 때.",
            "프로젝트가 내장 알고리즘이 수용할 수 없는 독특한 모델 아키텍처를 요구할 때."
        ],
        "Explanation": "커스텀 모델을 구축하는 것은 문제 도메인이 기존 내장 알고리즘이 효과적으로 다루지 못하는 전문 요구 사항이나 지식을 가질 때 유리합니다. 또한, 프로젝트가 SageMaker의 내장 옵션으로 구현할 수 없는 독특한 모델 아키텍처를 포함하는 경우, 특정 애플리케이션의 요구를 충족하기 위해 커스텀 모델이 필요합니다.",
        "Other Options": [
            "이 옵션은 도메인 특정 데이터가 성능을 향상시킬 수 있지만, 내장 알고리즘이 충족할 수 없는 독특한 요구 사항이 없는 한 커스텀 모델을 구축할 정당성을 제공하지 않기 때문에 잘못된 것입니다.",
            "이 옵션은 SageMaker의 내장 알고리즘이 일반적으로 다양한 평가 지표를 지원하므로 잘못된 것입니다. 필요한 지표가 내장 알고리즘을 통해 제공된다면 커스텀 모델을 만들 필요가 없습니다.",
            "이 옵션은 엔지니어가 빠르게 시작해야 할 경우 내장 알고리즘이 사용의 용이성과 신속한 배포를 위해 설계되었으므로 잘못된 것입니다. 이러한 상황에서는 내장 알고리즘이 선호되는 선택입니다."
        ]
    },
    {
        "Question Number": "52",
        "Situation": "머신 러닝 엔지니어가 AWS에 머신 러닝 모델을 배포하는 임무를 맡았습니다. 엔지니어는 배포가 보안, 확장성 및 모니터링에 대한 AWS 모범 사례를 준수하는지 확인하고자 합니다.",
        "Question": "머신 러닝 엔지니어가 수요에 따라 확장성과 자동 스케일링을 보장하면서 모델을 배포하기 위해 주로 어떤 AWS 서비스를 사용해야 합니까?",
        "Options": {
            "1": "AWS Lambda",
            "2": "AWS Fargate",
            "3": "Amazon SageMaker",
            "4": "Amazon EC2"
        },
        "Correct Answer": "Amazon SageMaker",
        "Explanation": "Amazon SageMaker는 머신 러닝 모델 배포를 위해 특별히 설계되었으며, 자동 스케일링, 모니터링 및 보안 모범 사례를 위한 내장 기능을 포함하고 있어 원활한 배포 프로세스에 가장 적합한 옵션입니다.",
        "Other Options": [
            "AWS Lambda는 이벤트에 응답하여 코드를 실행하는 데 사용할 수 있는 서버리스 컴퓨팅 서비스이지만, 대규모로 머신 러닝 모델을 배포하기 위해 주로 설계되지 않았으며, 더 큰 모델이나 장기 실행 추론 프로세스를 처리하는 데 제한이 있을 수 있습니다.",
            "Amazon EC2는 유연한 컴퓨팅 용량을 제공하지만, 스케일링 및 모니터링을 위한 수동 설정 및 관리가 필요하여 머신 러닝 모델의 효율적인 배포를 위한 모범 사례를 따르지 않을 수 있습니다.",
            "AWS Fargate는 마이크로서비스 아키텍처에 적합한 컨테이너를 위한 서버리스 컴퓨팅 엔진입니다. 애플리케이션 배포에 사용할 수 있지만, 머신 러닝 모델 배포를 위한 Amazon SageMaker가 제공하는 전문 기능을 제공하지 않습니다."
        ]
    },
    {
        "Question Number": "53",
        "Situation": "데이터 과학자가 이메일이 스팸인지 아닌지를 분류하는 머신 러닝 모델을 개발하는 임무를 맡았습니다. 다양한 알고리즘을 탐색한 후, 데이터 과학자는 두 클래스를 효과적으로 분리하기 위해 서포트 벡터 머신(SVM)을 구현하기로 결정했습니다. 데이터셋에는 이메일 콘텐츠의 다양한 측면을 나타내는 수많은 특성이 포함되어 있습니다. 데이터 과학자는 스팸과 비스팸 클래스 간의 마진을 최대화하는 최적의 초평면을 결정해야 합니다.",
        "Question": "다음 접근 방식 중 어떤 것이 데이터 과학자가 SVM을 사용하여 스팸과 비스팸 이메일을 분리하기 위한 최적의 초평면을 식별하는 데 가장 도움이 될까요?",
        "Options": {
            "1": "원시 이메일 특성에 직접적으로 경량 하강법을 적용하여 분류 오류를 최소화합니다.",
            "2": "SVM을 사용하기 전에 두 클래스를 분리하는 최상의 분할을 찾기 위해 결정 트리를 구현합니다.",
            "3": "커널 트릭을 활용하여 특성 공간을 변환하여 클래스의 비선형 분리를 가능하게 합니다.",
            "4": "k-평균 클러스터링을 사용하여 유사성에 따라 이메일을 그룹화한 다음 클러스터링된 데이터에 SVM을 적용합니다."
        },
        "Correct Answer": "커널 트릭을 활용하여 특성 공간을 변환하여 클래스의 비선형 분리를 가능하게 합니다.",
        "Explanation": "커널 트릭은 SVM이 데이터 포인트의 좌표를 명시적으로 계산하지 않고도 더 높은 차원의 공간에서 작동할 수 있게 합니다. 이는 클래스 간의 관계가 선형이 아닐 때 특히 유용하며, 모델이 마진을 최대화하고 두 클래스를 효과적으로 분리하는 최적의 초평면을 찾을 수 있게 합니다.",
        "Other Options": [
            "원시 이메일 특성에 직접적으로 경량 하강법을 적용하는 것은 SVM의 강점을 활용하지 않으며, 최적의 초평면을 효과적으로 찾지 못할 수 있습니다. SVM은 마진을 최대화하기 위해 다른 최적화 방법을 사용합니다.",
            "SVM을 사용하기 전에 결정 트리를 구현하는 것은 SVM의 최적 초평면을 찾는 데 직접적으로 기여하지 않습니다. 결정 트리와 SVM은 각기 다른 알고리즘으로, 분류를 위한 고유한 메커니즘을 가지고 있습니다.",
            "k-평균 클러스터링을 사용하여 이메일을 그룹화하는 것은 데이터 구조를 이해하는 데 도움이 될 수 있지만, SVM에 필요한 단계는 아니며, SVM은 레이블이 있는 훈련 데이터를 기반으로 초평면을 직접 찾습니다."
        ]
    },
    {
        "Question Number": "54",
        "Situation": "머신 러닝 엔지니어가 고객 이탈 예측 모델을 개발하는 임무를 맡았습니다. 데이터셋은 크고 상당한 양의 노이즈를 포함하고 있습니다. 엔지니어는 모델의 성능을 평가하고 보지 못한 데이터에 잘 일반화되는지 확인하기 위해 교차 검증을 구현하기로 결정했습니다.",
        "Question": "이 시나리오에서 k-겹 교차 검증을 사용하는 주요 이점은 무엇입니까?",
        "Options": {
            "1": "검증 세트에서 완벽한 모델 성능을 보장합니다.",
            "2": "단일 훈련-테스트 분할과 관련된 편향을 줄입니다.",
            "3": "모델이 전체 데이터셋에서 동시에 학습할 수 있게 합니다.",
            "4": "모델 훈련을 위한 데이터셋 크기를 늘리는 데 도움이 됩니다."
        },
        "Correct Answer": "단일 훈련-테스트 분할과 관련된 편향을 줄입니다.",
        "Explanation": "k-겹 교차 검증은 과적합의 위험을 완화하고 여러 훈련-테스트 분할에 걸쳐 결과를 평균화하여 모델 성능에 대한 보다 신뢰할 수 있는 추정치를 제공합니다. 이 접근 방식은 모델 평가가 데이터의 단일 분할에 의해 크게 영향을 받지 않도록 보장하는 데 도움이 됩니다.",
        "Other Options": [
            "이 옵션은 잘못되었습니다. k-겹 교차 검증은 데이터셋의 최대 활용을 보장하지만, 모델이 전체 데이터셋에서 한 번에 학습할 수 있게 하지는 않으며, 대신 각 겹에서 서로 다른 하위 집합에 대해 훈련합니다.",
            "이 옵션은 잘못되었습니다. k-겹 교차 검증은 완벽한 성능을 보장하지 않으며, 오히려 보지 못한 데이터에서 모델이 어떻게 수행될지를 추정합니다.",
            "이 옵션은 잘못되었습니다. k-겹 교차 검증은 데이터셋의 크기를 늘리지 않으며, 기존 데이터를 k개의 하위 집합으로 분할하여 강력한 평가를 보장합니다."
        ]
    },
    {
        "Question Number": "55",
        "Situation": "머신 러닝 엔지니어가 AWS에서 ML 모델의 새로운 배포를 위한 보안 설정을 구성하고 있습니다. 엔지니어는 권한이 있는 사용자와 시스템만 모델의 API 엔드포인트에 접근할 수 있도록 해야 합니다.",
        "Question": "엔지니어가 구현해야 할 보안 그룹 구성은 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "인바운드 트래픽을 포트 80과 포트 443으로만 제한",
            "2": "모든 IP 주소에 대한 아웃바운드 트래픽 허용",
            "3": "지정된 VPC 서브넷에서만 트래픽 허용",
            "4": "특정 IP 주소로의 인바운드 트래픽 제한",
            "5": "모든 IP 주소에서의 인바운드 트래픽 허용"
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "특정 IP 주소로의 인바운드 트래픽 제한",
            "지정된 VPC 서브넷에서만 트래픽 허용"
        ],
        "Explanation": "특정 IP 주소로의 인바운드 트래픽 제한은 지정된 사용자나 시스템만 API에 접근할 수 있도록 하여 보안을 강화합니다. 마찬가지로, 지정된 VPC 서브넷에서만 트래픽을 허용하면 해당 서브넷 내의 리소스만 모델의 배포와 통신할 수 있어 접근을 더욱 제한하고 잠재적인 공격 표면을 줄입니다.",
        "Other Options": [
            "모든 IP 주소에서의 인바운드 트래픽 허용은 API를 전체 인터넷에 노출시켜 무단 접근의 위험을 증가시키므로 안전하지 않습니다.",
            "모든 IP 주소에 대한 아웃바운드 트래픽 허용은 보안 모범 사례가 아니며, 데이터 유출이나 잠재적으로 해로운 외부 서비스와의 연결을 허용할 수 있습니다.",
            "인바운드 트래픽을 포트 80과 포트 443으로만 제한하는 것은 충분한 보안을 제공하지 않으며, 신뢰할 수 있는 엔티티로 트래픽의 출처를 제한하는 것이 중요합니다."
        ]
    },
    {
        "Question Number": "56",
        "Situation": "데이터 과학자가 K-평균 군집화를 사용하여 고객 세분화 프로젝트를 진행하고 있습니다. 군집화를 수행한 후, 데이터 세트에 대한 최적의 군집 수를 결정하고자 합니다. 이 목적을 위해 엘보우 방법을 사용하기로 결정했습니다.",
        "Question": "군집 분석에서 엘보우 방법의 주요 목적은 무엇입니까?",
        "Options": {
            "1": "다양한 반복에서 군집의 안정성을 평가하기 위해",
            "2": "군집의 최대 실루엣 점수를 식별하기 위해",
            "3": "다양한 차원에서 데이터 분포를 시각화하기 위해",
            "4": "설명된 분산을 플로팅하여 최적의 군집 수를 결정하기 위해"
        },
        "Correct Answer": "설명된 분산을 플로팅하여 최적의 군집 수를 결정하기 위해",
        "Explanation": "엘보우 방법은 설명된 분산(또는 관성)을 군집 수에 대해 플로팅하여 최적의 군집 수를 식별하는 데 사용됩니다. 감소율이 급격히 변화하는 지점(‘엘보우’)이 사용해야 할 적절한 군집 수를 나타냅니다.",
        "Other Options": [
            "최대 실루엣 점수는 군집이 얼마나 잘 분리되어 있는지를 평가하는 데 사용되지만, 엘보우 방법과는 직접적인 관련이 없습니다.",
            "다양한 차원에서 데이터 분포를 시각화하는 것은 데이터를 이해하는 데 중요하지만, 최적의 군집을 결정하기 위한 엘보우 방법과는 관련이 없습니다.",
            "설명된 분산은 엘보우 방법의 핵심 요소이지만, 이 방법의 구체적인 초점은 더 많은 군집을 추가해도 설명된 분산이 크게 개선되지 않는 지점을 찾는 것입니다."
        ]
    },
    {
        "Question Number": "57",
        "Situation": "한 회사가 리뷰를 분류하고 전반적인 감정을 결정하기 위한 고객 피드백 분석 도구를 개발하고 있습니다. 그들은 대량의 텍스트 데이터를 처리하고 실시간 통찰력을 제공할 수 있는 솔루션이 필요합니다. 이 솔루션을 구현하기 위해 Amazon SageMaker의 기능을 사용하는 것을 고려하고 있습니다.",
        "Question": "텍스트 분류 및 감정 분석을 위해 회사가 사용할 Amazon SageMaker 기능은 무엇입니까? 효율성과 단어 임베딩 지원을 고려할 때.",
        "Options": {
            "1": "단어2벡트 모드를 사용하는 Amazon SageMaker BlazingText",
            "2": "Amazon Comprehend의 엔티티 인식 기능",
            "3": "텍스트 분류 모드를 사용하는 Amazon SageMaker BlazingText",
            "4": "회귀 분석을 위한 Amazon SageMaker 내장 XGBoost 알고리즘"
        },
        "Correct Answer": "텍스트 분류 모드를 사용하는 Amazon SageMaker BlazingText",
        "Explanation": "Amazon SageMaker BlazingText의 텍스트 분류 모드는 감정 분석과 같은 작업을 위해 특별히 설계되었으며, 대량의 텍스트 데이터를 효율적으로 처리할 수 있어 회사의 요구 사항에 이상적인 선택입니다.",
        "Other Options": [
            "Amazon SageMaker 내장 XGBoost는 다양한 머신 러닝 작업에 유용할 수 있지만, 텍스트 분류나 감정 분석에 맞춰져 있지 않습니다.",
            "Amazon Comprehend의 엔티티 인식 기능은 텍스트 내의 특정 엔티티를 식별하는 데 중점을 두며, 전반적인 감정을 분류하거나 텍스트를 범주화하는 것과는 관련이 없습니다.",
            "단어2벡트 모드를 사용하는 Amazon SageMaker BlazingText는 주로 단어 임베딩을 생성하는 데 사용되며, 텍스트 분류 및 감정 분석의 필요를 직접적으로 해결하지 않을 수 있습니다."
        ]
    },
    {
        "Question Number": "58",
        "Situation": "기계 학습 엔지니어가 최근 정확도가 크게 떨어진 배포된 모델의 성능을 모니터링하고 있습니다. 이 모델은 생산 환경에서 사용되고 있으며, 엔지니어는 문제를 완화하기 위한 잠재적 원인과 해결책을 식별해야 합니다.",
        "Question": "모델 성능 저하를 진단하기 위해 엔지니어가 취해야 할 최선의 초기 단계는 무엇입니까?",
        "Options": {
            "1": "모델의 하이퍼파라미터를 분석하여 조정할 가능성을 검토합니다.",
            "2": "원본 데이터셋으로 모델을 재훈련합니다.",
            "3": "입력 데이터의 드리프트나 분포의 변화를 검사합니다.",
            "4": "훈련 데이터의 변화나 이상을 검토합니다."
        },
        "Correct Answer": "입력 데이터의 드리프트나 분포의 변화를 검사합니다.",
        "Explanation": "입력 데이터의 드리프트나 분포의 변화를 검사하는 것은 매우 중요합니다. 생산 환경에서 모델이 마주치는 데이터가 훈련에 사용된 데이터와 다르면 모델의 성능이 저하될 수 있습니다. 이 단계는 모델이 현재 데이터 맥락에 여전히 적합한지를 식별하는 데 도움이 됩니다.",
        "Other Options": [
            "훈련 데이터의 변화나 이상을 검토하는 것은 중요하지만, 현재 모델이 처리하고 있는 즉각적인 입력 데이터를 다루지 않습니다. 모델의 성능 문제는 종종 훈련 데이터보다 입력 데이터와 관련이 있습니다.",
            "모델의 하이퍼파라미터를 분석하는 것은 나중에 필요할 수 있지만, 특히 모델이 이전에 잘 작동했을 경우 성능 저하의 초기 원인일 가능성은 낮습니다.",
            "원본 데이터셋으로 모델을 재훈련하는 것은 현재 입력 데이터가 변화했다면 문제를 해결하지 못할 수 있습니다. 재훈련 결정을 내리기 전에 입력 데이터의 특성을 먼저 이해하는 것이 중요합니다."
        ]
    },
    {
        "Question Number": "59",
        "Situation": "한 소매 회사가 전자 상거래 플랫폼에서 고객 경험을 향상시키기 위해 추천 시스템을 구현하려고 합니다. 그들은 클릭, 구매 및 제품 평가를 포함한 고객 상호작용의 대규모 데이터셋을 보유하고 있습니다. 회사는 모델이 관련 추천을 제공할 뿐만 아니라 새로운 데이터가 들어올 때마다 적응할 수 있도록 하기를 원합니다. 어떤 접근 방식이 이러한 요구 사항을 가장 잘 충족할까요?",
        "Question": "어떤 접근 방식이 추천 시스템이 시간이 지남에 따라 새로운 데이터에 적응하도록 가장 잘 보장할까요?",
        "Options": {
            "1": "새로운 고객 상호작용 데이터가 제공될 때마다 모델을 점진적으로 업데이트하는 온라인 학습 알고리즘을 활용합니다.",
            "2": "협업 필터링과 콘텐츠 기반 필터링을 결합한 하이브리드 모델을 배포하되, 분기별로만 업데이트합니다.",
            "3": "제품 속성에만 의존하고 사용자 상호작용을 고려하지 않는 콘텐츠 기반 필터링 시스템을 구현합니다.",
            "4": "역사적 데이터를 기반으로 정적 모델을 생성하고 몇 개월마다 주기적으로 재훈련하는 협업 필터링 기법을 사용합니다."
        },
        "Correct Answer": "새로운 고객 상호작용 데이터가 제공될 때마다 모델을 점진적으로 업데이트하는 온라인 학습 알고리즘을 활용합니다.",
        "Explanation": "온라인 학습 알고리즘을 사용하면 새로운 상호작용 데이터가 수집됨에 따라 모델이 실시간으로 적응할 수 있습니다. 이는 추천이 관련성을 유지하고 최신 고객 행동에 따라 지속적으로 개선되도록 보장합니다.",
        "Other Options": [
            "정적 모델을 생성하는 협업 필터링 기법은 다음 재훈련 주기까지 새로운 데이터에 적응하지 않기 때문에 빠르게 구식이 될 수 있으며, 이는 동적인 전자 상거래 환경에 적합하지 않습니다.",
            "사용자 상호작용을 무시하는 콘텐츠 기반 필터링 시스템은 모델이 사용할 수 있는 풍부한 참여 데이터를 활용하는 능력을 제한하여 개인화된 추천이 줄어듭니다.",
            "협업 필터링과 콘텐츠 기반 필터링을 결합한 하이브리드 모델은 효과적일 수 있지만, 분기별로 업데이트하면 사용자 선호도나 제품 가용성의 변화에 신속하게 반응하지 못합니다."
        ]
    },
    {
        "Question Number": "60",
        "Situation": "데이터 과학자가 자연어 처리를 위해 텍스트 데이터셋을 준비하는 임무를 맡았습니다. 이 데이터셋에는 결측값, 관련 없는 구문 및 모델 성능을 저해할 수 있는 불용어가 포함된 여러 항목이 있습니다.",
        "Question": "데이터 과학자가 데이터셋에서 결측 데이터, 손상된 데이터 및 불용어를 식별하고 처리하기 위한 가장 효과적인 접근 방식은 무엇입니까?",
        "Options": {
            "1": "AWS Lambda를 활용하여 데이터셋에서 결측 데이터와 손상된 항목을 제거하고 불용어를 필터링하는 기능을 생성합니다.",
            "2": "AWS Glue를 사용하여 결측 및 손상된 항목을 필터링하는 데이터 정리 파이프라인을 구현하고, NLTK를 사용하여 텍스트에서 불용어를 제거합니다.",
            "3": "Amazon SageMaker Data Wrangler를 활용하여 결측 데이터 패턴을 시각화하고 불용어 제거를 포함한 데이터 정리 작업을 적용합니다.",
            "4": "Amazon Comprehend를 사용하여 텍스트 데이터에서 불용어와 결측값을 분석한 후, 데이터셋을 수동으로 편집하여 문제를 수정합니다."
        },
        "Correct Answer": "Amazon SageMaker Data Wrangler를 활용하여 결측 데이터 패턴을 시각화하고 불용어 제거를 포함한 데이터 정리 작업을 적용합니다.",
        "Explanation": "Amazon SageMaker Data Wrangler는 데이터 품질을 시각화하는 직관적인 인터페이스를 제공하여 결측값과 손상된 데이터를 식별하기 쉽게 만듭니다. 또한 불용어 제거를 포함한 데이터 정리를 위한 내장 기능이 있어 이 작업에 포괄적인 도구입니다.",
        "Other Options": [
            "AWS Glue는 주로 ETL 프로세스에 사용되며, Data Wrangler와 같은 수준의 즉각적인 시각화 및 데이터셋과의 상호작용을 제공하지 않을 수 있습니다. 데이터 정리를 수행할 수 있지만, 불용어를 효율적으로 처리하는 내장 기능이 부족합니다.",
            "Amazon Comprehend는 개체 인식 및 감정 분석과 같은 자연어 처리 작업을 위해 설계되었지만, 데이터셋에서 결측값이나 불용어를 직접 식별하고 정리하는 데 가장 효과적인 도구는 아닙니다.",
            "AWS Lambda는 데이터 정리 작업을 자동화할 수 있지만, 결측 데이터, 손상된 항목 및 불용어를 관리하기 위해 사용자 정의 코딩이 필요합니다. 이 접근 방식은 Data Wrangler와 같은 전용 도구에 비해 덜 효율적이고 오류가 발생할 가능성이 더 높습니다."
        ]
    },
    {
        "Question Number": "61",
        "Situation": "기계 학습 전문가가 이전 구매를 기반으로 사용자에게 제품을 추천하는 추천 시스템을 배포했습니다. 시간이 지남에 따라 모델의 성능이 저하되어 덜 관련성 있는 추천이 이루어지고 있습니다. 전문가는 새로운 사용자 데이터를 정기적으로 사용하여 모델을 업데이트할지, 아니면 몇 개월마다 더 큰 데이터셋으로 재훈련할지를 고려하고 있습니다.",
        "Question": "추천 시스템의 성능을 유지하기 위한 최선의 접근 방식은 무엇입니까?",
        "Options": {
            "1": "몇 개월마다 더 큰 데이터셋으로 모델을 재훈련합니다.",
            "2": "업데이트 없이 기존 모델에 의존합니다.",
            "3": "새로운 사용자 데이터를 사용하여 모델을 실시간으로 업데이트합니다.",
            "4": "두 가지 방법을 결합한 하이브리드 접근 방식을 사용합니다."
        },
        "Correct Answer": "두 가지 방법을 결합한 하이브리드 접근 방식을 사용합니다.",
        "Explanation": "하이브리드 접근 방식은 추천 시스템이 새로운 트렌드와 사용자 행동에 실시간으로 적응할 수 있게 하며, 주기적인 재훈련 동안 더 큰 데이터셋의 통찰력을 활용할 수 있게 합니다. 이 전략은 모델이 시간이 지나도 관련성과 정확성을 유지하도록 보장하며, 즉각적인 반응성과 역사적 데이터로부터의 포괄적인 학습의 균형을 맞춥니다.",
        "Other Options": [
            "몇 개월마다 더 큰 데이터셋으로 모델을 재훈련하는 것은 사용자 선호도의 즉각적인 변화를 해결하지 못할 수 있으며, 대기 기간 동안 구식 추천으로 이어질 수 있습니다.",
            "새로운 사용자 데이터만으로 모델을 실시간으로 업데이트하는 것은 노이즈와 불안정을 초래할 수 있으며, 더 큰 데이터셋에서 얻은 광범위한 통찰력을 활용하지 못합니다.",
            "업데이트 없이 기존 모델에 의존하면 사용자 선호도가 변화하고 새로운 데이터가 제공됨에 따라 성능이 계속 저하됩니다."
        ]
    },
    {
        "Question Number": "62",
        "Situation": "기계 학습 엔지니어가 안전한 접근과 데이터 프라이버시를 보장하기 위해 Virtual Private Cloud (VPC) 내에서 기계 학습 모델을 배포하고 있습니다. 이 모델은 실시간 예측을 위해 Amazon RDS 데이터베이스와 상호작용해야 합니다. 엔지니어는 VPC 설정이 모델이 공용 인터넷에 노출되지 않고 작동할 수 있도록 보장하고 싶어합니다.",
        "Question": "RDS 데이터베이스와 통신할 수 있도록 하면서 VPC 내에서 기계 학습 모델을 안전하게 배포하기 위한 최선의 접근 방식은 무엇입니까?",
        "Options": {
            "1": "공용 서브넷에서 AWS Lambda를 사용하여 예측을 위한 모델을 트리거합니다.",
            "2": "모델을 VPC 외부의 EC2 인스턴스에서 호스팅하여 무제한 인터넷 접근을 가능하게 합니다.",
            "3": "모델을 공용 서브넷에 배치하고 인터넷에서의 수신 트래픽을 허용하도록 보안 그룹을 구성합니다.",
            "4": "모델을 개인 서브넷에 배포하고 필요에 따라 아웃바운드 인터넷 접근을 위한 NAT 게이트웨이를 구성합니다."
        },
        "Correct Answer": "모델을 개인 서브넷에 배포하고 필요에 따라 아웃바운드 인터넷 접근을 위한 NAT 게이트웨이를 구성합니다.",
        "Explanation": "모델을 개인 서브넷에 배포하면 인터넷에서 직접 접근할 수 없게 되어 보안이 강화됩니다. NAT 게이트웨이는 모델이 RDS 데이터베이스나 다른 AWS 서비스에 접근하기 위해 아웃바운드 요청을 할 수 있도록 하면서도 모델이 직접 인터넷 접근으로부터 격리되도록 합니다.",
        "Other Options": [
            "모델을 공용 서브넷에 배치하면 인터넷에 노출되어 보안 위험이 발생하며 안전한 접근 요구 사항에 위배됩니다.",
            "모델을 VPC 외부의 EC2 인스턴스에서 호스팅하면 VPC 격리의 이점을 제거하고 데이터 프라이버시 문제를 초래할 수 있습니다.",
            "공용 서브넷에서 AWS Lambda를 사용하는 것은 안전한 배포 요구 사항과 일치하지 않으며, Lambda 기능을 공용 인터넷에 노출시킵니다."
        ]
    },
    {
        "Question Number": "63",
        "Situation": "ML 엔지니어가 크기, 위치 및 침실 수와 같은 다양한 특성을 기반으로 주택 가격을 예측하는 임무를 맡고 있습니다. 데이터셋에는 숫자형 및 범주형 특성이 모두 포함되어 있으며, 엔지니어는 예측 정확성을 개선하기 위해 앙상블 방법을 활용하고자 합니다.",
        "Question": "이 회귀 문제를 효과적으로 처리하기 위해 엔지니어가 고려해야 할 모델링 접근 방식의 조합은 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "데이터셋의 공간적 관계를 포착하기 위해 합성곱 신경망을 사용합니다.",
            "2": "강력한 예측 모델을 구축하기 위해 그래디언트 부스팅 머신을 적용합니다.",
            "3": "과적합을 줄이기 위해 결정 트리의 앙상블을 활용합니다.",
            "4": "데이터에 맞추기 위해 선형 커널을 가진 서포트 벡터 머신을 구현합니다.",
            "5": "혼합 데이터 유형을 처리할 수 있는 랜덤 포레스트 회귀기를 활용합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "혼합 데이터 유형을 처리할 수 있는 랜덤 포레스트 회귀기를 활용합니다.",
            "강력한 예측 모델을 구축하기 위해 그래디언트 부스팅 머신을 적용합니다."
        ],
        "Explanation": "랜덤 포레스트 회귀기를 사용하는 것은 숫자형 및 범주형 특성을 효과적으로 관리할 수 있기 때문에 혼합 데이터 유형에 적합합니다. 또한, 그래디언트 부스팅 머신은 여러 약한 학습자를 결합하여 예측 성능을 향상시킬 수 있는 강력한 앙상블 방법으로, 전체적으로 더 정확한 모델을 제공합니다.",
        "Other Options": [
            "선형 커널을 가진 서포트 벡터 머신을 구현하는 것은 데이터의 복잡성을 효과적으로 포착하지 못할 수 있으며, 특히 관계가 비선형인 경우 주택 가격 예측에서 자주 발생합니다.",
            "합성곱 신경망을 사용하는 것은 이 문제에 부적합합니다. CNN은 주로 이미지 데이터와 공간적 관계를 위해 설계되었으며, 주택 특성과 같은 구조화된 표 형식 데이터와는 관련이 없습니다.",
            "결정 트리의 앙상블을 활용하는 것은 과적합을 줄일 수 있지만, 랜덤 포레스트 접근 방식보다 덜 구체적이며, 랜덤 포레스트는 과적합을 방지하는 메커니즘을 내재적으로 포함하고 혼합 데이터 유형을 효과적으로 처리합니다."
        ]
    },
    {
        "Question Number": "64",
        "Situation": "한 소매 회사가 과거 데이터를 기반으로 고객 이탈을 예측하고자 합니다. 고객 인구 통계, 구매 이력, 고객 서비스 상호작용 등 다양한 데이터 소스를 보유하고 있습니다. 비즈니스 팀은 머신 러닝 기법을 사용하여 이 문제에 어떻게 접근할 수 있을지 이해하고자 합니다.",
        "Question": "머신 러닝 전문가가 이 비즈니스 문제를 ML 문제로 어떻게 정의해야 할까요?",
        "Options": {
            "1": "미래 판매를 예측하기 위해 시계열 분석을 구현합니다.",
            "2": "고객 상호작용을 최적화하기 위해 강화 학습 모델을 생성합니다.",
            "3": "유사한 고객을 그룹화하기 위해 비지도 클러스터링 모델을 개발합니다.",
            "4": "이탈을 예측하기 위해 감독 분류 문제로 공식화합니다."
        },
        "Correct Answer": "이탈을 예측하기 위해 감독 분류 문제로 공식화합니다.",
        "Explanation": "고객 이탈을 예측하는 문제는 감독 분류 문제로 정의할 수 있으며, 모델은 과거 데이터를 학습하여 고객의 속성과 과거 행동에 따라 이탈 여부를 분류합니다.",
        "Other Options": [
            "비지도 클러스터링 모델을 개발하는 것은 이탈 예측과 직접적으로 관련이 없으며, 클러스터링은 미리 정의된 레이블 없이 데이터를 그룹화하는 데 사용됩니다.",
            "강화 학습 모델을 생성하는 것은 부적절합니다. 여기서의 목표는 환경에서의 상호작용을 최적화하는 것이 아니라 과거 데이터를 기반으로 특정 결과(이탈)를 예측하는 것이기 때문입니다.",
            "시계열 분석을 구현하는 것은 과거 추세를 기반으로 미래 값을 예측하는 데 중점을 두며, 특정 시점에서 고객 이탈을 예측하는 목표와 일치하지 않습니다."
        ]
    },
    {
        "Question Number": "65",
        "Situation": "데이터 과학자가 이진 분류 모델의 성능을 평가하고 있습니다. 모델의 예측은 생산 환경에서의 효과성을 판단하기 위해 다양한 메트릭을 사용하여 평가되고 있습니다.",
        "Question": "상당한 클래스 불균형이 있을 때 모델 성능을 평가하기 위한 최선의 평가 메트릭은 무엇인가요?",
        "Options": {
            "1": "제곱근 평균 제곱 오차 (RMSE)",
            "2": "정확도",
            "3": "F1 점수",
            "4": "곡선 아래 면적 (AUC) - 수신기 작동 특성 (ROC)"
        },
        "Correct Answer": "F1 점수",
        "Explanation": "F1 점수는 클래스 불균형이 있는 경우 모델 성능을 평가하는 데 가장 적합한 선택입니다. 이는 정밀도와 재현율을 모두 고려하여 두 가지 간의 균형을 제공합니다. 특히 거짓 긍정과 거짓 부정의 비용이 다르거나 소수 클래스에 초점을 맞출 때 유용합니다.",
        "Other Options": [
            "정확도는 클래스 불균형의 경우 오해를 불러일으킬 수 있으며, 단순히 다수 클래스 예측을 반영하여 모델 성능에 대한 잘못된 인식을 줄 수 있습니다.",
            "제곱근 평균 제곱 오차 (RMSE)는 주로 회귀 작업에 사용되며, 분류에는 적합하지 않으며 이진 분류 모델을 평가하는 데 의미 있는 통찰력을 제공하지 않습니다.",
            "곡선 아래 면적 (AUC) - 수신기 작동 특성 (ROC)은 진짜 긍정 비율과 거짓 긍정 비율 간의 균형을 이해하는 데 유용하지만, 정밀도와 재현율 간의 균형을 직접적으로 고려하지 않습니다."
        ]
    }
]