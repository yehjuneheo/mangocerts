[
    {
        "Question Number": "1",
        "Situation": "데이터 분석가는 Amazon S3에 저장된 판매 데이터와 Amazon Redshift의 성과 지표를 시각화하기 위한 대화형 대시보드를 만들어야 합니다. 이 대시보드는 사용자가 데이터를 필터링하고 자세한 분석을 위해 데이터를 세분화할 수 있도록 해야 합니다.",
        "Question": "분석가는 실시간 데이터 시각화를 위해 Amazon S3와 Amazon Redshift 모두에 연결할 수 있는 대화형 대시보드를 구축하기 위해 어떤 AWS 서비스를 사용해야 합니까?",
        "Options": {
            "1": "AWS Glue",
            "2": "Amazon SageMaker",
            "3": "Amazon QuickSight",
            "4": "Amazon Managed Grafana"
        },
        "Correct Answer": "Amazon QuickSight",
        "Explanation": "Amazon QuickSight는 사용자가 대화형 대시보드와 시각화를 생성할 수 있도록 하는 확장 가능하고 서버리스이며 임베디드 BI 서비스로, 실시간 데이터 분석을 위해 Amazon S3와 Amazon Redshift 모두에 연결하는 데 이상적인 선택입니다.",
        "Other Options": [
            "AWS Glue는 주로 ETL(추출, 변환, 적재) 프로세스를 위한 데이터 통합 서비스로, 시각화 기능이나 대시보드를 제공하지 않습니다.",
            "Amazon SageMaker는 기계 학습 모델을 구축, 훈련 및 배포하기 위해 설계된 기계 학습 서비스로, 데이터 시각화나 대시보드 생성에 중점을 두지 않습니다.",
            "Amazon Managed Grafana는 모니터링 및 가시성을 위한 서비스로, 일반적으로 시계열 데이터와 함께 사용되지만 비즈니스 인텔리전스 대시보드를 위해 Amazon S3 및 Redshift에 직접 연결하는 기본 기능이 없습니다."
        ]
    },
    {
        "Question Number": "2",
        "Situation": "한 회사가 IoT 장치에서 들어오는 실시간 센서 데이터를 처리하고 변환하기 위한 서버리스 데이터 파이프라인을 구축하고 있습니다. 팀은 데이터 수집 및 변환 프로세스에서 운영 오버헤드를 최소화하면서 확장성과 신뢰성을 보장하는 것을 목표로 하고 있습니다.",
        "Question": "센서 데이터를 효율적으로 수집하고 변환하기 위한 서버리스 워크플로우를 구현하는 가장 좋은 방법은 무엇입니까?",
        "Options": {
            "1": "AWS Lambda 함수를 사용하여 들어오는 데이터를 처리하고 결과를 Amazon DynamoDB에 직접 기록하여 변환된 데이터에 대한 저지연 액세스를 보장합니다.",
            "2": "Amazon EventBridge를 통합하여 센서 데이터 이벤트를 수신할 때 AWS Lambda 함수를 트리거하여 데이터 변환을 수행하고 결과를 Amazon RDS에 저장합니다.",
            "3": "Amazon Kinesis Data Streams를 설정하여 실시간 데이터를 캡처하고 AWS Glue를 사용하여 Amazon S3에 저장된 데이터에 대해 배치 변환을 수행합니다.",
            "4": "Amazon S3 이벤트 알림을 사용하여 즉각적인 처리를 위한 Lambda 함수를 트리거하고 변환된 데이터를 Amazon Redshift에 저장하여 분석 쿼리를 수행합니다."
        },
        "Correct Answer": "Amazon S3 이벤트 알림을 사용하여 즉각적인 처리를 위한 Lambda 함수를 트리거하고 변환된 데이터를 Amazon Redshift에 저장하여 분석 쿼리를 수행합니다.",
        "Explanation": "이 옵션은 AWS Lambda를 사용하여 서버리스 워크플로우를 시작하기 위해 S3 이벤트 알림을 효과적으로 활용하며, 이는 들어오는 데이터의 실시간 처리를 위한 이상적인 방법입니다. 변환된 데이터를 Amazon Redshift에 저장하면 효율적인 분석 쿼리가 가능하여 회사의 요구에 적합합니다.",
        "Other Options": [
            "AWS Lambda와 DynamoDB를 사용하는 것은 저지연 액세스를 제공하지만, DynamoDB가 Redshift에 비해 복잡한 쿼리를 처리하는 데 한계가 있어 대규모 데이터 분석에는 최선의 선택이 아닐 수 있습니다.",
            "Amazon Kinesis Data Streams를 사용하는 것은 실시간 수집에 좋은 선택이지만, AWS Glue로 배치 변환을 수행하면 지연이 발생할 수 있으며 즉각적인 처리 요구와 일치하지 않습니다.",
            "Amazon EventBridge를 사용하여 Lambda 함수를 트리거하는 것은 가능하지만, Amazon RDS에 데이터를 저장하면 Amazon Redshift만큼의 확장성과 분석 성능을 제공하지 않을 수 있습니다."
        ]
    },
    {
        "Question Number": "3",
        "Situation": "한 금융 서비스 회사가 거래 데이터베이스 요구를 위해 Amazon RDS를 사용하고 있습니다. 그들은 특정 거래가 동시 액세스로 인해 차단되고 있어 타임아웃과 불량한 사용자 경험이 발생하고 있음을 발견했습니다. 거래 처리를 개선하고 데이터 액세스 문제를 방지하기 위해 데이터 엔지니어는 데이터 무결성을 보장하면서 효율적인 액세스를 허용하는 잠금 전략을 구현하려고 합니다.",
        "Question": "데이터 엔지니어가 잠금을 효과적으로 관리하기 위해 어떤 전략을 구현해야 합니까? (두 가지 선택)",
        "Options": {
            "1": "무한 차단을 피하기 위해 잠금에 대한 타임아웃 설정을 구현합니다.",
            "2": "데이터베이스 연결을 효율적으로 관리하기 위해 연결 풀러를 사용합니다.",
            "3": "잠금 대기 통계를 분석하여 경합 문제를 식별하고 해결합니다.",
            "4": "잠금 경합을 최소화하기 위해 낙관적 동시성 제어를 사용합니다.",
            "5": "읽기 트래픽을 기본 인스턴스에서 오프로드하기 위해 읽기 복제본을 구성합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "잠금 경합을 최소화하기 위해 낙관적 동시성 제어를 사용합니다.",
            "무한 차단을 피하기 위해 잠금에 대한 타임아웃 설정을 구현합니다."
        ],
        "Explanation": "낙관적 동시성 제어를 사용하면 초기에는 리소스를 잠그지 않고 거래를 진행할 수 있어 잠금 경합을 줄일 수 있습니다. 잠금에 대한 타임아웃을 구현하면 거래가 무한히 차단되지 않도록 하여 시스템이 교착 상태나 긴 대기 시간에서 회복할 수 있도록 합니다.",
        "Other Options": [
            "읽기 복제본을 구성하는 것은 주로 읽기 성능에 도움이 되며 잠금 관리 문제를 직접적으로 해결하지 않습니다.",
            "연결 풀러를 사용하면 연결 관리를 개선하지만 거래의 잠금 및 경합 문제를 해결하지는 않습니다.",
            "잠금 대기 통계를 분석하면 경합 문제에 대한 통찰력을 제공할 수 있지만, 잠금 관리 문제를 적극적으로 해결하지는 않습니다."
        ]
    },
    {
        "Question Number": "4",
        "Situation": "한 회사가 Amazon S3에 대량의 데이터를 저장하고 있으며, 이에는 비용 절감을 위해 아카이브할 수 있는 자주 접근하지 않는 데이터가 포함되어 있습니다. 데이터 라이프사이클 관리 전략은 시간이 지남에 따라 데이터를 더 비용 효율적인 스토리지 클래스로 전환하는 것을 포함합니다. 데이터는 비활성 상태가 일정 기간 지속된 후 자동으로 S3 Standard에서 S3 Glacier로 이동해야 합니다.",
        "Question": "데이터를 S3 Glacier로 전환하기 위한 S3 라이프사이클 정책을 구현하기 위해 어떤 조치를 취해야 합니까?",
        "Options": {
            "1": "마지막 수정 날짜를 기준으로 객체를 S3 Glacier로 수동으로 이동합니다.",
            "2": "30일 이상 된 객체에 대한 접근을 제한하는 S3 버킷 정책을 설정합니다.",
            "3": "30일 후에 객체를 S3 Glacier로 전환하는 라이프사이클 규칙을 생성합니다.",
            "4": "객체 전환 관리를 위해 S3 버킷에서 버전 관리를 활성화합니다."
        },
        "Correct Answer": "30일 후에 객체를 S3 Glacier로 전환하는 라이프사이클 규칙을 생성합니다.",
        "Explanation": "라이프사이클 규칙을 생성하는 것은 지정된 기간 후에 객체를 S3 Glacier로 자동 전환하는 올바른 접근 방식입니다. 이는 수동 개입 없이 객체가 더 비용 효율적인 스토리지 클래스로 이동되도록 보장하며, 회사의 데이터 관리 전략에 부합합니다.",
        "Other Options": [
            "버킷 정책을 설정하면 접근이 제한되지만, 객체를 S3 Glacier로 자동 전환하는 기본 요구 사항을 충족하지 않습니다.",
            "객체를 S3 Glacier로 수동으로 이동하는 것은 비효율적이며, 이 목적을 위해 설계된 S3의 자동 라이프사이클 관리 기능을 활용하지 않습니다.",
            "S3 버킷에서 버전 관리를 활성화하는 것은 객체를 다른 스토리지 클래스로 전환해야 하는 요구 사항을 해결하지 않으며, 단순히 객체의 여러 버전을 유지합니다."
        ]
    },
    {
        "Question Number": "5",
        "Situation": "다국적 기업이 여러 국가로 사업을 확장하고 있으며, 각국의 데이터 주권 법률에 따라 데이터 저장 및 처리 위치가 규정되어 있습니다. 이 회사는 이러한 규정을 준수하면서 운영 효율성을 유지하기 위해 AWS 서비스를 활용하고 있습니다. 데이터 접근성과 성능을 저해하지 않으면서 데이터 거주지를 관리할 수 있는 솔루션이 필요합니다.",
        "Question": "AWS 서비스를 사용하면서 데이터 주권 요구 사항을 준수하기 위한 회사의 필요를 가장 잘 해결하는 접근 방식은 무엇입니까?",
        "Options": {
            "1": "각 국가의 데이터 주권 법률에 맞는 AWS 리전을 활용하고 해당 리전 간에 데이터를 복제합니다.",
            "2": "데이터 저장을 위해 Amazon S3를 사용하고, 데이터를 전 세계적으로 사용할 수 있도록 교차 리전 복제를 구성합니다.",
            "3": "각 국가의 AWS 리전에서 Amazon RDS 인스턴스를 배포하여 지역 데이터 저장 및 처리 요구 사항을 충족합니다.",
            "4": "모든 데이터를 단일 AWS 리전에 저장하여 관리 간소화 및 운영 오버헤드를 줄입니다."
        },
        "Correct Answer": "각 국가의 AWS 리전에서 Amazon RDS 인스턴스를 배포하여 지역 데이터 저장 및 처리 요구 사항을 충족합니다.",
        "Explanation": "각 국가의 AWS 리전에서 Amazon RDS 인스턴스를 배포하면 데이터가 지역 데이터 주권 법률에 의해 정의된 법적 경계 내에서 저장되고 처리됩니다. 이 접근 방식은 규정 준수 요구 사항에 부합하면서도 지역 운영을 위한 성능과 접근성을 유지합니다.",
        "Other Options": [
            "데이터 주권 법률에 맞는 AWS 리전을 활용하고 데이터를 복제하는 것은 복제가 올바르게 관리되지 않을 경우 규정 준수 위험을 초래할 수 있습니다. 데이터가 요구되는 관할권 외부에 저장될 수 있기 때문입니다.",
            "모든 데이터를 단일 AWS 리전에 저장하는 것은 데이터 주권 규정을 준수하지 않으며, 일반적으로 데이터가 특정 지리적 경계 내에 유지되어야 하므로 회사가 법적 처벌에 노출될 수 있습니다.",
            "Amazon S3를 사용하여 교차 리전 복제를 수행하는 것은 데이터가 발생한 관할권 외부의 리전으로 복제될 경우 데이터 주권 법률을 위반할 수 있으며, 이는 규정 준수 노력을 저해할 수 있습니다."
        ]
    },
    {
        "Question Number": "6",
        "Situation": "한 조직이 실시간 분석을 위해 대규모 데이터 세트를 처리하고 있으며, 데이터 수집 및 변환을 위한 확장 가능한 솔루션이 필요합니다. 이 솔루션은 스트리밍 데이터를 처리할 수 있어야 하며, 데이터 웨어하우스에 로드하기 전에 즉시 변환을 수행할 수 있는 기능을 제공해야 합니다.",
        "Question": "분산 컴퓨팅 환경에서 실시간 데이터 수집 및 변환을 위한 최상의 솔루션을 제공하는 서비스는 무엇입니까?",
        "Options": {
            "1": "Amazon DynamoDB Streams를 활용하여 변경 사항을 캡처하고, Amazon EMR을 사용하여 데이터를 처리 및 변환한 후 Amazon RDS에 로드합니다.",
            "2": "Amazon Kinesis Data Streams를 사용하여 스트리밍 데이터를 수집하고, AWS Lambda를 사용하여 변환 및 Amazon Redshift에 로드합니다.",
            "3": "Amazon SQS를 활용하여 들어오는 데이터를 큐에 저장하고, AWS Batch를 사용하여 변환을 수행하고 데이터 레이크에 로드합니다.",
            "4": "AWS Glue를 구현하여 데이터를 크롤링하고 카탈로그화한 후, Amazon S3에 원시 데이터를 저장하고 Amazon Athena로 처리합니다."
        },
        "Correct Answer": "Amazon Kinesis Data Streams를 사용하여 스트리밍 데이터를 수집하고, AWS Lambda를 사용하여 변환 및 Amazon Redshift에 로드합니다.",
        "Explanation": "Amazon Kinesis Data Streams는 실시간 데이터 수집을 위해 설계되었으며 높은 처리량을 처리할 수 있습니다. AWS Lambda와 결합하면 서버리스 변환과 즉각적인 데이터 로딩이 가능하여 실시간 분석에 이상적입니다.",
        "Other Options": [
            "AWS Glue는 주로 배치 처리 및 ETL 작업에 사용되며, 실시간 수집에는 적합하지 않습니다. Amazon S3에 저장된 데이터를 처리할 수 있고 Athena와 통합할 수 있지만, 실시간 분석에 필요한 즉각성을 제공하지 않습니다.",
            "Amazon DynamoDB Streams는 DynamoDB 테이블의 변경 사항을 캡처할 수 있지만 일반적인 스트리밍 데이터 수집에는 적합하지 않습니다. EMR은 대규모 데이터 세트를 처리할 수 있지만, 이 시나리오에서 요구되는 실시간 변환에 최적화되어 있지 않습니다.",
            "Amazon SQS는 분산 구성 요소 간의 신뢰할 수 있는 통신을 제공하는 메시지 큐 서비스이지만, 실시간 데이터 수집을 본래 지원하지 않습니다. AWS Batch도 배치 처리에 설계되어 있으며, 실시간 분석에 필요한 즉각적인 변환에는 적합하지 않습니다."
        ]
    },
    {
        "Question Number": "7",
        "Situation": "소매 회사가 Amazon Redshift에서 분석을 위해 대량의 고객 데이터를 처리하고 있습니다. 분석을 수행하기 전에 시스템에 로드되는 데이터가 완전하고 일관되며 정확하고 무결성을 유지하는지 확인해야 합니다.",
        "Question": "Amazon Redshift에서 ETL 프로세스 중 데이터 무결성과 정확성을 검증하기 위한 최선의 접근 방식은 무엇입니까?",
        "Options": {
            "1": "원시 데이터를 저장하기 위해 Amazon S3 버킷을 설정하고 Amazon Redshift에 로드된 후 데이터 무결성을 검증하기 위해 별도의 보고 작업을 실행합니다.",
            "2": "AWS Glue를 사용하여 Amazon Redshift에 데이터를 로드하기 전에 중복 및 null 값을 확인하는 데이터 검증 단계를 구현합니다.",
            "3": "데이터를 로드한 후 검증을 실행하고 검증 기준을 충족하지 않는 레코드를 거부하는 Redshift 저장 프로시저를 생성합니다.",
            "4": "Amazon CloudWatch를 사용하여 ETL 작업을 모니터링하고 실패에 대해 경고하지만 데이터 검증은 수행하지 않습니다."
        },
        "Correct Answer": "AWS Glue를 사용하여 Amazon Redshift에 데이터를 로드하기 전에 중복 및 null 값을 확인하는 데이터 검증 단계를 구현합니다.",
        "Explanation": "AWS Glue를 사용하여 데이터 검증 단계를 구현하면 데이터가 Amazon Redshift 환경에 들어가기 전에 완전성, 일관성 및 정확성과 같은 데이터 품질 검사가 수행됩니다. 이 사전 예방적 접근 방식은 손상된 데이터 분석의 위험을 최소화하고 전반적인 데이터 무결성을 향상시킵니다.",
        "Other Options": [
            "Amazon CloudWatch를 사용하여 ETL 작업을 모니터링하더라도 검증이 없으면 데이터 품질 문제를 해결하지 못합니다. 중복 또는 null 값이 있는 데이터가 로드되면 모니터링만으로는 잘못된 분석을 방지할 수 없습니다.",
            "원시 데이터를 S3에 저장하고 Redshift에 로드한 후 검증하면 데이터 품질 문제를 식별하는 데 지연이 발생합니다. 이 반응적 접근 방식은 자원의 낭비와 부정확한 분석을 초래할 수 있습니다.",
            "Redshift에서 로드 후 검증을 위한 저장 프로시저를 생성하는 것은 나쁜 데이터가 처음에 시스템에 들어가는 것을 방지하지 않습니다. 이 방법은 추가 처리가 필요하므로 효율성이 떨어지고 성능 문제를 초래할 수 있습니다."
        ]
    },
    {
        "Question Number": "8",
        "Situation": "소매 회사는 Amazon S3 버킷에서 Amazon Redshift로 대량의 판매 데이터를 정기적으로 수집하고 변환해야 합니다. 이 과정이 효율적이고 데이터 전송 비용을 최소화하기를 원합니다.",
        "Question": "Amazon S3에서 Amazon Redshift로 데이터 수집 및 변환을 자동화하기 위한 가장 효과적인 솔루션은 무엇입니까?",
        "Options": {
            "1": "S3 객체 생성 시 트리거되는 AWS Lambda 함수를 설정하여 데이터를 추출하고 Amazon Redshift에 로드합니다.",
            "2": "AWS Glue를 사용하여 Amazon S3에서 데이터를 직접 읽고 정기적으로 Amazon Redshift에 로드하는 ETL 작업을 생성합니다.",
            "3": "필요한 변환이 적용되도록 Amazon S3에서 Amazon Redshift로 데이터를 전송하기 위해 Amazon AppFlow를 구현합니다.",
            "4": "AWS Data Pipeline을 활용하여 Amazon S3에서 Amazon Redshift로 데이터 이동을 조정하고 변환을 위한 사용자 지정 스크립트를 사용합니다."
        },
        "Correct Answer": "AWS Glue를 사용하여 Amazon S3에서 데이터를 직접 읽고 정기적으로 Amazon Redshift에 로드하는 ETL 작업을 생성합니다.",
        "Explanation": "AWS Glue는 ETL(추출, 변환, 로드) 작업을 위해 설계되었으며 Amazon S3 및 Amazon Redshift와 원활하게 통합됩니다. 자동 스키마 추론을 허용하고 정기적으로 실행되도록 예약할 수 있어 배치 데이터 수집 및 변환을 위한 가장 효율적인 솔루션입니다.",
        "Other Options": [
            "AWS Lambda 함수를 사용하여 S3 객체 생성 시 트리거하는 것은 작고 이벤트 기반 작업에 적합하지만 대량 배치 처리에는 적합하지 않아 성능 문제를 초래할 수 있습니다.",
            "AWS Data Pipeline은 데이터 이동을 조정할 수 있지만 AWS Glue에 비해 더 많은 관리 및 구성이 필요하므로 이 특정 작업에 덜 효율적입니다.",
            "Amazon AppFlow는 주로 SaaS 애플리케이션과 AWS 서비스 간의 데이터 통합에 사용되며 일부 변환을 처리할 수 있지만 대규모 배치 처리에 최적화되어 있지 않아 AWS Glue보다 더 높은 비용이 발생할 수 있습니다."
        ]
    },
    {
        "Question Number": "9",
        "Situation": "금융 서비스 회사가 민감한 고객 데이터를 처리하는 새로운 애플리케이션을 개발하고 있습니다. 규정을 준수하고 데이터 프라이버시를 보장하기 위해 데이터 익명화 기술을 구현해야 합니다. 목표는 개인 식별 정보(PII)를 보호하면서도 비즈니스 통찰력을 위한 데이터 분석을 가능하게 하는 것입니다.",
        "Question": "데이터 분석을 위한 유용성을 유지하면서 데이터를 효과적으로 익명화하기 위해 데이터 엔지니어가 선택해야 할 방법은 무엇입니까?",
        "Options": {
            "1": "Amazon Redshift를 사용하여 쿼리 결과에서 PII를 숨기면서 비민감 데이터에 대한 접근을 허용하는 데이터 마스킹 기술을 적용합니다.",
            "2": "AWS Glue를 사용하여 PII 필드에 토큰화를 적용하고 변환된 데이터를 Amazon S3에 저장하는 작업을 생성합니다.",
            "3": "AWS Lake Formation을 활용하여 사용자 역할에 따라 PII 데이터에 대한 접근을 제한하는 데이터 접근 정책을 시행합니다.",
            "4": "AWS Key Management Service(KMS)를 구현하여 민감한 데이터를 Amazon RDS에 저장하기 전에 암호화합니다."
        },
        "Correct Answer": "AWS Glue를 사용하여 PII 필드에 토큰화를 적용하고 변환된 데이터를 Amazon S3에 저장하는 작업을 생성합니다.",
        "Explanation": "토큰화는 민감한 PII를 비민감한 동등물(토큰)로 효과적으로 대체하여 원본 데이터를 노출하지 않고 데이터 분석을 가능하게 합니다. 이 방법은 데이터 프라이버시 규정을 준수하면서도 데이터에서 의미 있는 통찰력을 도출할 수 있는 능력을 유지합니다.",
        "Other Options": [
            "AWS Key Management Service(KMS)를 구현하여 암호화를 수행하면 데이터가 정지 상태에서 보호되지만 데이터 익명화는 이루어지지 않습니다. 암호화된 데이터는 여전히 원본 형태로 접근할 수 있어 데이터 익명화 요구 사항을 충족하지 않습니다.",
            "AWS Lake Formation을 활용하여 접근 정책을 시행하는 것은 거버넌스 및 보안에 중요하지만 데이터 익명화 메커니즘을 제공하지 않습니다. 이는 단순히 데이터에 접근할 수 있는 사람을 제어할 뿐, 데이터를 변환하지는 않습니다.",
            "Amazon Redshift에서 데이터 마스킹 기술을 적용하면 쿼리 결과에서 PII를 숨길 수 있지만 진정한 익명화를 제공하지 않을 수 있습니다. 마스킹된 데이터는 때때로 역설계될 수 있어 회사의 규정에서 요구하는 대로 PII를 완전히 보호하지 못합니다."
        ]
    },
    {
        "Question Number": "10",
        "Situation": "한 회사가 웹사이트 클릭스트림 및 IoT 장치 텔레메트리를 포함한 여러 실시간 소스에서 대량의 데이터를 분석하고 있습니다. 데이터 엔지니어는 이 들어오는 데이터를 효율적으로 저장하고 처리할 수 있는 솔루션이 필요하며, 분석을 위한 낮은 대기 시간을 보장해야 합니다.",
        "Question": "높은 처리량과 낮은 대기 시간 요구 사항을 가진 실시간 데이터 스트림을 처리하는 데 가장 적합한 AWS 서비스는 무엇입니까?",
        "Options": {
            "1": "Amazon Kinesis Data Streams를 사용하여 실시간 데이터 스트림을 수집, 버퍼링 및 처리합니다.",
            "2": "Amazon RDS를 사용하여 구조화된 데이터를 저장하고 SQL 쿼리를 실행합니다.",
            "3": "Amazon MSK를 사용하여 분산 스트리밍 플랫폼에서 데이터를 관리하고 처리합니다.",
            "4": "Amazon S3를 사용하여 배치 처리 및 분석을 위한 원시 데이터를 저장합니다."
        },
        "Correct Answer": "Amazon Kinesis Data Streams를 사용하여 실시간 데이터 스트림을 수집, 버퍼링 및 처리합니다.",
        "Explanation": "Amazon Kinesis Data Streams는 실시간 데이터 처리를 위해 특별히 설계되어 있으며, 낮은 대기 시간으로 대량의 데이터 스트림을 수집하고 처리할 수 있습니다. 이는 실시간으로 높은 처리량의 데이터를 처리하는 데 필요한 기능을 제공합니다.",
        "Other Options": [
            "Amazon RDS는 구조화된 데이터와 전통적인 SQL 쿼리에 최적화된 관계형 데이터베이스 서비스로, 실시간 데이터 스트림의 낮은 대기 시간 요구 사항을 충족하지 못할 수 있습니다.",
            "Amazon S3는 대량의 데이터를 저장하는 데 뛰어나지만, 실시간 데이터 수집 및 처리에 최적화되어 있지 않아 이 시나리오의 요구 사항에 적합하지 않습니다.",
            "Amazon MSK는 Apache Kafka를 위한 관리형 서비스로, 분산 스트리밍에 적합하지만 실시간 데이터 수집을 위해 Amazon Kinesis보다 더 복잡한 설정 및 관리가 필요합니다."
        ]
    },
    {
        "Question Number": "11",
        "Situation": "한 의료 기관이 규정 준수를 보장하고 데이터 접근성을 개선하기 위해 환자 데이터를 AWS로 마이그레이션하고 있습니다. 그들은 비구조화된 데이터 저장을 위해 Amazon S3를 사용하기로 선택했으며, 구조화된 데이터에 대한 다양한 데이터베이스 옵션도 고려하고 있습니다. 이 기관은 높은 가용성, 확장성 및 복잡한 쿼리를 지원하면서 ACID 준수를 보장하는 데이터베이스를 선택해야 합니다. 어떤 데이터베이스 서비스를 선택해야 할까요?",
        "Question": "구조화된 환자 데이터 저장을 위한 높은 가용성, 확장성 및 ACID 준수 요구 사항을 가장 잘 충족하는 AWS 데이터베이스 서비스는 무엇입니까?",
        "Options": {
            "1": "ACID 준수를 제공하고 복잡한 SQL 쿼리를 지원하며 관리되고 확장 가능한 Amazon RDS for PostgreSQL.",
            "2": "복잡한 쿼리를 지원하고 수요에 따라 자동으로 확장할 수 있으며 ACID 준수를 제공하는 Amazon Aurora Serverless.",
            "3": "키-값 쌍에 대한 높은 가용성과 확장성을 제공하지만 ACID 트랜잭션이 없는 Amazon DynamoDB.",
            "4": "대량의 데이터 세트를 처리할 수 있지만 데이터베이스 서비스가 아니며 ACID 트랜잭션이 없는 Amazon S3와 Athena."
        },
        "Correct Answer": "복잡한 쿼리를 지원하고 수요에 따라 자동으로 확장할 수 있으며 ACID 준수를 제공하는 Amazon Aurora Serverless.",
        "Explanation": "Amazon Aurora Serverless는 사용량에 따라 자동으로 확장할 수 있는 관계형 데이터베이스의 이점을 결합한 훌륭한 선택입니다. 복잡한 SQL 쿼리를 지원하며 ACID 준수를 유지하여 민감한 환자 기록을 효과적으로 처리하는 데 필수적입니다.",
        "Other Options": [
            "Amazon DynamoDB는 주로 키-값 및 문서 데이터 모델을 지원하는 NoSQL 데이터베이스로, 복잡한 SQL 쿼리 및 ACID 트랜잭션의 필요를 충족하지 못하므로 적합하지 않습니다.",
            "Amazon RDS for PostgreSQL은 ACID 준수를 갖춘 구조화된 데이터에 대한 실행 가능한 옵션이지만, Aurora Serverless와 달리 수요에 따라 자동으로 확장되지 않을 수 있습니다.",
            "Amazon S3와 Athena는 데이터베이스 서비스가 아니라 S3의 데이터를 쿼리할 수 있는 데이터 레이크 솔루션이므로 적절한 선택이 아니며 ACID 준수를 제공하지 않습니다."
        ]
    },
    {
        "Question Number": "12",
        "Situation": "데이터 엔지니어가 소매 분석 애플리케이션을 위해 대량의 데이터를 처리하는 Amazon Redshift 클러스터를 관리하고 있습니다. 팀은 예기치 않은 실패가 발생할 경우 클러스터의 데이터를 복구할 수 있도록 해야 합니다. 데이터 엔지니어는 Amazon Redshift의 스냅샷 기능을 이해하고 이를 효과적으로 재해 복구에 활용하는 방법을 알아야 합니다.",
        "Question": "Amazon Redshift 스냅샷에 대한 다음 설명 중 TRUE인 것은 무엇입니까?",
        "Options": {
            "1": "자동 스냅샷은 무기한 보존되며 다른 AWS 계정과 공유할 수 있습니다.",
            "2": "재해 복구를 강화하기 위해 교차 리전 스냅샷을 생성할 수 있으며 기본 보존 기간은 7일입니다.",
            "3": "수동 스냅샷은 8시간마다 또는 5GB의 데이터가 변경될 때 자동으로 생성됩니다.",
            "4": "Amazon Redshift의 감사 로그는 클러스터의 로컬 스토리지에 저장되며 30일 후에 삭제됩니다."
        },
        "Correct Answer": "재해 복구를 강화하기 위해 교차 리전 스냅샷을 생성할 수 있으며 기본 보존 기간은 7일입니다.",
        "Explanation": "교차 리전 스냅샷을 사용하면 재해 복구 목적으로 스냅샷을 다른 AWS 리전으로 복사할 수 있으며, 기본 보존 기간은 7일로 필요에 따라 구성할 수 있습니다.",
        "Other Options": [
            "자동 스냅샷은 8시간마다 또는 5GB의 데이터가 변경된 후에 생성되지만 무기한 보존되지 않으며 다른 계정과 공유할 수 없으므로 이 옵션은 잘못되었습니다.",
            "수동 스냅샷은 자동으로 생성되지 않으며 사용자가 수동으로 생성해야 합니다. 이 설명은 수동 스냅샷의 성격을 잘못 설명하고 있어 잘못된 것입니다.",
            "Amazon Redshift의 감사 로그는 로컬 스토리지에 저장되지 않고 Amazon S3에 저장되며, 30일의 고정 보존 기간이 없으므로 이 옵션은 잘못되었습니다."
        ]
    },
    {
        "Question Number": "13",
        "Situation": "데이터 엔지니어링 팀은 Amazon Redshift에 저장된 데이터의 정확성과 신뢰성을 향상시키는 임무를 맡았습니다. 그들은 사용자가 데이터의 출처와 적용된 변환을 이해할 수 있도록 데이터 세트의 데이터 계보를 추적하는 솔루션을 구현하고자 합니다.",
        "Question": "Amazon Redshift에서 데이터 계보를 구현하여 데이터의 정확성과 신뢰성을 보장하기 위한 최선의 접근 방식은 무엇입니까?",
        "Options": {
            "1": "AWS Step Functions를 사용하여 ETL 프로세스를 배포하고 데이터 변환을 처리하며 각 단계를 계보 목적으로 별도의 로그 파일에 문서화합니다.",
            "2": "AWS Glue Data Catalog를 사용하여 데이터 소스와 변환을 추적하기 위한 중앙 집중식 메타데이터 저장소를 생성합니다. 데이터 무결성을 확인하기 위해 정기적인 감사를 예약합니다.",
            "3": "AWS Lake Formation을 통합하여 데이터 접근을 관리하고 데이터 거버넌스를 위한 내장 기능을 통해 데이터 계보 추적을 구현합니다.",
            "4": "Amazon Redshift의 로깅 기능을 활성화하여 쿼리 및 데이터 변경 사항을 추적합니다. 계보 추적을 위해 로그를 구문 분석하는 사용자 정의 솔루션을 구현합니다."
        },
        "Correct Answer": "AWS Lake Formation을 통합하여 데이터 접근을 관리하고 데이터 거버넌스를 위한 내장 기능을 통해 데이터 계보 추적을 구현합니다.",
        "Explanation": "AWS Lake Formation을 통합하면 데이터 접근을 관리하고 데이터 계보 추적을 구현하기 위한 강력한 프레임워크를 제공합니다. Lake Formation의 기능은 세밀한 데이터 거버넌스를 가능하게 하여 데이터 세트의 출처와 변환을 추적하기 쉽게 만들어 데이터의 정확성과 신뢰성을 향상시킵니다.",
        "Other Options": [
            "AWS Glue Data Catalog를 사용하는 것은 메타데이터 관리에 좋은 접근 방식이지만, 계보 추적을 포함한 Lake Formation이 제공하는 전체 데이터 거버넌스 기능을 포함하지 않습니다.",
            "Amazon Redshift의 로깅 기능을 활성화하면 변경 사항을 추적하는 데 도움이 될 수 있지만, 계보 추적을 위해 로그를 구문 분석하는 것은 복잡할 수 있으며 Lake Formation의 내장 기능에 비해 명확한 계보 뷰를 제공하지 않을 수 있습니다.",
            "AWS Step Functions로 ETL 프로세스를 배포하면 변환을 문서화하는 데 도움이 될 수 있지만, AWS Lake Formation이 제공하는 중앙 집중식 거버넌스 및 계보 추적 기능이 부족하여 전체 데이터 신뢰성을 보장하는 데 덜 효율적입니다."
        ]
    },
    {
        "Question Number": "14",
        "Situation": "데이터 엔지니어는 Amazon S3에 저장된 대규모 데이터 세트를 정리하고 변환하여 분석을 준비하는 임무를 맡았습니다. 그들은 AWS Glue DataBrew를 사용하여 데이터를 시각적으로 조작하고 보고 목적으로 깨끗한 데이터 세트를 만들기로 결정했습니다.",
        "Question": "데이터 엔지니어가 반복적인 데이터 업데이트를 위한 데이터 변환 프로세스를 자동화하기 위해 AWS Glue DataBrew의 어떤 기능을 사용해야 합니까?",
        "Options": {
            "1": "변환된 데이터 세트를 Amazon Redshift로 내보내 추가 처리를 수행합니다.",
            "2": "새로운 DataBrew 프로젝트를 생성하고 각 데이터 업데이트에 대해 작업을 수동으로 실행합니다.",
            "3": "DataBrew에서 수행된 변환을 저장하기 위해 AWS Glue Data Catalog를 설정합니다.",
            "4": "DataBrew 레시피를 사용하여 변환을 정기적으로 예약합니다."
        },
        "Correct Answer": "DataBrew 레시피를 사용하여 변환을 정기적으로 예약합니다.",
        "Explanation": "AWS Glue DataBrew는 사용자가 새로운 데이터가 소스에 추가될 때마다 자동으로 실행되도록 예약할 수 있는 레시피를 생성할 수 있게 해줍니다. 이는 데이터 변환 프로세스가 자동화되고 효율적으로 이루어지도록 하여 반복적인 데이터 업데이트를 수동 개입 없이 수용할 수 있게 합니다.",
        "Other Options": [
            "새로운 DataBrew 프로젝트를 생성하고 각 데이터 업데이트에 대해 작업을 수동으로 실행하는 것은 효율적이지 않으며, DataBrew 사용의 주요 장점인 자동화를 활용하지 않습니다.",
            "AWS Glue Data Catalog를 설정하여 변환을 저장하는 것은 변환 프로세스를 자동화하지 않으며, 단순히 데이터를 카탈로그화할 뿐 변환을 적용하지 않습니다.",
            "변환된 데이터 세트를 Amazon Redshift로 내보내는 것은 DataBrew 내에서 변환 프로세스를 자동화하는 방법이 아니며, 데이터 로딩을 위한 추가 단계가 필요합니다."
        ]
    },
    {
        "Question Number": "15",
        "Situation": "금융 서비스 회사는 매일 대량의 거래 데이터를 처리하고 Amazon Kinesis Data Firehose를 사용하여 이 데이터를 Amazon S3로 수집합니다. 그런 다음 AWS Lambda를 사용하여 데이터를 변환한 후 Amazon Redshift에 저장하여 분석합니다. 이 회사는 효율적인 데이터 처리를 유지하면서 비용을 최적화할 방법을 찾고 있습니다.",
        "Question": "어떤 솔루션이 회사가 데이터 수집 및 변환과 관련된 비용을 줄이면서 효율적인 처리를 보장하는 데 도움이 될까요?",
        "Options": {
            "1": "비용을 줄이기 위해 AWS Lambda 대신 AWS Glue를 데이터 변환에 구현합니다.",
            "2": "비수기 시간에 데이터를 배치로 처리하기 위해 AWS Lambda 함수를 예약합니다.",
            "3": "Amazon S3에서 데이터의 보존 기간을 줄여 저장 비용을 낮춥니다.",
            "4": "실시간 처리를 위해 Kinesis Data Firehose 대신 Amazon Kinesis Data Streams를 사용합니다."
        },
        "Correct Answer": "비수기 시간에 데이터를 배치로 처리하기 위해 AWS Lambda 함수를 예약합니다.",
        "Explanation": "비수기 시간에 데이터를 배치로 처리하기 위해 AWS Lambda 함수를 예약하면 Lambda 호출과 관련된 비용을 줄일 수 있으며, 수요가 낮은 시간에 리소스를 최적화하여 사용할 수 있습니다. 이 접근 방식은 데이터 처리의 효율성을 유지하면서 비용 절감으로 이어질 수 있습니다.",
        "Other Options": [
            "Amazon Kinesis Data Streams를 사용하면 더 많은 실시간 처리 기능을 제공할 수 있지만, 데이터 수집 및 보존에 따른 가격 모델로 인해 비용이 증가할 수 있어 Kinesis Data Firehose에 비해 비용 효율적이지 않을 수 있습니다.",
            "AWS Glue를 데이터 변환에 구현하면 ETL 프로세스를 단순화할 수 있지만, 데이터 변환 작업을 Lambda 함수가 효율적으로 처리할 수 있는 경우 AWS Lambda에 비해 반드시 비용을 줄이지는 않을 수 있습니다.",
            "Amazon S3에서 데이터의 보존 기간을 줄이면 저장 비용을 낮출 수 있지만, 데이터 수집 및 변환 프로세스와 관련된 비용을 직접적으로 해결하지는 않습니다."
        ]
    },
    {
        "Question Number": "16",
        "Situation": "한 회사가 사용자 상호작용 및 시스템 성능 메트릭을 포함하는 로그를 생성하는 웹 애플리케이션을 배포했습니다. 현재 로그는 Amazon S3에 일반 텍스트 형식으로 저장되고 있습니다. 데이터 엔지니어는 이 데이터를 효율적으로 기록, 모니터링 및 분석할 수 있는 솔루션을 구현해야 하며, 이 데이터가 쉽게 검색 가능하고 다른 AWS 서비스와 통합될 수 있도록 해야 합니다.",
        "Question": "데이터 엔지니어가 애플리케이션 데이터를 효과적으로 기록하기 위해 어떤 솔루션을 구현해야 합니까?",
        "Options": {
            "1": "쿼리가 용이하도록 Amazon RDS와 같은 관계형 데이터베이스에 로그를 저장합니다.",
            "2": "로그를 Amazon DynamoDB에 직접 기록하는 사용자 정의 로깅 메커니즘을 구현합니다.",
            "3": "Amazon CloudWatch Logs를 사용하여 애플리케이션의 로그 데이터를 수집하고 저장합니다.",
            "4": "로그를 Amazon S3에 업로드하기 전에 Gzip을 사용하여 압축 형식으로 저장합니다."
        },
        "Correct Answer": "Amazon CloudWatch Logs를 사용하여 애플리케이션의 로그 데이터를 수집하고 저장합니다.",
        "Explanation": "Amazon CloudWatch Logs는 로그 데이터를 수집하고 모니터링하기 위해 특별히 설계되었으며, 필터링, 검색 및 다른 서비스와의 통합과 같은 기능을 제공하여 애플리케이션 데이터 로깅을 위한 가장 효율적인 선택입니다.",
        "Other Options": [
            "로그를 압축 형식으로 저장하면 저장 비용이 줄어들지만, CloudWatch Logs가 제공하는 향상된 모니터링 및 검색 기능을 제공하지 않습니다.",
            "DynamoDB에 사용자 정의 로깅 메커니즘을 구현하면 로깅 프로세스가 복잡해지고 CloudWatch와 같은 관리형 서비스를 사용할 때보다 추가 비용과 관리 오버헤드가 발생할 수 있습니다.",
            "Amazon RDS에 로그를 저장하는 것은 가장 효율적인 접근 방식이 아니며, 더 많은 관리가 필요하고 CloudWatch가 제공하는 로그 분석을 위한 통합 및 기능 수준이 동일하지 않습니다."
        ]
    },
    {
        "Question Number": "17",
        "Situation": "데이터 엔지니어링 팀은 IoT 장치에서 생성된 대량의 실시간 데이터 스트림을 처리하는 임무를 맡고 있습니다. 그들은 데이터 수집, 변환 및 분석을 효율적으로 처리할 수 있는 솔루션이 필요하며, 변동하는 작업 부하를 수용하기 위해 자동으로 확장할 수 있어야 합니다.",
        "Question": "팀이 IoT 애플리케이션의 실시간 데이터 수집 및 처리를 효율적으로 처리하기 위해 어떤 AWS 서비스를 사용해야 합니까?",
        "Options": {
            "1": "AWS Glue",
            "2": "Amazon RDS",
            "3": "Amazon Redshift",
            "4": "Amazon Kinesis Data Streams"
        },
        "Correct Answer": "Amazon Kinesis Data Streams",
        "Explanation": "Amazon Kinesis Data Streams는 실시간 데이터 수집 및 처리를 위해 특별히 설계되었습니다. 이 서비스는 애플리케이션이 분산 방식으로 스트리밍 데이터를 처리할 수 있도록 하며, 변동하는 작업 부하를 처리하기 위해 자동으로 확장할 수 있어 IoT 장치와 관련된 사용 사례에 적합합니다.",
        "Other Options": [
            "AWS Glue는 주로 배치 데이터 처리 및 ETL 작업에 사용되며, 변환을 처리할 수 있지만 실시간 스트리밍 데이터 수집에는 적합하지 않습니다.",
            "Amazon RDS는 관계형 데이터베이스 서비스로, 실시간 데이터 수집이나 스트리밍 데이터 처리 기능을 제공하지 않습니다.",
            "Amazon Redshift는 대규모 데이터 세트에 대한 복잡한 쿼리 및 분석에 최적화된 데이터 웨어하우징 솔루션이지만, 실시간 데이터 수집을 위해 설계되지 않았습니다."
        ]
    },
    {
        "Question Number": "18",
        "Situation": "한 회사가 Amazon S3에 저장된 대량의 데이터를 관리하고 있으며, 데이터 수명 주기를 기반으로 저장 비용을 최적화하려고 합니다. 그들은 자주 접근되는 데이터가 있지만, 시간이 지남에 따라 접근이 크게 줄어드는 데이터를 가지고 있습니다. 그들은 중요한 데이터를 잃지 않으면서 비용을 효과적으로 줄이는 솔루션을 구현하고자 합니다.",
        "Question": "데이터 엔지니어가 데이터 수명 주기를 기반으로 저장 비용을 최적화하기 위해 고려해야 할 전략은 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "오래된 데이터를 Amazon S3 Glacier로 전환하여 장기 저장합니다.",
            "2": "모든 데이터를 Amazon S3 Standard에 유지하여 고가용성을 확보합니다.",
            "3": "Amazon S3 Intelligent-Tiering을 사용하여 데이터 접근 계층 간에 자동으로 이동합니다.",
            "4": "모든 데이터를 Amazon EFS에 아카이브하여 저비용 저장을 합니다.",
            "5": "지정된 기간 후에 데이터를 자동으로 삭제하는 수명 주기 정책을 구현합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "오래된 데이터를 Amazon S3 Glacier로 전환하여 장기 저장합니다.",
            "지정된 기간 후에 데이터를 자동으로 삭제하는 수명 주기 정책을 구현합니다."
        ],
        "Explanation": "오래된 데이터를 Amazon S3 Glacier로 전환하는 것은 자주 접근되지 않는 데이터의 장기 저장을 위한 비용 효율적인 솔루션을 제공합니다. 수명 주기 정책을 구현하면 데이터의 자동 관리를 가능하게 하여 더 이상 필요하지 않은 데이터가 삭제되도록 하여 저장 비용을 더욱 최적화할 수 있습니다.",
        "Other Options": [
            "모든 데이터를 Amazon S3 Standard에 유지하는 것은 오래되고 자주 접근되지 않는 데이터에 대해 비용 효율적이지 않으며, S3 Glacier와 같은 대안 옵션에 비해 더 높은 저장 비용이 발생합니다.",
            "Amazon S3 Intelligent-Tiering을 사용하는 것은 모든 데이터에 대해 최선의 선택이 아닐 수 있으며, 데이터 수명 주기에 대한 명확한 이해가 있을 경우 추가 모니터링 비용이 발생하고 자주 접근되지 않는 데이터에 대해 비용 절감 효과가 크지 않을 수 있습니다.",
            "모든 데이터를 Amazon EFS에 아카이브하는 것은 장기 저장을 위한 비용 효율적인 솔루션이 아니며, EFS는 고성능 파일 저장을 위해 설계되었고 일반적으로 대량의 데이터를 저장하는 데 S3보다 더 비쌉니다."
        ]
    },
    {
        "Question Number": "19",
        "Situation": "소매 회사가 Amazon S3에 저장된 거래 데이터를 분석하여 고객 구매 패턴과 트렌드를 파악하고자 합니다. 그들은 데이터를 수집하고, 분석을 위해 변환하며, 쿼리를 위해 Amazon Redshift에 로드하는 솔루션을 구현하고자 합니다. 이 솔루션은 비용을 최소화하면서 확장성과 유지 관리성을 보장해야 합니다.",
        "Question": "이 데이터 수집 및 변환 워크플로우에 가장 효과적인 AWS 서비스 조합은 무엇입니까?",
        "Options": {
            "1": "Amazon S3의 데이터를 처리하기 위해 Amazon EMR 클러스터를 설정한 다음 AWS Lambda를 사용하여 Amazon Redshift에 로드합니다.",
            "2": "Amazon Kinesis Data Firehose를 구현하여 데이터를 직접 Amazon Redshift로 스트리밍하여 분석합니다.",
            "3": "AWS Glue를 사용하여 Amazon S3에서 Amazon Redshift로 데이터를 추출, 변환 및 로드(ETL)합니다.",
            "4": "AWS Data Pipeline을 활용하여 Amazon S3에서 Amazon Redshift로의 데이터 전송을 변환 단계와 함께 조정합니다."
        },
        "Correct Answer": "AWS Glue를 사용하여 Amazon S3에서 Amazon Redshift로 데이터를 추출, 변환 및 로드(ETL)합니다.",
        "Explanation": "AWS Glue는 Amazon S3에서 Amazon Redshift로 데이터를 효율적으로 추출, 변환 및 로드할 수 있는 완전 관리형 ETL 서비스입니다. 서버리스 기능을 제공하여 데이터 변환 작업에 대해 비용 효율적이고 확장 가능합니다.",
        "Other Options": [
            "Amazon EMR 클러스터를 설정하면 클러스터 및 추가 리소스를 관리해야 하므로 AWS Glue에 비해 운영 오버헤드와 비용이 더 높아집니다.",
            "Amazon Kinesis Data Firehose는 배치 처리 및 변환보다는 실시간 데이터 스트리밍에 더 적합하여 역사적 거래 데이터를 분석하는 데 필요한 기능이 부족합니다.",
            "AWS Data Pipeline은 유효한 옵션이지만 AWS Glue의 서버리스 특성에 비해 더 많은 구성 및 유지 관리가 필요하므로 운영 복잡성이 증가합니다."
        ]
    },
    {
        "Question Number": "20",
        "Situation": "한 회사가 AWS에 마이크로서비스 애플리케이션을 배포하고 모든 애플리케이션 로그가 Amazon CloudWatch Logs를 사용하여 캡처되고 모니터링되도록 하기를 원합니다. 그들은 서비스에 대한 로그 그룹 및 로그 스트림의 구성을 자동화해야 합니다.",
        "Question": "애플리케이션에 대한 Amazon CloudWatch Logs 구성을 자동화하는 가장 좋은 방법은 무엇입니까?",
        "Options": {
            "1": "AWS Config를 활용하여 로그 그룹 구성의 변경 사항을 모니터링하고 변경 사항이 발생하면 관리자를 알립니다.",
            "2": "EC2 인스턴스에서 애플리케이션 요구 사항에 따라 로그 그룹 및 스트림을 생성하는 스크립트를 실행하는 크론 작업을 구현합니다.",
            "3": "새 서비스가 배포될 때마다 CloudWatch Logs 콘솔에서 수동으로 로그 그룹 및 스트림을 생성합니다.",
            "4": "AWS CloudFormation 서비스를 사용하여 애플리케이션 스택의 일부로 로그 그룹 및 로그 스트림 구성을 정의하고 배포합니다."
        },
        "Correct Answer": "AWS CloudFormation 서비스를 사용하여 애플리케이션 스택의 일부로 로그 그룹 및 로그 스트림 구성을 정의하고 배포합니다.",
        "Explanation": "AWS CloudFormation을 사용하면 일관되고 반복 가능한 인프라를 코드로 관리할 수 있어 애플리케이션 배포 프로세스의 일환으로 CloudWatch Logs 구성을 자동으로 생성하고 관리할 수 있습니다.",
        "Other Options": [
            "이 옵션은 수동으로 로그 그룹 및 스트림을 생성하는 것이 비효율적이고 확장 가능하지 않기 때문에 잘못된 것입니다. 특히 마이크로서비스는 자주 변경될 수 있습니다.",
            "이 옵션은 EC2 인스턴스에서 크론 작업에 의존하는 것이 로그 구성을 관리하는 데 불필요한 복잡성과 잠재적인 실패 지점을 도입하기 때문에 잘못된 것입니다.",
            "이 옵션은 AWS Config가 주로 리소스 구성의 준수 및 변경 사항 모니터링에 사용되므로 로그 그룹 및 스트림 생성을 자동화하는 데 적합하지 않기 때문에 잘못된 것입니다."
        ]
    },
    {
        "Question Number": "21",
        "Situation": "한 금융 서비스 회사가 매일 대량의 거래 데이터를 처리하고 있으며 이 데이터를 AWS로 수집하여 분석 및 보고를 해야 합니다. 이 회사는 현재 실시간 처리와 배치 처리를 혼합하여 사용하고 있습니다. 데이터 수집 프로세스를 간소화하고 지연 시간을 최소화하며 데이터 일관성을 보장하기 위해 적절한 수집 전략을 선택하고자 합니다.",
        "Question": "AWS 내에서 거래 데이터의 실시간 및 배치 처리를 모두 충족하는 데이터 수집 전략은 무엇입니까?",
        "Options": {
            "1": "AWS Data Pipeline을 구현하여 온프레미스 데이터베이스에서 Amazon S3로 정기적으로 데이터를 수집하는 배치 작업을 예약합니다.",
            "2": "Amazon S3 이벤트 알림을 활용하여 AWS Glue 작업을 트리거하여 데이터를 Redshift 클러스터로 실시간 수집합니다.",
            "3": "정기적으로 온프레미스 소스에서 데이터를 가져와 Amazon S3로 전송하는 예약된 AWS Lambda 함수를 설정합니다.",
            "4": "Amazon Kinesis Data Streams를 사용하여 거래 데이터를 실시간으로 수집하고 AWS Glue를 사용하여 매일 밤 데이터를 Amazon S3로 배치 변환합니다."
        },
        "Correct Answer": "Amazon Kinesis Data Streams를 사용하여 거래 데이터를 실시간으로 수집하고 AWS Glue를 사용하여 매일 밤 데이터를 Amazon S3로 배치 변환합니다.",
        "Explanation": "이 옵션은 실시간 및 배치 처리 요구 사항을 효과적으로 결합합니다. Amazon Kinesis Data Streams는 거래 데이터를 저지연으로 수집할 수 있게 해주며, AWS Glue는 변환 및 야간 배치 처리를 처리할 수 있어 회사의 요구에 맞는 다재다능한 솔루션입니다.",
        "Other Options": [
            "AWS Data Pipeline은 배치 작업을 예약할 수 있지만 실시간 수집을 본질적으로 지원하지 않으므로 회사의 중요한 요구 사항을 충족하지 않습니다.",
            "예약된 AWS Lambda 함수를 설정하는 것은 배치 수집에 효과적일 수 있지만 실시간 처리 기능이 부족하여 즉각적인 거래 데이터 분석에 비효율적입니다.",
            "S3 이벤트 알림을 사용하여 실시간 수집을 하는 것은 데이터를 배치 변환하는 요구 사항과 일치하지 않으며, 이벤트 기반 처리에만 집중하고 있어 확실한 배치 통합 전략이 없습니다."
        ]
    },
    {
        "Question Number": "22",
        "Situation": "한 금융 서비스 회사가 결제 처리 시스템의 스트리밍 데이터에 대한 실시간 거래 및 분석을 처리하려고 합니다. 이들은 AWS에서 이 데이터 수집을 처리할 수 있는 신뢰할 수 있고 확장 가능한 솔루션을 설정하고자 합니다.",
        "Question": "회사가 결제 거래에서 실시간으로 스트리밍 데이터를 수집하고 처리하기 위해 선택해야 할 AWS 서비스는 무엇입니까?",
        "Options": {
            "1": "Amazon S3",
            "2": "Amazon Kinesis Data Streams",
            "3": "AWS Lambda",
            "4": "Amazon Redshift"
        },
        "Correct Answer": "Amazon Kinesis Data Streams",
        "Explanation": "Amazon Kinesis Data Streams는 실시간 데이터 수집 및 처리를 위해 특별히 설계되어, 결제 거래의 스트리밍 데이터를 효율적으로 처리하는 데 가장 적합한 선택입니다.",
        "Other Options": [
            "AWS Lambda는 데이터를 처리할 수 있는 서버리스 컴퓨팅 서비스이지만, Kinesis와 같은 다른 서비스와 결합되지 않으면 스트림에서 데이터 수집을 본래적으로 처리하지 않습니다.",
            "Amazon S3는 주로 저장 서비스이며 실시간 데이터 수집 기능을 제공하지 않으며, 스트리밍 수집보다는 데이터의 배치 처리를 더 잘 지원합니다.",
            "Amazon Redshift는 대규모 데이터 세트에 대한 분석을 최적화한 데이터 웨어하우스 서비스이지만, 실시간 스트리밍 데이터 수집을 위해 설계되지 않았습니다."
        ]
    },
    {
        "Question Number": "23",
        "Situation": "한 기술 스타트업이 소셜 미디어 피드, 고객 거래 로그, IoT 장치의 센서 데이터 등 다양한 출처에서 다양한 데이터 유형을 처리하기 위한 데이터 수집 파이프라인을 설계하고 있습니다. 데이터 엔지니어는 높은 속도로 도착하는 대량의 데이터를 수용할 수 있는 수집 프로세스를 보장해야 하며, 구조화된 데이터와 비구조화된 데이터 형식을 모두 처리할 수 있는 능력을 유지해야 합니다. 또한, 파이프라인은 데이터 출처와 유형의 미래 성장을 처리할 수 있도록 확장 가능해야 합니다.",
        "Question": "데이터 엔지니어가 데이터 수집 프로세스를 최적화하기 위해 구현해야 할 전략은 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "모든 수신 데이터를 사전 처리 없이 Amazon S3에 저장합니다.",
            "2": "도착하는 데이터를 즉시 처리하기 위해 AWS Lambda 함수를 배포합니다.",
            "3": "여러 출처에서 실시간 데이터를 캡처하기 위해 Amazon Kinesis Data Streams를 활용합니다.",
            "4": "도착하는 데이터의 즉각적인 쿼리를 처리하기 위해 Amazon Redshift를 설정합니다.",
            "5": "분석을 위해 수신 데이터를 카탈로그화하고 변환하기 위해 AWS Glue를 구현합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "여러 출처에서 실시간 데이터를 캡처하기 위해 Amazon Kinesis Data Streams를 활용합니다.",
            "도착하는 데이터를 즉시 처리하기 위해 AWS Lambda 함수를 배포합니다."
        ],
        "Explanation": "Amazon Kinesis Data Streams를 활용하면 다양한 출처에서 높은 속도로 실시간 데이터 수집이 가능하여, 대량의 수신 데이터를 처리하는 데 필수적입니다. AWS Lambda 함수를 배포하면 도착하는 데이터의 서버리스 처리가 가능하여 즉각적인 변환 및 분석이 가능해지며, 이는 구조화된 데이터와 비구조화된 데이터 모두에 필수적입니다.",
        "Other Options": [
            "AWS Glue를 구현하는 것은 유익하지만, 주로 배치 처리 및 변환에 사용되며, 여기서 요구되는 실시간 수집에는 적합하지 않습니다.",
            "모든 수신 데이터를 사전 처리 없이 Amazon S3에 저장하는 것은 수집 프로세스를 최적화하지 않으며, 단순히 저장소 역할을 할 뿐 데이터 속도나 다양성을 효과적으로 처리하지 않습니다.",
            "즉각적인 쿼리를 위해 Amazon Redshift를 설정하는 것은 초기 데이터 수집을 위한 최적의 전략이 아니며, 이는 실시간 데이터 캡처보다는 분석 작업에 맞춰 설계되었습니다."
        ]
    },
    {
        "Question Number": "24",
        "Situation": "한 소매 회사의 데이터 엔지니어링 팀은 Amazon S3에 있는 파티션 데이터 세트가 잘 관리되고 AWS Glue Data Catalog와 동기화되도록 해야 합니다. 팀은 데이터 검색을 간소화하고 분석 작업의 성능을 향상시키는 것을 목표로 하고 있습니다.",
        "Question": "AWS Glue Data Catalog와 파티션을 효과적으로 동기화하기 위한 단계의 조합은 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "Amazon S3에 생성된 각 새로운 파티션에 대해 AWS Glue Data Catalog 항목을 수동으로 업데이트합니다.",
            "2": "Amazon S3에 새로운 데이터가 추가될 때마다 AWS Glue Data Catalog 업데이트를 트리거하기 위해 AWS Lambda 함수를 활용합니다.",
            "3": "AWS Glue Crawlers를 구현하여 Amazon S3의 데이터를 스캔하고 새로운 파티션으로 AWS Glue Data Catalog를 업데이트합니다.",
            "4": "Amazon Athena를 활용하여 Amazon S3의 데이터를 쿼리하고 AWS Glue Data Catalog에 파티션 메타데이터를 생성합니다.",
            "5": "AWS Glue ETL 작업을 사용하여 AWS Glue Data Catalog에서 파티션을 자동으로 생성하고 업데이트합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "AWS Glue ETL 작업을 사용하여 AWS Glue Data Catalog에서 파티션을 자동으로 생성하고 업데이트합니다.",
            "AWS Glue Crawlers를 구현하여 Amazon S3의 데이터를 스캔하고 새로운 파티션으로 AWS Glue Data Catalog를 업데이트합니다."
        ],
        "Explanation": "AWS Glue ETL 작업을 사용하면 AWS Glue Data Catalog에서 파티션을 자동으로 생성하고 업데이트할 수 있어, 카탈로그가 Amazon S3에 저장된 데이터의 현재 구조를 반영하도록 보장합니다. 또한, AWS Glue Crawlers는 S3 버킷을 정기적으로 스캔하도록 예약할 수 있어 새로운 파티션을 식별하고 Data Catalog를 자동으로 업데이트하여 데이터 검색 가능성과 관리성을 향상시킵니다.",
        "Other Options": [
            "AWS Glue Data Catalog를 수동으로 업데이트하는 것은 비효율적이며 오류가 발생하기 쉬워, 대규모 데이터 세트의 경우 정기적인 동기화에 실용적이지 않습니다.",
            "Amazon Athena를 사용하여 파티션 메타데이터를 생성하는 것은 Data Catalog를 직접 업데이트하지 않으며, 기존 카탈로그 항목에 의존하고 자동 동기화를 촉진하지 않습니다.",
            "AWS Lambda 함수는 이벤트를 트리거할 수 있지만, 파티션 동기화를 적절히 관리하기 위해 상당한 사용자 정의 및 추가 복잡성이 필요하므로 Glue 작업이나 Crawlers를 사용하는 것보다 효율적이지 않습니다."
        ]
    },
    {
        "Question Number": "25",
        "Situation": "데이터 엔지니어가 Amazon Redshift 클러스터를 관리하고 있으며, 증가된 작업량으로 인해 성능 문제가 발생하고 있습니다. 엔지니어는 성능을 개선하고 주요 성능 지표를 추적할 수 있도록 모니터링이 설정되도록 클러스터의 크기를 조정해야 합니다.",
        "Question": "데이터 엔지니어가 예상 작업량에 충분한 리소스를 보장하면서 클러스터의 크기를 조정하기 위해 실행해야 할 명령은 무엇입니까?",
        "Options": {
            "1": "aws redshift change-cluster-configuration --cluster-identifier my-redshift-cluster --node-type dc2.8xlarge --number-of-nodes 3",
            "2": "aws redshift update-cluster --cluster-identifier my-redshift-cluster --node-type ra3.xlplus --number-of-nodes 6",
            "3": "aws redshift resize-cluster --cluster-identifier my-redshift-cluster --node-type dc2.xlarge --number-of-nodes 2",
            "4": "aws redshift modify-cluster --cluster-identifier my-redshift-cluster --node-type dc2.large --number-of-nodes 4"
        },
        "Correct Answer": "aws redshift modify-cluster --cluster-identifier my-redshift-cluster --node-type dc2.large --number-of-nodes 4",
        "Explanation": "이 명령은 클러스터 식별자, 노드 유형 및 노드 수를 지정하여 기존 Redshift 클러스터를 올바르게 수정하며, 성능을 유지하면서 크기를 조정하기 위한 모범 사례에 부합합니다.",
        "Other Options": [
            "이 옵션은 AWS CLI에서 존재하지 않는 잘못된 명령 'resize-cluster'를 사용하고 있습니다. 명령은 'modify-cluster'여야 합니다.",
            "이 옵션은 Amazon Redshift 클러스터의 크기를 조정하기 위한 유효한 명령이 아닌 'update-cluster'를 사용하고 있습니다; 올바른 명령은 'modify-cluster'입니다.",
            "이 옵션은 Redshift 클러스터의 크기를 조정하기 위한 유효한 AWS CLI 명령이 아닌 'change-cluster-configuration'을 잘못 사용하고 있습니다. 올바른 명령은 'modify-cluster'입니다."
        ]
    },
    {
        "Question Number": "26",
        "Situation": "헬스케어 애플리케이션은 HIPAA와 같은 규정을 준수하기 위해 민감한 환자 정보를 포함한 모든 애플리케이션 데이터를 기록해야 합니다. 조직은 안전한 로깅을 제공하고 감사 목적으로 로그에 쉽게 접근할 수 있는 솔루션을 찾고 있습니다.",
        "Question": "데이터 거버넌스 규정을 준수하면서 애플리케이션 데이터를 안전하게 로깅하는 데 가장 적합한 AWS 서비스는 무엇입니까?",
        "Options": {
            "1": "Amazon S3 with Server Access Logging",
            "2": "AWS Config",
            "3": "AWS CloudTrail",
            "4": "Amazon CloudWatch Logs"
        },
        "Correct Answer": "Amazon CloudWatch Logs",
        "Explanation": "Amazon CloudWatch Logs는 애플리케이션 데이터를 로깅하기 위해 특별히 설계되었으며 AWS 서비스와 쉽게 통합될 수 있습니다. 안전한 저장소, 알림 설정 기능을 제공하며, 규정 준수를 지원하여 헬스케어 애플리케이션에서 민감한 정보를 로깅하는 데 가장 적합한 옵션입니다.",
        "Other Options": [
            "AWS CloudTrail은 AWS 리소스에서 수행된 API 호출 및 작업을 로깅하는 데 중점을 두지만 애플리케이션 로그에 특별히 맞춰져 있지 않아 애플리케이션 데이터 로깅에는 적합하지 않습니다.",
            "AWS Config는 주로 AWS 리소스 구성 및 규정 준수를 추적하는 데 사용되지만 애플리케이션 데이터를 로깅하기 위해 설계되지 않았으므로 요구 사항을 충족하지 않습니다.",
            "Amazon S3 with Server Access Logging은 S3 버킷에 대한 요청의 기본 로깅을 제공하지만 구조화된 애플리케이션 로그 기능이 부족하고 민감한 데이터에 대한 안전한 접근 제어를 보장하지 않습니다."
        ]
    },
    {
        "Question Number": "27",
        "Situation": "소매 회사는 판매 거래, 고객 피드백 및 재고 업데이트를 포함한 다양한 출처에서 데이터를 정기적으로 수집하여 AWS로 분석합니다. 이들은 데이터 웨어하우스에 로드하기 전에 변환을 위해 이 데이터를 효율적으로 스테이징할 방법이 필요합니다.",
        "Question": "이 소매 회사의 데이터 수집 파이프라인을 최적화하기 위한 중간 데이터 스테이징 솔루션은 무엇입니까?",
        "Options": {
            "1": "Amazon RDS 인스턴스를 설정하여 수집된 데이터를 임시로 저장합니다.",
            "2": "AWS Data Pipeline을 사용하여 데이터를 Amazon Redshift로 직접 전송합니다.",
            "3": "Amazon S3를 원시 데이터의 스테이징 영역으로 활용한 후 AWS Glue로 처리합니다.",
            "4": "Amazon DynamoDB를 사용하여 변환을 위한 수신 데이터를 스테이징합니다."
        },
        "Correct Answer": "Amazon S3를 원시 데이터의 스테이징 영역으로 활용한 후 AWS Glue로 처리합니다.",
        "Explanation": "Amazon S3를 스테이징 영역으로 활용하면 원시 데이터를 확장 가능하고 비용 효율적으로 저장할 수 있으며, 이후 AWS Glue로 변환할 수 있습니다. 이 접근 방식은 다양한 출처에서 대량의 데이터를 효율적으로 처리하는 데 이상적입니다.",
        "Other Options": [
            "Amazon RDS 인스턴스를 설정하는 것은 추가 관리 오버헤드와 비용이 발생하므로 S3에 비해 임시 데이터 스테이징에 덜 최적입니다.",
            "Amazon DynamoDB는 키-값 및 문서 저장을 위해 설계되었으며, 대량의 배치 데이터를 스테이징하는 데 사용하면 비효율적이고 비용 효과적이지 않습니다.",
            "AWS Data Pipeline을 사용하여 데이터를 Amazon Redshift로 직접 전송하면 스테이징 프로세스를 우회하게 되어 데이터에 대한 필요한 변환을 수행할 수 있는 능력이 제한될 수 있습니다."
        ]
    },
    {
        "Question Number": "28",
        "Situation": "데이터베이스 관리자는 AWS에 호스팅된 PostgreSQL 데이터베이스에서 사용자 권한을 관리하는 임무를 맡고 있습니다. 그들은 새로운 사용자를 생성하고, 특정 데이터베이스에 대한 접근 권한을 부여하며, 필요에 따라 작업을 수행할 수 있도록 해야 합니다. 관리자는 더 이상 필요하지 않을 때 사용자의 접근 권한을 제거해야 합니다. 이 과정에는 사용자 생성, 권한 부여 및 필요 시 권한 회수가 포함됩니다.",
        "Question": "다음 SQL 명령 중 사용자 생성, 특정 데이터베이스에 대한 권한 부여 및 그 권한 회수를 성공적으로 수행할 수 있는 것은 무엇입니까?",
        "Options": {
            "1": "CREATE USER newuser WITH PASSWORD 'securepassword'; GRANT SELECT ON TABLE mytable TO newuser; REVOKE SELECT ON TABLE mytable FROM newuser;",
            "2": "CREATE USER newuser WITH PASSWORD 'securepassword'; GRANT ALL PRIVILEGES ON DATABASE mydb TO newuser; DROP USER newuser;",
            "3": "CREATE USER newuser WITH PASSWORD 'securepassword'; GRANT ALL PRIVILEGES ON DATABASE mydb TO newuser; REVOKE ALL PRIVILEGES ON DATABASE mydb FROM newuser;",
            "4": "CREATE USER newuser; GRANT CONNECT ON DATABASE mydb TO newuser; REVOKE CONNECT ON DATABASE mydb FROM newuser;"
        },
        "Correct Answer": "CREATE USER newuser WITH PASSWORD 'securepassword'; GRANT ALL PRIVILEGES ON DATABASE mydb TO newuser; REVOKE ALL PRIVILEGES ON DATABASE mydb FROM newuser;",
        "Explanation": "이 옵션은 비밀번호로 사용자를 생성하고, 특정 데이터베이스에 대한 모든 권한을 해당 사용자에게 부여한 다음, 모든 권한을 회수하는 순서를 올바르게 따릅니다. 이는 사용자 관리의 요구 사항을 충족합니다.",
        "Other Options": [
            "이 옵션은 CONNECT 권한만 부여하므로 사용자가 데이터베이스에서 필요한 모든 작업을 수행하는 데 충분하지 않을 수 있습니다.",
            "이 옵션은 테이블에 대한 SELECT 권한만 다루므로 전체 데이터베이스에 대한 권한 부여 요구 사항을 충족하지 않습니다.",
            "이 옵션은 권한을 부여한 후 사용자를 삭제하는 것을 포함하므로 권한을 회수하면서 사용자를 유지해야 하는 요구 사항과 일치하지 않습니다."
        ]
    },
    {
        "Question Number": "29",
        "Situation": "금융 서비스 회사는 실시간으로 거래를 처리하기 위해 이벤트 기반 아키텍처를 구축하고 있습니다. 그들은 거래 파일이 업로드되는 S3 버킷의 이벤트에 의해 트리거되는 AWS Lambda 함수를 활용하고 있습니다. 데이터 엔지니어는 비동기 아키텍처의 특성으로 인해 일부 거래가 누락되고 있음을 발견했습니다. 모든 거래가 신뢰성 있게 캡처되고 처리되도록 하기 위해 회사는 강력한 솔루션을 구현해야 합니다.",
        "Question": "데이터 엔지니어가 신뢰할 수 있는 이벤트 처리를 보장하기 위해 어떤 단계를 밟아야 합니까? (두 가지 선택)",
        "Options": {
            "1": "Amazon SQS를 사용하여 Lambda가 처리하기 전에 S3에서 이벤트를 큐에 넣습니다.",
            "2": "AWS Step Functions를 구현하여 Lambda 함수 실행의 워크플로를 관리합니다.",
            "3": "Amazon Kinesis Data Streams를 설정하여 실시간으로 이벤트를 캡처하고 처리합니다.",
            "4": "S3 이벤트 알림을 활성화하여 여러 Lambda 함수를 직접 호출합니다.",
            "5": "Lambda 함수가 실패한 이벤트 처리를 처리할 수 있도록 데드레터 큐를 생성합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Amazon SQS를 사용하여 Lambda가 처리하기 전에 S3에서 이벤트를 큐에 넣습니다.",
            "Lambda 함수가 실패한 이벤트 처리를 처리할 수 있도록 데드레터 큐를 생성합니다."
        ],
        "Explanation": "Amazon SQS를 사용하여 이벤트를 큐에 넣으면 신뢰할 수 있는 메시지 전달이 가능하며, Lambda가 이벤트를 처리하지 못할 경우 재시도할 수 있으므로 거래가 누락되지 않도록 보장합니다. 데드레터 큐를 구현하면 여러 번의 시도 후에도 처리에 실패한 이벤트를 캡처하여 추가 조사 및 수동 처리가 가능하게 합니다.",
        "Other Options": [
            "AWS Step Functions를 구현하는 것은 워크플로 관리에 유용하지만 비동기 처리로 인해 누락된 거래 문제를 직접적으로 해결하지는 않습니다.",
            "Amazon Kinesis Data Streams를 사용하는 것은 실시간 데이터 처리에 좋은 대안이지만 S3 이벤트 처리의 특정 시나리오에 불필요한 복잡성을 도입할 수 있습니다.",
            "S3 이벤트 알림을 활성화하여 Lambda 함수를 직접 호출하면 처리 오류가 발생할 경우 거래가 누락될 수 있으며, 전달을 보장하는 큐잉 메커니즘이 없기 때문입니다."
        ]
    },
    {
        "Question Number": "30",
        "Situation": "데이터 엔지니어는 Amazon S3에서 데이터를 처리하고 Amazon Redshift에 로드하는 ETL 파이프라인을 유지 관리하는 책임이 있습니다. 데이터 엔지니어는 Glue 작업이 타임아웃 오류로 인해 자주 실패하는 것을 발견했습니다. 현재 작업은 기본 작업자 유형으로 실행되며 처리되는 데이터의 크기에 맞게 최적화되지 않았습니다.",
        "Question": "데이터 엔지니어가 최소한의 코드 변경으로 타임아웃 문제를 해결하기 위해 어떤 조치를 취해야 합니까?",
        "Options": {
            "1": "AWS Glue 콘솔에서 작업의 타임아웃 설정을 늘려 더 긴 실행 시간을 허용합니다.",
            "2": "Glue 작업을 데이터 트래픽이 적은 비혼잡 시간에 실행되도록 예약합니다.",
            "3": "Glue 작업 구성에서 작업자 유형을 G.2X로 변경하여 더 많은 리소스를 제공합니다.",
            "4": "ETL 스크립트를 수정하여 데이터를 더 작은 배치로 처리하여 실행 시간을 줄입니다."
        },
        "Correct Answer": "Glue 작업 구성에서 작업자 유형을 G.2X로 변경하여 더 많은 리소스를 제공합니다.",
        "Explanation": "작업자 유형을 G.2X로 변경하면 Glue 작업에 할당된 리소스가 증가하여 처리 시간을 크게 줄이고 기존 코드에 최소한의 변경으로 타임아웃 오류를 피할 수 있습니다.",
        "Other Options": [
            "작업의 타임아웃 설정을 늘리면 타임아웃으로 인해 작업이 실패하는 것을 방지할 수 있지만, 기본 실행 시간을 초과하는 원인인 리소스 제약 문제를 해결하지는 않습니다.",
            "ETL 스크립트를 수정하여 데이터를 더 작은 배치로 처리하는 것은 코드에 상당한 변경을 수반할 수 있으며, 복잡성이 증가하고 추가 처리 시간이 소요될 수 있습니다.",
            "Glue 작업을 비혼잡 시간에 실행되도록 예약하면 리소스 경쟁을 줄일 수 있지만, 타임아웃 오류를 유발하는 근본적인 리소스 제한 문제를 해결하지는 않습니다."
        ]
    },
    {
        "Question Number": "31",
        "Situation": "한 금융 서비스 회사가 거래 로그 및 시장 피드를 포함한 다양한 출처에서 실시간 데이터 스트림을 처리하고 있습니다. 이들은 Amazon Kinesis Data Streams를 사용하여 데이터를 수집하고 처리하고 있습니다. 회사는 Kinesis 스트림에 도착하는 각 레코드를 처리하기 위해 특정 AWS Lambda 함수를 트리거해야 합니다. 이를 통해 복잡한 변환 및 계산을 실시간으로 수행한 후 결과를 Amazon DynamoDB 테이블에 저장할 수 있습니다.",
        "Question": "데이터 엔지니어링 팀은 Kinesis Data Stream에서 들어오는 각 레코드에 대해 Lambda 함수가 호출되도록 보장하기 위해 어떤 접근 방식을 사용해야 합니까?",
        "Options": {
            "1": "Kinesis Data Stream을 AWS Lambda 함수의 이벤트 소스로 구성하여 들어오는 레코드로 자동 호출되도록 합니다.",
            "2": "Kinesis Data Firehose를 사용하여 데이터를 Lambda 함수로 직접 전송하여 처리합니다.",
            "3": "Kinesis Data Stream에서 메시지를 수신하고 Lambda 함수를 트리거하는 Amazon SQS 큐를 구현합니다.",
            "4": "정기적으로 Kinesis Data Stream을 폴링하여 레코드를 처리하는 예약된 AWS Lambda 함수를 생성합니다."
        },
        "Correct Answer": "Kinesis Data Stream을 AWS Lambda 함수의 이벤트 소스로 구성하여 들어오는 레코드로 자동 호출되도록 합니다.",
        "Explanation": "Kinesis Data Stream을 Lambda 함수의 이벤트 소스로 구성하면 들어오는 각 레코드에 대해 함수가 자동으로 호출되어 데이터가 도착하는 즉시 실시간으로 처리할 수 있습니다.",
        "Other Options": [
            "Kinesis Data Firehose를 사용하는 것은 레코드별로 Lambda 함수를 호출하는 데 적합하지 않으며, Firehose는 데이터를 목적지로 전송하기 전에 배치 처리하도록 설계되었습니다.",
            "예약된 Lambda 함수를 생성하면 설정된 간격으로만 스트림을 폴링하므로 처리에 지연이 발생하고 들어오는 데이터의 실시간 처리를 제공하지 못합니다.",
            "Amazon SQS 큐를 구현하면 불필요한 복잡성이 추가되고 지연이 발생합니다. Kinesis의 메시지는 먼저 SQS로 전송된 후 Lambda에 의해 처리되어야 합니다."
        ]
    },
    {
        "Question Number": "32",
        "Situation": "한 스타트업이 다양한 출처에서 데이터를 수집하고 처리하여 Amazon S3에 저장하는 데이터 파이프라인을 개발하고 있습니다. 팀은 파이프라인이 실패에 강하고 데이터 부하 증가에 따라 확장 가능하며 모니터링과 유지 관리가 용이하도록 보장하고자 합니다. 그들은 ETL 워크플로를 조정하기 위해 다양한 AWS 서비스를 고려하고 있습니다.",
        "Question": "데이터 파이프라인을 위한 강력하고 확장 가능한 ETL 워크플로를 구축하기 위한 최선의 옵션은 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "Amazon Managed Workflows for Apache Airflow (Amazon MWAA)를 활용하여 복잡한 데이터 변환 워크플로를 설계합니다.",
            "2": "Amazon EventBridge를 사용하여 순차적인 데이터 처리를 위해 Lambda 함수를 트리거합니다.",
            "3": "AWS Step Functions를 구현하여 ETL 작업을 조정하고 실패를 관리합니다.",
            "4": "AWS Glue를 활용하여 작업 부하에 따라 자동으로 확장되는 ETL 작업을 생성합니다.",
            "5": "Amazon Kinesis Data Firehose를 사용하여 실시간 데이터 수집 및 변환을 수행합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "AWS Step Functions를 구현하여 ETL 작업을 조정하고 실패를 관리합니다.",
            "Amazon Managed Workflows for Apache Airflow (Amazon MWAA)를 활용하여 복잡한 데이터 변환 워크플로를 설계합니다."
        ],
        "Explanation": "AWS Step Functions는 작업의 순서를 정의하고 오류 처리를 관리할 수 있도록 하여 강력한 워크플로를 구축할 수 있는 방법을 제공합니다. Amazon MWAA는 종속성과 일정을 가진 복잡한 워크플로를 설계할 수 있는 유연성을 제공하여 ETL 프로세스를 조정하는 데 유리합니다. 두 서비스 모두 데이터 파이프라인의 확장성과 내결함성을 향상시킵니다.",
        "Other Options": [
            "Amazon Kinesis Data Firehose는 주로 실시간 데이터 수집에 사용되지만, 순차적인 ETL 작업이나 오류 처리를 효과적으로 관리하는 데 필요한 조정 기능을 제공하지 않습니다.",
            "AWS Glue는 확장 가능한 ETL 작업을 생성할 수 있지만, 여러 작업의 조정을 본질적으로 관리하지 않으며 Step Functions나 MWAA와 같은 수준의 오류 처리 및 모니터링을 제공하지 않습니다.",
            "Amazon EventBridge는 이벤트 기반 아키텍처에 사용되며 작업을 트리거할 수 있지만, ETL 프로세스에서 일반적인 복잡한 워크플로를 관리하기 위한 포괄적인 조정 프레임워크를 제공하지 않습니다."
        ]
    },
    {
        "Question Number": "33",
        "Situation": "한 금융 기관이 민감한 데이터에 대한 엄격한 접근 제어가 필요한 새로운 데이터 처리 파이프라인을 구현하고 있습니다. 데이터 엔지니어는 데이터 처리를 위해 필요한 리소스에 접근할 수 있는 권한이 있는 역할만 접근할 수 있도록 AWS Identity and Access Management (IAM)를 구성해야 합니다. 엔지니어는 또한 개발자들이 프로덕션 리소스에 영향을 주지 않고 자신의 구성을 테스트할 수 있는 방법을 제공해야 합니다.",
        "Question": "개발자가 IAM 구성을 테스트할 수 있도록 하면서 데이터 처리 파이프라인을 위한 안전한 환경을 만드는 최선의 접근 방식은 무엇입니까?",
        "Options": {
            "1": "모든 리소스에 대한 전체 접근을 허용하는 IAM 정책을 생성하고 이를 개발자의 IAM 역할에 연결하여 테스트합니다.",
            "2": "AWS Organizations를 사용하여 개발 및 프로덕션을 위한 별도의 계정을 생성하여 리소스가 격리되도록 합니다.",
            "3": "S3 버킷에 대한 리소스 기반 정책을 구현하여 IAM 역할을 수정하지 않고 개발자의 데이터 접근을 제어합니다.",
            "4": "테스트 환경에 대한 권한이 제한된 새로운 IAM 역할을 생성하고 개발자가 이 역할을 맡을 수 있도록 합니다."
        },
        "Correct Answer": "테스트 환경에 대한 권한이 제한된 새로운 IAM 역할을 생성하고 개발자가 이 역할을 맡을 수 있도록 합니다.",
        "Explanation": "테스트 환경을 위해 특별히 새로운 IAM 역할을 생성하면 개발자가 프로덕션 리소스를 노출하지 않고 자신의 구성을 테스트할 수 있습니다. 이는 개발 작업에 필요한 접근을 제공하면서 보안을 유지합니다.",
        "Other Options": [
            "AWS Organizations를 사용하여 별도의 계정을 생성하는 것은 효과적일 수 있지만, 여러 계정 간의 리소스 및 권한 관리를 위한 불필요한 복잡성과 오버헤드를 초래할 수 있습니다.",
            "S3 버킷에 대한 리소스 기반 정책을 구현하는 것은 IAM 역할 관리를 포괄적으로 다루지 않으며, 테스트 과정에서 필요한 다른 서비스나 리소스에 대한 접근을 제한할 수 있습니다.",
            "모든 리소스에 대한 전체 접근을 허용하는 IAM 정책을 부여하는 것은 안전하지 않으며, 개발자가 잠재적으로 민감한 프로덕션 데이터에 무제한으로 접근할 수 있게 하여 상당한 위험을 초래합니다."
        ]
    },
    {
        "Question Number": "34",
        "Situation": "데이터 엔지니어링 팀은 Amazon RDS 데이터베이스에서 데이터를 추출하고 변환한 후 Amazon Redshift 데이터 웨어하우스에 로드하는 일련의 ETL 프로세스를 조정하는 임무를 맡고 있습니다. 팀은 워크플로우를 쉽게 정의하고, 재시도를 처리하며, 작업 간의 종속성을 관리할 수 있는 솔루션이 필요합니다. 또한 다른 AWS 서비스와 잘 통합될 수 있는 서비스를 찾고 있습니다.",
        "Question": "설명된 ETL 프로세스를 조정하는 데 가장 적합한 AWS 서비스는 무엇입니까?",
        "Options": {
            "1": "AWS Step Functions",
            "2": "Amazon EventBridge",
            "3": "Amazon MWAA",
            "4": "AWS Glue"
        },
        "Correct Answer": "AWS Step Functions",
        "Explanation": "AWS Step Functions는 분산 애플리케이션의 구성 요소를 조정하기 위해 설계되었으며 ETL 프로세스를 조정하는 데 이상적입니다. 재시도 및 종속성 관리를 포함한 워크플로우 정의를 허용하며, Amazon RDS 및 Amazon Redshift와 같은 서비스와 원활하게 통합됩니다.",
        "Other Options": [
            "Amazon MWAA는 Apache Airflow 워크플로우 실행에 주로 초점을 맞추고 있으며, ETL 프로세스에 적합하지만 AWS Step Functions에 비해 조정에 대한 통합 및 단순성을 제공하지 않을 수 있습니다.",
            "AWS Glue는 주로 데이터 카탈로그 및 ETL 서비스이지만, 복잡한 워크플로우 및 작업 종속성을 관리하는 조정 기능을 AWS Step Functions만큼 효과적으로 제공하지 않습니다.",
            "Amazon EventBridge는 이벤트 기반 아키텍처 및 서비스 간 이벤트 라우팅에 사용되지만, ETL 프로세스에서 필요한 복잡한 워크플로우 조정이나 작업 종속성 관리를 위해 특별히 설계되지 않았습니다."
        ]
    },
    {
        "Question Number": "35",
        "Situation": "금융 기관이 데이터 저장소를 AWS로 마이그레이션하고 있으며, 민감한 고객 데이터가 무단 접근으로부터 보호되도록 해야 합니다. 그들은 AWS 서비스 전반에 걸쳐 데이터 보안 및 거버넌스를 구현하기 위한 다양한 방법을 고려하고 있습니다.",
        "Question": "어떤 방법이 금융 기관이 AWS 서비스 전반에 걸쳐 민감한 데이터를 무단 접근으로부터 보호하는 데 가장 도움이 될까요?",
        "Options": {
            "1": "Amazon S3 서버 측 암호화를 사용하여 데이터가 정지 상태일 때 PKI를 통해 암호화합니다.",
            "2": "서비스 간의 쉬운 공유를 위해 공개 액세스가 활성화된 Amazon S3 버킷에 민감한 데이터를 저장합니다.",
            "3": "AWS Identity and Access Management (IAM) 역할 및 정책을 활용하여 리소스에 대한 최소 권한 액세스를 시행합니다.",
            "4": "AWS CloudTrail을 활성화하여 모든 데이터 접근 이벤트를 기록하지만 민감한 데이터에 대한 접근을 제한하지 않습니다."
        },
        "Correct Answer": "AWS Identity and Access Management (IAM) 역할 및 정책을 활용하여 리소스에 대한 최소 권한 액세스를 시행합니다.",
        "Explanation": "AWS IAM 역할 및 정책을 사용하여 최소 권한 액세스를 시행하는 것은 민감한 데이터를 보호하는 데 필수적인 방법입니다. 이를 통해 권한이 부여된 사용자와 서비스만 특정 리소스에 접근할 수 있도록 하여 무단 접근의 위험을 최소화합니다.",
        "Other Options": [
            "공개 액세스가 활성화된 Amazon S3 버킷에 민감한 데이터를 저장하면 인터넷에 있는 누구나 데이터에 접근할 수 있으므로 무단 접근의 위험이 크게 증가합니다.",
            "AWS CloudTrail을 활성화하여 데이터 접근 이벤트를 기록하더라도 접근을 제한하지 않으면 데이터 자체를 보호하지 않으며, 접근 로그만 제공할 뿐 데이터 보안에는 충분하지 않습니다.",
            "Amazon S3 서버 측 암호화를 사용하여 PKI를 사용하는 것은 올바른 방법이 아닙니다. S3 암호화는 일반적으로 AWS 관리 키 또는 고객 관리 키를 사용하며, PKI는 S3 암호화의 표준 관행이 아닙니다."
        ]
    },
    {
        "Question Number": "36",
        "Situation": "데이터 엔지니어링 팀은 Amazon Redshift에서 데이터 분포에 상당한 왜곡이 있는 대규모 데이터 세트로 작업하고 있습니다. 그들은 노드 간의 불균형한 작업 부하 분포로 인해 성능 문제를 경험하고 있습니다. 팀은 데이터 왜곡을 줄이고 쿼리 성능을 개선하기 위한 메커니즘을 구현하고자 합니다.",
        "Question": "Amazon Redshift에서 데이터 왜곡을 해결하기 위해 구현할 수 있는 전략은 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "분포 키를 사용하여 데이터를 노드 간에 고르게 분배합니다.",
            "2": "정렬 키를 활용하여 데이터 접근 패턴을 최적화하고 쿼리 성능을 개선합니다.",
            "3": "Redshift 클러스터의 슬라이스 수를 늘려 더 많은 동시 쿼리를 처리합니다.",
            "4": "왜곡된 테이블에 대해 ALL 분포 스타일을 적용하여 모든 노드에 데이터를 복제합니다.",
            "5": "특정 기준에 따라 대규모 테이블을 분할하기 위해 데이터 샤딩을 구현합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "분포 키를 사용하여 데이터를 노드 간에 고르게 분배합니다.",
            "특정 기준에 따라 대규모 테이블을 분할하기 위해 데이터 샤딩을 구현합니다."
        ],
        "Explanation": "분포 키를 사용하면 데이터를 노드 간에 고르게 분산시켜 데이터 왜곡 문제를 완화하는 데 도움이 됩니다. 데이터 샤딩을 구현하면 팀이 특정 기준에 따라 대규모 테이블을 분할할 수 있어 각 노드가 한 번에 처리해야 하는 데이터 양을 줄여 성능을 더욱 개선할 수 있습니다.",
        "Other Options": [
            "슬라이스 수를 늘리면 더 많은 동시 쿼리를 허용할 수 있지만, 데이터 왜곡의 근본적인 문제는 해결되지 않으며, 이는 데이터가 이러한 슬라이스 간에 어떻게 분배되는지와 관련이 있습니다.",
            "ALL 분포 스타일을 적용하면 데이터 중복으로 인해 저장 비용이 증가하고 성능이 저하될 수 있으며, 왜곡 문제를 해결하지 않습니다.",
            "정렬 키를 활용하는 것은 데이터 왜곡을 직접적으로 해결하지 않으며, 정렬된 쿼리의 성능을 개선하지만 데이터가 노드 간에 어떻게 분배되는지는 변경하지 않습니다."
        ]
    },
    {
        "Question Number": "37",
        "Situation": "소매 회사가 다양한 데이터 소스를 통합하여 통합 고객 프로필을 생성하고 있습니다. 데이터 소스에는 Amazon S3에 저장된 고객 거래, API를 통한 소셜 미디어 상호작용, Amazon RDS 데이터베이스의 고객 서비스 로그가 포함됩니다. 팀은 데이터가 정확하게 카탈로그화되고 AWS Glue를 사용하여 분석을 위해 쉽게 소비될 수 있도록 하는 것을 목표로 하고 있습니다.",
        "Question": "다양한 소스에서 데이터를 소비하기 위해 데이터 카탈로그를 가장 효과적으로 활용할 수 있는 접근 방식은 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "AWS Glue 데이터 카탈로그를 설정하여 데이터 소스를 등록하고 데이터 변경에 따라 ETL 작업을 트리거하는 Lambda 함수를 구성합니다.",
            "2": "AWS Glue의 ETL 작업을 사용하여 데이터 카탈로그 없이 S3 및 RDS의 원시 데이터에 직접 접근하고, 분석 전에 데이터를 인라인으로 변환합니다.",
            "3": "AWS Glue 크롤러를 배포하여 데이터 소스를 지속적으로 모니터링하고 카탈로그화하며, Amazon Athena를 활성화하여 카탈로그화된 데이터를 쿼리합니다.",
            "4": "AWS Lake Formation을 구현하여 데이터 카탈로그의 데이터 접근을 관리하고 다양한 소스에서 데이터 수집 프로세스를 간소화합니다.",
            "5": "AWS Glue 데이터 카탈로그를 생성하고 크롤러를 구성하여 S3 및 RDS 데이터 소스를 스캔하고, 스키마가 자동으로 업데이트되도록 합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "AWS Glue 데이터 카탈로그를 생성하고 크롤러를 구성하여 S3 및 RDS 데이터 소스를 스캔하고, 스키마가 자동으로 업데이트되도록 합니다.",
            "AWS Lake Formation을 구현하여 데이터 카탈로그의 데이터 접근을 관리하고 다양한 소스에서 데이터 수집 프로세스를 간소화합니다."
        ],
        "Explanation": "이 두 가지 접근 방식은 AWS Glue의 기능을 활용하여 스키마 변경을 자동으로 추적하고 권한을 관리하는 중앙 집중식 데이터 카탈로그를 생성하여 다양한 소스에서 데이터를 분석을 위해 쉽게 소비할 수 있도록 합니다.",
        "Other Options": [
            "이 옵션은 데이터 카탈로그를 활용하지 않으며, 이는 스키마 관리 및 데이터 발견에 필수적입니다. 카탈로그 없이 직접 접근하면 불일치 및 데이터 통합의 어려움이 발생할 수 있습니다.",
            "이 옵션은 AWS Glue 데이터 카탈로그에 의존하지 않으며, 여러 소스에서 데이터를 통합하는 데 중요한 자동화된 스키마 업데이트 및 데이터 발견의 이점을 놓치고 있습니다.",
            "이 옵션은 데이터 카탈로그를 포함하지만 데이터 소스와의 직접 연결이 부족합니다. 데이터 변경에 따라 ETL 작업을 트리거하는 것은 크롤러를 사용하여 카탈로그를 업데이트하는 것보다 더 복잡합니다."
        ]
    },
    {
        "Question Number": "38",
        "Situation": "회사는 컴플라이언스 및 보안 목적을 위해 AWS 리소스에 대한 모든 API 호출을 모니터링하고 감사할 수 있도록 보장하고자 합니다. 회사는 API 호출에 대한 자세한 정보를 기록하는 솔루션을 설정해야 하며, 여기에는 호출한 사람과 호출이 발생한 시간이 포함됩니다. 이를 위해 AWS CloudTrail을 사용하는 것을 고려하고 있습니다.",
        "Question": "회사가 감사 및 컴플라이언스를 위해 AWS 리소스에 대한 API 호출을 추적하기 위해 활성화해야 하는 AWS 서비스는 무엇입니까?",
        "Options": {
            "1": "AWS Config를 구현하여 리소스 구성의 변경 사항을 추적합니다.",
            "2": "AWS Security Hub를 설정하여 보안 발견 사항을 집계합니다.",
            "3": "AWS CloudTrail을 활성화하여 모든 리전에서 API 호출을 기록합니다.",
            "4": "Amazon CloudWatch를 사용하여 API 호출 메트릭을 실시간으로 모니터링합니다."
        },
        "Correct Answer": "AWS CloudTrail을 활성화하여 모든 리전에서 API 호출을 기록합니다.",
        "Explanation": "AWS CloudTrail은 AWS 리소스에 대한 API 호출을 기록하고 모니터링하도록 특별히 설계되었습니다. 호출한 사람, 호출이 발생한 시간 및 위치에 대한 자세한 정보를 제공하여 감사 및 컴플라이언스에 이상적인 솔루션입니다.",
        "Other Options": [
            "Amazon CloudWatch는 메트릭을 모니터링하고 기록하는 데 주로 사용되며, 자세한 API 호출 기록을 추적하는 데는 적합하지 않으므로 포괄적인 감사 요구 사항을 충족하지 않습니다.",
            "AWS Config는 AWS 리소스의 구성 변경 사항을 모니터링하는 데 중점을 두지만 API 호출을 기록하지 않으므로 API 활동 추적에 적합하지 않습니다.",
            "AWS Security Hub는 다양한 AWS 서비스의 보안 발견 사항을 집계하지만 API 호출을 특별히 추적하지 않으므로 API 활동 감사 목적에 부적합합니다."
        ]
    },
    {
        "Question Number": "39",
        "Situation": "회사는 데이터 웨어하우스를 AWS로 마이그레이션하고 있으며, 데이터의 특성(스키마 및 데이터 유형 등)이 향후 변경 사항을 수용할 수 있도록 일관되게 관리되어야 합니다.",
        "Question": "회사가 데이터 웨어하우스 내에서 데이터 특성의 변경을 효과적으로 관리하기 위해 사용해야 하는 AWS 서비스는 무엇입니까?",
        "Options": {
            "1": "Amazon RDS를 사용하여 자동 스키마 마이그레이션이 가능한 관계형 데이터베이스를 생성합니다.",
            "2": "AWS Glue 데이터 카탈로그를 사용하여 Amazon S3에 저장된 데이터의 메타데이터 중앙 저장소를 유지합니다.",
            "3": "AWS Lake Formation을 사용하여 세분화된 접근 제어가 있는 안전한 데이터 레이크를 설정하고 관리합니다.",
            "4": "Amazon Redshift Spectrum을 사용하여 데이터를 데이터 웨어하우스에 로드할 필요 없이 S3에서 직접 쿼리합니다."
        },
        "Correct Answer": "AWS Glue 데이터 카탈로그를 사용하여 Amazon S3에 저장된 데이터의 메타데이터 중앙 저장소를 유지합니다.",
        "Explanation": "AWS Glue 데이터 카탈로그는 데이터 세트의 메타데이터를 관리하고 규제하도록 특별히 설계되어 있으며, 스키마 변경 및 데이터 유형을 효율적으로 추적할 수 있어 데이터 웨어하우스의 데이터 특성 변화에 대한 적응력이 필수적입니다.",
        "Other Options": [
            "Amazon Redshift Spectrum은 S3의 데이터를 쿼리할 수 있지만 스키마 및 데이터 유형 변경 관리를 위한 포괄적인 솔루션을 제공하지 않습니다.",
            "AWS Lake Formation은 데이터 레이크 관리 및 접근 제어에 중점을 두지만 스키마 진화를 위한 메타데이터 관리를 특별히 다루지 않습니다.",
            "Amazon RDS는 관계형 데이터베이스 서비스이지만 데이터 웨어하우스 환경에서 메타데이터 또는 스키마 변경 관리를 위해 설계되지 않았습니다."
        ]
    },
    {
        "Question Number": "40",
        "Situation": "소매 회사가 Amazon S3에 저장된 판매 데이터를 시각화하기 위해 인터랙티브 대시보드를 만들고자 합니다. 비기술 사용자가 코드를 작성하지 않고도 맞춤형 시각화를 생성할 수 있는 도구가 필요합니다. 이 도구는 데이터 분석을 위한 준비 및 정리 기능도 지원해야 합니다.",
        "Question": "회사가 판매 데이터를 효과적으로 시각화하고 준비하기 위해 어떤 AWS 서비스를 사용해야 합니까?",
        "Options": {
            "1": "Amazon Athena",
            "2": "AWS Glue DataBrew",
            "3": "Amazon QuickSight",
            "4": "AWS Data Pipeline"
        },
        "Correct Answer": "Amazon QuickSight",
        "Explanation": "Amazon QuickSight는 사용자가 Amazon S3와 같은 데이터 소스에서 인터랙티브 대시보드와 시각화를 생성할 수 있도록 하는 비즈니스 인텔리전스 서비스입니다. 비기술 사용자에게 친숙하며 데이터 준비 기능도 지원하여 회사의 요구에 적합합니다.",
        "Other Options": [
            "AWS Glue DataBrew는 주로 데이터 준비 및 정리에 중점을 두고 있지만 대시보드와 같은 직접적인 시각화 기능을 제공하지 않습니다. 이는 시각화 도구라기보다는 데이터 조작 도구입니다.",
            "AWS Data Pipeline은 다양한 AWS 서비스 간의 데이터 처리 및 이동을 위한 서비스입니다. 시각화 기능을 제공하지 않으며, 데이터 워크플로우 관리에 더 적합합니다.",
            "Amazon Athena는 사용자가 SQL을 사용하여 Amazon S3의 데이터를 분석할 수 있도록 하는 인터랙티브 쿼리 서비스입니다. 데이터 쿼리에 유용하지만 인터랙티브 대시보드나 시각화를 생성하기 위한 내장 도구를 제공하지 않습니다."
        ]
    },
    {
        "Question Number": "41",
        "Situation": "금융 서비스 회사가 데이터 처리 파이프라인을 AWS로 마이그레이션하고 있습니다. 현재는 매일 대량의 거래 데이터를 처리하는 배치 처리 시스템을 사용하고 있습니다. 회사는 데이터 통찰력의 속도를 개선하기 위해 보다 실시간 데이터 처리 시스템으로 전환하고자 합니다. 모바일 앱 및 결제 게이트웨이와 같은 여러 소스에서 스트리밍 데이터를 수집하고 처리하기 위해 다양한 AWS 서비스를 고려하고 있습니다.",
        "Question": "이 시나리오에서 실시간 데이터 수집 및 처리를 처리하기에 가장 적합한 AWS 서비스는 무엇입니까?",
        "Options": {
            "1": "AWS Glue",
            "2": "AWS Lambda",
            "3": "Amazon Kinesis Data Streams",
            "4": "Amazon EMR"
        },
        "Correct Answer": "Amazon Kinesis Data Streams",
        "Explanation": "Amazon Kinesis Data Streams는 실시간 데이터 수집 및 처리를 위해 특별히 설계되어 다양한 소스에서 스트리밍 데이터를 지속적으로 수집하고 처리할 수 있도록 합니다. 낮은 대기 시간 처리 기능을 제공하여 시기적절한 통찰력이 필요한 시나리오에 이상적입니다.",
        "Other Options": [
            "AWS Lambda는 데이터를 처리하는 데 사용할 수 있는 서버리스 컴퓨팅 서비스이지만 주로 데이터 수집 도구는 아닙니다. 실시간 데이터 스트림을 관리하기보다는 이벤트에 응답하여 코드를 실행하는 데 더 적합합니다.",
            "Amazon EMR은 주로 배치 모드에서 빅 데이터 처리 및 분석에 사용됩니다. 스트리밍 데이터를 처리할 수 있지만 실시간 수집에 최적화되어 있지 않으며 Kinesis에 비해 더 많은 지연을 초래할 수 있습니다.",
            "AWS Glue는 배치 처리 및 데이터 카탈로깅을 위해 설계된 ETL 서비스입니다. 실시간 스트리밍 데이터 수집을 위해 구축되지 않았으므로 데이터 스트림의 즉각적인 처리가 필요한 시나리오에는 덜 적합합니다."
        ]
    },
    {
        "Question Number": "42",
        "Situation": "금융 서비스 회사가 처리 노드 간 데이터 분포의 불균형으로 인해 데이터 처리 작업에서 성능 문제를 겪고 있습니다. 데이터 스큐를 처리하고 작업 부하가 균형 잡히고 효율적이도록 하기 위한 메커니즘을 구현해야 합니다.",
        "Question": "회사가 데이터 스큐 메커니즘을 효과적으로 구현하기 위해 사용할 수 있는 전략은 무엇입니까?",
        "Options": {
            "1": "해시 파티셔닝을 사용하여 데이터를 노드 간에 고르게 분배합니다.",
            "2": "큐 기반 아키텍처를 활용하여 데이터를 단계적으로 처리합니다.",
            "3": "데이터 복제를 구현하여 스큐된 데이터의 여러 복사본을 생성합니다.",
            "4": "처리 노드의 크기를 늘려 더 큰 데이터 볼륨을 처리합니다."
        },
        "Correct Answer": "해시 파티셔닝을 사용하여 데이터를 노드 간에 고르게 분배합니다.",
        "Explanation": "해시 파티셔닝은 데이터 스큐를 해결하기 위한 효과적인 전략으로, 처리 노드 간에 데이터를 고르게 분배할 수 있습니다. 이는 작업 부하를 균형 있게 유지하고 불균형한 데이터 분포로 인한 병목 현상을 줄여 전체 성능을 향상시킵니다.",
        "Other Options": [
            "처리 노드의 크기를 늘리는 것은 데이터 스큐의 근본적인 문제를 해결하지 않으며, 성능 문제를 일시적으로 완화할 수 있지만 근본 원인을 해결하지 않고 비효율성과 더 높은 비용을 초래할 수 있습니다.",
            "데이터 복제를 구현하면 데이터의 추가 복사본을 생성할 수 있지만 스큐 문제를 해결하지 않으며, 저장 비용과 복잡성이 증가할 수 있고 처리 효율성을 개선하지 않습니다.",
            "큐 기반 아키텍처를 활용하면 데이터를 순차적으로 처리하게 되어 처리 시간이 길어질 수 있으며, 작업 부하를 노드 간에 분배하지 않기 때문에 데이터 스큐 문제를 효과적으로 해결하지 못할 수 있습니다."
        ]
    },
    {
        "Question Number": "43",
        "Situation": "한 금융 서비스 회사가 다양한 AWS 서비스에 저장된 방대한 데이터 세트를 관리하고 목록화하려고 합니다. 이 회사는 데이터가 쉽게 검색되고 관리될 수 있도록 하며, 데이터 분석가가 데이터 세트를 신속하게 찾고 접근할 수 있도록 하기를 원합니다.",
        "Question": "다른 AWS 서비스와 통합되는 중앙 집중식 데이터 카탈로그를 생성하는 데 가장 적합한 AWS 서비스는 무엇입니까?",
        "Options": {
            "1": "Amazon S3",
            "2": "Amazon Redshift",
            "3": "AWS Glue Data Catalog",
            "4": "Amazon RDS"
        },
        "Correct Answer": "AWS Glue Data Catalog",
        "Explanation": "AWS Glue Data Catalog는 중앙 집중식 메타데이터 저장소를 생성하기 위해 특별히 설계되었으며, 다양한 AWS 데이터 서비스와 긴밀하게 통합되어 있습니다. 이를 통해 사용자는 AWS 전역에서 데이터를 발견하고 관리할 수 있어, 회사의 요구 사항에 이상적인 선택입니다.",
        "Other Options": [
            "Amazon S3는 주로 저장 서비스이며, 내장된 메타데이터 관리 또는 목록화 기능을 제공하지 않습니다.",
            "Amazon RDS는 관계형 데이터베이스 서비스로, 데이터 카탈로그 역할을 하지 않으며 메타데이터 저장보다는 데이터베이스 관리에 중점을 둡니다.",
            "Amazon Redshift는 분석 기능을 제공하는 데이터 웨어하우스 서비스지만, AWS Glue에 있는 전용 데이터 카탈로그 기능이 부족합니다."
        ]
    },
    {
        "Question Number": "44",
        "Situation": "한 금융 기관이 애플리케이션을 AWS로 마이그레이션하고 있으며, 사용자가 직무 기능을 수행하는 데 필요한 최소한의 권한만 가지도록 보장하고자 합니다. 보안 팀은 이 마이그레이션 동안 민감한 데이터와 자원을 보호하기 위해 최소 권한 원칙을 구현할 필요성을 강조합니다.",
        "Question": "최소 권한 원칙을 준수하면서 사용자 권한을 효과적으로 관리하는 데 사용할 수 있는 AWS 서비스는 무엇입니까?",
        "Options": {
            "1": "Amazon Cognito",
            "2": "AWS Key Management Service (KMS)",
            "3": "AWS Organizations",
            "4": "AWS Identity and Access Management (IAM)"
        },
        "Correct Answer": "AWS Identity and Access Management (IAM)",
        "Explanation": "AWS Identity and Access Management (IAM)는 AWS 사용자 및 그룹을 생성하고 관리하며, AWS 자원에 대한 접근을 허용하거나 거부하는 권한을 할당할 수 있게 해줍니다. 이 서비스는 사용자가 역할에 필요한 권한만 부여하여 최소 권한 원칙을 적용하는 데 필수적입니다.",
        "Other Options": [
            "Amazon Cognito는 주로 웹 및 모바일 애플리케이션의 사용자 인증 및 접근 제어에 사용되지만, AWS 자원에 대한 포괄적인 권한 관리를 제공하지 않습니다.",
            "AWS Key Management Service (KMS)는 애플리케이션의 암호화 키 관리를 중심으로 하며, AWS 자원에 대한 사용자 권한이나 접근 제어를 직접 처리하지 않습니다.",
            "AWS Organizations는 여러 AWS 계정을 관리하는 데 도움을 주고 계정 간 정책 관리를 지원하지만, AWS 자원에 대한 사용자 또는 그룹 수준에서 권한을 직접 할당하지 않습니다."
        ]
    },
    {
        "Question Number": "45",
        "Situation": "한 조직이 Amazon Redshift에서 구조가 자주 변경되는 대규모 데이터 세트를 관리하고 있으며, 새로운 열의 추가 및 기존 열의 수정이 포함됩니다. 데이터 엔지니어링 팀은 상당한 다운타임이나 데이터 손실 없이 이러한 스키마 변경을 처리할 수 있는 전략을 구현해야 합니다.",
        "Question": "데이터 엔지니어가 데이터 세트의 변경을 효과적으로 관리하기 위해 구현해야 할 스키마 진화 기법은 무엇입니까?",
        "Options": {
            "1": "스키마 변경이 발생할 때마다 데이터 세트를 완전히 다시 로드합니다.",
            "2": "필요에 따라 새로운 열을 추가하고 기존 열을 수정하기 위해 ALTER TABLE 명령을 구현합니다.",
            "3": "여러 스키마 버전을 유지하고 데이터를 점진적으로 마이그레이션하기 위해 버전 관리 접근 방식을 사용합니다.",
            "4": "데이터 세트가 생성된 후 변경을 허용하지 않는 고정 스키마를 활용합니다."
        },
        "Correct Answer": "필요에 따라 새로운 열을 추가하고 기존 열을 수정하기 위해 ALTER TABLE 명령을 구현합니다.",
        "Explanation": "ALTER TABLE 명령을 사용하면 전체 데이터 세트를 다시 로드할 필요 없이 동적 스키마 변경이 가능합니다. 이 방법은 효율적이며 다운타임을 최소화하여 스키마 진화를 원활하게 수용합니다.",
        "Other Options": [
            "버전 관리 접근 방식을 사용하는 것은 복잡성을 추가할 수 있으며, 여러 스키마 버전을 관리하는 데 상당한 오버헤드가 필요할 수 있어 자주 변경되는 경우에는 이상적이지 않습니다.",
            "데이터 세트를 완전히 다시 로드하는 것은 비효율적이며, 특히 대규모 데이터 세트의 경우 상당한 다운타임을 초래할 수 있어 고가용성이 요구되는 환경에는 적합하지 않습니다.",
            "고정 스키마를 활용하는 것은 유연성과 적응성을 제한하여 데이터 요구 사항이 지속적으로 진화하는 시나리오에서는 비현실적입니다."
        ]
    },
    {
        "Question Number": "46",
        "Situation": "한 금융 서비스 회사가 온프레미스 파일 저장소를 클라우드로 마이그레이션하고 있습니다. 그들은 기존 데이터 처리 워크플로우와 원활하게 통합되면서 안전하고 확장 가능한 파일 전송 기능을 제공하는 솔루션이 필요합니다.",
        "Question": "안전하게 파일을 전송하고 데이터 처리 시스템과 확장 가능하게 통합하는 데 가장 적합한 AWS 서비스는 무엇입니까?",
        "Options": {
            "1": "AWS Snowball을 사용하여 대량의 데이터를 Amazon S3로 전송합니다.",
            "2": "AWS Transfer Family를 사용하여 SFTP, FTPS 및 FTP 파일 전송을 수행합니다.",
            "3": "AWS DataSync를 사용하여 자동화된 데이터 전송 및 동기화를 수행합니다.",
            "4": "Amazon S3 Batch Operations를 사용하여 대량 파일 관리 작업을 수행합니다."
        },
        "Correct Answer": "AWS Transfer Family를 사용하여 SFTP, FTPS 및 FTP 파일 전송을 수행합니다.",
        "Explanation": "AWS Transfer Family는 SFTP, FTPS 및 FTP 프로토콜을 지원하는 완전 관리형 서비스로, Amazon S3 및 기타 AWS 서비스와 쉽게 통합되는 안전하고 확장 가능한 파일 전송을 가능하게 합니다. 이는 온프레미스 시스템에서 클라우드로 전환하는 조직에 이상적입니다.",
        "Other Options": [
            "AWS DataSync는 주로 온프레미스 저장소와 AWS 간의 자동화된 데이터 전송 및 동기화를 위해 설계되었지만, SFTP 또는 FTPS와 같은 직접 파일 전송 프로토콜을 지원하지 않아 이 특정 요구 사항에 덜 적합합니다.",
            "Amazon S3 Batch Operations는 대량의 S3 객체를 관리하는 데 사용되지만, 직접 파일 전송을 촉진하지 않으므로 이 시나리오에서 중요한 필요를 충족하지 못합니다.",
            "AWS Snowball은 대량의 데이터 세트를 물리적 형식으로 AWS로 전송하기 위한 것이며, 실시간으로 안전하고 확장 가능한 파일 전송에 필요하지 않아 이 상황과는 덜 관련이 있습니다."
        ]
    },
    {
        "Question Number": "47",
        "Situation": "한 금융 서비스 회사가 거래 데이터에 대한 실시간 분석이 필요한 새로운 애플리케이션을 개발하고 있습니다. 그들은 데이터에 대한 낮은 대기 시간 접근을 유지하면서 높은 읽기 및 쓰기 처리량을 기대하고 있습니다. 데이터는 복잡한 쿼리 기능도 지원해야 합니다.",
        "Question": "이 시나리오에서 실시간 분석 및 복잡한 쿼리 요구 사항을 가장 잘 충족하는 데이터 저장 솔루션은 무엇입니까?",
        "Options": {
            "1": "Amazon DynamoDB를 구현하여 낮은 대기 시간 접근 및 확장성을 제공합니다.",
            "2": "Amazon Redshift의 데이터 웨어하우징 기능과 배치 처리를 활용합니다.",
            "3": "Amazon EMR을 사용하여 Apache Spark를 사용하여 대규모 데이터 세트를 실시간으로 처리합니다.",
            "4": "Amazon RDS와 읽기 복제본을 사용하여 높은 가용성과 복잡한 쿼리 기능을 달성합니다."
        },
        "Correct Answer": "Amazon DynamoDB를 구현하여 낮은 대기 시간 접근 및 확장성을 제공합니다.",
        "Explanation": "Amazon DynamoDB는 빠르고 예측 가능한 성능과 원활한 확장성을 제공하는 완전 관리형 NoSQL 데이터베이스입니다. 이는 낮은 대기 시간 데이터 접근이 필요하고 높은 읽기 및 쓰기 처리량을 처리할 수 있는 애플리케이션에 이상적입니다. 프로비저닝된 처리량 기능을 통해 필요한 읽기 및 쓰기 용량을 지정할 수 있어 거래 데이터에 대한 실시간 분석에 적합합니다.",
        "Other Options": [
            "Amazon RDS는 일반적으로 전통적인 데이터베이스 작업에 사용되는 관계형 데이터베이스 서비스입니다. 복잡한 쿼리를 지원할 수 있지만, 높은 속도의 거래 데이터에 대한 실시간 분석에 필요한 낮은 대기 시간 접근 및 확장성을 제공하지 않을 수 있습니다.",
            "Amazon Redshift는 데이터 웨어하우징을 위해 설계되었으며 대규모 데이터 세트에 대한 복잡한 쿼리에 최적화되어 있습니다. 그러나 배치 지향적 특성으로 인해 실시간 분석에는 적합하지 않으며, 쿼리하기 전에 데이터를 로드해야 합니다.",
            "Amazon EMR은 Apache Spark와 같은 프레임워크를 사용하여 대규모 데이터 처리를 수행할 수 있는 클라우드 빅 데이터 플랫폼입니다. 실시간으로 데이터를 처리할 수 있지만, 배치 처리에 더 적합하며 즉각적인 접근이 필요한 애플리케이션에 대해 수용할 수 없는 대기 시간을 초래할 수 있습니다."
        ]
    },
    {
        "Question Number": "48",
        "Situation": "한 회사가 Amazon S3에 저장된 데이터에 대한 실시간 접근이 필요한 새로운 애플리케이션을 개발하고 있습니다. 이 애플리케이션은 다른 시스템이 데이터를 효율적으로 검색할 수 있도록 RESTful API를 노출해야 합니다. 회사는 낮은 대기 시간과 확장성을 보장하면서 이 솔루션을 구현하기 위해 AWS 서비스를 사용하고자 합니다.",
        "Question": "Amazon S3의 데이터에 접근하기 위한 RESTful API 생성을 가장 잘 촉진하는 AWS 서비스 조합은 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "AWS Lambda와 Amazon API Gateway",
            "2": "Amazon EC2와 커스텀 웹 서버",
            "3": "AWS AppSync와 Amazon DynamoDB",
            "4": "AWS Glue와 Amazon Kinesis Data Streams",
            "5": "Amazon API Gateway와 Amazon S3"
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "AWS Lambda와 Amazon API Gateway",
            "Amazon API Gateway와 Amazon S3"
        ],
        "Explanation": "AWS Lambda와 Amazon API Gateway를 사용하면 Amazon S3에 저장된 데이터에 접근하기 위해 Lambda 함수를 트리거할 수 있는 서버리스 RESTful API를 생성할 수 있습니다. 이 조합은 효율적이고 확장 가능하며, 트래픽에 자동으로 조정되고 낮은 대기 시간을 제공합니다. 또한, Amazon API Gateway를 Amazon S3와 직접 사용하면 S3에서 정적 콘텐츠를 직접 제공할 수 있어 간단한 데이터 검색 사용 사례에 적합합니다.",
        "Other Options": [
            "Amazon EC2와 커스텀 웹 서버는 인스턴스와 웹 서버를 실행하고 유지 관리해야 하므로 더 많은 관리 및 확장 고려 사항이 필요하며, 서버리스 옵션에 비해 덜 효율적입니다.",
            "AWS AppSync와 Amazon DynamoDB는 실시간 데이터 쿼리 및 동기화에 더 적합하지만, Amazon S3에서 데이터를 접근하는 요구 사항을 직접적으로 해결하지 않습니다.",
            "AWS Glue와 Amazon Kinesis Data Streams는 주로 데이터 변환 및 실시간 분석에 사용되며, S3의 데이터에 접근하기 위한 RESTful API 생성을 위한 필요와 일치하지 않습니다."
        ]
    },
    {
        "Question Number": "49",
        "Situation": "한 회사가 대규모 데이터 세트에 대한 실시간 데이터 처리 및 분석이 필요한 데이터 분석 솔루션을 구축하고 있습니다. 데이터는 IoT 장치 및 거래 시스템을 포함한 다양한 출처에서 생성됩니다. 이 회사는 고속 데이터 처리를 처리하면서 즉석 분석을 위한 효율적인 쿼리 기능을 제공할 수 있는 저장 서비스가 필요합니다. 이 요구 사항에 가장 적합한 AWS 서비스 조합은 무엇입니까?",
        "Question": "대규모 데이터 세트에 대한 실시간 데이터 처리 및 분석 요구를 가장 잘 충족하는 AWS 서비스 조합은 무엇입니까?",
        "Options": {
            "1": "관계형 데이터 저장을 위해 Amazon RDS를 사용하고 대규모 데이터 세트의 배치 처리를 위해 Amazon EMR을 사용합니다.",
            "2": "실시간 데이터 저장을 위해 Amazon DynamoDB를 구현하고 ETL 처리를 위해 AWS Glue를 사용합니다.",
            "3": "데이터 웨어하우징을 위해 Amazon Redshift를 활용하고 실시간 데이터 수집을 위해 Amazon Kinesis를 사용합니다.",
            "4": "데이터 저장을 위해 Amazon S3를 사용하고 데이터를 쿼리하기 위해 Amazon Athena를 사용합니다."
        },
        "Correct Answer": "데이터 웨어하우징을 위해 Amazon Redshift를 활용하고 실시간 데이터 수집을 위해 Amazon Kinesis를 사용합니다.",
        "Explanation": "이 조합은 Amazon Kinesis를 사용한 실시간 데이터 수집을 위한 강력한 솔루션을 제공하며, 스트리밍 데이터를 즉시 처리할 수 있습니다. 그런 다음 Amazon Redshift를 사용하여 대규모 데이터 세트에 대한 복잡한 쿼리 기능을 활용하여 데이터 분석 요구에 이상적인 선택이 됩니다.",
        "Other Options": [
            "Amazon S3와 Amazon Athena는 S3에 저장된 데이터를 쿼리하는 데 훌륭하지만, 이 시나리오에서 주요 요구 사항인 실시간 데이터 처리에 최적화되어 있지 않습니다.",
            "Amazon RDS는 거래 작업에 맞춰 설계되었으며, 실시간 분석에 필요한 고속 데이터를 효율적으로 처리하지 못할 수 있습니다. 또한, Amazon EMR을 사용한 배치 처리는 실시간 처리 요구를 충족하지 않습니다.",
            "Amazon DynamoDB를 사용하는 것은 고속 데이터 저장에 적합하지만, 필요한 즉석 분석을 위해 필수적인 Amazon Redshift가 제공하는 포괄적인 쿼리 기능과 분석 기능이 부족합니다."
        ]
    },
    {
        "Question Number": "50",
        "Situation": "한 회사가 Amazon Kinesis 및 AWS Database Migration Service (AWS DMS)를 포함한 여러 출처에서 실시간 스트리밍 데이터를 수집하고 처리해야 합니다. 데이터 엔지니어는 이 스트리밍 데이터를 효율적으로 캡처하고 Amazon S3에 저장하여 추가 분석을 위한 형식으로 변환하는 솔루션을 설계하는 임무를 맡고 있습니다.",
        "Question": "Kinesis와 AWS DMS에서 스트리밍 데이터의 지속적인 수집 및 변환을 가장 잘 지원하는 솔루션은 무엇입니까?",
        "Options": {
            "1": "Amazon Kinesis Data Firehose를 사용하여 Kinesis와 AWS DMS에서 데이터를 변환 없이 직접 Amazon S3에 기록합니다.",
            "2": "Amazon MSK 클러스터를 구현하여 Kinesis와 AWS DMS에서 데이터를 수집한 후 AWS Glue를 사용하여 처리하고 Amazon S3에 저장합니다.",
            "3": "AWS Lambda 함수를 사용하여 Kinesis와 AWS DMS에서 데이터를 처리하고 필요에 따라 변환하여 배치로 Amazon S3에 전송합니다.",
            "4": "AWS Glue Streaming ETL을 활용하여 Kinesis Data Streams와 AWS DMS에서 데이터를 읽고, 실시간으로 데이터를 변환한 후 Amazon S3에 기록합니다."
        },
        "Correct Answer": "AWS Glue Streaming ETL을 활용하여 Kinesis Data Streams와 AWS DMS에서 데이터를 읽고, 실시간으로 데이터를 변환한 후 Amazon S3에 기록합니다.",
        "Explanation": "AWS Glue Streaming ETL은 실시간 스트리밍 데이터를 처리하도록 특별히 설계되었으며, Kinesis 및 AWS DMS와 같은 출처에서 직접 데이터를 처리할 수 있습니다. 이는 데이터를 실시간으로 변환할 수 있게 하며, 추가 분석을 위해 Amazon S3에 출력을 기록하는 데 적합합니다.",
        "Other Options": [
            "Amazon Kinesis Data Firehose는 데이터를 Amazon S3에 기록하기 전에 변환 기능을 제공하지 않으므로, 데이터를 변환하는 요구 사항을 충족하지 않습니다.",
            "AWS Lambda는 Kinesis와 AWS DMS에서 데이터를 처리할 수 있지만, 배치 처리에 사용하면 지연이 발생할 수 있어 지속적인 실시간 수집 및 변환에 덜 적합합니다.",
            "Amazon MSK는 데이터를 수집할 수 있지만, AWS DMS에서 직접 데이터를 수집하지는 않습니다. 또한, MSK를 사용하면 데이터를 Amazon S3에 저장하기 전에 변환을 수행할 다른 서비스나 프로세스가 필요합니다."
        ]
    },
    {
        "Question Number": "51",
        "Situation": "데이터 엔지니어링 팀은 머신 러닝 모델에 사용되는 대규모 데이터 세트의 정확성과 청결성을 보장하는 임무를 맡고 있습니다. 그들은 기존 AWS 서비스와 쉽게 통합되고 데이터 프로파일링 및 정리 기능을 허용하는 솔루션을 활용하고자 합니다.",
        "Question": "머신 러닝 모델에 사용되기 전에 데이터를 프로파일링, 정리 및 변환할 수 있는 사용자 친화적인 인터페이스를 제공하는 AWS 서비스는 무엇입니까?",
        "Options": {
            "1": "AWS Glue DataBrew",
            "2": "AWS Lambda",
            "3": "Amazon QuickSight",
            "4": "Amazon SageMaker Data Wrangler"
        },
        "Correct Answer": "AWS Glue DataBrew",
        "Explanation": "AWS Glue DataBrew는 데이터 준비를 위해 특별히 설계되어 사용자가 프로그래밍 지식 없이도 시각적 인터페이스를 통해 데이터를 프로파일링, 정리 및 변환할 수 있게 하여 머신 러닝 모델의 데이터 품질 향상에 집중하는 데이터 엔지니어에게 이상적입니다.",
        "Other Options": [
            "Amazon SageMaker Data Wrangler는 머신 러닝을 위한 데이터 준비에 주로 초점을 맞추고 있지만, AWS Glue DataBrew와 같은 수준의 데이터 프로파일링 및 정리 기능이 부족합니다.",
            "AWS Lambda는 이벤트에 응답하여 코드를 실행하는 서버리스 컴퓨팅 서비스이지만, 데이터 프로파일링 및 정리에 필요한 도구를 제공하지 않습니다.",
            "Amazon QuickSight는 데이터 시각화 및 분석을 허용하는 비즈니스 인텔리전스 서비스이지만, 데이터 정리 및 변환 프로세스에 중점을 두지 않습니다."
        ]
    },
    {
        "Question Number": "52",
        "Situation": "데이터 엔지니어는 Amazon RDS 데이터베이스와 Amazon EC2에서 실행되는 애플리케이션 간에 전송되는 민감한 데이터가 전송 중에 암호화되도록 하는 임무를 맡았습니다. 데이터 엔지니어는 아키텍처의 복잡성을 크게 증가시키지 않으면서 데이터 보안에 대한 업계 모범 사례를 충족하는 솔루션을 구현해야 합니다.",
        "Question": "이 시나리오에서 데이터 엔지니어가 전송 중 암호화를 활성화하기 위해 어떤 접근 방식을 취해야 합니까?",
        "Options": {
            "1": "AWS VPN을 사용하여 Amazon RDS와 EC2의 애플리케이션 간에 보안 터널을 생성합니다.",
            "2": "VPC Peering을 구성하여 Amazon RDS와 EC2 인스턴스 간의 보안 연결을 보장합니다.",
            "3": "EC2에서 실행되는 애플리케이션에서 Amazon RDS 데이터베이스에 대한 연결에 SSL/TLS를 활성화합니다.",
            "4": "AWS Direct Connect를 구현하여 서비스 간의 전용 연결을 설정합니다."
        },
        "Correct Answer": "EC2에서 실행되는 애플리케이션에서 Amazon RDS 데이터베이스에 대한 연결에 SSL/TLS를 활성화합니다.",
        "Explanation": "연결에 SSL/TLS를 활성화하면 Amazon RDS 데이터베이스와 EC2의 애플리케이션 간에 전송 중인 데이터가 암호화되어 데이터 보안과 무결성을 보장하면서 아키텍처를 간단하고 효과적으로 유지할 수 있습니다.",
        "Other Options": [
            "AWS VPN을 사용하면 보안 터널을 생성하지만, 이 사용 사례에 대해 불필요한 복잡성을 추가하며, SSL/TLS 암호화로 효과적으로 처리할 수 있습니다.",
            "AWS Direct Connect는 주로 AWS에 전용 네트워크 연결을 설정하는 데 사용되며, EC2와 RDS 간의 전송 중 데이터 보안을 위해 필요하지 않습니다. SSL/TLS로 충분합니다.",
            "VPC Peering은 VPC 간의 통신을 허용하지만, 전송 중 데이터에 대한 암호화를 본질적으로 제공하지 않으므로 연결 암호화 요구 사항에 대한 충분한 솔루션이 아닙니다."
        ]
    },
    {
        "Question Number": "53",
        "Situation": "소매 회사는 Amazon Kinesis Data Analytics를 사용하여 여러 위치에서 실시간 판매 데이터를 처리하고 있습니다. 회사는 고객 구매 행동 및 재고 수준에 대한 통찰력을 얻고자 합니다. 그들은 낮은 대기 시간을 보장하고 다양한 입력 데이터 형식을 지원하며 데이터 양이 변동할 때 자동으로 확장할 수 있는 솔루션을 설계해야 합니다.",
        "Question": "회사가 목표를 달성하는 데 도움이 되는 Amazon Kinesis Data Analytics의 두 가지 기능은 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "Kinesis Data Streams와 통합하기 위한 표준 ANSI SQL 지원.",
            "2": "기록의 정확한 한 번 처리를 보장하기 위한 인메모리 상태 관리.",
            "3": "대규모 데이터 세트를 효율적으로 처리하기 위한 배치 처리 작업 지원.",
            "4": "예측 분석을 위한 내장 기계 학습 기능.",
            "5": "기본 인프라를 자동으로 관리하는 서버리스 아키텍처."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "기본 인프라를 자동으로 관리하는 서버리스 아키텍처.",
            "Kinesis Data Streams와 통합하기 위한 표준 ANSI SQL 지원."
        ],
        "Explanation": "Amazon Kinesis Data Analytics의 서버리스 아키텍처는 회사가 인프라 관리에 대한 걱정 없이 데이터 분석에 집중할 수 있게 해주며, 표준 ANSI SQL 지원은 Kinesis Data Streams와의 쉬운 통합을 가능하게 하여 스트리밍 데이터의 실시간 처리를 지원합니다.",
        "Other Options": [
            "배치 처리 작업은 Kinesis Data Analytics의 기능이 아니며, 이는 배치 처리보다는 실시간 스트림 처리를 위해 설계되었습니다.",
            "인메모리 상태 관리는 기능이지만, 정확한 한 번 처리를 보장하지는 않습니다. 이는 상태 관리와 애플리케이션 설계의 조합입니다.",
            "Kinesis Data Analytics는 내장된 기계 학습 기능을 제공하지 않으며, 주로 실시간 분석 및 SQL 처리에 중점을 두고 있습니다."
        ]
    },
    {
        "Question Number": "54",
        "Situation": "데이터 엔지니어는 새로운 분석 플랫폼을 위해 다양한 데이터 유형을 수용할 수 있는 데이터 저장 솔루션을 설계하는 임무를 맡았습니다. 이 플랫폼은 관계형 데이터베이스의 구조화된 데이터, JSON 및 XML 파일의 반구조화된 데이터, 텍스트 및 이미지와 같은 비구조화된 데이터를 처리해야 합니다. 엔지니어는 이 다양한 데이터를 효과적으로 모델링하고 저장하기 위해 다양한 AWS 서비스를 고려하고 있습니다. 어떤 AWS 서비스 조합이 이 요구 사항을 가장 잘 지원할까요?",
        "Question": "데이터 엔지니어가 구조화된, 반구조화된 및 비구조화된 데이터를 효과적으로 관리하기 위해 선택해야 할 AWS 서비스 조합은 무엇입니까?",
        "Options": {
            "1": "Amazon S3와 Amazon DynamoDB 및 Amazon Redshift",
            "2": "Amazon Redshift와 Amazon S3 및 AWS Lambda",
            "3": "Amazon Aurora와 Amazon S3 및 AWS Glue",
            "4": "Amazon RDS와 Amazon S3 및 Amazon DynamoDB"
        },
        "Correct Answer": "Amazon S3와 Amazon DynamoDB 및 Amazon Redshift",
        "Explanation": "이 조합은 데이터 엔지니어가 분석에 최적화된 Amazon Redshift에 구조화된 데이터를 저장하고, Amazon S3를 사용하여 반구조화된 데이터와 비구조화된 데이터를 처리할 수 있게 해줍니다. 또한, Amazon DynamoDB는 반구조화된 데이터 형식을 효율적으로 저장하고 쿼리할 수 있어 분석 플랫폼에서 요구하는 다양한 데이터 유형에 대한 포괄적인 솔루션을 제공합니다.",
        "Other Options": [
            "이 옵션은 Amazon RDS가 구조화된 데이터를 관리할 수 있지만, 비구조화된 데이터를 Amazon S3만큼 효율적으로 처리하지 못하기 때문에 잘못된 것입니다. 또한, 이 시나리오에서 DynamoDB는 일반적으로 RDS와 함께 사용되지 않습니다.",
            "이 옵션은 Amazon S3가 반구조화된 데이터와 비구조화된 데이터를 저장할 수 있지만, 주로 구조화된 데이터에 대해 DynamoDB를 사용하는 것은 최적이 아닙니다. 또한, Redshift는 반구조화된 데이터의 기본 저장소보다는 구조화된 데이터 분석에 더 적합합니다.",
            "이 옵션은 Amazon Aurora가 구조화된 데이터에 적합하지만 비구조화된 데이터를 효과적으로 관리하지 못하기 때문에 잘못된 것입니다. AWS Glue는 ETL 프로세스에 유용하지만 비구조화된 데이터의 저장 솔루션으로는 적합하지 않습니다."
        ]
    },
    {
        "Question Number": "55",
        "Situation": "한 금융 서비스 회사가 다양한 출처의 거래 데이터를 처리하기 위한 실시간 분석 솔루션을 구축하고 있습니다. 그들은 데이터 수집 및 변환을 위한 서버리스 아키텍처를 구현하여 확장성과 낮은 운영 오버헤드를 보장하고자 합니다. 데이터 엔지니어는 이를 달성하기 위해 적절한 AWS 서비스를 선택하는 책임이 있습니다.",
        "Question": "데이터 수집 및 변환을 위한 서버리스 워크플로우를 효과적으로 지원하는 AWS 서비스 조합은 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "확장을 위해 사용자 정의 데이터 처리 애플리케이션을 실행하기 위해 Amazon EC2 인스턴스를 활용합니다.",
            "2": "AWS Glue를 활용하여 데이터를 변환하고 Amazon S3에 로드하는 ETL 작업을 생성합니다.",
            "3": "Amazon Kinesis Data Streams를 사용하여 거래 데이터를 실시간으로 캡처하고 처리합니다.",
            "4": "AWS Lambda 함수를 구현하여 데이터를 변환하고 결과를 DynamoDB에 저장합니다.",
            "5": "Amazon S3 이벤트 알림을 사용하여 업로드된 거래 파일을 처리하기 위해 Lambda 함수를 트리거합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Amazon Kinesis Data Streams를 사용하여 거래 데이터를 실시간으로 캡처하고 처리합니다.",
            "Amazon S3 이벤트 알림을 사용하여 업로드된 거래 파일을 처리하기 위해 Lambda 함수를 트리거합니다."
        ],
        "Explanation": "Amazon Kinesis Data Streams를 사용하면 회사가 실시간 거래 데이터를 효율적으로 수집하고 처리할 수 있습니다. S3 이벤트 알림을 통해 Lambda 함수를 트리거하는 이 설정은 수신 데이터의 양에 따라 자동으로 확장되는 완전한 서버리스 아키텍처를 제공하여 낮은 운영 오버헤드와 높은 반응성을 보장합니다.",
        "Other Options": [
            "EC2 인스턴스를 구현하면 기본 인프라를 관리해야 하므로 서버리스 아키텍처의 목표와 모순됩니다. 이 접근 방식은 운영 복잡성과 비용을 증가시킬 것입니다.",
            "AWS Glue는 ETL 작업에 사용할 수 있지만 Kinesis와 Lambda가 함께 사용되는 방식에서 엄밀히 서버리스 솔루션은 아닙니다. Glue 작업은 실시간 처리에 비해 더 많은 오버헤드를 포함할 수 있습니다.",
            "Lambda는 데이터를 변환할 수 있지만 이벤트 알림 없이 S3와만 사용하는 경우 실시간 분석에 필요한 즉각적인 처리를 제공하지 않으며 Kinesis의 이점을 놓칠 것입니다."
        ]
    },
    {
        "Question Number": "56",
        "Situation": "데이터 분석가는 대규모 데이터 분석에 사용되는 Amazon Redshift 클러스터의 성능을 최적화하는 임무를 맡고 있습니다. 분석가는 클러스터가 쿼리 성능에 영향을 주지 않으면서 증가하는 작업 부하를 처리할 수 있도록 하고, 비용도 최소화해야 합니다.",
        "Question": "분석가는 이러한 목표를 달성하기 위해 어떤 전략을 구현할 수 있습니까? (두 가지 선택)",
        "Options": {
            "1": "비용을 낮추기 위해 컴퓨트 노드 수를 줄입니다. 노드 수가 적을수록 오버헤드가 감소합니다.",
            "2": "집약적 쿼리에 대한 성능 향상을 위해 노드 유형을 Dense Compute (DC)로 변경합니다.",
            "3": "저렴한 비용으로 대량의 데이터를 수용하기 위해 Dense Storage (DS) 노드 유형을 활용합니다.",
            "4": "작업 부하를 보다 효과적으로 분산하기 위해 클러스터에 더 많은 컴퓨트 노드를 추가하여 확장합니다.",
            "5": "업데이트를 적용할 때 다운타임을 최소화하기 위해 비혼잡 시간 동안 유지 관리 창을 사용합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "클러스터에 더 많은 컴퓨트 노드를 추가하여 작업 부하를 보다 효과적으로 분산합니다.",
            "집약적 쿼리에 대한 성능 향상을 위해 노드 유형을 Dense Compute (DC)로 변경합니다."
        ],
        "Explanation": "클러스터에 컴퓨트 노드를 추가하여 확장하면 작업 부하의 더 나은 분산과 쿼리 성능 향상을 이룰 수 있습니다. Dense Compute (DC) 노드 유형으로 변경하는 것은 성능 집약적인 작업에 이상적이며, SSD 스토리지를 활용하여 데이터 검색 및 처리 속도를 향상시킵니다.",
        "Other Options": [
            "컴퓨트 노드 수를 줄이면 쿼리 시간이 증가하고 성능 저하가 발생할 가능성이 높아져 증가하는 작업 부하에서 성능을 유지하려는 목표와 모순됩니다.",
            "유지 관리 창을 사용하는 것은 업데이트에 중요하지만 쿼리 성능이나 작업 부하 처리 능력에 직접적인 영향을 미치지 않으므로 최적화 작업과는 덜 관련이 있습니다.",
            "Dense Storage (DS) 노드를 활용하는 것은 대량의 데이터에 대해 비용 효율적이지만 성능 집약적인 쿼리에는 덜 최적화되어 있어 분석가의 성능 최적화 목표와는 반대입니다."
        ]
    },
    {
        "Question Number": "57",
        "Situation": "한 금융 서비스 회사가 실시간 거래 데이터를 처리하기 위한 데이터 파이프라인을 구현했습니다. 그들은 거래 금액에서 이상이 감지될 경우 운영 팀에 즉시 알림이 가도록 하고자 합니다. 알림 시스템을 구현하기 위해 AWS 서비스를 사용하는 것을 고려하고 있습니다.",
        "Question": "거래 이상이 감지될 때 회사가 운영 팀에 알림을 보낼 수 있도록 가장 잘 지원하는 AWS 서비스 조합은 무엇입니까?",
        "Options": {
            "1": "Amazon Kinesis Data Streams를 사용하여 이상을 감지하고, 이후 Amazon SNS 알림을 트리거하여 운영 팀에 경고합니다.",
            "2": "AWS Step Functions를 활용하여 이상 감지 프로세스를 조정하고 Amazon SNS를 사용하여 알림을 보냅니다.",
            "3": "Amazon SQS를 구현하여 거래 데이터를 큐에 저장하고, AWS Lambda를 사용하여 큐를 처리하고 Amazon SNS를 통해 알림을 보냅니다.",
            "4": "Amazon CloudWatch를 설정하여 거래 메트릭을 모니터링하고, 이를 Amazon SQS에 알림을 보내도록 구성합니다."
        },
        "Correct Answer": "Amazon Kinesis Data Streams를 사용하여 이상을 감지하고, 이후 Amazon SNS 알림을 트리거하여 운영 팀에 경고합니다.",
        "Explanation": "Amazon Kinesis Data Streams를 사용하면 거래 데이터를 실시간으로 처리하여 발생하는 이상을 감지할 수 있습니다. 이를 Amazon SNS와 통합하면 지연 없이 운영 팀에 즉시 알림이 전송됩니다.",
        "Other Options": [
            "Amazon SQS를 사용하여 거래 데이터를 큐에 저장하는 것은 작업 부하 관리를 도울 수 있지만, 본질적으로 이상을 감지하지는 않습니다. AWS Lambda가 알림을 보낼 수 있지만, 감지 메커니즘은 별도로 설정해야 하므로 실시간 알림에 덜 효율적입니다.",
            "Amazon CloudWatch는 메트릭 모니터링에 유용하지만, Amazon SQS에 알림을 보내는 것은 운영 팀에 즉각적인 알림을 제공하지 않습니다. 대신 CloudWatch는 적시에 알림을 위해 Amazon SNS에 직접 알림을 보내야 합니다.",
            "AWS Step Functions는 워크플로우 조정에 유용하지만, 이상 감지 및 알림이라는 간단한 작업에 불필요한 복잡성을 추가합니다. 실시간 감지를 위해 Kinesis를 직접 사용하고 알림을 위해 SNS를 사용하는 것이 더 간단한 접근 방식입니다."
        ]
    },
    {
        "Question Number": "58",
        "Situation": "데이터 엔지니어가 다양한 AWS 서비스에 프로그래밍 방식으로 접근해야 하는 애플리케이션을 구축하는 임무를 맡았습니다. 이 애플리케이션은 Amazon S3, Amazon DynamoDB 및 AWS Lambda와 같은 AWS 서비스와의 원활한 통합이 필요합니다. 데이터 엔지니어는 애플리케이션이 다양한 AWS 리소스를 효율적이고 안전하게 관리할 수 있도록 하기를 원합니다.",
        "Question": "애플리케이션의 요구를 가장 잘 지원하는 SDK 기능은 무엇입니까? (두 가지 선택)",
        "Options": {
            "1": "AWS SDK for .NET을 활용하여 고급 보안 기능을 갖춘 Windows 기반 애플리케이션에서 AWS 리소스를 관리합니다.",
            "2": "AWS SDK for Go를 사용하여 데이터를 처리하고 다른 AWS 서비스와 상호작용할 수 있는 Lambda 함수를 작성합니다.",
            "3": "AWS SDK for Python (Boto3)을 사용하여 Amazon DynamoDB와 상호작용하여 데이터를 검색하고 업데이트합니다.",
            "4": "AWS SDK for JavaScript를 활용하여 파일 업로드 및 다운로드를 위해 Amazon S3 API를 호출합니다.",
            "5": "AWS SDK for Ruby를 구현하여 인증 없이 애플리케이션에서 직접 AWS 서비스를 호출합니다."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "AWS SDK for JavaScript를 활용하여 파일 업로드 및 다운로드를 위해 Amazon S3 API를 호출합니다.",
            "AWS SDK for Python (Boto3)을 사용하여 Amazon DynamoDB와 상호작용하여 데이터를 검색하고 업데이트합니다."
        ],
        "Explanation": "AWS SDK for JavaScript를 사용하면 애플리케이션이 파일 작업을 위해 Amazon S3와 쉽게 상호작용할 수 있으며, Boto3는 데이터를 관리하기 위해 DynamoDB에 효율적으로 접근할 수 있게 해 주므로 두 가지 옵션 모두 애플리케이션의 요구에 적합합니다.",
        "Other Options": [
            "AWS SDK for .NET은 AWS 리소스를 관리할 수 있지만, 다른 옵션들이 지정된 서비스에 대해 더 직접적이고 효율적인 통합을 제공하므로 이 시나리오에 가장 좋은 선택은 아닙니다.",
            "AWS SDK for Go는 다양한 서비스와 상호작용할 수 있지만, 현재 요구 사항에 대해 S3 또는 DynamoDB에 대한 특정 요구를 효과적으로 해결하지 못하므로 가장 적합한 옵션이 아닐 수 있습니다.",
            "AWS SDK for Ruby는 AWS 서비스를 호출할 수 있지만, 본질적으로 인증을 관리하지 않습니다. 적절한 인증 없이 직접 호출하는 것은 보안 위험이므로 이 옵션은 적합하지 않습니다."
        ]
    },
    {
        "Question Number": "59",
        "Situation": "금융 기관이 Amazon S3에 계좌 정보 및 거래 기록을 포함한 민감한 고객 데이터를 저장하고 있습니다. 보안 팀은 특정 애플리케이션이 처리 및 보고를 위해 데이터에 접근할 수 있도록 하면서 데이터가 안전하게 저장되도록 해야 합니다. 이 데이터가 포함된 S3 버킷에 대한 가장 효과적인 접근 제어 조치를 구현하고자 합니다.",
        "Question": "필요한 애플리케이션이 올바르게 작동하도록 하면서 S3 버킷에 대한 접근을 안전하게 보호하는 가장 좋은 방법은 무엇입니까?",
        "Options": {
            "1": "버킷 정책을 사용하여 접근이 필요한 애플리케이션과 연결된 특정 IAM 역할에 권한을 부여합니다.",
            "2": "민감한 데이터의 우발적인 삭제를 방지하기 위해 S3 버킷 버전 관리를 구성합니다.",
            "3": "인터넷을 통해 모든 애플리케이션의 접근을 허용하기 위해 S3 버킷에서 공개 접근을 활성화합니다.",
            "4": "모든 S3 버킷에 접근할 수 있는 단일 IAM 사용자를 생성하고 모든 애플리케이션에 자격 증명을 제공합니다."
        },
        "Correct Answer": "버킷 정책을 사용하여 접근이 필요한 애플리케이션과 연결된 특정 IAM 역할에 권한을 부여합니다.",
        "Explanation": "버킷 정책을 사용하여 특정 IAM 역할에 권한을 부여하면 세분화된 접근 제어가 가능해져, 권한이 있는 애플리케이션만 S3 버킷에 저장된 민감한 데이터에 접근할 수 있도록 보장합니다. 이 방법은 최소 권한 원칙을 준수하며 전반적인 보안을 강화합니다.",
        "Other Options": [
            "S3 버킷에서 공개 접근을 활성화하면 민감한 고객 데이터가 인터넷에 노출되어 심각한 보안 위험을 초래하며 데이터 보호를 위한 모범 사례를 위반합니다.",
            "모든 S3 버킷에 무제한 접근이 가능한 단일 IAM 사용자를 생성하는 것은 최소 권한 원칙을 위반하며, 자격 증명이 손상될 경우 보안 취약점을 초래할 수 있습니다.",
            "S3 버킷 버전 관리를 구성하는 것은 우발적인 삭제로부터 데이터 보호에 유용하지만, 버킷에 대한 접근을 직접적으로 제어하지 않으므로 민감한 데이터를 보호하기 위한 적절한 솔루션이 아닙니다."
        ]
    },
    {
        "Question Number": "60",
        "Situation": "의료 기관이 다양한 시스템에서 환자 데이터의 정확성과 신뢰성을 보장하기 위한 데이터 관리 전략을 구현하고 있습니다. 그들은 데이터가 다양한 데이터 저장소를 통해 이동할 때 데이터의 출처와 변환을 추적하기 위해 데이터 계보를 활용하고자 합니다.",
        "Question": "이 시나리오에서 데이터 계보를 구현하기 위한 가장 좋은 방법은 무엇입니까?",
        "Options": {
            "1": "Amazon DynamoDB Streams를 사용하여 환자 데이터의 변화를 캡처하고 계보 추적을 위한 별도의 감사 테이블을 유지합니다.",
            "2": "AWS Glue를 활용하여 ETL 프로세스를 통해 데이터 소스, 변환 및 목적지를 추적하는 데이터 카탈로그를 생성하고 유지합니다.",
            "3": "감사 목적으로 모든 데이터 쓰기 및 읽기 작업을 기록하기 위해 Amazon S3 이벤트 알림을 구현합니다.",
            "4": "모든 데이터 저장소에 대한 API 호출을 기록하기 위해 AWS CloudTrail을 설정하고 이 로그를 데이터 계보 추적에 사용합니다."
        },
        "Correct Answer": "AWS Glue를 활용하여 ETL 프로세스를 통해 데이터 소스, 변환 및 목적지를 추적하는 데이터 카탈로그를 생성하고 유지합니다.",
        "Explanation": "AWS Glue를 활용하면 조직이 데이터의 출처, 변환 및 저장 위치를 자동으로 추적하는 포괄적인 데이터 카탈로그를 생성할 수 있습니다. 이는 여러 시스템에서 데이터의 정확성과 신뢰성을 유지하는 데 필수적입니다.",
        "Other Options": [
            "Amazon S3 이벤트 알림을 구현하면 저장 수준에서 데이터 작업만 기록되며, 다양한 시스템에서 데이터 변환에 대한 포괄적인 뷰를 제공하지 않습니다.",
            "Amazon DynamoDB Streams를 사용하면 DynamoDB의 변화만 추적되며, 조직 내 여러 데이터 저장소에서 데이터 계보에 대한 가시성을 제공하지 않습니다.",
            "AWS CloudTrail을 설정하면 API 호출을 기록하지만, 데이터 계보, 변환을 구체적으로 추적하거나 다양한 시스템 내에서 데이터가 처리되는 방식을 포괄적으로 보여주지 않습니다."
        ]
    },
    {
        "Question Number": "61",
        "Situation": "금융 서비스 회사가 Amazon Redshift를 활용하여 민감한 고객 정보가 포함된 대규모 데이터 세트를 분석하고 있습니다. 이 회사는 조직 내 다양한 팀이 민감한 데이터를 보호하면서 작업을 수행할 수 있도록 적절한 접근 제어를 보장해야 합니다. 그들은 사용자 권한을 효과적으로 관리하고 역할 요구 사항에 따라 특정 데이터에 대한 접근을 제한할 수 있는 솔루션이 필요합니다.",
        "Question": "Amazon Redshift에서 접근 제어를 효과적으로 관리하기 위해 어떤 접근 방식을 구현해야 합니까?",
        "Options": {
            "1": "특정 IP 주소로 데이터베이스 접근을 제한하기 위해 네트워크 ACL을 설정합니다.",
            "2": "IAM 정책을 사용하여 Amazon Redshift 클러스터 및 데이터에 대한 접근을 제어합니다.",
            "3": "사용자 그룹을 생성하고 그룹 수준에서 권한을 할당하여 접근 관리를 용이하게 합니다.",
            "4": "Amazon Redshift의 열 수준 보안을 활용하여 민감한 데이터 필드에 대한 접근을 제한합니다."
        },
        "Correct Answer": "사용자 그룹을 생성하고 그룹 수준에서 권한을 할당하여 접근 관리를 용이하게 합니다.",
        "Explanation": "Amazon Redshift에서 사용자 그룹을 생성하면 개별 사용자보다 그룹에 접근 제어를 할당할 수 있어 권한 관리가 더 간소화됩니다. 이는 팀 구성원이 변경되거나 조직 내 역할이 발전함에 따라 권한을 부여하거나 철회하는 과정을 단순화합니다.",
        "Other Options": [
            "IAM 정책은 AWS 리소스에 대한 접근을 제어하는 데 주로 사용되며, Amazon Redshift 내에서 권한을 직접 관리하는 데는 적합하지 않습니다. IAM 역할은 Amazon Redshift와 통합될 수 있지만, 데이터베이스 내에서 데이터 접근 제어에 대한 동일한 세분성을 제공하지 않습니다.",
            "네트워크 ACL은 Amazon Redshift 내에서 사용자 접근을 관리하는 적절한 방법이 아니며, 데이터베이스 사용자 권한을 관리하기보다는 서브넷 수준에서 트래픽을 제어하는 데 중점을 둡니다. 이 옵션은 역할 기반 접근 제어 요구 사항을 해결하지 않습니다.",
            "열 수준 보안은 테이블 내 특정 필드에 대한 접근을 제한하는 데 사용할 수 있는 기능이지만, 전체 사용자 접근 관리를 위한 완전한 솔루션을 제공하지 않습니다. 포괄적인 접근 제어를 보장하기 위해 사용자 그룹과 함께 사용해야 합니다."
        ]
    },
    {
        "Question Number": "62",
        "Situation": "소매 회사가 거래 데이터베이스와 제3자 API를 포함한 다양한 출처에서 데이터를 통합하기 위해 ETL 파이프라인을 만들고자 합니다. 이 파이프라인은 데이터 변환 및 중앙 집중식 데이터 웨어하우스로의 로딩을 용이하게 하여 보고 및 분석을 지원해야 합니다. 솔루션은 증가하는 데이터 양에 따라 확장 가능해야 하며, 종속성을 자동으로 관리할 수 있어야 합니다.",
        "Question": "확장성과 자동 종속성 관리를 보장하면서 이 ETL 파이프라인을 구축하기 위한 최상의 AWS 서비스는 무엇입니까?",
        "Options": {
            "1": "ETL 오케스트레이션 및 실행을 위해 AWS Glue를 사용합니다.",
            "2": "데이터 처리를 위해 Amazon EMR에서 Spark 작업을 실행합니다.",
            "3": "사용자 정의 데이터 변환을 수행하기 위해 AWS Lambda 함수를 사용합니다.",
            "4": "데이터를 웨어하우스로 스트리밍하기 위해 Amazon Kinesis Data Firehose를 사용합니다."
        },
        "Correct Answer": "ETL 오케스트레이션 및 실행을 위해 AWS Glue를 사용합니다.",
        "Explanation": "AWS Glue는 ETL 프로세스를 위해 특별히 설계되어 데이터 수집, 변환 및 데이터 웨어하우스로의 로딩을 쉽게 오케스트레이션할 수 있습니다. 서버리스 아키텍처를 제공하여 데이터 양에 따라 자동으로 확장되며, 작업 종속성을 효과적으로 관리하는 기능도 포함되어 있습니다.",
        "Other Options": [
            "AWS Lambda는 이벤트 기반 처리에 적합하지만, 여러 데이터 출처 간의 종속성을 관리하는 데 필요한 전체 오케스트레이션 기능이 부족합니다.",
            "Amazon EMR은 대규모 데이터 세트를 처리하는 데 강력하지만, 더 많은 관리 오버헤드가 필요하며 AWS Glue에 비해 ETL 오케스트레이션에 특별히 맞춰져 있지 않습니다.",
            "Amazon Kinesis Data Firehose는 주로 데이터 스트리밍 수집에 사용되며, 복잡한 변환을 수행하거나 전체 ETL 파이프라인을 오케스트레이션하는 데는 적합하지 않습니다."
        ]
    },
    {
        "Question Number": "63",
        "Situation": "금융 기관의 보안 팀이 AWS 인프라에서 생성된 로그를 분석하여 잠재적인 보안 위협을 탐지하고자 합니다. 그들은 실시간 쿼리 기능을 제공하고 기존 AWS 서비스와 잘 통합되는 솔루션을 선호합니다. 이 솔루션은 모니터링 목적으로 로그 데이터를 효과적으로 시각화할 수 있어야 합니다.",
        "Question": "보안 팀이 로그를 효과적으로 분석하고 시각화하기 위해 어떤 AWS 서비스를 사용해야 합니까?",
        "Options": {
            "1": "AWS Glue Data Catalog",
            "2": "Amazon QuickSight",
            "3": "AWS Config",
            "4": "Amazon CloudWatch Logs Insights"
        },
        "Correct Answer": "Amazon CloudWatch Logs Insights",
        "Explanation": "Amazon CloudWatch Logs Insights는 로그 데이터를 실시간으로 쿼리하고 분석하기 위해 특별히 설계되었습니다. 강력한 쿼리 기능을 제공하며 다른 AWS 서비스와 원활하게 통합되어 로그 데이터에서 보안 위협을 모니터링하고 탐지하는 데 이상적인 선택입니다.",
        "Other Options": [
            "AWS Config는 AWS 리소스의 구성을 평가, 감사 및 평가하는 데 주로 사용되며, 로그 데이터를 분석하는 데는 적합하지 않습니다.",
            "Amazon QuickSight는 사용자가 데이터 소스에 대한 시각화를 생성하고 분석을 수행할 수 있는 비즈니스 인텔리전스 서비스이지만, 실시간 로그 분석을 위해 특별히 설계된 것은 아닙니다.",
            "AWS Glue Data Catalog는 ETL 프로세스를 위한 메타데이터 저장소로, 데이터를 조직하고 관리하는 데 도움을 주지만, 로그 분석에 필요한 실시간 쿼리 기능을 제공하지 않습니다."
        ]
    },
    {
        "Question Number": "64",
        "Situation": "데이터 엔지니어는 소매 분석 프로젝트의 ETL 프로세스 동안 데이터 품질을 보장하는 임무를 맡고 있습니다. 파이프라인은 여러 출처에서 고객 거래 데이터를 수집하며, 데이터를 추가로 처리하기 전에 중요한 필드가 누락되지 않았는지 확인하는 것이 중요합니다. 데이터 엔지니어는 들어오는 데이터에서 빈 필드를 자동으로 확인하고 불일치를 검토를 위해 기록하는 솔루션을 구현해야 합니다.",
        "Question": "데이터 엔지니어가 ETL 프로세스 동안 빈 필드에 대한 데이터 품질 검사를 효율적으로 수행하기 위해 어떤 접근 방식을 취해야 합니까?",
        "Options": {
            "1": "AWS Glue 작업을 구현하여 들어오는 데이터를 읽고 빈 필드를 확인한 후 유효한 레코드만 목적지에 기록합니다.",
            "2": "Amazon Kinesis Data Firehose를 사용하여 들어오는 데이터를 스트리밍하고, 저장하기 전에 빈 필드에 대한 레코드를 검증하는 Lambda 함수를 구성합니다.",
            "3": "AWS Step Functions를 활용하여 데이터에서 빈 필드를 확인하는 작업을 포함하는 ETL 워크플로를 조정합니다.",
            "4": "Amazon S3 이벤트 알림을 생성하여 S3에 저장된 후 빈 필드에 대한 들어오는 데이터를 검증하는 Lambda 함수를 트리거합니다."
        },
        "Correct Answer": "AWS Glue 작업을 구현하여 들어오는 데이터를 읽고 빈 필드를 확인한 후 유효한 레코드만 목적지에 기록합니다.",
        "Explanation": "이 옵션은 AWS Glue의 ETL 기능과 원활하게 통합되어 데이터를 목적지에 기록하기 전에 효과적인 데이터 변환 및 검증을 한 번에 수행하여 데이터 품질을 사전에 보장합니다.",
        "Other Options": [
            "이 옵션은 Kinesis Data Firehose를 사용하는 것이 데이터 검증보다는 데이터 스트리밍 수집에 더 적합하므로 잘못된 것입니다. 또한 저장 전에 빈 필드 검사를 원활하게 처리하지 못할 수 있습니다.",
            "이 옵션은 S3에 데이터가 저장된 후 검증을 트리거하지만, 초기 저장 시 잘못된 데이터가 저장되는 것을 방지하지 않기 때문에 잘못된 것입니다. 이는 하류 문제를 초래할 수 있습니다.",
            "이 옵션은 AWS Step Functions가 워크플로를 조정할 수 있지만, 단일 AWS Glue 작업으로 효율적으로 처리할 수 있는 작업에 불필요한 복잡성을 추가하므로 잘못된 것입니다."
        ]
    },
    {
        "Question Number": "65",
        "Situation": "금융 서비스 회사는 모바일 앱, 웹 플랫폼 및 제3자 API를 포함한 다양한 출처에서 대량의 실시간 거래 데이터를 처리하고 있습니다. 이 데이터는 효율적으로 수집되어 분석을 위해 사용 가능해야 하며, 구조화된 형식과 비구조화된 형식을 모두 수용해야 합니다.",
        "Question": "이 다양한 데이터를 신속하고 유연하게 수집하고 처리하기 위해 가장 적합한 AWS 서비스는 무엇입니까?",
        "Options": {
            "1": "실시간 데이터 수집을 위해 Amazon Kinesis Data Streams를 사용합니다.",
            "2": "일정한 방식으로 데이터를 처리하기 위해 AWS Batch를 사용합니다.",
            "3": "거래 데이터의 배치 저장을 위해 Amazon S3를 사용합니다.",
            "4": "구조화된 거래 데이터를 저장하기 위해 Amazon RDS를 사용합니다."
        },
        "Correct Answer": "실시간 데이터 수집을 위해 Amazon Kinesis Data Streams를 사용합니다.",
        "Explanation": "Amazon Kinesis Data Streams는 실시간 데이터 수집 및 처리를 위해 설계되어 있으며, 여러 출처에서 고속 데이터를 처리하는 데 이상적입니다. 구조화된 데이터 형식과 비구조화된 데이터 형식을 모두 지원하여 회사가 데이터 스트림을 실시간으로 캡처, 처리 및 분석할 수 있도록 하여 분석 요구를 효과적으로 충족합니다.",
        "Other Options": [
            "Amazon S3는 주로 저장 서비스이며 실시간 수집 기능을 제공하지 않으므로, 실시간 거래 데이터를 처리하는 데 필요한 속도와 유연성에 적합하지 않습니다.",
            "AWS Batch는 배치 처리를 위해 설계되었으며 실시간 데이터 수집에 최적화되어 있지 않습니다. 따라서 고속 거래 데이터를 효과적으로 처리하는 회사의 요구를 충족하지 못합니다.",
            "Amazon RDS는 구조화된 데이터 저장에 적합한 관계형 데이터베이스 서비스이지만, 다양한 출처에서 실시간 데이터를 수집하고 처리하는 기능이 부족하여 이 시나리오에 필수적입니다."
        ]
    }
]