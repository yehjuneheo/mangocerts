[
    {
        "Question Number": "1",
        "Situation": "Une entreprise adopte une infrastructure immuable pour le déploiement de ses applications. Elle souhaite s'assurer que tous les changements d'infrastructure sont effectués en remplaçant les ressources plutôt qu'en les modifiant sur place, dans le but d'améliorer la cohérence et de faciliter les retours en arrière.",
        "Question": "Lequel des éléments suivants décrit le mieux le principe de l'infrastructure immuable et ses avantages ? (Choisissez deux.)",
        "Options": {
            "1": "L'infrastructure immuable garantit que les serveurs et les ressources sont toujours modifiés sur place, empêchant ainsi la nécessité de remplacer les ressources.",
            "2": "L'infrastructure immuable implique de remplacer entièrement les serveurs ou les composants d'infrastructure lorsque des changements sont nécessaires, garantissant qu'aucun changement n'est appliqué aux instances en cours d'exécution et facilitant les retours en arrière.",
            "3": "L'infrastructure immuable élimine le besoin de contrôle de version, car chaque mise à jour est automatiquement intégrée aux ressources existantes.",
            "4": "L'infrastructure immuable repose sur des configurations manuelles des serveurs, garantissant qu'aucune automatisation n'est utilisée pendant le processus de déploiement.",
            "5": "L'infrastructure immuable améliore la cohérence en garantissant que tous les déploiements sont identiques et réduit la dérive de configuration."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "L'infrastructure immuable implique de remplacer entièrement les serveurs ou les composants d'infrastructure lorsque des changements sont nécessaires, garantissant qu'aucun changement n'est appliqué aux instances en cours d'exécution et facilitant les retours en arrière.",
            "L'infrastructure immuable améliore la cohérence en garantissant que tous les déploiements sont identiques et réduit la dérive de configuration."
        ],
        "Explanation": "L'infrastructure immuable est un principe selon lequel les serveurs ou les composants d'infrastructure sont entièrement remplacés lorsque des changements sont nécessaires, plutôt que de les modifier sur place. Cela garantit qu'aucun changement n'est appliqué aux instances en cours d'exécution, ce qui facilite les retours en arrière. Elle améliore également la cohérence en garantissant que tous les déploiements sont identiques, ce qui réduit la dérive de configuration. Cette approche peut réduire considérablement le risque d'incohérences et d'erreurs dans l'infrastructure, la rendant plus fiable et plus facile à gérer.",
        "Other Options": [
            "L'infrastructure immuable n'implique pas de modifier les serveurs et les ressources sur place. Au lieu de cela, elle consiste à les remplacer entièrement lorsque des changements sont nécessaires.",
            "L'infrastructure immuable n'élimine pas le besoin de contrôle de version. En fait, le contrôle de version est crucial dans une infrastructure immuable pour suivre toutes les différentes versions des composants d'infrastructure.",
            "L'infrastructure immuable ne repose pas sur des configurations manuelles des serveurs. Au contraire, elle implique souvent l'automatisation pour garantir que tous les déploiements sont identiques et pour faciliter le remplacement des serveurs ou des composants d'infrastructure lorsque des changements sont nécessaires."
        ]
    },
    {
        "Question Number": "2",
        "Situation": "Une entreprise de vente au détail gère un site e-commerce sur des instances Amazon EC2 derrière un Application Load Balancer. L'entreprise connaît des variations de trafic et souhaite s'assurer que l'application s'adapte automatiquement pour gérer des charges variables tout en minimisant les coûts.",
        "Question": "Quelles configurations un architecte de solutions devrait-il mettre en œuvre pour répondre à ces exigences ? (Choisissez deux.)",
        "Options": {
            "1": "Configurer un groupe Auto Scaling avec un nombre fixe d'instances EC2 et utiliser des instances réservées pour des économies de coûts.",
            "2": "Utiliser des instances Spot avec un groupe Auto Scaling pour gérer le trafic variable.",
            "3": "Mettre en place un groupe Auto Scaling avec des politiques de mise à l'échelle basées sur le suivi des cibles en fonction de l'utilisation du CPU.",
            "4": "Déployer l'application sur AWS Elastic Beanstalk avec des politiques de mise à l'échelle manuelles.",
            "5": "Mettre en œuvre une mise à l'échelle prédictive en utilisant Amazon CloudWatch pour prévoir le trafic et ajuster la capacité de manière proactive."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Utiliser des instances Spot avec un groupe Auto Scaling pour gérer le trafic variable.",
            "Mettre en place un groupe Auto Scaling avec des politiques de mise à l'échelle basées sur le suivi des cibles en fonction de l'utilisation du CPU."
        ],
        "Explanation": "Les instances Spot avec un groupe Auto Scaling sont un choix rentable pour gérer le trafic variable car elles vous permettent de tirer parti de la capacité EC2 inutilisée dans le cloud AWS. Les instances Spot sont disponibles avec jusqu'à 90 % de réduction par rapport aux prix à la demande. Un groupe Auto Scaling avec des politiques de mise à l'échelle basées sur le suivi des cibles en fonction de l'utilisation du CPU permet à l'application de s'adapter automatiquement en fonction de la demande. Lorsque la demande augmente, de nouvelles instances sont automatiquement ajoutées et lorsque la demande diminue, les instances sont automatiquement supprimées. Cela garantit que vous n'utilisez (et ne payez) que ce dont vous avez besoin.",
        "Other Options": [
            "Configurer un groupe Auto Scaling avec un nombre fixe d'instances EC2 et utiliser des instances réservées pour des économies de coûts n'est pas la meilleure option pour gérer le trafic variable car cela ne permet pas de mise à l'échelle automatique en fonction de la demande. Les instances réservées offrent des économies par rapport aux instances à la demande, mais elles ne fournissent pas la flexibilité nécessaire pour des variations de trafic.",
            "Déployer l'application sur AWS Elastic Beanstalk avec des politiques de mise à l'échelle manuelles n'est pas la meilleure option car cela ne permet pas de mise à l'échelle automatique. La mise à l'échelle manuelle nécessite une intervention manuelle pour ajouter ou supprimer des instances, ce qui n'est pas idéal pour gérer des variations de trafic.",
            "Mettre en œuvre une mise à l'échelle prédictive en utilisant Amazon CloudWatch pour prévoir le trafic et ajuster la capacité de manière proactive peut être une bonne option pour certains cas d'utilisation, mais ce n'est pas la solution la plus rentable pour ce scénario particulier. La mise à l'échelle prédictive utilise des algorithmes d'apprentissage automatique pour prédire les modèles de trafic futurs et ajuster la capacité en conséquence, ce qui peut être plus coûteux que d'autres options."
        ]
    },
    {
        "Question Number": "3",
        "Situation": "Une entreprise gère une application web qui connaît des variations de trafic. Elle doit s'assurer que l'application peut gérer un fort trafic pendant les heures de pointe sans surprovisionner les ressources.",
        "Question": "Quelle stratégie de mise à l'échelle l'entreprise devrait-elle utiliser pour mieux gérer la variabilité du trafic et la rentabilité ?",
        "Options": {
            "1": "Utiliser la mise à l'échelle horizontale en ajoutant plus d'instances EC2 derrière un équilibreur de charge pour répartir le trafic, garantissant que les ressources s'adaptent en réponse aux changements de demande.",
            "2": "Utiliser la mise à l'échelle verticale en augmentant la taille des instances EC2 pour gérer plus de trafic, bien que cela puisse ne pas offrir autant de flexibilité lors des pics de trafic.",
            "3": "Utiliser une combinaison de mise à l'échelle horizontale et verticale, où la mise à l'échelle horizontale est utilisée pour des changements de trafic mineurs, et la mise à l'échelle verticale est utilisée pour gérer des pics extrêmes.",
            "4": "Utiliser la mise à l'échelle manuelle, en ajustant les tailles des instances EC2 et le nombre d'instances en fonction des prévisions des modèles de trafic."
        },
        "Correct Answer": "Utiliser la mise à l'échelle horizontale en ajoutant plus d'instances EC2 derrière un équilibreur de charge pour répartir le trafic, garantissant que les ressources s'adaptent en réponse aux changements de demande.",
        "Explanation": "La mise à l'échelle horizontale est la stratégie la plus efficace pour gérer le trafic fluctuant car elle permet à l'application d'ajouter ou de supprimer des instances en fonction de la demande en temps réel. Cette approche garantit que pendant les heures de pointe, des instances EC2 supplémentaires peuvent être provisionnées pour gérer l'augmentation du trafic, tandis que pendant les heures creuses, les instances peuvent être réduites pour économiser des coûts. Cette capacité de mise à l'échelle dynamique offre à la fois flexibilité et rentabilité, car les ressources ne sont utilisées que lorsque cela est nécessaire.",
        "Other Options": [
            "La mise à l'échelle verticale consiste à augmenter la taille des instances EC2 existantes pour gérer plus de trafic. Bien que cela puisse être efficace, cela présente des limitations en termes de flexibilité et peut entraîner des temps d'arrêt lors des opérations de mise à l'échelle. De plus, il existe une limite de taille maximale pour les instances, qui peut ne pas être suffisante lors de pics de trafic extrêmes.",
            "Une combinaison de mise à l'échelle horizontale et verticale peut offrir des avantages, mais cela complique la stratégie de mise à l'échelle et peut ne pas être aussi efficace que l'utilisation de la mise à l'échelle horizontale seule. La mise à l'échelle horizontale est généralement préférée pour gérer le trafic variable car elle permet un contrôle plus granulaire sur l'allocation des ressources.",
            "La mise à l'échelle manuelle repose sur des prévisions des modèles de trafic, qui peuvent être inexactes. Cette approche ne fournit pas l'agilité nécessaire pour répondre aux changements de trafic soudains, ce qui peut entraîner des problèmes de performance lors de pics inattendus et des coûts inutiles pendant les périodes de faible trafic."
        ]
    },
    {
        "Question Number": "4",
        "Situation": "Une organisation de santé doit s'assurer que toutes les données stockées dans Amazon RDS pour PostgreSQL sont chiffrées au repos et que les clés de chiffrement sont gérées de manière sécurisée. L'organisation doit se conformer à des exigences réglementaires strictes en matière de protection des données.",
        "Question": "Quelle solution répondra à ces exigences ?",
        "Options": {
            "1": "Activer le chiffrement au repos en utilisant le chiffrement Amazon RDS et gérer les clés avec AWS Key Management Service (KMS).",
            "2": "Utiliser Amazon S3 pour stocker les sauvegardes de la base de données et activer le chiffrement S3.",
            "3": "Mettre en œuvre SSL/TLS pour les données en transit et s'appuyer sur le chiffrement par défaut de RDS.",
            "4": "Chiffrer les données au sein de l'application avant de les stocker dans la base de données RDS."
        },
        "Correct Answer": "Activer le chiffrement au repos en utilisant le chiffrement Amazon RDS et gérer les clés avec AWS Key Management Service (KMS).",
        "Explanation": "Cette option répond directement à l'exigence de chiffrer les données au repos dans Amazon RDS pour PostgreSQL. Amazon RDS offre des capacités de chiffrement intégrées qui peuvent être activées pour garantir que toutes les données stockées dans la base de données sont chiffrées. De plus, l'utilisation d'AWS Key Management Service (KMS) permet une gestion sécurisée des clés de chiffrement, ce qui est crucial pour se conformer aux exigences réglementaires concernant la protection des données. Cette solution garantit à la fois le chiffrement et la gestion sécurisée des clés de manière transparente.",
        "Other Options": [
            "Utiliser Amazon S3 pour stocker les sauvegardes de la base de données et activer le chiffrement S3 ne répond pas à l'exigence de chiffrer les données au repos au sein de la base de données RDS elle-même. Bien que le chiffrement S3 soit utile pour les sauvegardes, il ne traite pas le chiffrement des données de la base de données en direct stockées dans RDS.",
            "Mettre en œuvre SSL/TLS pour les données en transit est important pour sécuriser les données lorsqu'elles circulent entre le client et la base de données, mais cela ne fournit pas de chiffrement pour les données au repos. De plus, s'appuyer sur le chiffrement par défaut de RDS peut ne pas répondre à des exigences réglementaires spécifiques, car cela ne permet pas de gestion personnalisée des clés ou de vérifications de conformité.",
            "Chiffrer les données au sein de l'application avant de les stocker dans la base de données RDS est une approche valide, mais elle nécessite des efforts de développement supplémentaires et peut compliquer l'accès et la gestion des données. De plus, cela n'utilise pas les fonctionnalités de chiffrement intégrées de RDS, qui sont conçues pour simplifier la conformité aux réglementations sur la protection des données."
        ]
    },
    {
        "Question Number": "5",
        "Situation": "Une entreprise migre son application sur site vers AWS. L'application se compose d'un serveur web, d'un serveur d'application et d'un serveur de base de données. L'entreprise souhaite s'assurer que le serveur de base de données n'est pas directement accessible depuis Internet et ne peut être accédé que par le serveur d'application.",
        "Question": "Quelles configurations réseau répondront à ces exigences ? (Choisissez deux.)",
        "Options": {
            "1": "Placer le serveur web et le serveur d'application dans un sous-réseau public et le serveur de base de données dans un sous-réseau privé. Configurer les groupes de sécurité pour autoriser le trafic uniquement du serveur d'application vers le serveur de base de données.",
            "2": "Placer tous les serveurs dans un sous-réseau public et utiliser des ACL réseau pour restreindre l'accès au serveur de base de données.",
            "3": "Placer le serveur web dans un sous-réseau public et les serveurs d'application et de base de données dans des sous-réseaux privés séparés. Utiliser des groupes de sécurité pour autoriser le trafic uniquement du serveur web vers le serveur d'application et du serveur d'application vers le serveur de base de données.",
            "4": "Placer le serveur web et le serveur de base de données dans un sous-réseau public et le serveur d'application dans un sous-réseau privé. Utiliser des groupes de sécurité pour autoriser le trafic uniquement du serveur web vers le serveur d'application.",
            "5": "Utiliser AWS Transit Gateway pour gérer le routage entre les sous-réseaux et restreindre l'accès au serveur de base de données via des tables de routage."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Placer le serveur web et le serveur d'application dans un sous-réseau public et le serveur de base de données dans un sous-réseau privé. Configurer les groupes de sécurité pour autoriser le trafic uniquement du serveur d'application vers le serveur de base de données.",
            "Placer le serveur web dans un sous-réseau public et les serveurs d'application et de base de données dans des sous-réseaux privés séparés. Utiliser des groupes de sécurité pour autoriser le trafic uniquement du serveur web vers le serveur d'application et du serveur d'application vers le serveur de base de données."
        ],
        "Explanation": "Les réponses correctes sont les options qui placent le serveur web et le serveur d'application dans un sous-réseau public, et le serveur de base de données dans un sous-réseau privé. Cette configuration garantit que le serveur de base de données n'est pas directement accessible depuis Internet, comme requis. Les groupes de sécurité sont ensuite utilisés pour contrôler le trafic, permettant uniquement au serveur d'application d'accéder au serveur de base de données. Dans la deuxième option correcte, les serveurs d'application et de base de données sont dans des sous-réseaux privés séparés, ce qui ajoute une couche supplémentaire de sécurité et d'isolement.",
        "Other Options": [
            "Placer tous les serveurs dans un sous-réseau public et utiliser des ACL réseau pour restreindre l'accès au serveur de base de données n'est pas une bonne pratique. Cela expose tous les serveurs à Internet, ce qui augmente le risque de violations de sécurité.",
            "Placer le serveur web et le serveur de base de données dans un sous-réseau public et le serveur d'application dans un sous-réseau privé ne répond pas à l'exigence que le serveur de base de données soit inaccessible depuis Internet.",
            "Utiliser AWS Transit Gateway pour gérer le routage entre les sous-réseaux et restreindre l'accès au serveur de base de données via des tables de routage n'est pas la méthode la plus efficace ou sécurisée. Cela peut être complexe à gérer et ne fournit pas le même niveau de sécurité que l'utilisation de sous-réseaux privés et publics avec des groupes de sécurité."
        ]
    },
    {
        "Question Number": "6",
        "Situation": "Une entreprise de vente au détail exploite un site e-commerce hébergé sur des instances Amazon EC2 derrière un Application Load Balancer. Le site connaît des variations de trafic, en particulier pendant les saisons de shopping de pointe, et l'entreprise souhaite s'assurer que l'application s'adapte automatiquement pour gérer des charges variables sans encourir de coûts inutiles pendant les périodes de faible trafic. L'équipe recherche une configuration optimale pour soutenir l'évolutivité automatique tout en minimisant ses coûts d'infrastructure.",
        "Question": "Quelle configuration un architecte de solutions devrait-il mettre en œuvre pour répondre à ces exigences ?",
        "Options": {
            "1": "Configurer un groupe Auto Scaling avec un nombre fixe d'instances EC2 et réserver de la capacité avec des Instances Réservées pour des économies de coûts à long terme.",
            "2": "Utiliser des Instances Spot au sein d'un groupe Auto Scaling pour gérer le trafic fluctuant, permettant aux instances de s'adapter pendant les charges de pointe tout en réduisant les coûts.",
            "3": "Mettre en place un groupe Auto Scaling avec des politiques de mise à l'échelle basées sur le suivi des cibles en fonction de l'utilisation du CPU pour ajuster dynamiquement la capacité selon la demande.",
            "4": "Déployer l'application sur AWS Elastic Beanstalk et utiliser des politiques de mise à l'échelle manuelles pour ajouter ou retirer des instances à mesure que les modèles de trafic changent."
        },
        "Correct Answer": "Mettre en place un groupe Auto Scaling avec des politiques de mise à l'échelle basées sur le suivi des cibles en fonction de l'utilisation du CPU pour ajuster dynamiquement la capacité selon la demande.",
        "Explanation": "Mettre en place un groupe Auto Scaling avec des politiques de mise à l'échelle basées sur le suivi des cibles permet à l'application d'ajuster automatiquement le nombre d'instances EC2 en fonction de la demande en temps réel, spécifiquement l'utilisation du CPU dans ce cas. Cette configuration garantit que l'application peut s'adapter pendant les périodes de trafic de pointe pour gérer des charges accrues et se réduire pendant les périodes de faible trafic pour minimiser les coûts. Les politiques de mise à l'échelle basées sur le suivi des cibles sont simples à mettre en œuvre et à gérer, offrant un équilibre entre performance et efficacité des coûts.",
        "Other Options": [
            "Configurer un groupe Auto Scaling avec un nombre fixe d'instances EC2 ne permet pas une mise à l'échelle dynamique en fonction des modèles de trafic. Bien que les Instances Réservées puissent offrir des économies de coûts pour une utilisation à long terme, cette approche ne répond pas efficacement aux besoins de trafic fluctuant, car elle ne réduit pas la capacité pendant les périodes de faible trafic.",
            "Utiliser des Instances Spot au sein d'un groupe Auto Scaling peut réduire les coûts, mais les Instances Spot peuvent être résiliées par AWS avec peu de préavis, ce qui peut entraîner une instabilité de l'application pendant les charges de pointe. Cette option n'est pas idéale pour une entreprise de vente au détail qui nécessite une disponibilité constante pendant les saisons de shopping à fort trafic.",
            "Déployer l'application sur AWS Elastic Beanstalk avec des politiques de mise à l'échelle manuelles ne fournit pas la mise à l'échelle automatique nécessaire pour les modèles de trafic fluctuants. La mise à l'échelle manuelle nécessite une intervention humaine pour ajuster le nombre d'instances, ce qui peut entraîner des retards et des problèmes de performance potentiels pendant les périodes de pointe."
        ]
    },
    {
        "Question Number": "7",
        "Situation": "Une entreprise de médias dispose de plusieurs VPC dans différents comptes AWS et souhaite permettre une communication privée et rentable entre les VPC sans passer par Internet public. Elle souhaite également réduire les coûts de transfert de données associés à cette configuration.",
        "Question": "Quelle configuration réseau serait la solution la plus rentable ?",
        "Options": {
            "1": "Utiliser le VPC Peering entre chaque VPC",
            "2": "Utiliser AWS Transit Gateway pour une communication centralisée entre les VPC",
            "3": "Acheminer le trafic via des passerelles NAT pour un accès sécurisé",
            "4": "Établir une connexion VPN pour chaque VPC"
        },
        "Correct Answer": "Utiliser AWS Transit Gateway pour une communication centralisée entre les VPC",
        "Explanation": "AWS Transit Gateway est conçu pour simplifier la gestion de plusieurs VPC et permet une communication privée et rentable entre eux. Il permet un modèle de hub-and-spoke où tous les VPC peuvent se connecter à une passerelle centrale, réduisant ainsi la complexité et le coût associés à la gestion de plusieurs connexions de peering VPC. De plus, Transit Gateway peut aider à réduire les coûts de transfert de données en consolidant le trafic à travers un point unique plutôt que d'exiger plusieurs connexions de peering, ce qui peut entraîner des frais de transfert de données plus élevés.",
        "Other Options": [
            "Utiliser le VPC Peering entre chaque VPC peut devenir complexe et coûteux à mesure que le nombre de VPC augmente. Chaque VPC nécessiterait une connexion de peering distincte, entraînant une explosion combinatoire de connexions et un surcoût de gestion, ainsi que des coûts de transfert de données potentiellement plus élevés en raison de la nature du peering VPC.",
            "Acheminer le trafic via des passerelles NAT n'est pas adapté à la communication VPC-à-VPC car les passerelles NAT sont principalement utilisées pour l'accès Internet sortant depuis des sous-réseaux privés. Cette option ne faciliterait pas la communication directe entre les VPC et entraînerait des coûts supplémentaires pour le transfert de données via la passerelle NAT.",
            "Établir une connexion VPN pour chaque VPC serait inefficace et coûteux, surtout lorsqu'il s'agit de plusieurs VPC. Chaque connexion VPN entraîne des coûts et ajoute de la complexité à l'architecture réseau. De plus, les connexions VPN ont généralement un débit inférieur par rapport à d'autres options et peuvent introduire de la latence."
        ]
    },
    {
        "Question Number": "8",
        "Situation": "Une entreprise déploie une application web qui doit être protégée contre les attaques courantes basées sur le web telles que l'injection SQL et le cross-site scripting.",
        "Question": "Quel service AWS devrait être utilisé pour fournir cette protection ?",
        "Options": {
            "1": "AWS Shield",
            "2": "AWS WAF (Web Application Firewall)",
            "3": "Amazon Macie",
            "4": "Amazon GuardDuty"
        },
        "Correct Answer": "AWS WAF (Web Application Firewall)",
        "Explanation": "AWS WAF (Web Application Firewall) est spécifiquement conçu pour protéger les applications web contre les attaques courantes basées sur le web telles que l'injection SQL et le cross-site scripting (XSS). Il permet aux utilisateurs de créer des règles qui filtrent et surveillent les requêtes HTTP en fonction de conditions personnalisables, bloquant efficacement le trafic malveillant avant qu'il n'atteigne l'application. Cela en fait le choix le plus approprié pour le scénario décrit.",
        "Other Options": [
            "AWS Shield est un service de protection DDoS géré qui protège les applications contre les attaques par déni de service distribué. Bien qu'il fournisse des fonctionnalités de sécurité importantes, il ne traite pas spécifiquement des vulnérabilités d'injection SQL ou de cross-site scripting.",
            "Amazon Macie est un service de sécurité des données et de confidentialité qui utilise l'apprentissage automatique pour découvrir, classer et protéger les données sensibles stockées dans AWS. Il n'est pas conçu pour protéger les applications web contre les attaques basées sur le web.",
            "Amazon GuardDuty est un service de détection des menaces qui surveille en continu les activités malveillantes et les comportements non autorisés pour protéger les comptes et les charges de travail AWS. Bien qu'il améliore la sécurité globale, il ne fournit pas spécifiquement de protection contre les attaques par injection SQL ou cross-site scripting."
        ]
    },
    {
        "Question Number": "9",
        "Situation": "Une entreprise déploie une application web multi-niveaux sur AWS. L'application se compose d'une couche frontale sur des instances Amazon EC2 et d'une base de données backend sur Amazon RDS. L'entreprise exige que la base de données ne soit pas directement accessible depuis Internet et que seule la couche frontale puisse communiquer avec la base de données.",
        "Question": "Quelle configuration réseau le solutions architecte devrait-il mettre en œuvre ?",
        "Options": {
            "1": "Placer à la fois les couches frontale et base de données dans un sous-réseau public et utiliser des groupes de sécurité pour restreindre l'accès.",
            "2": "Placer la couche frontale dans un sous-réseau public et la couche de base de données dans un sous-réseau privé. Configurer les groupes de sécurité pour permettre uniquement aux instances frontales de communiquer avec la base de données.",
            "3": "Placer les deux couches dans des sous-réseaux privés et utiliser une passerelle NAT pour l'accès Internet.",
            "4": "Utiliser une passerelle Internet et des tables de routage pour contrôler l'accès entre les couches frontale et base de données."
        },
        "Correct Answer": "Placer la couche frontale dans un sous-réseau public et la couche de base de données dans un sous-réseau privé. Configurer les groupes de sécurité pour permettre uniquement aux instances frontales de communiquer avec la base de données.",
        "Explanation": "Cette configuration garantit que la base de données n'est pas directement accessible depuis Internet, car elle se trouve dans un sous-réseau privé. La couche frontale, qui se trouve dans un sous-réseau public, peut communiquer avec la base de données via des groupes de sécurité qui autorisent le trafic uniquement depuis les instances frontales. Cette configuration respecte les meilleures pratiques en matière de sécurité et d'architecture dans AWS, garantissant que la base de données est protégée contre l'accès externe tout en étant accessible à la couche d'application qui en a besoin.",
        "Other Options": [
            "Placer à la fois les couches frontale et base de données dans un sous-réseau public expose la base de données à Internet, ce qui viole l'exigence selon laquelle la base de données ne doit pas être directement accessible depuis Internet.",
            "Bien que placer les deux couches dans des sous-réseaux privés améliore la sécurité, cela n'autorise pas la couche frontale à communiquer avec la base de données à moins que des configurations supplémentaires (comme une passerelle NAT) ne soient mises en œuvre, ce qui est inutile dans ce scénario puisque la couche frontale doit être publique.",
            "Utiliser une passerelle Internet et des tables de routage pour contrôler l'accès exposerait la base de données à Internet, ce qui contredirait l'exigence de garder la base de données inaccessible depuis Internet."
        ]
    },
    {
        "Question Number": "10",
        "Situation": "Une plateforme de commerce électronique souhaite migrer sa base de données vers AWS mais veut minimiser les changements de code. Leur base de données sur site existante est PostgreSQL, et ils ont besoin d'une solution gérée qui prend en charge la haute disponibilité et l'évolutivité en lecture.",
        "Question": "Quel moteur de base de données sur AWS répondrait le mieux à ces exigences ?",
        "Options": {
            "1": "Amazon DynamoDB",
            "2": "Amazon Aurora avec compatibilité PostgreSQL",
            "3": "Amazon RDS pour MySQL",
            "4": "Amazon DocumentDB"
        },
        "Correct Answer": "Amazon Aurora avec compatibilité PostgreSQL",
        "Explanation": "Amazon Aurora avec compatibilité PostgreSQL est le meilleur choix pour migrer d'une base de données PostgreSQL sur site car il est conçu pour être compatible avec PostgreSQL, ce qui signifie qu'il nécessite des changements de code minimes lors de la migration. Aurora offre également une haute disponibilité grâce à ses déploiements multi-AZ et des capacités d'évolutivité en lecture avec des réplicas de lecture, ce qui le rend adapté aux plateformes de commerce électronique nécessitant des performances fiables et une évolutivité.",
        "Other Options": [
            "Amazon DynamoDB est un service de base de données NoSQL qui ne prend pas en charge les requêtes SQL ni les fonctionnalités PostgreSQL sur lesquelles l'application existante s'appuie probablement. Migrer vers DynamoDB nécessiterait des changements de code significatifs et une réarchitecture complète de l'application.",
            "Amazon RDS pour MySQL est un service de base de données relationnelle géré, mais il est basé sur MySQL, pas sur PostgreSQL. Migrer vers RDS pour MySQL nécessiterait des changements de code substantiels pour adapter l'application à la syntaxe et aux fonctionnalités de MySQL, ce qui n'est pas idéal pour minimiser les changements de code.",
            "Amazon DocumentDB est un service de base de données de documents géré qui est compatible avec MongoDB. Comme DynamoDB, il n'est pas compatible avec PostgreSQL et nécessiterait une refonte complète du modèle de données et du code de l'application, ce qui le rend inadapté à ce scénario de migration."
        ]
    },
    {
        "Question Number": "11",
        "Situation": "Une entreprise prévoit d'utiliser Amazon Aurora pour une solution de base de données hautement disponible. Elle souhaite s'assurer d'avoir des performances de lecture rapides et une disponibilité améliorée sans avoir à gérer la provision de stockage.",
        "Question": "Quelles fonctionnalités d'Amazon Aurora le rendent adapté à cette exigence, et comment son architecture diffère-t-elle de celle de RDS standard ? (Choisissez deux.)",
        "Options": {
            "1": "Aurora utilise un volume de cluster partagé à travers plusieurs zones de disponibilité (AZ) avec un stockage basé sur SSD, permettant des IOPS élevés et une faible latence. Il inclut un point de terminaison de cluster pour les opérations d'écriture et des points de terminaison de lecture pour distribuer le trafic de lecture entre les réplicas, ce qui améliore les performances de lecture.",
            "2": "Aurora nécessite un stockage local sur chaque instance, donc le stockage doit être provisionné et géré séparément, permettant un meilleur contrôle sur la distribution des données.",
            "3": "Aurora évolue automatiquement verticalement au sein d'une seule AZ, sans avoir besoin de plusieurs instances ou réplicas, garantissant une haute disponibilité avec une configuration minimale.",
            "4": "Aurora repose sur une gestion manuelle du stockage, où l'instance principale doit gérer à la fois le trafic de lecture et d'écriture, ce qui le rend adapté uniquement aux petites bases de données avec de faibles exigences en I/O.",
            "5": "L'architecture d'Aurora sépare le calcul et le stockage, permettant une mise à l'échelle indépendante de chacun, et fournit une tolérance aux pannes intégrée en répliquant les données à travers plusieurs AZ."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Aurora utilise un volume de cluster partagé à travers plusieurs zones de disponibilité (AZ) avec un stockage basé sur SSD, permettant des IOPS élevés et une faible latence. Il inclut un point de terminaison de cluster pour les opérations d'écriture et des points de terminaison de lecture pour distribuer le trafic de lecture entre les réplicas, ce qui améliore les performances de lecture.",
            "L'architecture d'Aurora sépare le calcul et le stockage, permettant une mise à l'échelle indépendante de chacun, et fournit une tolérance aux pannes intégrée en répliquant les données à travers plusieurs AZ."
        ],
        "Explanation": "Amazon Aurora est conçu pour une haute disponibilité et durabilité. Il utilise un volume de cluster partagé qui s'étend sur plusieurs zones de disponibilité, chaque AZ ayant une copie de la base de données. Cette architecture permet des IOPS élevés et une faible latence, ce qui améliore les performances de lecture. Aurora sépare également le calcul et le stockage, ce qui permet à chacun de se mettre à l'échelle indépendamment. Cette séparation fournit également une tolérance aux pannes intégrée en répliquant les données à travers plusieurs AZ.",
        "Other Options": [
            "Aurora ne nécessite pas de stockage local sur chaque instance. Au lieu de cela, il utilise un volume de stockage partagé qui s'étend sur plusieurs AZ. Par conséquent, le stockage n'a pas besoin d'être provisionné et géré séparément.",
            "Aurora ne s'évolue pas automatiquement verticalement au sein d'une seule AZ. Au lieu de cela, il utilise une architecture distribuée qui s'étend sur plusieurs AZ. Cette architecture permet une haute disponibilité et une tolérance aux pannes.",
            "Aurora ne repose pas sur une gestion manuelle du stockage. Au lieu de cela, il gère automatiquement le stockage, le faisant évoluer vers le haut et vers le bas selon les besoins. L'instance principale n'a pas à gérer à la fois le trafic de lecture et d'écriture, car Aurora fournit un point de terminaison de cluster pour les opérations d'écriture et des points de terminaison de lecture pour les opérations de lecture."
        ]
    },
    {
        "Question Number": "12",
        "Situation": "Une application de médias sociaux stocke les publications des utilisateurs et doit optimiser sa base de données pour des opérations de lecture à fort volume et des mises à jour fréquentes. L'application nécessite également des analyses en temps réel sur l'engagement des utilisateurs.",
        "Question": "Quelle solution de base de données le spécialiste des solutions devrait-il recommander pour gérer efficacement les modèles d'accès mixtes ?",
        "Options": {
            "1": "Amazon RDS pour PostgreSQL avec des réplicas de lecture et Amazon Redshift pour l'analyse.",
            "2": "Amazon DynamoDB avec capacité provisionnée et DynamoDB Streams intégré avec AWS Lambda pour le traitement en temps réel.",
            "3": "Amazon Aurora Serverless avec configuration multi-maître pour gérer les opérations de lecture et d'écriture.",
            "4": "Amazon S3 avec Amazon Athena pour les requêtes et Amazon Kinesis pour l'analyse en temps réel."
        },
        "Correct Answer": "Amazon DynamoDB avec capacité provisionnée et DynamoDB Streams intégré avec AWS Lambda pour le traitement en temps réel.",
        "Explanation": "Amazon DynamoDB est un service de base de données NoSQL entièrement géré qui offre des performances élevées pour les opérations de lecture et d'écriture, ce qui le rend idéal pour les applications avec des modèles d'accès mixtes. Sa capacité provisionnée permet de s'adapter aux besoins de l'application, garantissant qu'elle peut gérer efficacement des opérations de lecture à fort volume. De plus, DynamoDB Streams peut être utilisé pour capturer les changements apportés aux éléments de la base de données, ce qui peut ensuite déclencher des fonctions AWS Lambda pour le traitement et l'analyse en temps réel sur l'engagement des utilisateurs. Cette combinaison permet à la fois un stockage de données efficace et des analyses en temps réel, répondant ainsi efficacement aux exigences de l'application.",
        "Other Options": [
            "Amazon RDS pour PostgreSQL avec des réplicas de lecture et Amazon Redshift pour l'analyse n'est pas le meilleur choix car bien que RDS puisse gérer les opérations de lecture avec des réplicas de lecture, il peut ne pas évoluer aussi efficacement pour des opérations d'écriture à fort volume par rapport à DynamoDB. De plus, l'utilisation de Redshift pour l'analyse introduit une latence, car il est optimisé pour le traitement par lots plutôt que pour l'analyse en temps réel.",
            "Amazon Aurora Serverless avec configuration multi-maître pourrait gérer les opérations de lecture et d'écriture, mais il peut ne pas fournir le même niveau d'évolutivité et de performances pour des modèles d'accès à fort volume que DynamoDB. Aurora est également plus adapté aux données relationnelles et peut ne pas être aussi efficace pour l'analyse en temps réel par rapport à l'intégration de DynamoDB avec Lambda.",
            "Amazon S3 avec Amazon Athena pour les requêtes et Amazon Kinesis pour l'analyse en temps réel n'est pas adapté car S3 est principalement un service de stockage et ne prend pas en charge efficacement les opérations d'écriture à haute fréquence. Bien que Kinesis puisse gérer des flux de données en temps réel, la combinaison ne fournit pas une solution robuste pour des modèles d'accès mixtes comme le fait DynamoDB."
        ]
    },
    {
        "Question Number": "13",
        "Situation": "Une grande entreprise avec plusieurs départements utilise des comptes AWS séparés pour chaque unité commerciale et souhaite surveiller et contrôler les coûts liés au réseau. Ils ont besoin d'un moyen d'identifier et d'allouer les dépenses réseau, telles que les coûts de VPC, de passerelle NAT et de transfert de données, aux départements appropriés afin d'assurer une distribution précise des coûts et une responsabilité au sein de l'organisation.",
        "Question": "Quelle fonctionnalité de gestion des coûts AWS les aiderait le mieux à atteindre cet objectif ?",
        "Options": {
            "1": "Activer les balises d'allocation des coûts pour les ressources réseau, en attribuant des balises par département pour allouer avec précision les coûts liés au réseau",
            "2": "Configurer des Clouds Privés Virtuels (VPC) séparés pour chaque département et surveiller les coûts de chaque VPC individuellement",
            "3": "Utiliser AWS Trusted Advisor pour surveiller régulièrement et optimiser l'utilisation du réseau et obtenir des recommandations pour des économies de coûts",
            "4": "Établir différentes Zones de Disponibilité pour chaque département afin de suivre les coûts de transfert de données par zone"
        },
        "Correct Answer": "Activer les balises d'allocation des coûts pour les ressources réseau, en attribuant des balises par département pour allouer avec précision les coûts liés au réseau",
        "Explanation": "L'activation des balises d'allocation des coûts pour les ressources réseau permet à l'entreprise de catégoriser et de suivre les coûts associés à des départements spécifiques. En attribuant des balises aux ressources telles que les VPC, les passerelles NAT et le transfert de données, l'organisation peut générer des rapports de coûts détaillés qui reflètent les dépenses engagées par chaque département. Cette méthode fournit un moyen clair et organisé d'allouer les coûts liés au réseau, garantissant responsabilité et transparence au sein des unités commerciales.",
        "Other Options": [
            "Configurer des Clouds Privés Virtuels (VPC) séparés pour chaque département peut aider à isoler les ressources, mais cela ne fournit pas en soi un mécanisme pour suivre et allouer les coûts. Sans balisage ou stratégie de gestion des coûts, il serait difficile de distribuer avec précision les coûts entre les départements.",
            "Utiliser AWS Trusted Advisor peut fournir des informations et des recommandations pour optimiser l'utilisation des ressources et réaliser des économies de coûts, mais cela n'alloue pas directement les coûts à des départements spécifiques. Cela se concentre davantage sur les meilleures pratiques et l'optimisation des coûts plutôt que sur le suivi et l'allocation détaillés des coûts.",
            "Établir différentes Zones de Disponibilité pour chaque département ne corrèle pas directement avec le suivi des coûts de transfert de données. Les Zones de Disponibilité concernent principalement la redondance et la disponibilité plutôt que l'allocation des coûts. Les coûts de transfert de données sont généralement engagés en fonction des ressources utilisées et de leurs configurations, et non des zones elles-mêmes."
        ]
    },
    {
        "Question Number": "14",
        "Situation": "Une startup développe un tableau de bord en temps réel qui affiche des métriques en direct provenant de divers appareils IoT. Le tableau de bord nécessite une ingestion rapide des données et un accès à faible latence aux dernières métriques pour garantir des mises à jour en temps opportun. La solution doit également gérer des volumes de données variables à mesure que le nombre d'appareils augmente.",
        "Question": "Quel service AWS le concepteur de solutions devrait-il utiliser pour répondre à ces exigences de taille et de vitesse ? (Choisissez deux.)",
        "Options": {
            "1": "Amazon S3 avec Amazon Athena",
            "2": "Amazon Kinesis Data Streams",
            "3": "AWS Batch avec des Instances Spot Amazon EC2",
            "4": "Amazon RDS avec des réplicas en lecture",
            "5": "Amazon DynamoDB avec DynamoDB Streams"
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Amazon Kinesis Data Streams",
            "Amazon DynamoDB avec DynamoDB Streams"
        ],
        "Explanation": "Amazon Kinesis Data Streams est conçu pour le streaming de données en temps réel. Il peut capturer en continu des gigaoctets de données par seconde provenant de centaines de milliers de sources, ce qui en fait un bon choix pour gérer l'ingestion rapide des données et l'accès à faible latence requis par le tableau de bord. Amazon DynamoDB avec DynamoDB Streams est également un bon choix car il fournit un accès à faible latence aux données et peut gérer des charges de trafic élevées, ce qui est utile lorsque le nombre d'appareils augmente. DynamoDB Streams capture une séquence ordonnée dans le temps des modifications au niveau des éléments dans n'importe quelle table DynamoDB et stocke ces données pendant 24 heures.",
        "Other Options": [
            "Amazon S3 avec Amazon Athena : Cette combinaison est plus adaptée pour stocker et interroger de grands ensembles de données, pas pour l'ingestion de données en temps réel et l'accès à faible latence.",
            "AWS Batch avec des Instances Spot Amazon EC2 : Cela est plus adapté pour des travaux de traitement par lots et non pour l'ingestion de données en temps réel et l'accès à faible latence.",
            "Amazon RDS avec des réplicas en lecture : Bien que cela puisse aider à distribuer le trafic de lecture, ce n'est pas conçu pour l'ingestion de données en temps réel ou pour gérer des volumes de données variables provenant potentiellement de milliers d'appareils."
        ]
    },
    {
        "Question Number": "15",
        "Situation": "Une application de médias sociaux reçoit un grand volume de requêtes de lecture, les utilisateurs récupérant fréquemment des informations de profil et des fils d'actualités. L'application rencontre actuellement des problèmes de latence car elle interroge directement une base de données Amazon Aurora pour chaque requête de lecture. L'équipe de développement souhaite améliorer les performances de lecture et réduire la charge sur la base de données de manière rentable, et elle est ouverte à apporter de légers changements à l'application.",
        "Question": "Quelle solution le concepteur de solutions devrait-il recommander ?",
        "Options": {
            "1": "Mettre en œuvre Amazon ElastiCache avec Redis pour mettre en cache les données fréquemment consultées et réduire les requêtes à la base de données",
            "2": "Activer des réplicas en lecture sur la base de données Amazon Aurora pour distribuer la charge de lecture",
            "3": "Utiliser Amazon RDS Proxy pour regrouper et partager les connexions à la base de données afin d'améliorer les performances",
            "4": "Stocker les données fréquemment consultées dans Amazon S3 et y accéder directement depuis l'application"
        },
        "Correct Answer": "Mettre en œuvre Amazon ElastiCache avec Redis pour mettre en cache les données fréquemment consultées et réduire les requêtes à la base de données",
        "Explanation": "Mettre en œuvre Amazon ElastiCache avec Redis est la solution la plus efficace pour améliorer les performances de lecture et réduire la charge sur la base de données Amazon Aurora. En mettant en cache les données fréquemment consultées, telles que les profils d'utilisateurs et les fils d'actualités, l'application peut servir les requêtes de lecture directement depuis le cache au lieu d'interroger la base de données pour chaque requête. Cela réduit considérablement la latence et la charge sur la base de données, entraînant des économies de coûts et une meilleure expérience utilisateur. ElastiCache est conçu pour une récupération rapide des données, ce qui le rend idéal pour les applications avec un volume élevé de requêtes de lecture.",
        "Other Options": [
            "Activer des réplicas en lecture sur la base de données Amazon Aurora peut aider à distribuer la charge de lecture, mais cela ne résout pas les problèmes de latence aussi efficacement que le caching. Les réplicas en lecture peuvent toujours engendrer des coûts et ne peuvent pas fournir les améliorations de performances immédiates nécessaires pour des requêtes de lecture à volume élevé.",
            "Utiliser Amazon RDS Proxy pour regrouper et partager les connexions à la base de données peut améliorer les performances en réduisant le coût de l'établissement des connexions, mais cela ne réduit pas directement le nombre de requêtes de lecture envoyées à la base de données. Cette option peut aider à la gestion des connexions mais ne résout pas le problème de latence sous-jacent causé par un volume élevé de requêtes de lecture.",
            "Stocker les données fréquemment consultées dans Amazon S3 et y accéder directement depuis l'application n'est pas idéal pour la récupération de données en temps réel, car S3 est conçu pour le stockage d'objets et peut introduire une latence supplémentaire. Cette approche est plus adaptée pour du contenu statique plutôt que pour des données dynamiques nécessitant des mises à jour fréquentes, ce qui la rend moins efficace pour les besoins de l'application."
        ]
    },
    {
        "Question Number": "16",
        "Situation": "Une entreprise financière nécessite une solution de stockage de fichiers entièrement gérée sur AWS qui peut supporter un haut IOPS, une faible latence et des fonctionnalités natives du système de fichiers Windows pour stocker et traiter des données sensibles des clients. Le système doit fournir un accès sécurisé via SMB et s'intégrer à l'Active Directory local de l'entreprise pour l'authentification des utilisateurs.",
        "Question": "Quelle configuration de service AWS répondrait le mieux à ces exigences ? (Choisissez deux.)",
        "Options": {
            "1": "Amazon S3 avec Transfer Acceleration pour un accès à haute vitesse",
            "2": "Amazon FSx for Windows File Server dans un déploiement Multi-AZ",
            "3": "Amazon EFS avec chiffrement au repos et en transit",
            "4": "AWS Storage Gateway avec Cached Volumes",
            "5": "Amazon FSx for NetApp ONTAP avec intégration Active Directory"
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Amazon FSx for Windows File Server dans un déploiement Multi-AZ",
            "AWS Storage Gateway avec Cached Volumes"
        ],
        "Explanation": "Amazon FSx for Windows File Server dans un déploiement Multi-AZ est un système de fichiers natif Microsoft Windows entièrement géré qui peut supporter un haut IOPS, une faible latence et des fonctionnalités natives du système de fichiers Windows. Il fournit également un accès sécurisé via SMB et s'intègre à l'Active Directory local pour l'authentification des utilisateurs, ce qui répond à toutes les exigences énoncées. AWS Storage Gateway avec Cached Volumes peut être utilisé pour fournir un accès à faible latence aux données dans AWS depuis des applications locales en stockant les données fréquemment consultées localement tout en conservant toutes les données dans Amazon S3. Il prend également en charge l'intégration avec l'Active Directory local pour l'authentification des utilisateurs.",
        "Other Options": [
            "Amazon S3 avec Transfer Acceleration pour un accès à haute vitesse ne prend pas en charge les fonctionnalités natives du système de fichiers Windows et le protocole SMB. Il ne s'intègre pas non plus à l'Active Directory local pour l'authentification des utilisateurs.",
            "Amazon EFS avec chiffrement au repos et en transit est un système de fichiers entièrement géré qui n'est pas conçu pour un haut IOPS, une faible latence, et ne prend pas en charge les fonctionnalités natives du système de fichiers Windows ou le protocole SMB.",
            "Amazon FSx for NetApp ONTAP avec intégration Active Directory est un service de système de fichiers entièrement géré qui prend en charge le protocole SMB et s'intègre à l'Active Directory local, mais il ne prend pas en charge les fonctionnalités natives du système de fichiers Windows."
        ]
    },
    {
        "Question Number": "17",
        "Situation": "Une entreprise exécute une application web sur des instances EC2 derrière un Application Load Balancer (ALB). L'application doit router le trafic en fonction des chemins d'URL, avec des services spécifiques gérant certains types de requêtes. Ils souhaitent également s'assurer que le trafic est réparti de manière uniforme entre les instances pour éviter qu'une seule instance ne soit surchargée pendant les périodes de fort trafic.",
        "Question": "Quelle configuration l'entreprise devrait-elle appliquer pour atteindre un équilibrage de charge efficace ?",
        "Options": {
            "1": "Configurer l'ALB avec un routage basé sur le chemin pour diriger le trafic vers différents groupes cibles en fonction des chemins d'URL, en veillant à ce que le trafic soit équilibré uniformément entre les instances EC2 dans chaque groupe.",
            "2": "Configurer l'ALB pour router tout le trafic vers une seule instance EC2 pour la simplicité, mais utiliser l'Auto Scaling pour augmenter la taille de l'instance pendant les périodes de fort trafic.",
            "3": "Utiliser un Classic Load Balancer (CLB) au lieu de l'ALB pour prendre en charge le routage basé sur le chemin et distribuer le trafic en fonction de plusieurs points de terminaison d'application.",
            "4": "Mettre en place plusieurs ALB, chacun servant le trafic pour un domaine d'application différent, et diriger manuellement le trafic vers chaque ALB en fonction des modèles de trafic."
        },
        "Correct Answer": "Configurer l'ALB avec un routage basé sur le chemin pour diriger le trafic vers différents groupes cibles en fonction des chemins d'URL, en veillant à ce que le trafic soit équilibré uniformément entre les instances EC2 dans chaque groupe.",
        "Explanation": "Configurer l'ALB avec un routage basé sur le chemin permet à l'entreprise de diriger le trafic vers différents groupes cibles en fonction des chemins d'URL des requêtes entrantes. Cela signifie que des services spécifiques peuvent gérer des types de requêtes spécifiques, ce qui est essentiel pour l'architecture de l'application. De plus, l'ALB équilibre automatiquement le trafic entre les instances EC2 dans chaque groupe cible, garantissant qu'aucune instance unique ne soit surchargée pendant les périodes de fort trafic. Cette configuration est optimale pour gérer le trafic efficacement et maintenir la performance de l'application.",
        "Other Options": [
            "Configurer l'ALB pour router tout le trafic vers une seule instance EC2 n'est pas une solution viable pour l'équilibrage de charge, car cela va à l'encontre de l'objectif d'utiliser un équilibreur de charge. Cela entraînerait une surcharge potentielle sur cette instance unique, surtout pendant les périodes de fort trafic, et ne tirerait pas parti des avantages d'avoir plusieurs instances.",
            "Utiliser un Classic Load Balancer (CLB) au lieu de l'ALB est incorrect car les CLB ne prennent pas en charge le routage basé sur le chemin. Les ALB sont spécifiquement conçus pour des fonctionnalités de routage avancées, y compris le routage basé sur le chemin, ce qui est nécessaire pour l'exigence de l'entreprise de diriger le trafic en fonction des chemins d'URL.",
            "Mettre en place plusieurs ALB pour différents domaines d'application et diriger manuellement le trafic vers chaque ALB ajoute une complexité inutile à l'architecture. Il serait plus efficace d'utiliser un seul ALB avec un routage basé sur le chemin pour gérer le trafic pour plusieurs services, ce qui simplifie la configuration et réduit les frais généraux opérationnels."
        ]
    },
    {
        "Question Number": "18",
        "Situation": "Une organisation utilise AWS Organizations et souhaite mettre en œuvre une limite de permissions sur plusieurs comptes pour empêcher certaines actions, même pour les utilisateurs ayant un accès administratif complet. L'organisation souhaite également maintenir les frais généraux administratifs à un niveau bas.",
        "Question": "Quel type d'architecture de Service Control Policy (SCP) répondrait le mieux à ces exigences, et quel effet cela aura-t-il sur les permissions des utilisateurs IAM au sein de l'organisation ?",
        "Options": {
            "1": "Utiliser une architecture de liste d'autorisation pour autoriser explicitement uniquement des services spécifiques, limitant toutes les autres actions pour une meilleure sécurité et un meilleur contrôle.",
            "2": "Utiliser une architecture de liste de refus pour refuser des actions spécifiques, permettant toutes les autres actions par défaut, ce qui minimise les frais généraux de gestion.",
            "3": "Utiliser une architecture de liste de refus pour refuser explicitement toutes les actions, nécessitant l'ajout manuel de permissions pour chaque service nécessaire.",
            "4": "Utiliser une architecture de liste d'autorisation pour autoriser des actions uniquement pour l'utilisateur root, bloquant les permissions pour tous les utilisateurs IAM au sein de l'organisation."
        },
        "Correct Answer": "Utiliser une architecture de liste de refus pour refuser des actions spécifiques, permettant toutes les autres actions par défaut, ce qui minimise les frais généraux de gestion.",
        "Explanation": "Une architecture de liste de refus est efficace dans ce scénario car elle permet à l'organisation de spécifier uniquement les actions qui doivent être refusées, tandis que toutes les autres actions restent autorisées par défaut. Cette approche minimise les frais généraux administratifs puisque l'organisation n'a pas besoin de gérer une liste exhaustive d'actions autorisées. Au lieu de cela, elle peut se concentrer sur l'identification et le refus uniquement des actions spécifiques qui posent un risque, maintenant ainsi la flexibilité pour les utilisateurs IAM d'effectuer leurs tâches sans restrictions inutiles.",
        "Other Options": [
            "Utiliser une architecture de liste d'autorisation nécessiterait que l'organisation définisse et autorise explicitement uniquement des services spécifiques, ce qui pourrait entraîner une augmentation des frais généraux administratifs car elle devrait continuellement mettre à jour la liste des services autorisés chaque fois que de nouveaux services sont introduits ou que des services existants doivent être modifiés.",
            "Une architecture de liste de refus qui refuse explicitement toutes les actions serait trop restrictive et impraticable, car elle exigerait que l'organisation ajoute manuellement des permissions pour chaque service nécessaire. Cela créerait des frais généraux de gestion significatifs et pourrait nuire à la productivité, car les utilisateurs seraient bloqués dans l'exécution d'actions nécessaires à moins d'être explicitement autorisés.",
            "Utiliser une architecture de liste d'autorisation pour autoriser des actions uniquement pour l'utilisateur root bloquerait effectivement les permissions pour tous les utilisateurs IAM au sein de l'organisation, ce qui contredit l'exigence d'autoriser les utilisateurs ayant un accès administratif à effectuer leurs tâches. Cela ne répondrait pas à l'objectif de l'organisation de mettre en œuvre une limite de permissions tout en permettant des actions nécessaires pour les utilisateurs IAM."
        ]
    },
    {
        "Question Number": "19",
        "Situation": "Une entreprise utilise AWS Key Management Service (KMS) pour sécuriser des données sensibles. L'entreprise souhaite s'assurer que les clés utilisées pour chiffrer ces données sont gérées et stockées en toute sécurité au sein d'AWS, sans jamais quitter l'environnement AWS.",
        "Question": "Quelle caractéristique d'AWS KMS garantit que les clés de chiffrement restent sécurisées et au sein de l'infrastructure AWS, et quel type de chiffrement prend-elle en charge ?",
        "Options": {
            "1": "Les clés KMS sont isolées dans une région KMS dédiée et ne prennent en charge que le chiffrement symétrique.",
            "2": "Les clés KMS ne quittent jamais AWS KMS et prennent en charge à la fois le chiffrement symétrique et asymétrique.",
            "3": "Les clés KMS peuvent être exportées d'AWS pour un usage externe et ne prennent en charge que le chiffrement asymétrique.",
            "4": "Les clés KMS sont partagées entre plusieurs comptes AWS et ne prennent en charge que le chiffrement symétrique."
        },
        "Correct Answer": "Les clés KMS ne quittent jamais AWS KMS et prennent en charge à la fois le chiffrement symétrique et asymétrique.",
        "Explanation": "AWS Key Management Service (KMS) est conçu pour gérer les clés de chiffrement de manière sécurisée au sein de l'environnement AWS. L'une de ses caractéristiques clés est que les clés de chiffrement ne sont jamais exposées en dehors de l'infrastructure AWS, garantissant ainsi leur sécurité. De plus, AWS KMS prend en charge à la fois le chiffrement symétrique (où la même clé est utilisée pour le chiffrement et le déchiffrement) et le chiffrement asymétrique (où une paire de clés est utilisée). Cette flexibilité permet aux utilisateurs de choisir la méthode de chiffrement appropriée en fonction de leurs exigences de sécurité.",
        "Other Options": [
            "Les clés KMS sont isolées dans une région KMS dédiée et ne prennent en charge que le chiffrement symétrique. Cette option est incorrecte car bien que les clés KMS soient effectivement spécifiques à une région, elles prennent en charge à la fois le chiffrement symétrique et asymétrique, pas seulement le symétrique.",
            "Les clés KMS peuvent être exportées d'AWS pour un usage externe et ne prennent en charge que le chiffrement asymétrique. Cette option est incorrecte car les clés KMS ne peuvent pas être exportées pour un usage externe ; elles sont conçues pour rester au sein d'AWS. De plus, KMS prend en charge à la fois le chiffrement symétrique et asymétrique, pas seulement asymétrique.",
            "Les clés KMS sont partagées entre plusieurs comptes AWS et ne prennent en charge que le chiffrement symétrique. Cette option est incorrecte car bien que les clés KMS puissent être partagées entre comptes via des politiques de ressources, elles prennent en charge à la fois le chiffrement symétrique et asymétrique, pas seulement le symétrique."
        ]
    },
    {
        "Question Number": "20",
        "Situation": "Une entreprise a déployé un Application Load Balancer (ALB) à travers plusieurs zones de disponibilité (AZ) et a activé l'équilibrage de charge inter-zones pour distribuer le trafic entrant.",
        "Question": "Comment l'équilibrage de charge inter-zones améliore-t-il la distribution de la charge, et quel avantage offre-t-il pour gérer les pics de trafic dans une AZ ?",
        "Options": {
            "1": "L'équilibrage de charge inter-zones permet à chaque nœud de l'équilibreur de charge de router le trafic uniquement vers des cibles au sein de sa propre AZ, offrant ainsi une isolation et une résilience en cas de défaillance de l'AZ.",
            "2": "L'équilibrage de charge inter-zones permet à chaque nœud de l'équilibreur de charge de router le trafic de manière équilibrée entre les cibles dans toutes les AZ, garantissant une distribution de charge plus équilibrée et réduisant le risque de surcharge des cibles dans une AZ.",
            "3": "L'équilibrage de charge inter-zones route le trafic vers une seule cible par demande, réduisant la latence et améliorant les performances pour les utilisateurs dans chaque AZ.",
            "4": "L'équilibrage de charge inter-zones n'est efficace que dans des configurations à une seule AZ et n'a aucun impact lorsque plusieurs AZ sont impliquées."
        },
        "Correct Answer": "L'équilibrage de charge inter-zones permet à chaque nœud de l'équilibreur de charge de router le trafic de manière équilibrée entre les cibles dans toutes les AZ, garantissant une distribution de charge plus équilibrée et réduisant le risque de surcharge des cibles dans une AZ.",
        "Explanation": "L'équilibrage de charge inter-zones permet à l'Application Load Balancer de distribuer le trafic entrant de manière uniforme entre toutes les cibles enregistrées dans différentes zones de disponibilité, plutôt que seulement vers les cibles dans la même AZ que le nœud de l'équilibreur de charge. Cela signifie que si une AZ connaît un pic de trafic, l'équilibreur de charge peut diriger le trafic vers des cibles dans d'autres AZ, empêchant ainsi qu'une seule AZ ne devienne un goulot d'étranglement. Cette capacité améliore la résilience et les performances globales de l'application, en particulier lors des pics de trafic.",
        "Other Options": [
            "L'équilibrage de charge inter-zones permet à chaque nœud de l'équilibreur de charge de router le trafic uniquement vers des cibles au sein de sa propre AZ, offrant ainsi une isolation et une résilience en cas de défaillance de l'AZ. C'est incorrect car l'équilibrage de charge inter-zones permet spécifiquement de router le trafic à travers plusieurs AZ, ce qui est l'opposé de router uniquement au sein d'une seule AZ.",
            "Cette option est incorrecte car elle déforme la fonctionnalité de l'équilibrage de charge inter-zones. Bien qu'il vise à équilibrer la charge, il le fait en distribuant le trafic entre toutes les AZ, pas seulement en garantissant une distribution uniforme parmi les cibles d'une seule AZ.",
            "Cette option est incorrecte car l'équilibrage de charge inter-zones ne limite pas le trafic à une seule cible par demande. Au contraire, il distribue le trafic entre plusieurs cibles, ce qui aide à gérer la charge efficacement et à améliorer les performances."
        ]
    },
    {
        "Question Number": "21",
        "Situation": "Une plateforme de médias sociaux souhaite surveiller et analyser le contenu généré par les utilisateurs en temps réel pour détecter et répondre rapidement aux publications inappropriées. La plateforme a besoin d'une solution évolutive pour traiter des flux de données continus provenant de millions d'utilisateurs simultanément.",
        "Question": "Quels services AWS le concepteur de solutions devrait-il recommander pour le traitement des données en streaming dans ce scénario ? (Choisissez deux.)",
        "Options": {
            "1": "Amazon Simple Queue Service (SQS)",
            "2": "Amazon Kinesis Data Streams",
            "3": "Amazon Managed Streaming for Apache Kafka (MSK)",
            "4": "AWS Lambda avec des déclencheurs planifiés",
            "5": "Amazon EventBridge"
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Amazon Kinesis Data Streams",
            "Amazon Managed Streaming for Apache Kafka (MSK)"
        ],
        "Explanation": "Amazon Kinesis Data Streams est conçu pour collecter, traiter et analyser des données en streaming en temps réel afin que vous puissiez obtenir des informations en temps opportun et réagir rapidement à de nouvelles informations. Il peut gérer n'importe quelle quantité de données en streaming et traiter des données provenant de centaines de milliers de sources avec des latences très faibles. Amazon Managed Streaming for Apache Kafka (MSK) est un service entièrement géré qui facilite la création et l'exécution d'applications utilisant Apache Kafka pour traiter des données en streaming. Il est particulièrement adapté aux tâches de traitement de données en temps réel à fort volume.",
        "Other Options": [
            "Amazon Simple Queue Service (SQS) est un service de mise en file d'attente de messages entièrement géré qui vous permet de découpler et de faire évoluer des microservices, des systèmes distribués et des applications sans serveur. Cependant, il n'est pas conçu pour le traitement de données en streaming en temps réel.",
            "AWS Lambda avec des déclencheurs planifiés est un service de calcul qui vous permet d'exécuter du code sans provisionner ou gérer des serveurs. Bien que Lambda puisse traiter des changements de fichiers en temps réel, l'option 'déclencheurs planifiés' ne correspond pas à l'exigence de temps réel du scénario.",
            "Amazon EventBridge est un bus d'événements sans serveur qui facilite la connexion d'applications en utilisant des données provenant de vos propres applications, d'applications SaaS intégrées et de services AWS. Il n'est pas spécifiquement conçu pour le traitement de données en streaming en temps réel."
        ]
    },
    {
        "Question Number": "22",
        "Situation": "Une entreprise doit chiffrer de grands fichiers dépassant 4 Ko en utilisant AWS Key Management Service (KMS). Le processus de chiffrement doit impliquer à la fois une version en texte clair pour une utilisation immédiate et une version sécurisée à stocker avec les données chiffrées.",
        "Question": "Quelle fonctionnalité de KMS l'entreprise devrait-elle utiliser pour répondre à ces exigences, et comment gère-t-elle le chiffrement de données plus grandes que 4 Ko ?",
        "Options": {
            "1": "Utiliser la clé KMS directement pour chiffrer les données, car KMS prend en charge des fichiers de toute taille sans étapes supplémentaires.",
            "2": "Générer une clé de chiffrement de données (DEK) avec KMS, utiliser la DEK en texte clair pour chiffrer les données, et stocker la DEK chiffrée avec les données chiffrées.",
            "3": "Utiliser une clé KMS gérée par le client avec une politique personnalisée pour permettre le chiffrement de grands fichiers et maintenir à la fois des copies en texte clair et chiffrées.",
            "4": "Chiffrer les données directement dans KMS en les divisant en morceaux de 4 Ko, en chiffrant chaque morceau séparément, et en les réassemblant après déchiffrement."
        },
        "Correct Answer": "Générer une clé de chiffrement de données (DEK) avec KMS, utiliser la DEK en texte clair pour chiffrer les données, et stocker la DEK chiffrée avec les données chiffrées.",
        "Explanation": "AWS Key Management Service (KMS) a une limite de 4 Ko pour les opérations de chiffrement direct. Pour chiffrer des fichiers plus grands, l'approche recommandée est de générer une clé de chiffrement de données (DEK) en utilisant KMS. La DEK est ensuite utilisée pour chiffrer les données, permettant le chiffrement de fichiers plus grands que 4 Ko. La DEK en texte clair peut être utilisée pour le déchiffrement immédiat, tandis que la DEK chiffrée (chiffrée avec la clé KMS) est stockée avec les données chiffrées pour un accès sécurisé. Cette méthode garantit que le processus de chiffrement est efficace et évolutif pour les grands fichiers.",
        "Other Options": [
            "Utiliser la clé KMS directement pour chiffrer les données est incorrect car KMS a une limite de taille de 4 Ko pour les opérations de chiffrement. Les fichiers plus grands doivent être traités différemment, comme en utilisant une DEK.",
            "Bien que générer une DEK soit correct, l'option ne précise pas que la DEK doit être stockée sous forme chiffrée avec les données chiffrées. Cela est crucial pour maintenir la sécurité et permettre un déchiffrement ultérieur.",
            "Utiliser une clé KMS gérée par le client avec une politique personnalisée ne traite pas directement la limitation de taille du chiffrement KMS. La méthode de chiffrement de grands fichiers nécessite toujours l'utilisation d'une DEK, quelle que soit la politique de gestion des clés."
        ]
    },
    {
        "Question Number": "23",
        "Situation": "Une entreprise doit s'assurer que son environnement AWS respecte les meilleures pratiques de sécurité et les normes de conformité. L'entreprise souhaite une surveillance continue de ses ressources AWS pour détecter les vulnérabilités potentielles en matière de sécurité et garantir la conformité.",
        "Question": "Quels services AWS l'architecte de solutions devrait-il recommander ? (Choisissez deux.)",
        "Options": {
            "1": "AWS Config",
            "2": "Amazon GuardDuty",
            "3": "AWS Security Hub",
            "4": "AWS CloudTrail",
            "5": "AWS Shield Advanced"
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "AWS Security Hub",
            "Amazon GuardDuty"
        ],
        "Explanation": "AWS Security Hub fournit une vue d'ensemble complète de vos alertes de sécurité prioritaires et de l'état de conformité à travers les comptes AWS. Il agrège, organise et priorise vos alertes de sécurité, ou résultats, provenant de plusieurs services AWS, tels qu'Amazon GuardDuty, Amazon Inspector et Amazon Macie, ainsi que des solutions partenaires AWS. Amazon GuardDuty est un service de détection des menaces qui surveille en continu les activités malveillantes et les comportements non autorisés pour protéger vos comptes et charges de travail AWS. Il analyse des milliards d'événements provenant de plusieurs sources de données AWS, telles que les journaux d'événements AWS CloudTrail, les journaux de flux Amazon VPC et les journaux DNS.",
        "Other Options": [
            "AWS Config est un service qui vous permet d'évaluer, d'auditer et d'évaluer les configurations de vos ressources AWS. Il ne fournit pas de surveillance continue des vulnérabilités potentielles en matière de sécurité.",
            "AWS CloudTrail est un service qui permet la gouvernance, la conformité, l'audit opérationnel et l'audit des risques de votre compte AWS. Cependant, il ne fournit pas de surveillance continue des vulnérabilités potentielles en matière de sécurité.",
            "AWS Shield Advanced fournit une protection DDoS et une protection des coûts, mais ne fournit pas de surveillance continue des vulnérabilités potentielles en matière de sécurité."
        ]
    },
    {
        "Question Number": "24",
        "Situation": "Une entreprise de commerce électronique multinationale nécessite une solution de base de données hautement disponible qui offre un accès en lecture à faible latence aux clients dans plusieurs régions. Pour garantir la résilience et se protéger contre les pannes régionales, l'entreprise nécessite également une configuration de récupération après sinistre inter-régionale avec un impact minimal sur les performances de la base de données principale. De plus, elle a besoin d'une réplication quasi en temps réel vers les régions secondaires pour les mises à jour de données les plus rapides possibles.",
        "Question": "Quelle solution de base de données AWS répondrait le mieux à ces exigences ?",
        "Options": {
            "1": "Déployer Amazon RDS avec Multi-AZ pour améliorer la haute disponibilité au sein d'une seule région AWS",
            "2": "Utiliser Aurora Global Database pour activer des répliques de lecture inter-régionales, offrant un accès en lecture à faible latence et une réplication quasi en temps réel avec un impact minimal sur la base de données principale",
            "3": "Configurer les tables globales Amazon DynamoDB pour réaliser une réplication multi-régionale et un accès à faible latence pour les charges de travail NoSQL",
            "4": "Mettre en place Amazon Redshift avec des instantanés inter-régionaux pour créer une sauvegarde dans chaque région pour la récupération après sinistre"
        },
        "Correct Answer": "Utiliser Aurora Global Database pour activer des répliques de lecture inter-régionales, offrant un accès en lecture à faible latence et une réplication quasi en temps réel avec un impact minimal sur la base de données principale",
        "Explanation": "Aurora Global Database est spécifiquement conçu pour les applications ayant une portée mondiale qui nécessitent des lectures à faible latence et une haute disponibilité dans plusieurs régions. Il permet une réplication quasi en temps réel des données vers des régions secondaires, ce qui garantit que les clients dans ces régions peuvent accéder aux données rapidement et efficacement. De plus, il offre une résilience contre les pannes régionales, car la base de données peut basculer vers une région secondaire avec un impact minimal sur les performances de la base de données principale. Cela en fait la meilleure solution pour les exigences de l'entreprise en matière de haute disponibilité, d'accès à faible latence et de récupération après sinistre inter-régionale.",
        "Other Options": [
            "Déployer Amazon RDS avec Multi-AZ améliore la haute disponibilité au sein d'une seule région AWS mais ne fournit pas de réplication inter-régionale ni de capacités de récupération après sinistre. Par conséquent, cela ne répond pas à l'exigence de résilience contre les pannes régionales.",
            "Utiliser Aurora Global Database est le bon choix, donc cette option n'est pas applicable en tant qu'alternative. C'est la meilleure solution pour les exigences énoncées.",
            "Configurer les tables globales Amazon DynamoDB fournirait une réplication multi-régionale et un accès à faible latence, mais il est principalement adapté aux charges de travail NoSQL. Le scénario ne précise pas la nécessité d'une base de données NoSQL, et Aurora Global Database est une option plus appropriée pour les besoins de bases de données relationnelles avec les exigences spécifiées."
        ]
    },
    {
        "Question Number": "25",
        "Situation": "Une entreprise exécute une application web critique dans AWS et doit configurer des quotas de service pour gérer l'utilisation dans un environnement de secours. Elle souhaite s'assurer que sa charge de travail peut évoluer en fonction de la demande sans dépasser les limites de service, et elle souhaite également appliquer un throttling pour éviter les interruptions de service.",
        "Question": "Quelles étapes l'entreprise devrait-elle suivre pour gérer les quotas de service et le throttling pour l'environnement de secours ?",
        "Options": {
            "1": "Utiliser AWS Service Quotas pour définir des limites d'utilisation des services, et configurer AWS Lambda pour faire évoluer automatiquement les ressources en fonction de ces quotas tout en appliquant un throttling pour maintenir la stabilité du service.",
            "2": "Configurer des groupes Auto Scaling pour faire évoluer les instances EC2 en fonction de la charge de travail, et ajuster manuellement les quotas de service dans la console de gestion AWS pour gérer le trafic de pointe.",
            "3": "Utiliser Amazon API Gateway pour définir des limites de throttling sur les requêtes API, et configurer CloudWatch pour surveiller l'utilisation dans l'environnement de secours afin de s'assurer que les limites ne sont pas dépassées.",
            "4": "Utiliser Amazon SQS pour mettre en file d'attente les requêtes excessives et retarder le traitement pour éviter le throttling, tout en configurant AWS Lambda pour une mise à l'échelle automatique."
        },
        "Correct Answer": "Utiliser Amazon API Gateway pour définir des limites de throttling sur les requêtes API, et configurer CloudWatch pour surveiller l'utilisation dans l'environnement de secours afin de s'assurer que les limites ne sont pas dépassées.",
        "Explanation": "Utiliser Amazon API Gateway pour définir des limites de throttling est un moyen efficace de gérer le nombre de requêtes pouvant être traitées par l'application web, évitant ainsi les interruptions de service dues à une charge excessive. API Gateway vous permet de définir des plans d'utilisation qui peuvent limiter les requêtes et définir des quotas, garantissant que l'application reste stable sous des charges variées. De plus, l'intégration de CloudWatch pour la surveillance permet à l'entreprise de suivre les métriques d'utilisation en temps réel, facilitant la gestion proactive des limites de service et garantissant qu'elles ne dépassent pas les seuils définis.",
        "Other Options": [
            "Utiliser AWS Service Quotas pour définir des limites d'utilisation des services et configurer AWS Lambda pour une mise à l'échelle automatique ne traite pas directement du throttling pour les requêtes API. Bien que cela aide à gérer les limites de service, cela manque des capacités spécifiques de throttling qu'API Gateway fournit, qui sont cruciales pour maintenir la stabilité du service sous charge.",
            "Configurer des groupes Auto Scaling pour faire évoluer les instances EC2 est une bonne pratique pour gérer les augmentations de charge de travail, mais cela ne gère pas intrinsèquement les quotas de service ni n'applique de throttling. Ajuster manuellement les quotas de service peut entraîner des retards et des interruptions potentielles de service si cela n'est pas fait en temps réel, ce qui n'est pas idéal pour un environnement de secours qui doit répondre rapidement aux changements de demande.",
            "Utiliser Amazon SQS pour mettre en file d'attente les requêtes excessives est une approche valide pour gérer la charge, mais cela n'applique pas directement le throttling aux requêtes API. Bien que SQS puisse aider à éviter de surcharger les services backend, cela ne fournit pas le même niveau de contrôle sur les taux de requêtes qu'API Gateway, et cela peut introduire une latence dans le traitement des requêtes."
        ]
    },
    {
        "Question Number": "26",
        "Situation": "Une entreprise de santé, HealthSecure, est soumise à des réglementations strictes de conformité exigeant une surveillance continue et une documentation de la configuration de ses ressources cloud. HealthSecure a choisi AWS Config pour suivre et auditer les changements dans son environnement AWS afin d'assurer la conformité avec des normes telles que HIPAA. Elle a besoin d'une solution capable d'évaluer les ressources par rapport à des règles de conformité spécifiques et de remédier automatiquement aux ressources non conformes. Cependant, HealthSecure souhaite également comprendre les limitations d'AWS Config, en particulier s'il peut empêcher activement les changements de configuration ou s'il ne fournit que des capacités de surveillance et d'alerte.",
        "Question": "Comment AWS Config soutient-il la gestion de la conformité et le suivi de la configuration des ressources dans un compte AWS, et quelles sont certaines limitations associées à son fonctionnement ?",
        "Options": {
            "1": "AWS Config permet aux utilisateurs de suivre les changements de configuration à travers les ressources et empêche les changements non autorisés en appliquant la conformité en temps réel.",
            "2": "AWS Config surveille et enregistre les changements de configuration à travers les ressources prises en charge, permet l'audit pour les normes de conformité, et peut automatiquement remédier aux ressources non conformes grâce à l'intégration avec AWS Lambda. Cependant, il ne prévient pas activement les changements.",
            "3": "AWS Config ne fournit que des instantanés de configuration à des intervalles spécifiques, ce qui limite son efficacité pour la gestion de la conformité, car la surveillance en temps réel n'est pas prise en charge.",
            "4": "AWS Config fonctionne uniquement dans une seule région et ne peut pas agréger des données à travers plusieurs comptes, ce qui le rend adapté uniquement aux environnements isolés où les ressources restent statiques."
        },
        "Correct Answer": "AWS Config surveille et enregistre les changements de configuration à travers les ressources prises en charge, permet l'audit pour les normes de conformité, et peut automatiquement remédier aux ressources non conformes grâce à l'intégration avec AWS Lambda. Cependant, il ne prévient pas activement les changements.",
        "Explanation": "AWS Config est conçu pour fournir une surveillance continue des configurations des ressources AWS et pour suivre les changements au fil du temps. Il permet aux utilisateurs d'évaluer leurs ressources par rapport aux règles de conformité et peut déclencher des actions de remédiation via AWS Lambda lorsque des configurations non conformes sont détectées. Cependant, il est important de noter qu'AWS Config n'a pas la capacité d'empêcher activement les changements de configuration ; il surveille et alerte uniquement sur les changements qui se produisent, ce qui en fait un outil puissant pour la gestion de la conformité mais pas un outil préventif.",
        "Other Options": [
            "AWS Config ne prévient pas les changements non autorisés en temps réel ; il surveille et alerte uniquement sur les changements après qu'ils se soient produits.",
            "AWS Config fournit une surveillance quasi en temps réel et ne se limite pas à des instantanés de configuration à des intervalles spécifiques ; il enregistre continuellement les changements de configuration.",
            "AWS Config peut fonctionner à travers plusieurs régions et comptes lorsqu'il est utilisé avec AWS Organizations, permettant une vue plus complète des configurations des ressources à travers toute une organisation."
        ]
    },
    {
        "Question Number": "27",
        "Situation": "Une organisation de recherche scientifique stocke de grands ensembles de données dans Amazon S3 qui sont fréquemment accessibles par des utilisateurs externes. Pour minimiser les coûts, elle souhaite que les utilisateurs externes couvrent le coût d'accès aux données au lieu de l'organisation elle-même.",
        "Question": "Quelle configuration S3 devraient-ils utiliser pour répondre à cette exigence ?",
        "Options": {
            "1": "Activer S3 Transfer Acceleration",
            "2": "Configurer un bucket S3 avec Requester Pays activé",
            "3": "Utiliser S3 Intelligent-Tiering pour la classe de stockage",
            "4": "Activer la réplication inter-régions pour le partage des coûts"
        },
        "Correct Answer": "Configurer un bucket S3 avec Requester Pays activé",
        "Explanation": "Activer Requester Pays sur un bucket S3 permet aux utilisateurs externes qui accèdent aux données d'encourir les coûts associés à leurs requêtes. Cela signifie que lorsque les utilisateurs accèdent aux données, ils seront facturés pour le transfert de données et les requêtes, déplaçant effectivement le fardeau des coûts de l'organisation vers les utilisateurs accédant aux données. Cette configuration est spécifiquement conçue pour des scénarios où les données sont partagées avec des parties externes, ce qui en fait l'option la plus adaptée à l'exigence de l'organisation de minimiser les coûts.",
        "Other Options": [
            "Activer S3 Transfer Acceleration accélère le transfert de fichiers vers et depuis S3, mais cela ne change pas qui paie pour l'accès aux données. Les coûts d'utilisation de Transfer Acceleration sont toujours à la charge du propriétaire du bucket, pas du demandeur.",
            "Bien que S3 Intelligent-Tiering soit une classe de stockage qui déplace automatiquement les données entre deux niveaux d'accès en fonction des modèles d'accès changeants, cela ne traite pas de l'allocation des coûts pour l'accès aux données. L'organisation serait toujours responsable des coûts associés à la récupération des données.",
            "Activer la réplication inter-régions est utilisé pour répliquer automatiquement des données à travers différentes régions AWS pour la redondance et la disponibilité. Cette fonctionnalité n'est pas liée au partage des coûts pour l'accès aux données et entraînerait des coûts supplémentaires pour l'organisation sans répondre à l'exigence d'avoir des utilisateurs externes couvrant les coûts d'accès."
        ]
    },
    {
        "Question Number": "28",
        "Situation": "Une entreprise de services financiers gère une base de données transactionnelle qui connaît des charges de travail variables, y compris des périodes de pointe nécessitant un haut niveau d'IOPS et une capacité de stockage importante. L'entreprise vise à optimiser les coûts tout en garantissant des performances pendant les périodes de pointe.",
        "Question": "Quelle configuration de stockage Amazon RDS le concepteur de solutions devrait-il recommander pour répondre à ces exigences ?",
        "Options": {
            "1": "Provisionner un stockage SSD à usage général (gp3) avec l'évolutivité automatique activée.",
            "2": "Utiliser un stockage magnétique avec des sauvegardes automatisées et des capacités de snapshot.",
            "3": "Provisionner un stockage SSD IOPS provisionnés (io1) avec des IOPS réglés au maximum requis pendant les périodes de pointe.",
            "4": "Mettre en œuvre Amazon Aurora avec son évolutivité de stockage intégrée et ses capacités de haute performance."
        },
        "Correct Answer": "Mettre en œuvre Amazon Aurora avec son évolutivité de stockage intégrée et ses capacités de haute performance.",
        "Explanation": "Amazon Aurora est conçu pour une haute performance et une disponibilité, ce qui en fait un excellent choix pour les applications avec des charges de travail variables. Il évolue automatiquement le stockage jusqu'à 128 To selon les besoins, ce qui est bénéfique pendant les périodes de pointe nécessitant un haut niveau d'IOPS et une capacité de stockage importante. Aurora offre également un débit élevé et une faible latence, garantissant que les performances sont maintenues même sous de lourdes charges, optimisant ainsi les coûts tout en répondant aux exigences de performance.",
        "Other Options": [
            "Provisionner un stockage SSD à usage général (gp3) avec l'évolutivité automatique activée est une bonne option pour les charges de travail générales, mais cela peut ne pas fournir le même niveau de performance et d'évolutivité qu'Amazon Aurora pendant les périodes de pointe, en particulier pour les bases de données transactionnelles qui nécessitent des IOPS élevés et constants.",
            "Utiliser un stockage magnétique avec des sauvegardes automatisées et des capacités de snapshot n'est pas adapté aux exigences de haute performance. Le stockage magnétique est plus lent et ne fournit pas les IOPS nécessaires pour les charges de travail transactionnelles, le rendant inadéquat pour les besoins de performance de pointe.",
            "Provisionner un stockage SSD IOPS provisionnés (io1) avec des IOPS réglés au maximum requis pendant les périodes de pointe peut être efficace, mais cela peut être coûteux et peut ne pas offrir le même niveau d'évolutivité automatique et d'optimisation des performances qu'Amazon Aurora, surtout si les charges de travail sont variables et imprévisibles."
        ]
    },
    {
        "Question Number": "29",
        "Situation": "Une organisation de recherche doit migrer 80 To de données scientifiques de son stockage NFS sur site vers Amazon S3. Les données sont fréquemment mises à jour, et l'organisation souhaite s'assurer que tout changement effectué sur site est synchronisé de manière incrémentielle vers AWS. Ils sont également préoccupés par la saturation de leur bande passante réseau pendant les heures de travail.",
        "Question": "Quelles fonctionnalités d'AWS DataSync le concepteur de solutions devrait-il mettre en avant comme avantages pour cette migration ? (Choisissez deux.)",
        "Options": {
            "1": "Validation des données pendant le transfert pour garantir l'intégrité des données",
            "2": "Réplication multi-région pour la reprise après sinistre",
            "3": "Limiteur de bande passante pour contrôler l'utilisation du réseau pendant les heures de pointe",
            "4": "Support pour la synchronisation en temps réel avec zéro latence",
            "5": "Récupération automatique des erreurs de transit pour un transfert fiable"
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Validation des données pendant le transfert pour garantir l'intégrité des données",
            "Limiteur de bande passante pour contrôler l'utilisation du réseau pendant les heures de pointe"
        ],
        "Explanation": "La validation des données pendant le transfert est une fonctionnalité clé d'AWS DataSync qui garantit l'intégrité des données. Elle vérifie que les données lues à partir de l'emplacement source correspondent aux données écrites à la destination, garantissant ainsi que les données ne sont pas corrompues pendant le transfert. Cela est crucial pour l'organisation de recherche car elle doit s'assurer de l'intégrité de ses données scientifiques. La fonctionnalité de limiteur de bande passante permet à l'organisation de contrôler l'utilisation du réseau pendant les heures de pointe. Cela est important car l'organisation est préoccupée par la saturation de leur bande passante réseau pendant les heures de travail. AWS DataSync permet aux utilisateurs de définir une limite sur la bande passante utilisée par DataSync, empêchant ainsi le réseau de devenir saturé.",
        "Other Options": [
            "La réplication multi-région pour la reprise après sinistre n'est pas une fonctionnalité d'AWS DataSync. C'est une fonctionnalité d'Amazon S3, pas de DataSync. DataSync est utilisé pour transférer des données vers et depuis les services de stockage AWS, il ne fournit pas de réplication multi-région.",
            "Le support pour la synchronisation en temps réel avec zéro latence n'est pas une fonctionnalité d'AWS DataSync. Bien que DataSync prenne en charge des tâches de transfert de données programmées ou à la demande, il ne fournit pas de synchronisation en temps réel avec zéro latence.",
            "La récupération automatique des erreurs de transit pour un transfert fiable n'est pas une fonctionnalité spécifique d'AWS DataSync. Bien que DataSync dispose d'une gestion des erreurs robuste, il ne fournit pas spécifiquement une fonctionnalité de 'récupération automatique des erreurs de transit'."
        ]
    },
    {
        "Question Number": "30",
        "Situation": "Une entreprise développe une application d'analyse de données qui traite de grands volumes de fichiers journaux générés par ses serveurs web. L'application nécessite un accès à faible latence aux données de journal fréquemment consultées et doit prendre en charge des opérations de lecture et d'écriture simultanées à partir de plusieurs instances. De plus, la solution de stockage doit évoluer automatiquement pour s'adapter à l'augmentation des volumes de données sans intervention manuelle.",
        "Question": "Quel service de stockage AWS le concepteur de solutions devrait-il recommander pour répondre à ces exigences ?",
        "Options": {
            "1": "Amazon S3 Standard",
            "2": "Amazon Elastic File System (Amazon EFS)",
            "3": "Amazon Elastic Block Store (Amazon EBS) IOPS provisionnés",
            "4": "Amazon FSx for Windows File Server"
        },
        "Correct Answer": "Amazon Elastic File System (Amazon EFS)",
        "Explanation": "Amazon Elastic File System (EFS) est conçu pour un accès à faible latence et peut prendre en charge des opérations de lecture et d'écriture simultanées à partir de plusieurs instances, ce qui le rend idéal pour les applications nécessitant un accès fréquent aux données. EFS évolue automatiquement à mesure que des données sont ajoutées ou supprimées, ce qui correspond parfaitement à l'exigence d'une solution de stockage qui s'adapte à l'augmentation des volumes de données sans intervention manuelle. De plus, EFS fournit un système de fichiers géré qui peut être accessible depuis plusieurs instances EC2, garantissant une haute disponibilité et durabilité pour les données de journal.",
        "Other Options": [
            "Amazon S3 Standard est un service de stockage d'objets optimisé pour la durabilité et l'évolutivité, mais il n'est pas conçu pour un accès à faible latence ou des opérations de lecture/écriture simultanées comme un système de fichiers. Il est mieux adapté pour stocker de grandes quantités de données non structurées plutôt que pour des applications nécessitant un accès fréquent et une faible latence.",
            "Amazon Elastic Block Store (Amazon EBS) IOPS provisionnés est un service de stockage en bloc qui fournit des performances élevées pour les instances EC2. Cependant, il n'est pas conçu pour un accès simultané à partir de plusieurs instances, car il est généralement attaché à une seule instance EC2 à la fois. Cela le rend moins adapté aux exigences d'opérations de lecture et d'écriture simultanées.",
            "Amazon FSx for Windows File Server est un système de fichiers Windows géré qui fournit un stockage de fichiers partagé. Bien qu'il prenne en charge l'accès simultané, il est plus complexe et peut ne pas évoluer automatiquement de la même manière qu'EFS. Il est également plus adapté aux environnements Windows, ce qui peut ne pas être nécessaire pour l'application décrite."
        ]
    },
    {
        "Question Number": "31",
        "Situation": "Une entreprise déploie une application web et souhaite s'assurer qu'elle peut évoluer dynamiquement tout en offrant une haute disponibilité à travers plusieurs Zones de Disponibilité (AZ). Elle souhaite utiliser un Application Load Balancer (ALB) pour distribuer le trafic de manière efficace.",
        "Question": "Laquelle des configurations suivantes permettrait le mieux à l'entreprise d'atteindre cet objectif ?",
        "Options": {
            "1": "Utiliser un ALB pour distribuer le trafic en fonction du chemin URL et rediriger les demandes vers différents groupes cibles, en veillant à ce que le trafic soit réparti uniformément entre plusieurs instances EC2.",
            "2": "Utiliser un Classic Load Balancer (CLB) pour distribuer le trafic uniquement en fonction de l'adresse IP sans routage par chemin URL.",
            "3": "Utiliser un ALB mais diriger tout le trafic vers une seule instance EC2 pour réduire la complexité et améliorer les performances.",
            "4": "Utiliser un ALB uniquement pour le contenu statique et diriger le trafic de contenu dynamique vers une seule instance EC2 pour maintenir un équilibrage de charge efficace."
        },
        "Correct Answer": "Utiliser un ALB pour distribuer le trafic en fonction du chemin URL et rediriger les demandes vers différents groupes cibles, en veillant à ce que le trafic soit réparti uniformément entre plusieurs instances EC2.",
        "Explanation": "Utiliser un Application Load Balancer (ALB) pour distribuer le trafic en fonction du chemin URL permet des capacités de routage avancées, permettant à l'application de gérer différents types de demandes de manière efficace. En redirigeant les demandes vers différents groupes cibles, l'ALB peut garantir que le trafic est réparti uniformément entre plusieurs instances EC2, ce qui est essentiel pour évoluer dynamiquement et maintenir une haute disponibilité à travers plusieurs Zones de Disponibilité (AZ). Cette configuration prend en charge à la fois l'évolutivité horizontale et l'utilisation efficace des ressources, qui sont critiques pour les applications web modernes.",
        "Other Options": [
            "Utiliser un Classic Load Balancer (CLB) pour distribuer le trafic uniquement en fonction de l'adresse IP sans routage par chemin URL limite la flexibilité et l'efficacité de la gestion du trafic. Les CLB ne prennent pas en charge les fonctionnalités de routage avancées comme le routage basé sur le chemin, ce qui peut entraîner une distribution inégale du trafic et potentiellement surcharger certaines instances tout en sous-utilisant d'autres.",
            "Diriger tout le trafic vers une seule instance EC2 compromet l'objectif d'utiliser un ALB pour l'équilibrage de charge. Cette configuration créerait un point de défaillance unique et annulerait les avantages de haute disponibilité et d'évolutivité, car elle ne tire pas parti de la capacité de l'ALB à distribuer le trafic entre plusieurs instances.",
            "Utiliser un ALB uniquement pour le contenu statique et diriger le trafic de contenu dynamique vers une seule instance EC2 limite les capacités de l'équilibreur de charge et peut entraîner des goulets d'étranglement en matière de performances. Cette approche ne tire pas parti de la capacité de l'ALB à distribuer à la fois du contenu statique et dynamique entre plusieurs instances, ce qui est crucial pour maintenir une haute disponibilité et une évolutivité."
        ]
    },
    {
        "Question Number": "32",
        "Situation": "Une entreprise de fabrication opère dans un endroit éloigné avec une connectivité Internet limitée. Elle a besoin de ressources de calcul locales pour analyser les données des machines et exécuter des applications, mais elle souhaite également avoir la possibilité de synchroniser les données avec AWS lorsque la connectivité est disponible.",
        "Question": "Quelle option de calcul hybride répondrait le mieux à ces exigences ?",
        "Options": {
            "1": "AWS Snowball Edge",
            "2": "AWS Lambda avec des points de terminaison VPC",
            "3": "Instances Amazon EC2 dans la région AWS la plus proche",
            "4": "Amazon EKS avec mise à l'échelle à la demande"
        },
        "Correct Answer": "AWS Snowball Edge",
        "Explanation": "AWS Snowball Edge est conçu pour le calcul en périphérie et le transfert de données dans des environnements avec une connectivité Internet limitée ou inexistante. Il permet aux utilisateurs d'exécuter des applications et d'analyser des données localement sur l'appareil, ce qui est idéal pour l'entreprise de fabrication dans un endroit éloigné. De plus, Snowball Edge prend en charge la synchronisation des données avec AWS lorsque la connectivité est disponible, ce qui en fait un choix parfait pour leurs besoins.",
        "Other Options": [
            "AWS Lambda avec des points de terminaison VPC n'est pas adapté car il nécessite une connexion Internet stable pour accéder aux services AWS. Dans un endroit éloigné avec une connectivité limitée, cette option ne fournirait pas les ressources de calcul locales nécessaires.",
            "Les instances Amazon EC2 dans la région AWS la plus proche ne répondraient pas aux besoins de l'entreprise car elles nécessitent une connectivité Internet constante pour accéder à ces instances. Cette option ne fournit pas de ressources de calcul locales pour l'analyse des données dans une zone éloignée.",
            "Amazon EKS avec mise à l'échelle à la demande dépend également d'une connexion Internet stable pour gérer les clusters Kubernetes dans le cloud. Cette option ne fonctionnerait pas efficacement dans un endroit éloigné avec une connectivité limitée, car elle ne fournit pas de ressources de calcul locales."
        ]
    },
    {
        "Question Number": "33",
        "Situation": "Une entreprise fintech conçoit une nouvelle plateforme d'analyse de données pour traiter de grands volumes de données de transaction en temps réel. Pour garantir des performances élevées, la plateforme doit traiter les données à mesure qu'elles arrivent, avec un délai minimal, et fournir rapidement des informations aux utilisateurs finaux.",
        "Question": "Quel choix architectural répondrait le plus efficacement à ces exigences de haute performance ?",
        "Options": {
            "1": "Traitement par lots des données de transaction à intervalles réguliers",
            "2": "Architecture orientée événements avec streaming de données en temps réel",
            "3": "Stockage de toutes les données de transaction dans une base de données relationnelle traditionnelle",
            "4": "Déploiement de tous les composants de l'application dans une seule zone de disponibilité pour un accès plus rapide"
        },
        "Correct Answer": "Architecture orientée événements avec streaming de données en temps réel",
        "Explanation": "L'architecture orientée événements avec streaming de données en temps réel est le choix le plus efficace pour traiter de grands volumes de données de transaction en temps réel. Cette architecture permet au système de réagir aux données entrantes à mesure qu'elles arrivent, permettant un traitement et une analyse immédiats. Elle prend en charge un débit élevé et une faible latence, qui sont critiques pour fournir des informations en temps opportun aux utilisateurs finaux. En utilisant des technologies telles que les files d'attente de messages et les frameworks de traitement de flux, la plateforme peut gérer efficacement des flux de données continus et fournir des résultats sans délais significatifs.",
        "Other Options": [
            "Le traitement par lots des données de transaction à intervalles réguliers n'est pas adapté aux exigences de haute performance qui nécessitent un traitement en temps réel. Cette approche introduit une latence car les données sont collectées et traitées par lots, ce qui peut retarder les informations et la réactivité.",
            "Le stockage de toutes les données de transaction dans une base de données relationnelle traditionnelle peut fournir un stockage de données structuré, mais il n'est pas optimisé pour le traitement en temps réel. Les bases de données relationnelles nécessitent généralement plus de temps pour les requêtes et peuvent ne pas gérer efficacement les flux de données à haute vitesse, entraînant des goulets d'étranglement en matière de performances.",
            "Le déploiement de tous les composants de l'application dans une seule zone de disponibilité pour un accès plus rapide n'améliore pas intrinsèquement les performances de traitement des données. Bien qu'il puisse réduire la latence pour un accès local, il ne répond pas au besoin de traitement de données en temps réel et pourrait créer un point de défaillance unique, compromettant la fiabilité du système."
        ]
    },
    {
        "Question Number": "34",
        "Situation": "Une entreprise de développement web héberge plusieurs applications sur AWS, avec des modèles de trafic variés. Pour optimiser les coûts, elle souhaite ne payer que pour ce qu'elle utilise et éviter de gérer directement des serveurs.",
        "Question": "Quelle approche répondrait le mieux à ces exigences ?",
        "Options": {
            "1": "Déployer des applications sur Amazon EC2 avec Auto Scaling",
            "2": "Utiliser des conteneurs sur Amazon ECS avec Fargate",
            "3": "Exécuter des applications sur des Instances Réservées",
            "4": "Utiliser Amazon S3 pour le contenu statique et Amazon RDS pour les bases de données"
        },
        "Correct Answer": "Utiliser des conteneurs sur Amazon ECS avec Fargate",
        "Explanation": "Utiliser Amazon ECS avec Fargate permet à l'entreprise de développement web d'exécuter ses applications dans des conteneurs sans avoir à gérer les serveurs sous-jacents. Fargate provisionne et gère automatiquement les ressources de calcul, ce qui signifie que l'entreprise ne paie que pour les ressources qu'elle utilise réellement en fonction des modèles de trafic de ses applications. Cette approche sans serveur est idéale pour optimiser les coûts tout en offrant la flexibilité de s'adapter à la demande.",
        "Other Options": [
            "Déployer des applications sur Amazon EC2 avec Auto Scaling nécessite de gérer des instances EC2, même si elles se mettent à l'échelle automatiquement. Cette approche peut ne pas répondre pleinement à l'exigence d'éviter la gestion directe des serveurs, car l'entreprise devrait tout de même s'occuper de la provision des instances et de leur maintenance.",
            "Exécuter des applications sur des Instances Réservées implique de s'engager sur un type et une taille d'instance spécifiques pour une durée d'un ou trois ans, ce qui ne correspond pas à l'objectif de ne payer que pour ce qu'elle utilise. Cette option est plus rentable pour des charges de travail prévisibles mais ne fournit pas la flexibilité nécessaire pour des modèles de trafic variés.",
            "Utiliser Amazon S3 pour le contenu statique et Amazon RDS pour les bases de données est une bonne approche pour des cas d'utilisation spécifiques, mais cela ne répond pas à l'exigence d'héberger des applications dynamiques. Cette option sépare la gestion du stockage et des bases de données mais ne fournit pas une solution complète pour exécuter des applications avec des modèles de trafic variés."
        ]
    },
    {
        "Question Number": "35",
        "Situation": "Une organisation de recherche doit stocker des données expérimentales dans une base de données pour analyse. Les données sont utilisées activement pendant les trois premiers mois, puis rarement consultées mais conservées pendant cinq ans pour des raisons de conformité. Elle souhaite minimiser les coûts de stockage à long terme.",
        "Question": "Quelle politique de conservation des données serait la plus rentable ?",
        "Options": {
            "1": "Stocker toutes les données dans une base de données haute performance avec des sauvegardes quotidiennes",
            "2": "Archiver les données sur Amazon S3 Glacier après trois mois",
            "3": "Supprimer les données après trois mois pour réduire les coûts de stockage",
            "4": "Déplacer les données vers un niveau de base de données à faible coût après trois mois"
        },
        "Correct Answer": "Archiver les données sur Amazon S3 Glacier après trois mois",
        "Explanation": "Archiver les données sur Amazon S3 Glacier après trois mois est la solution la plus rentable pour le stockage à long terme. S3 Glacier est conçu pour les données qui sont rarement consultées et offre des coûts de stockage significativement inférieurs par rapport aux bases de données haute performance. Étant donné que les données seront rarement consultées après les trois premiers mois mais doivent être conservées pour des raisons de conformité pendant cinq ans, S3 Glacier offre un équilibre approprié entre coût et accessibilité, permettant à l'organisation de minimiser ses dépenses tout en respectant ses exigences de conservation.",
        "Other Options": [
            "Stocker toutes les données dans une base de données haute performance avec des sauvegardes quotidiennes n'est pas rentable pour le stockage à long terme, surtout puisque les données ne seront pas activement utilisées après les trois premiers mois. Les bases de données haute performance sont généralement plus coûteuses, et les sauvegardes quotidiennes ajoutent des coûts supplémentaires qui sont inutiles pour des données qui seront rarement consultées.",
            "Supprimer les données après trois mois peut réduire les coûts de stockage, mais cela ne respecte pas l'exigence de conformité de conserver les données pendant cinq ans. Cette option exposerait l'organisation à des risques juridiques et réglementaires en raison de non-conformité.",
            "Déplacer les données vers un niveau de base de données à faible coût après trois mois est une meilleure option que de les conserver dans une base de données haute performance, mais cela peut encore être plus coûteux que de les archiver sur S3 Glacier. Les niveaux de base de données à faible coût peuvent encore engendrer des coûts plus élevés par rapport aux solutions d'archivage conçues pour un accès peu fréquent, rendant cette option moins optimale pour le stockage à long terme."
        ]
    },
    {
        "Question Number": "36",
        "Situation": "Une organisation doit se conformer à des politiques de conservation des données qui exigent que certains enregistrements soient conservés pendant au moins 7 ans.",
        "Question": "Quelle solution est la PLUS appropriée pour garantir la conformité tout en minimisant les coûts de stockage ?",
        "Options": {
            "1": "Stocker les données dans Amazon S3 Standard avec une politique de cycle de vie S3 pour transférer les données vers S3 Glacier",
            "2": "Stocker les données dans Amazon Elastic File System (EFS) avec le chiffrement activé",
            "3": "Utiliser Amazon RDS avec des sauvegardes automatisées configurées pour conserver les instantanés pendant 7 ans",
            "4": "Stocker les données dans Amazon DynamoDB avec des sauvegardes à la demande"
        },
        "Correct Answer": "Stocker les données dans Amazon S3 Standard avec une politique de cycle de vie S3 pour transférer les données vers S3 Glacier",
        "Explanation": "Cette option est la plus appropriée car elle permet une gestion du stockage rentable. Amazon S3 Standard est adapté aux données fréquemment consultées, tandis que S3 Glacier est conçu pour le stockage d'archives à long terme à un coût inférieur. En mettant en œuvre une politique de cycle de vie S3, l'organisation peut automatiquement transférer les données vers S3 Glacier après une période spécifiée, garantissant ainsi la conformité avec la politique de conservation de 7 ans tout en minimisant les coûts de stockage au fil du temps.",
        "Other Options": [
            "Stocker les données dans Amazon Elastic File System (EFS) avec le chiffrement activé n'est pas le meilleur choix pour le stockage à long terme en raison des coûts plus élevés associés à EFS par rapport à S3 Glacier. EFS est conçu pour un accès à faible latence et est plus coûteux pour le stockage de données qui sont rarement consultées.",
            "Utiliser Amazon RDS avec des sauvegardes automatisées configurées pour conserver les instantanés pendant 7 ans peut être coûteux et peut ne pas être nécessaire pour des données qui ne nécessitent pas les fonctionnalités d'une base de données relationnelle. RDS est généralement utilisé pour des données transactionnelles et peut engendrer des coûts plus élevés pour le stockage à long terme par rapport à S3 Glacier.",
            "Stocker les données dans Amazon DynamoDB avec des sauvegardes à la demande n'est également pas la solution la plus rentable pour la conservation à long terme. Bien que DynamoDB soit excellent pour des applications haute performance, son modèle de tarification pour les sauvegardes peut devenir coûteux au fil du temps, surtout pour des données qui doivent être conservées pendant plusieurs années."
        ]
    },
    {
        "Question Number": "37",
        "Situation": "Une entreprise a un bucket S3 nommé \"secretcatproject\" qui contient des données sensibles. L'entreprise doit permettre l'accès à ce bucket à des utilisateurs spécifiques dans un compte partenaire tout en s'assurant que les données restent sécurisées contre l'accès public.",
        "Question": "Quelle méthode l'entreprise devrait-elle utiliser pour accorder l'accès nécessaire tout en empêchant l'accès non autorisé par des utilisateurs anonymes ?",
        "Options": {
            "1": "Définir la politique du bucket pour permettre l'accès public à tous les utilisateurs afin de simplifier la gestion des accès.",
            "2": "Utiliser une politique de bucket S3 qui spécifie les rôles IAM du compte partenaire comme principaux avec la permission d'accéder au bucket.",
            "3": "Activer \"Bloquer l'accès public\" sur le bucket et utiliser des listes de contrôle d'accès (ACL) pour gérer l'accès pour le compte partenaire.",
            "4": "Attacher une politique IAM directement au bucket pour contrôler l'accès des utilisateurs dans le compte partenaire."
        },
        "Correct Answer": "Utiliser une politique de bucket S3 qui spécifie les rôles IAM du compte partenaire comme principaux avec la permission d'accéder au bucket.",
        "Explanation": "Utiliser une politique de bucket S3 pour spécifier les rôles IAM du compte partenaire comme principaux permet un contrôle précis sur qui peut accéder au bucket. Cette méthode garantit que seuls les utilisateurs désignés du compte partenaire peuvent accéder aux données sensibles, tout en empêchant tout accès public. Les politiques de bucket sont des outils puissants qui peuvent définir des permissions au niveau du bucket et peuvent inclure des conditions pour restreindre davantage l'accès, ce qui les rend idéales pour gérer l'accès aux données sensibles de manière sécurisée.",
        "Other Options": [
            "Définir la politique du bucket pour permettre l'accès public à tous les utilisateurs est très peu sécurisé et contredit l'exigence de garder les données sécurisées contre l'accès public. Cela exposerait les données sensibles à quiconque sur Internet, ce qui n'est pas acceptable.",
            "Bien que l'utilisation d'une politique de bucket S3 qui spécifie les rôles IAM du compte partenaire soit correcte, cette option ne mentionne pas explicitement l'utilisation des rôles IAM comme principaux, ce qui est un aspect crucial pour accorder l'accès de manière sécurisée. Par conséquent, elle est moins précise que la réponse correcte.",
            "Activer 'Bloquer l'accès public' est une bonne pratique pour empêcher l'accès public, mais utiliser des listes de contrôle d'accès (ACL) n'est pas la meilleure méthode pour gérer l'accès dans ce scénario. Les ACL peuvent être plus complexes et moins flexibles que les politiques de bucket, et elles ne fournissent pas le même niveau de clarté et de contrôle sur les permissions que les politiques de bucket."
        ]
    },
    {
        "Question Number": "38",
        "Situation": "Une entreprise utilise Amazon Route 53 pour gérer les enregistrements DNS de son domaine. Elle est préoccupée par les attaques potentielles contre le DNS, telles que le spoofing DNS et les attaques DDoS, et souhaite s'assurer que son infrastructure DNS est sécurisée.",
        "Question": "Quelles actions l'entreprise devrait-elle entreprendre pour améliorer la sécurité de sa configuration Route 53 ?",
        "Options": {
            "1": "Activer DNSSEC (Domain Name System Security Extensions) sur ses zones hébergées Route 53 pour s'assurer que les réponses DNS sont signées cryptographiquement, empêchant ainsi les attaques de spoofing DNS.",
            "2": "Utiliser Route 53 Resolver DNS Firewall pour filtrer les requêtes malveillantes et empêcher le trafic provenant d'adresses IP malveillantes connues, garantissant que seul le trafic légitime atteint ses ressources.",
            "3": "Configurer Route 53 pour n'utiliser que HTTP pour les requêtes DNS afin de simplifier la sécurité, car HTTP est moins sujet aux attaques DDoS par rapport à d'autres protocoles.",
            "4": "Mettre en place des vérifications de santé Route 53 pour surveiller les performances des requêtes DNS mais ne pas activer d'autres fonctionnalités de sécurité, en supposant que la sécurité DNS est couverte par d'autres services AWS."
        },
        "Correct Answer": "Activer DNSSEC (Domain Name System Security Extensions) sur ses zones hébergées Route 53 pour s'assurer que les réponses DNS sont signées cryptographiquement, empêchant ainsi les attaques de spoofing DNS.",
        "Explanation": "Activer DNSSEC sur les zones hébergées Route 53 ajoute une couche de sécurité en permettant aux réponses DNS d'être signées cryptographiquement. Cela garantit que les réponses sont authentiques et n'ont pas été altérées, empêchant efficacement les attaques de spoofing DNS. DNSSEC aide à vérifier l'intégrité des données DNS, rendant beaucoup plus difficile pour les attaquants de rediriger les utilisateurs vers des sites malveillants via des réponses DNS falsifiées.",
        "Other Options": [
            "Utiliser Route 53 Resolver DNS Firewall est une bonne pratique pour filtrer les requêtes malveillantes, mais cela ne traite pas directement le problème du spoofing DNS. Bien que cela puisse aider à atténuer certaines menaces, ce n'est pas aussi efficace que DNSSEC pour garantir l'authenticité des réponses DNS.",
            "Configurer Route 53 pour n'utiliser que HTTP pour les requêtes DNS est incorrect car les requêtes DNS utilisent généralement les protocoles UDP et TCP, et non HTTP. De plus, HTTP ne fournit pas intrinsèquement de sécurité contre les attaques DDoS ; au contraire, cela peut exposer l'infrastructure DNS à plus de risques. Utiliser des protocoles sécurisés comme DNS over HTTPS (DoH) ou DNS over TLS (DoT) serait plus approprié.",
            "Mettre en place des vérifications de santé Route 53 est utile pour surveiller les performances des requêtes DNS, mais cela n'améliore pas la sécurité. Compter uniquement sur les vérifications de santé sans activer d'autres fonctionnalités de sécurité laisse l'infrastructure DNS vulnérable aux attaques telles que le spoofing et les DDoS, qui peuvent être atténuées en mettant en œuvre DNSSEC et d'autres mesures de sécurité."
        ]
    },
    {
        "Question Number": "39",
        "Situation": "Une entreprise souhaite sécuriser les identifiants d'application pour une fonction AWS Lambda. La fonction doit se connecter à une base de données Amazon RDS.",
        "Question": "Quelle approche fournira le moyen le PLUS sécurisé de stocker et de gérer les identifiants de la base de données ?",
        "Options": {
            "1": "Stocker les identifiants de la base de données dans un fichier de configuration en texte clair au sein de la fonction Lambda",
            "2": "Utiliser des rôles IAM AWS avec des permissions pour accéder directement à la base de données",
            "3": "Stocker les identifiants de la base de données dans AWS Secrets Manager et accorder à la fonction Lambda les permissions pour récupérer les secrets",
            "4": "Stocker les identifiants de la base de données dans Amazon S3 avec le chiffrement côté serveur activé"
        },
        "Correct Answer": "Stocker les identifiants de la base de données dans AWS Secrets Manager et accorder à la fonction Lambda les permissions pour récupérer les secrets",
        "Explanation": "Utiliser AWS Secrets Manager pour stocker les identifiants de la base de données est l'approche la plus sécurisée car elle est spécifiquement conçue pour gérer des informations sensibles. Secrets Manager chiffre les identifiants au repos et fournit un contrôle d'accès granulaire via AWS IAM. Cela permet à la fonction Lambda de récupérer les identifiants de manière sécurisée sans les coder en dur dans le code ou les fichiers de configuration de la fonction. De plus, Secrets Manager peut automatiquement faire tourner les identifiants, renforçant ainsi la sécurité.",
        "Other Options": [
            "Stocker les identifiants de la base de données dans un fichier de configuration en texte clair au sein de la fonction Lambda est très peu sécurisé. Cela expose des informations sensibles directement dans le code, les rendant vulnérables à un accès non autorisé si le code est jamais exposé ou partagé.",
            "Utiliser des rôles IAM AWS avec des permissions pour accéder directement à la base de données ne répond pas au besoin de stocker les identifiants de la base de données de manière sécurisée. Bien que les rôles IAM puissent gérer les permissions d'accès, ils ne fournissent pas de mécanisme pour stocker en toute sécurité des informations sensibles comme les identifiants de la base de données.",
            "Stocker les identifiants de la base de données dans Amazon S3 avec le chiffrement côté serveur activé est mieux que le stockage en texte clair, mais ce n'est toujours pas aussi sécurisé que d'utiliser Secrets Manager. S3 n'est pas conçu pour gérer des secrets, et bien que le chiffrement côté serveur protège les données au repos, il ne fournit pas le même niveau de contrôle d'accès et de fonctionnalités de gestion des secrets que Secrets Manager offre."
        ]
    },
    {
        "Question Number": "40",
        "Situation": "Imaginez que vous êtes chargé de construire une connexion hautement résiliente entre votre centre de données sur site et AWS en utilisant AWS Direct Connect pour une application critique.",
        "Question": "Étant donné que Direct Connect est un lien physique sans résilience inhérente, quelle serait la meilleure approche pour garantir la tolérance aux pannes ?",
        "Options": {
            "1": "Déployer deux connexions Direct Connect dans des emplacements séparés (emplacements DX) au sein de la même région AWS pour fournir des chemins redondants en cas de défaillance d'une connexion.",
            "2": "Utiliser une seule connexion Direct Connect à haute bande passante pour réduire le risque de pannes dues à une surcharge.",
            "3": "Mettre en œuvre une connexion Direct Connect associée à une sauvegarde VPN pour maintenir la connectivité si le lien Direct Connect tombe en panne.",
            "4": "Établir des connexions Direct Connect dans différentes régions AWS pour garantir la connectivité si une région rencontre un problème."
        },
        "Correct Answer": "Mettre en œuvre une connexion Direct Connect associée à une sauvegarde VPN pour maintenir la connectivité si le lien Direct Connect tombe en panne.",
        "Explanation": "Mettre en œuvre une connexion Direct Connect associée à une sauvegarde VPN est la meilleure approche pour garantir la tolérance aux pannes car cela fournit un chemin secondaire pour la transmission des données. Si le lien Direct Connect échoue, le VPN peut prendre le relais, assurant ainsi une connectivité continue. Cette approche hybride tire parti de la fiabilité de Direct Connect tout en utilisant également le VPN basé sur Internet comme option de secours, renforçant ainsi la résilience globale.",
        "Other Options": [
            "Déployer deux connexions Direct Connect dans des emplacements séparés au sein de la même région AWS pourrait fournir une redondance, mais cela ne traite pas le potentiel d'une panne régionale ou d'autres problèmes pouvant affecter les deux connexions. De plus, cela peut ne pas être rentable par rapport à une solution hybride avec un VPN.",
            "Utiliser une seule connexion Direct Connect à haute bande passante ne fournit aucune tolérance aux pannes. Si cette connexion tombe en panne, il n'y aurait pas de chemin alternatif pour les données, entraînant un temps d'arrêt potentiel pour l'application critique.",
            "Établir des connexions Direct Connect dans différentes régions AWS pourrait fournir un certain niveau de redondance, mais cela peut introduire de la latence et de la complexité dans la gestion du trafic inter-régional. De plus, cela ne garantit pas que les deux connexions seront disponibles simultanément, surtout s'il y a des problèmes affectant les régions elles-mêmes."
        ]
    },
    {
        "Question Number": "41",
        "Situation": "Une entreprise prévoit de migrer ses applications vers AWS et souhaite comprendre les responsabilités en matière de sécurité qu'elle doit gérer dans le cadre du modèle de responsabilité partagée d'AWS. L'entreprise utilisera Amazon EC2 pour ses serveurs d'application, Amazon RDS pour ses bases de données et Amazon S3 pour le stockage de données.",
        "Question": "Quelles sont les responsabilités que l'entreprise conservera et celles qu'AWS gérera ?",
        "Options": {
            "1": "L'entreprise est responsable de la sécurité de l'infrastructure physique sous-jacente, tandis qu'AWS gère le chiffrement des données au repos.",
            "2": "AWS est responsable de la mise à jour des instances Amazon EC2, tandis que l'entreprise gère le filtrage du trafic réseau à l'aide de groupes de sécurité et de listes de contrôle d'accès réseau.",
            "3": "L'entreprise est responsable de la gestion des configurations de sécurité d'Amazon RDS, y compris la mise à jour du logiciel de base de données, tandis qu'AWS gère la sécurité des centres de données où les instances RDS sont hébergées.",
            "4": "AWS gère la sécurité des données clients stockées dans Amazon S3, tandis que l'entreprise est responsable de la configuration des autorisations d'accès et des paramètres de chiffrement pour ces données."
        },
        "Correct Answer": "L'entreprise est responsable de la gestion des configurations de sécurité d'Amazon RDS, y compris la mise à jour du logiciel de base de données, tandis qu'AWS gère la sécurité des centres de données où les instances RDS sont hébergées.",
        "Explanation": "Dans le modèle de responsabilité partagée d'AWS, AWS est responsable de la sécurité de l'infrastructure cloud, ce qui inclut la sécurité physique des centres de données et le matériel qui exécute les services AWS. Cependant, les clients sont responsables de la sécurité de leurs applications et de leurs données, y compris la gestion des configurations et des mises à jour de services comme Amazon RDS. Cela signifie que, tandis qu'AWS sécurise l'infrastructure sous-jacente, l'entreprise doit s'assurer que ses configurations de base de données sont sécurisées et à jour.",
        "Other Options": [
            "L'entreprise est responsable de la sécurité de ses applications et de ses données, pas de l'infrastructure physique sous-jacente, qui est gérée par AWS. AWS gère le chiffrement des données au repos, mais c'est à l'entreprise de l'implémenter pour ses données.",
            "AWS est responsable de la mise à jour de l'infrastructure sous-jacente, mais l'entreprise doit gérer la mise à jour du système d'exploitation et des applications pour les instances Amazon EC2. L'entreprise est également responsable de la configuration des groupes de sécurité et des listes de contrôle d'accès réseau pour le filtrage du trafic réseau.",
            "AWS gère la sécurité de l'infrastructure qui prend en charge Amazon S3, mais l'entreprise est responsable de la gestion des autorisations d'accès et des paramètres de chiffrement pour les données qu'elle stocke dans S3. AWS ne gère pas directement la sécurité des données des clients ; elle fournit les outils pour que les clients sécurisent leurs données."
        ]
    },
    {
        "Question Number": "42",
        "Situation": "Une entreprise conçoit un Cloud Privé Virtuel (VPC) avec plusieurs sous-réseaux répartis sur plusieurs Zones de Disponibilité (AZ). Elle doit s'assurer que chaque sous-réseau est défini de manière unique, ne chevauche pas d'autres sous-réseaux, et que certaines adresses IP sont réservées pour des fonctions spécifiques au sein de chaque sous-réseau.",
        "Question": "Quelles sont les directives qu'elle devrait suivre pour configurer correctement ses sous-réseaux et éviter les conflits d'IP ? (Choisissez deux.)",
        "Options": {
            "1": "Définir un bloc CIDR unique pour chaque sous-réseau, s'assurer qu'il chevauche d'autres sous-réseaux dans différentes AZ, et utiliser des adresses IP réservées pour les fonctions réseau et de diffusion.",
            "2": "Utiliser le même bloc CIDR pour tous les sous-réseaux au sein du VPC, permettant aux sous-réseaux de communiquer sans problème à travers les AZ, et réserver la première adresse IP de chaque sous-réseau pour DNS.",
            "3": "Attribuer des blocs CIDR non chevauchants à chaque sous-réseau au sein du VPC, avec un sous-réseau par AZ, et réserver des adresses IP spécifiques (comme les adresses réseau et de diffusion) selon les exigences d'AWS.",
            "4": "Allouer un seul grand bloc CIDR pour tous les sous-réseaux au sein du VPC et utiliser le Protocole de Configuration Dynamique d'Hôte (DHCP) pour prévenir les conflits d'IP entre les sous-réseaux.",
            "5": "S'assurer que le bloc CIDR de chaque sous-réseau est un sous-ensemble du bloc CIDR du VPC et planifier les plages IP pour accueillir la croissance future sans chevauchement."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Attribuer des blocs CIDR non chevauchants à chaque sous-réseau au sein du VPC, avec un sous-réseau par AZ, et réserver des adresses IP spécifiques (comme les adresses réseau et de diffusion) selon les exigences d'AWS.",
            "S'assurer que le bloc CIDR de chaque sous-réseau est un sous-ensemble du bloc CIDR du VPC et planifier les plages IP pour accueillir la croissance future sans chevauchement."
        ],
        "Explanation": "Les réponses correctes sont les options 3 et 5. L'option 3 est correcte car attribuer des blocs CIDR non chevauchants à chaque sous-réseau au sein du VPC garantit que chaque sous-réseau est défini de manière unique et ne conflit pas avec d'autres sous-réseaux. Réserver des adresses IP spécifiques pour les fonctions réseau et de diffusion est une pratique standard en conception de réseau. L'option 5 est correcte car le bloc CIDR de chaque sous-réseau doit être un sous-ensemble du bloc CIDR du VPC. Cela garantit que les adresses IP au sein du sous-réseau sont uniques au sein du VPC. Planifier les plages IP pour accueillir la croissance future sans chevauchement est une bonne pratique pour éviter d'éventuels conflits d'IP à l'avenir.",
        "Other Options": [
            "Des blocs CIDR chevauchants entre les sous-réseaux peuvent entraîner des conflits d'IP. De plus, bien qu'il soit vrai que certaines adresses IP doivent être réservées pour les fonctions réseau et de diffusion, cette option suggère incorrectement que des blocs CIDR chevauchants sont une bonne pratique.",
            "Utiliser le même bloc CIDR pour tous les sous-réseaux au sein du VPC peut entraîner des conflits d'IP. Bien qu'il soit vrai que la première adresse IP de chaque sous-réseau est généralement réservée pour DNS, cette option suggère incorrectement que l'utilisation du même bloc CIDR pour tous les sous-réseaux est une bonne pratique.",
            "Allouer un seul grand bloc CIDR pour tous les sous-réseaux au sein du VPC peut entraîner des conflits d'IP. Bien que le DHCP puisse aider à gérer les adresses IP au sein d'un sous-réseau, il ne peut pas prévenir les conflits d'IP entre des sous-réseaux partageant le même bloc CIDR."
        ]
    },
    {
        "Question Number": "43",
        "Situation": "Une entreprise utilise Amazon RDS pour ses besoins en base de données, mais elle s'inquiète de la scalabilité et de la disponibilité de ses connexions à la base de données. Elle souhaite améliorer la gestion des connexions à la base de données et garantir une haute disponibilité pour son application sans surcharger les instances RDS.",
        "Question": "Quel service AWS l'entreprise devrait-elle utiliser pour atteindre cet objectif, et quels en sont les avantages ?",
        "Options": {
            "1": "Utiliser Amazon RDS Proxy pour gérer les connexions à la base de données, en regroupant et en multipliant les connexions pour réduire la charge sur les instances RDS et améliorer la scalabilité.",
            "2": "Utiliser Amazon CloudFront comme proxy pour mettre en cache les requêtes de base de données et réduire la charge sur l'instance RDS.",
            "3": "Utiliser Amazon SQS pour mettre en file d'attente les demandes de base de données et les traiter séquentiellement, garantissant ainsi une haute disponibilité des connexions à la base de données.",
            "4": "Utiliser Amazon ElastiCache pour faire office de proxy et mettre en cache les requêtes de base de données afin de minimiser la charge sur la base de données."
        },
        "Correct Answer": "Utiliser Amazon RDS Proxy pour gérer les connexions à la base de données, en regroupant et en multipliant les connexions pour réduire la charge sur les instances RDS et améliorer la scalabilité.",
        "Explanation": "Amazon RDS Proxy est spécifiquement conçu pour améliorer la gestion des connexions à la base de données pour Amazon RDS. Il fournit un regroupement et une multiplication des connexions, ce qui aide à réduire le nombre de connexions qui doivent être établies avec les instances RDS. Cela améliore non seulement la scalabilité de l'application en permettant plus de connexions simultanées, mais renforce également la disponibilité en gérant les scénarios de basculement de manière transparente. En utilisant RDS Proxy, l'entreprise peut s'assurer que ses connexions à la base de données sont gérées efficacement, réduisant ainsi la charge sur les instances RDS et améliorant la performance globale de l'application.",
        "Other Options": [
            "Utiliser Amazon CloudFront comme proxy pour mettre en cache les requêtes de base de données est incorrect car CloudFront est principalement un réseau de distribution de contenu (CDN) conçu pour mettre en cache du contenu statique et accélérer la livraison des applications web, et non pour gérer les connexions à la base de données ou mettre en cache les requêtes de base de données.",
            "Utiliser Amazon SQS pour mettre en file d'attente les demandes de base de données n'est pas adapté à ce scénario car SQS est un service de mise en file d'attente de messages conçu pour découpler et mettre à l'échelle des microservices, des systèmes distribués et des applications sans serveur. Il ne gère pas directement les connexions à la base de données ni n'améliore leur disponibilité.",
            "Utiliser Amazon ElastiCache pour faire office de proxy et mettre en cache les requêtes de base de données n'est pas la meilleure option dans ce contexte. Bien qu'ElastiCache puisse être utilisé pour mettre en cache des données fréquemment consultées afin de réduire la charge sur la base de données, il ne gère pas les connexions à la base de données ni ne fournit de regroupement de connexions, ce qui est la principale préoccupation pour la scalabilité et la disponibilité dans ce scénario."
        ]
    },
    {
        "Question Number": "44",
        "Situation": "Une entreprise prévoit de migrer son application monolithique vers une architecture conteneurisée pour améliorer la scalabilité, la portabilité et la gestion des ressources. L'entreprise souhaite décomposer l'application monolithique en composants plus petits et plus gérables pour garantir une mise à l'échelle efficace lors des pics de trafic. Elle doit également s'assurer que l'application peut être facilement déplacée entre les environnements et les plateformes.",
        "Question": "Quelle est l'approche la plus efficace pour migrer leur application vers des conteneurs ?",
        "Options": {
            "1": "Conteneuriser chaque composant de l'application en créant des images Docker pour chaque microservice, et déployer les conteneurs sur Amazon ECS ou EKS pour l'orchestration et la gestion.",
            "2": "Migrer l'ensemble de l'application en tant que machine virtuelle vers AWS en utilisant Amazon EC2 et gérer l'application via un groupe de mise à l'échelle automatique EC2.",
            "3": "Utiliser AWS Lambda pour migrer l'application et la décomposer en fonctions sans serveur afin d'éliminer le besoin de conteneurs.",
            "4": "Stocker l'application dans Amazon S3 et utiliser AWS Fargate pour exécuter l'application dans un environnement de conteneurs géré."
        },
        "Correct Answer": "Conteneuriser chaque composant de l'application en créant des images Docker pour chaque microservice, et déployer les conteneurs sur Amazon ECS ou EKS pour l'orchestration et la gestion.",
        "Explanation": "Cette approche est la plus efficace pour migrer une application monolithique vers une architecture conteneurisée car elle permet de décomposer l'application en microservices plus petits et gérables. En créant des images Docker pour chaque composant, l'entreprise peut s'assurer que chaque microservice est déployable, évolutif et maintenable de manière indépendante. L'utilisation d'Amazon ECS (Elastic Container Service) ou d'EKS (Elastic Kubernetes Service) offre des capacités d'orchestration et de gestion robustes, permettant une mise à l'échelle efficace lors des pics de trafic et un déplacement sans faille entre différents environnements et plateformes.",
        "Other Options": [
            "Migrer l'ensemble de l'application en tant que machine virtuelle vers AWS en utilisant Amazon EC2 ne tire pas pleinement parti des avantages de la conteneurisation. Bien que cela permette une mise à l'échelle via des groupes de mise à l'échelle automatique EC2, cela ne décompose pas l'application monolithique en microservices, ce qui est essentiel pour atteindre la scalabilité et la gestion des ressources souhaitées.",
            "Utiliser AWS Lambda pour migrer l'application en fonctions sans serveur n'est pas adapté à toutes les applications, en particulier celles qui ne sont pas conçues pour être sans serveur. Cette approche peut nécessiter une réarchitecture significative de l'application et n'utilise pas de conteneurs, que l'entreprise cherche spécifiquement à mettre en œuvre.",
            "Stocker l'application dans Amazon S3 et utiliser AWS Fargate pour exécuter l'application dans un environnement de conteneurs géré n'est pas une solution complète. Bien que Fargate permette d'exécuter des conteneurs sans gérer de serveurs, le simple stockage de l'application dans S3 ne répond pas au besoin de décomposer l'application monolithique en microservices ou de créer des images Docker, qui sont critiques pour une conteneurisation efficace."
        ]
    },
    {
        "Question Number": "45",
        "Situation": "Une entreprise de services financiers doit sécuriser sa nouvelle application web avec HTTPS pour protéger les données des clients. Elle souhaite une solution qui simplifie l'émission, le déploiement et le renouvellement des certificats SSL/TLS afin d'éviter le risque de certificats expirés causant des temps d'arrêt. Avec la majorité de son infrastructure sur AWS, elle envisage AWS Certificate Manager (ACM) pour gérer les certificats à travers des services comme ELB, CloudFront et API Gateway.",
        "Question": "Comment AWS Certificate Manager (ACM) prend-il en charge la gestion sécurisée et automatisée des certificats SSL/TLS pour les besoins de l'entreprise ?",
        "Options": {
            "1": "ACM permet l'émission et le renouvellement manuels des certificats, offrant un contrôle sur le processus de renouvellement.",
            "2": "ACM émet, déploie et renouvelle automatiquement les certificats, s'intègre aux services AWS et propose des certificats gratuitement lorsqu'ils sont utilisés avec des ressources AWS.",
            "3": "ACM ne prend en charge que les certificats auto-signés, obligeant l'entreprise à gérer les renouvellements et la sécurité séparément.",
            "4": "ACM émet des certificats mais nécessite des outils tiers pour les renouvellements et ne s'intègre pas directement aux services AWS."
        },
        "Correct Answer": "ACM émet, déploie et renouvelle automatiquement les certificats, s'intègre aux services AWS et propose des certificats gratuitement lorsqu'ils sont utilisés avec des ressources AWS.",
        "Explanation": "AWS Certificate Manager (ACM) simplifie la gestion des certificats SSL/TLS en automatisant les processus d'émission, de déploiement et de renouvellement. Cela signifie que l'entreprise de services financiers peut éviter le risque de certificats expirés causant des temps d'arrêt, car ACM gère automatiquement les renouvellements. De plus, ACM s'intègre parfaitement avec divers services AWS tels qu'Elastic Load Balancing (ELB), CloudFront et API Gateway, et il fournit des certificats sans frais lorsqu'ils sont utilisés avec ces services, ce qui en fait une solution rentable pour sécuriser leur application web.",
        "Other Options": [
            "Bien qu'ACM permette l'émission et le renouvellement manuels des certificats, les besoins de l'entreprise sont axés sur l'automatisation pour éviter le risque de certificats expirés. Les processus manuels ne simplifieraient pas leur gestion des certificats comme requis.",
            "ACM ne prend pas en charge uniquement les certificats auto-signés. Il émet principalement des certificats publics qui sont approuvés par les navigateurs et les clients, ce qui est essentiel pour sécuriser les données des clients dans un environnement de production.",
            "ACM ne nécessite pas d'outils tiers pour les renouvellements ; il automatise le processus de renouvellement. De plus, ACM est conçu pour s'intégrer directement aux services AWS, ce qui est une fonctionnalité clé qui répond aux besoins d'infrastructure de l'entreprise."
        ]
    },
    {
        "Question Number": "46",
        "Situation": "",
        "Question": "Quelle fonctionnalité d'Amazon Redshift garantit la durabilité et la résilience des données en fournissant des capacités de sauvegarde et de récupération après sinistre ?",
        "Options": {
            "1": "Enhanced VPC Routing, qui permet un réseau personnalisé au sein d'un VPC.",
            "2": "Slices dans les nœuds de calcul, permettant la distribution des données et des requêtes sur plusieurs nœuds.",
            "3": "Automatic Snapshots to S3, où les données sont sauvegardées toutes les 8 heures ou par incréments de 5 Go vers Amazon S3 pour la durabilité.",
            "4": "Redshift Spectrum, permettant d'interroger directement des données dans S3 sans les charger dans Redshift."
        },
        "Correct Answer": "Automatic Snapshots to S3, où les données sont sauvegardées toutes les 8 heures ou par incréments de 5 Go vers Amazon S3 pour la durabilité.",
        "Explanation": "Amazon Redshift fournit Automatic Snapshots to S3 comme une fonctionnalité clé pour garantir la durabilité et la résilience des données. Cette fonctionnalité sauvegarde automatiquement les données stockées dans Redshift vers Amazon S3 toutes les 8 heures ou chaque fois que la taille des données augmente de 5 Go. Ces instantanés sont cruciaux pour la récupération après sinistre, car ils permettent aux utilisateurs de restaurer leurs données à un état précédent en cas de perte ou de corruption des données, garantissant ainsi l'intégrité et la disponibilité des données.",
        "Other Options": [
            "Enhanced VPC Routing est principalement axé sur l'amélioration de la sécurité réseau et de la gestion du trafic au sein d'un Cloud Privé Virtuel (VPC) et ne se rapporte pas directement à la durabilité des données ou aux capacités de sauvegarde.",
            "Slices dans les nœuds de calcul font référence à la manière dont les données sont distribuées et traitées sur plusieurs nœuds dans un cluster Redshift. Bien que cela améliore les performances et l'évolutivité, cela ne fournit pas de fonctionnalités de sauvegarde ou de récupération après sinistre.",
            "Redshift Spectrum permet aux utilisateurs d'interroger des données directement dans Amazon S3 sans les charger dans Redshift, ce qui est utile pour accéder à de grands ensembles de données mais ne fournit pas de capacités de sauvegarde ou de récupération après sinistre."
        ]
    },
    {
        "Question Number": "47",
        "Situation": "Une entreprise conçoit une architecture réseau sécurisée sur AWS, avec certaines ressources nécessitant un accès public et d'autres restreintes à un accès privé au sein d'un VPC. Ils souhaitent garantir que les données sensibles dans les services privés sont isolées d'Internet tout en permettant un accès sécurisé à certains services publics d'AWS.",
        "Question": "Quelle approche répond le mieux à leurs exigences de sécurité ?",
        "Options": {
            "1": "Déployer toutes les ressources dans la zone publique AWS avec des IP publiques, car cela simplifie l'accès et la gestion de la sécurité.",
            "2": "Placer des instances EC2 sensibles dans un sous-réseau privé au sein de la zone privée AWS, accéder à Internet via une passerelle NAT et utiliser un VPN ou Direct Connect pour un accès sécurisé aux locaux vers le VPC.",
            "3": "Utiliser des sous-réseaux publics pour des services sensibles et restreindre l'accès en appliquant des groupes de sécurité pour contrôler le trafic entrant et sortant.",
            "4": "Configurer des services privés dans des sous-réseaux publics pour accéder directement aux services AWS via Internet sans utiliser l'IGW ou le VPN."
        },
        "Correct Answer": "Placer des instances EC2 sensibles dans un sous-réseau privé au sein de la zone privée AWS, accéder à Internet via une passerelle NAT et utiliser un VPN ou Direct Connect pour un accès sécurisé aux locaux vers le VPC.",
        "Explanation": "Cette approche isole efficacement les données et ressources sensibles en les plaçant dans un sous-réseau privé, qui n'est pas directement accessible depuis Internet. L'utilisation d'une passerelle NAT permet à ces instances privées d'initier un trafic sortant vers Internet (pour des mises à jour, etc.) tout en empêchant le trafic entrant depuis Internet, maintenant ainsi la sécurité. De plus, l'utilisation d'un VPN ou de Direct Connect fournit une connexion sécurisée pour un accès aux locaux vers le VPC, garantissant que les données sensibles restent protégées de l'exposition publique.",
        "Other Options": [
            "Déployer toutes les ressources dans la zone publique AWS avec des IP publiques simplifie l'accès mais expose toutes les ressources à Internet, ce qui constitue un risque de sécurité important pour les données sensibles.",
            "Utiliser des sous-réseaux publics pour des services sensibles contredit l'exigence d'isolement d'Internet. Les sous-réseaux publics sont accessibles depuis Internet, ce qui pourrait entraîner un accès non autorisé aux données sensibles.",
            "Configurer des services privés dans des sous-réseaux publics pour accéder directement aux services AWS via Internet sans utiliser l'IGW ou le VPN n'est pas faisable, car les sous-réseaux publics sont intrinsèquement exposés à Internet, ce qui ne répond pas à l'exigence de sécurité d'isolement des données sensibles."
        ]
    },
    {
        "Question Number": "48",
        "Situation": "Une entreprise déploie une nouvelle application basée sur des microservices sur AWS. Chaque microservice est emballé dans un conteneur Docker. L'application nécessite une orchestration pour gérer les conteneurs, gérer l'évolutivité et garantir une haute disponibilité.",
        "Question": "Quel service AWS le responsable des solutions devrait-il recommander pour l'orchestration des conteneurs ?",
        "Options": {
            "1": "Amazon EC2 Auto Scaling",
            "2": "AWS Lambda",
            "3": "Amazon Elastic Kubernetes Service (EKS)",
            "4": "Amazon Elastic Container Service (ECS)"
        },
        "Correct Answer": "Amazon Elastic Kubernetes Service (EKS)",
        "Explanation": "Amazon Elastic Kubernetes Service (EKS) est un service entièrement géré qui facilite l'exécution de Kubernetes sur AWS sans avoir besoin d'installer et d'exploiter votre propre plan de contrôle ou vos nœuds Kubernetes. Il fournit l'orchestration nécessaire pour gérer les conteneurs Docker, y compris l'évolutivité et la haute disponibilité. EKS est particulièrement bien adapté aux architectures de microservices, car il permet le déploiement, l'évolutivité et la gestion d'applications conteneurisées utilisant Kubernetes, qui est un outil d'orchestration largement adopté dans l'industrie.",
        "Other Options": [
            "Amazon EC2 Auto Scaling est un service qui ajuste automatiquement le nombre d'instances EC2 en réponse à la demande. Bien qu'il puisse aider à évoluer les applications, il ne fournit pas de capacités d'orchestration de conteneurs spécifiquement pour la gestion des conteneurs Docker.",
            "AWS Lambda est un service de calcul sans serveur qui exécute du code en réponse à des événements et gère automatiquement les ressources de calcul nécessaires. Il n'est pas conçu pour l'orchestration de conteneurs et est plus adapté aux architectures basées sur des événements plutôt qu'à la gestion de plusieurs microservices dans des conteneurs.",
            "Amazon Elastic Container Service (ECS) est un autre service d'orchestration de conteneurs fourni par AWS. Bien qu'il soit capable de gérer des conteneurs Docker et puisse gérer l'évolutivité et la haute disponibilité, la question demande spécifiquement une orchestration, et EKS est souvent préféré pour les applications basées sur Kubernetes en raison de ses fonctionnalités étendues et de son soutien communautaire."
        ]
    },
    {
        "Question Number": "49",
        "Situation": "Une plateforme de commerce électronique en forte croissance souhaite gérer efficacement les demandes d'API entrantes alors qu'elle étend ses services backend pour gérer de forts volumes de trafic. Elle veut s'assurer que les demandes sont autorisées, validées, transformées et mises en cache pour des performances optimales. De plus, la plateforme cherche à surveiller les cycles de demande-réponse et à recueillir des métriques détaillées sur l'utilisation.",
        "Question": "Quel service AWS l'entreprise devrait-elle utiliser pour construire une couche de gestion d'API fiable et évolutive, et quelles fonctionnalités spécifiques de ce service soutiendraient ses exigences ?",
        "Options": {
            "1": "Amazon API Gateway, car il peut gérer l'autorisation, le throttling, la mise en cache, et s'intègre parfaitement avec AWS CloudWatch pour la surveillance en temps réel et la collecte de métriques.",
            "2": "AWS Lambda, puisqu'il fournit une capacité de calcul sans serveur et peut être utilisé pour gérer, autoriser et traiter chaque demande de manière indépendante.",
            "3": "Des instances Amazon EC2 avec NGINX pour gérer l'équilibrage de charge et la mise en cache, tout en utilisant des agents CloudWatch pour les métriques et la journalisation.",
            "4": "Amazon S3 avec des URL signées pour restreindre l'accès et CloudFront pour la mise en cache, car cela peut réduire la charge sur les services backend."
        },
        "Correct Answer": "Amazon API Gateway, car il peut gérer l'autorisation, le throttling, la mise en cache, et s'intègre parfaitement avec AWS CloudWatch pour la surveillance en temps réel et la collecte de métriques.",
        "Explanation": "Amazon API Gateway est spécifiquement conçu pour créer, déployer et gérer des API à grande échelle. Il fournit des fonctionnalités intégrées pour l'autorisation (en utilisant AWS IAM, des autorisateurs Lambda ou Amazon Cognito), la validation des demandes, la transformation des demandes et des réponses, et la mise en cache pour améliorer les performances. De plus, il s'intègre avec AWS CloudWatch, permettant à la plateforme de surveiller l'utilisation de l'API, de suivre les cycles de demande-réponse et de recueillir des métriques détaillées, ce qui correspond parfaitement aux exigences de l'entreprise pour gérer efficacement de forts volumes de trafic.",
        "Other Options": [
            "AWS Lambda est un service de calcul sans serveur qui peut traiter des demandes mais ne fournit pas une couche complète de gestion d'API. Bien qu'il puisse gérer l'autorisation et le traitement des demandes, il manque des fonctionnalités intégrées pour la mise en cache, le throttling et la surveillance complète que propose API Gateway.",
            "Des instances Amazon EC2 avec NGINX peuvent être configurées pour gérer l'équilibrage de charge et la mise en cache, mais cette approche nécessite plus de configuration manuelle et de gestion par rapport à API Gateway. De plus, bien que les agents CloudWatch puissent fournir des métriques, ils n'offrent pas le même niveau d'intégration et de facilité d'utilisation pour la gestion d'API qu'API Gateway.",
            "Amazon S3 avec des URL signées et CloudFront peut fournir un accès sécurisé et une mise en cache pour du contenu statique, mais il n'est pas adapté pour gérer des demandes d'API dynamiques. Cette solution manque des fonctionnalités nécessaires pour l'autorisation, la validation des demandes et la surveillance détaillée de l'utilisation de l'API, qui sont critiques pour les besoins de la plateforme de commerce électronique."
        ]
    },
    {
        "Question Number": "50",
        "Situation": "Une entreprise configure un VPC avec plusieurs sous-réseaux pour une application web à plusieurs niveaux. Le sous-réseau public de l'application doit permettre l'accès à Internet, et le sous-réseau privé ne doit autoriser que le trafic sortant vers Internet via une passerelle NAT.",
        "Question": "Quelle est la manière la plus efficace d'assurer un routage correct du trafic entre ces sous-réseaux ?",
        "Options": {
            "1": "Créer une table de routage pour le sous-réseau public avec une route par défaut (0.0.0.0/0) pointant vers une passerelle Internet, et créer une table de routage pour le sous-réseau privé avec une route vers la passerelle NAT.",
            "2": "Créer une seule table de routage pour les sous-réseaux public et privé et ajouter une route vers la passerelle NAT pour l'accès Internet sortant.",
            "3": "Créer une table de routage pour le sous-réseau privé qui pointe directement vers la passerelle Internet pour le trafic externe.",
            "4": "Utiliser Amazon Route 53 pour gérer le routage pour les deux sous-réseaux et acheminer tout le trafic vers un serveur DNS interne."
        },
        "Correct Answer": "Créer une table de routage pour le sous-réseau public avec une route par défaut (0.0.0.0/0) pointant vers une passerelle Internet, et créer une table de routage pour le sous-réseau privé avec une route vers la passerelle NAT.",
        "Explanation": "Cette option configure correctement le routage pour les sous-réseaux public et privé dans un VPC. Le sous-réseau public a besoin d'une table de routage qui dirige tout le trafic sortant (0.0.0.0/0) vers la passerelle Internet, permettant aux instances de ce sous-réseau d'accéder directement à Internet. Le sous-réseau privé, en revanche, ne doit pas avoir d'accès direct à Internet ; il doit plutôt acheminer le trafic sortant vers la passerelle NAT, qui gérera ensuite l'accès à Internet pour les instances dans le sous-réseau privé. Cette configuration garantit que le sous-réseau public peut servir le trafic web tout en maintenant la sécurité du sous-réseau privé.",
        "Other Options": [
            "Créer une seule table de routage pour les sous-réseaux public et privé et ajouter une route vers la passerelle NAT pour l'accès Internet sortant est incorrect car le sous-réseau public doit acheminer le trafic vers la passerelle Internet, et non vers la passerelle NAT. La passerelle NAT est uniquement pour le trafic sortant du sous-réseau privé.",
            "Créer une table de routage pour le sous-réseau privé qui pointe directement vers la passerelle Internet pour le trafic externe est incorrect car les sous-réseaux privés ne doivent pas avoir d'accès direct à Internet. Ils doivent acheminer le trafic via une passerelle NAT pour maintenir la sécurité et éviter une exposition directe à Internet.",
            "Utiliser Amazon Route 53 pour gérer le routage pour les deux sous-réseaux et acheminer tout le trafic vers un serveur DNS interne est incorrect car Route 53 est principalement un service DNS et ne gère pas le routage entre les sous-réseaux dans un VPC. Le routage est géré par des tables de routage, pas par des services DNS."
        ]
    },
    {
        "Question Number": "51",
        "Situation": "Une plateforme de partage de médias permet aux utilisateurs de télécharger des vidéos, qui sont ensuite automatiquement transcodées en plusieurs formats pour une lecture optimale sur différents appareils. La plateforme utilise Google comme fournisseur d'identité pour l'authentification des utilisateurs, et après une connexion réussie, les utilisateurs peuvent télécharger des vidéos dans un bucket Amazon S3. Une série de fonctions Lambda sont déclenchées pour traiter et charger les vidéos, initier des travaux de transcodage et mettre à jour les métadonnées dans une table DynamoDB.",
        "Question": "Quel avantage cette architecture sans serveur offre-t-elle à la plateforme ?",
        "Options": {
            "1": "Traitement vidéo garanti dans une durée fixe",
            "2": "Moins de frais généraux opérationnels avec une gestion minimale des serveurs requise",
            "3": "Intervention manuelle requise pour les tâches de transcodage vidéo",
            "4": "Serveurs dédiés pour gérer un trafic de téléchargement élevé"
        },
        "Correct Answer": "Moins de frais généraux opérationnels avec une gestion minimale des serveurs requise",
        "Explanation": "L'architecture sans serveur permet à la plateforme de tirer parti des services cloud comme AWS Lambda, S3 et DynamoDB sans avoir besoin de gérer les serveurs sous-jacents. Cela entraîne moins de frais généraux opérationnels, car la plateforme peut se concentrer sur le développement et l'évolutivité sans se soucier de la maintenance des serveurs, de la provisionnement ou des problèmes d'évolutivité. L'évolutivité automatique des fonctions Lambda et la nature gérée de S3 et DynamoDB réduisent encore le besoin d'intervention manuelle et de gestion des serveurs.",
        "Other Options": [
            "Le traitement vidéo garanti dans une durée fixe n'est pas un avantage de l'architecture sans serveur. Bien que les fonctions sans serveur puissent évoluer automatiquement, il n'y a aucune garantie sur la durée du traitement, car cela peut varier en fonction de la charge de travail et d'autres facteurs.",
            "L'intervention manuelle requise pour les tâches de transcodage vidéo contredit les avantages de l'architecture sans serveur, qui est conçue pour automatiser les processus. Dans ce scénario, l'utilisation de fonctions Lambda indique que les tâches de transcodage sont automatisées sans intervention manuelle.",
            "Des serveurs dédiés pour gérer un trafic de téléchargement élevé ne sont pas une caractéristique de l'architecture sans serveur. Au lieu de cela, les solutions sans serveur allouent dynamiquement des ressources selon les besoins, éliminant ainsi le besoin de serveurs dédiés et permettant une utilisation plus efficace des ressources."
        ]
    },
    {
        "Question Number": "52",
        "Situation": "Une entreprise de services financiers utilise AWS Key Management Service (KMS) pour gérer les clés de chiffrement des données sensibles des clients stockées sur plusieurs comptes AWS. L'équipe de sécurité doit mettre en œuvre des politiques d'accès pour s'assurer que seules les personnes et les applications autorisées peuvent accéder à des clés spécifiques, tout en empêchant l'accès non autorisé. Pour se conformer aux exigences réglementaires, elle doit également restreindre l'accès en fonction des rôles, des départements et des projets spécifiques.",
        "Question": "Quelles approches devraient-elles adopter pour appliquer efficacement ces politiques d'accès ? (Choisissez deux.)",
        "Options": {
            "1": "Utiliser des politiques basées sur les ressources dans KMS pour définir des autorisations d'accès spécifiques pour chaque clé et attribuer ces autorisations aux utilisateurs IAM, groupes et rôles concernés.",
            "2": "Créer des groupes de sécurité pour chaque département, attacher les clés de chiffrement pertinentes et appliquer des autorisations au niveau du réseau pour contrôler l'accès.",
            "3": "Mettre en œuvre des contrôles d'accès via des politiques de compartiment AWS S3 pour contrôler quels utilisateurs peuvent accéder aux données chiffrées par les clés.",
            "4": "Utiliser des rôles AWS Identity and Access Management (IAM) avec des autorisations de moindre privilège pour différents départements et projets.",
            "5": "S'appuyer sur AWS Shield pour gérer et appliquer les politiques d'accès aux clés de chiffrement sur toutes les ressources."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Utiliser des politiques basées sur les ressources dans KMS pour définir des autorisations d'accès spécifiques pour chaque clé et attribuer ces autorisations aux utilisateurs IAM, groupes et rôles concernés.",
            "Utiliser des rôles AWS Identity and Access Management (IAM) avec des autorisations de moindre privilège pour différents départements et projets."
        ],
        "Explanation": "Les bonnes réponses sont d'utiliser des politiques basées sur les ressources dans KMS et d'utiliser des rôles IAM avec des autorisations de moindre privilège. Les politiques basées sur les ressources dans KMS vous permettent de spécifier qui a accès à quelles clés, et vous pouvez attribuer ces autorisations aux utilisateurs IAM, groupes et rôles concernés. Cela correspond à l'exigence de restreindre l'accès en fonction des rôles, des départements et des projets spécifiques. Les rôles IAM avec des autorisations de moindre privilège sont également une bonne approche car ils garantissent que chaque département et projet n'a accès qu'aux ressources dont ils ont besoin, réduisant ainsi le risque d'accès non autorisé.",
        "Other Options": [
            "Créer des groupes de sécurité pour chaque département et attacher les clés de chiffrement pertinentes n'est pas une approche correcte car les groupes de sécurité dans AWS sont utilisés pour contrôler le trafic entrant et sortant au niveau de l'instance, et non pour gérer l'accès aux clés de chiffrement.",
            "Mettre en œuvre des contrôles d'accès via des politiques de compartiment AWS S3 n'est pas une approche correcte car bien que les politiques de compartiment S3 puissent contrôler qui peut accéder aux données dans un compartiment, elles ne gèrent pas l'accès aux clés de chiffrement KMS.",
            "S'appuyer sur AWS Shield pour gérer et appliquer les politiques d'accès aux clés de chiffrement n'est pas une approche correcte car AWS Shield est un service de protection contre les attaques par déni de service distribué (DDoS), et non un service pour gérer l'accès aux clés de chiffrement."
        ]
    },
    {
        "Question Number": "53",
        "Situation": "Une entreprise a besoin d'une stratégie de reprise après sinistre (DR) pour son application critique qui garantit que le système peut rapidement récupérer d'une défaillance tout en minimisant le temps d'arrêt. L'entreprise souhaite minimiser l'objectif de temps de récupération (RTO) et l'objectif de point de récupération (RPO), et est prête à mettre en œuvre une infrastructure supplémentaire dans une région secondaire pour maintenir l'application en fonctionnement avec un impact minimal sur les performances.",
        "Question": "Quelle stratégie DR l'entreprise devrait-elle mettre en œuvre ?",
        "Options": {
            "1": "Mettre en œuvre une stratégie de basculement actif-actif sur deux régions, en s'assurant que l'application fonctionne dans les deux régions à tout moment et que le trafic est distribué dynamiquement.",
            "2": "Mettre en œuvre une stratégie de veille chaude avec une infrastructure minimale fonctionnant dans la région secondaire, et augmenter les ressources lorsque le basculement est déclenché.",
            "3": "Mettre en œuvre une stratégie de sauvegarde et de restauration, où les données sont sauvegardées sur Amazon S3 et restaurées manuellement en cas de défaillance.",
            "4": "Mettre en œuvre une stratégie de lumière pilote avec une infrastructure minimale fonctionnant dans la région secondaire et ne s'élevant à pleine capacité que si nécessaire."
        },
        "Correct Answer": "Mettre en œuvre une stratégie de basculement actif-actif sur deux régions, en s'assurant que l'application fonctionne dans les deux régions à tout moment et que le trafic est distribué dynamiquement.",
        "Explanation": "Une stratégie de basculement actif-actif permet à l'application de fonctionner simultanément dans deux régions, ce qui signifie que les deux régions peuvent gérer le trafic à tout moment. Cette configuration minimise considérablement le temps d'arrêt, car il n'est pas nécessaire de basculer vers une région secondaire lors d'une défaillance ; l'application est déjà opérationnelle dans les deux emplacements. Cette approche minimise efficacement à la fois l'objectif de temps de récupération (RTO) et l'objectif de point de récupération (RPO) puisque les données sont continuellement synchronisées entre les deux régions, garantissant que les données les plus récentes sont toujours disponibles.",
        "Other Options": [
            "Mettre en œuvre une stratégie de veille chaude implique de maintenir une infrastructure minimale dans la région secondaire, qui peut être augmentée lorsque le basculement se produit. Bien que cela améliore les temps de récupération par rapport à une veille froide, cela nécessite toujours du temps pour augmenter les ressources, ce qui peut entraîner un temps d'arrêt accru et un RTO plus élevé par rapport à une configuration actif-actif.",
            "Une stratégie de sauvegarde et de restauration repose sur des sauvegardes périodiques des données, qui sont stockées dans un service comme Amazon S3. En cas de défaillance, le système doit être restauré manuellement à partir de ces sauvegardes. Cette approche entraîne généralement un RTO et un RPO plus longs, car il peut falloir un temps considérable pour restaurer l'application et les données, ce qui la rend inadaptée aux scénarios où un temps d'arrêt minimal est critique.",
            "Une stratégie de lumière pilote maintient une version minimale de l'application fonctionnant dans la région secondaire, qui peut être augmentée à pleine capacité lors d'un basculement. Bien que cela soit plus efficace qu'une veille froide, cela nécessite toujours du temps pour augmenter, entraînant un RTO plus long par rapport à une stratégie actif-actif, qui est toujours pleinement opérationnelle."
        ]
    },
    {
        "Question Number": "54",
        "Situation": "Une institution financière utilise le chiffrement pour protéger les données des clients stockées sur AWS et doit régulièrement faire tourner les clés de chiffrement et renouveler les certificats SSL pour rester conforme aux exigences réglementaires. L'institution doit automatiser la rotation des clés et le renouvellement des certificats pour éviter toute intervention manuelle et réduire le risque d'erreur humaine.",
        "Question": "Quelle approche l'institution devrait-elle adopter pour gérer efficacement la rotation des clés et le renouvellement des certificats dans son environnement AWS ?",
        "Options": {
            "1": "Activer la rotation automatique des clés dans AWS KMS et utiliser AWS Certificate Manager (ACM) pour renouveler automatiquement les certificats SSL/TLS pour les domaines gérés.",
            "2": "Faire tourner manuellement les clés KMS tous les 90 jours et renouveler les certificats SSL en demandant de nouveaux certificats à un fournisseur tiers.",
            "3": "Utiliser des politiques IAM pour imposer une rotation régulière des clés et un renouvellement des certificats à travers les comptes AWS.",
            "4": "Configurer AWS CloudTrail pour faire automatiquement tourner les clés de chiffrement et renouveler les certificats lorsqu'ils approchent de leur expiration."
        },
        "Correct Answer": "Activer la rotation automatique des clés dans AWS KMS et utiliser AWS Certificate Manager (ACM) pour renouveler automatiquement les certificats SSL/TLS pour les domaines gérés.",
        "Explanation": "Cette approche tire parti des services AWS conçus pour l'automatisation et la conformité. AWS Key Management Service (KMS) permet la rotation automatique des clés, ce qui garantit que les clés de chiffrement sont régulièrement renouvelées sans intervention manuelle, réduisant ainsi le risque d'erreur humaine. De plus, AWS Certificate Manager (ACM) peut renouveler automatiquement les certificats SSL/TLS pour les domaines gérés, rationalisant le processus et garantissant que les certificats sont toujours à jour. Cette combinaison répond efficacement aux besoins de l'institution en matière de conformité et de sécurité.",
        "Other Options": [
            "Faire tourner manuellement les clés KMS tous les 90 jours et renouveler les certificats SSL en demandant de nouveaux certificats à un fournisseur tiers est inefficace et sujet à des erreurs humaines. Cette approche n'automatise pas le processus, ce qui est crucial pour maintenir la conformité et réduire le risque d'oubli.",
            "Utiliser des politiques IAM pour imposer une rotation régulière des clés et un renouvellement des certificats à travers les comptes AWS n'automatise pas directement les processus. Les politiques IAM peuvent imposer des autorisations et des contrôles d'accès mais ne gèrent pas les tâches réelles de rotation ou de renouvellement, rendant cette option moins efficace pour les besoins de l'institution.",
            "Configurer AWS CloudTrail pour faire automatiquement tourner les clés de chiffrement et renouveler les certificats lorsqu'ils approchent de leur expiration est incorrect car CloudTrail est principalement un service de journalisation qui suit les appels API et les activités dans AWS. Il n'a pas la capacité d'effectuer une rotation automatique des clés ou un renouvellement des certificats."
        ]
    },
    {
        "Question Number": "55",
        "Situation": "Une grande entreprise avec plusieurs comptes AWS souhaite rationaliser son processus de facturation et assurer une gestion centralisée de ses comptes AWS. L'organisation souhaite également mettre en place des politiques pour des groupes spécifiques de comptes afin d'imposer des normes de sécurité et de conformité à travers les départements.",
        "Question": "Quelles fonctionnalités AWS devraient-elles utiliser pour atteindre ces exigences, et quel rôle joue le compte de gestion dans cette configuration ? (Choisissez deux.)",
        "Options": {
            "1": "Utiliser AWS Control Tower pour la gestion des comptes, avec le compte de gestion s'occupant de la fédération d'identité.",
            "2": "Mettre en place AWS Organizations avec la facturation consolidée, où le compte de gestion est responsable de la facturation et peut inviter d'autres comptes en tant que comptes membres.",
            "3": "Utiliser AWS Identity and Access Management (IAM) pour gérer les autorisations pour tous les comptes, avec le compte racine s'occupant de la facturation pour chaque compte.",
            "4": "Activer AWS Single Sign-On (SSO) et lier chaque compte, permettant au compte de gestion de gérer l'accès des utilisateurs et la facturation pour tous les comptes liés.",
            "5": "Mettre en œuvre des politiques de contrôle de service AWS (SCP) au sein d'AWS Organizations pour imposer des normes de sécurité et de conformité à travers les comptes membres."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Mettre en place AWS Organizations avec la facturation consolidée, où le compte de gestion est responsable de la facturation et peut inviter d'autres comptes en tant que comptes membres.",
            "Mettre en œuvre des politiques de contrôle de service AWS (SCP) au sein d'AWS Organizations pour imposer des normes de sécurité et de conformité à travers les comptes membres."
        ],
        "Explanation": "La mise en place d'AWS Organizations avec la facturation consolidée permet à l'organisation de centraliser son processus de facturation. Le compte de gestion dans cette configuration est responsable du paiement de tous les frais encourus par les comptes membres, et il peut inviter ou retirer d'autres comptes. Cette fonctionnalité permet également à l'organisation de consolider les méthodes de paiement, rendant le processus de facturation plus efficace. La mise en œuvre des politiques de contrôle de service AWS (SCP) au sein d'AWS Organizations permet à l'organisation de gérer centralement les autorisations à travers plusieurs comptes AWS. Les SCP peuvent être utilisés pour imposer des normes de sécurité et de conformité à tous les comptes membres, ce qui correspond à l'exigence de l'organisation de mettre en place des politiques pour des groupes spécifiques de comptes.",
        "Other Options": [
            "Bien qu'AWS Control Tower puisse être utilisé pour la gestion des comptes, il ne gère pas la fédération d'identité. La fédération d'identité est généralement gérée par AWS Identity and Access Management (IAM) ou AWS Single Sign-On (SSO).",
            "Bien qu'AWS Identity and Access Management (IAM) puisse être utilisé pour gérer les autorisations, le compte racine ne s'occupe pas de la facturation pour chaque compte. La facturation est généralement gérée par le compte de gestion dans AWS Organizations.",
            "Bien qu'AWS Single Sign-On (SSO) puisse être utilisé pour gérer l'accès des utilisateurs, il ne gère pas directement la facturation pour tous les comptes liés. La facturation est généralement gérée par le compte de gestion dans AWS Organizations."
        ]
    },
    {
        "Question Number": "56",
        "Situation": "Une entreprise a besoin d'une connexion réseau sécurisée et dédiée entre son centre de données sur site et son environnement AWS pour un accès à faible latence aux applications critiques. Elle est préoccupée par les risques de sécurité potentiels liés à la transmission de données sensibles sur Internet.",
        "Question": "Quelle solution AWS offre la meilleure option pour une connexion sécurisée et dédiée avec des performances réseau constantes ?",
        "Options": {
            "1": "Mettre en place une passerelle Internet (IGW) et utiliser des groupes de sécurité pour restreindre l'accès aux applications sur site.",
            "2": "Utiliser AWS VPN pour établir un tunnel IPsec sécurisé sur Internet, permettant une communication cryptée.",
            "3": "Mettre en œuvre AWS Direct Connect, offrant un lien réseau privé et dédié entre le centre de données sur site et AWS, avec un support pour le cryptage via une couche VPN supplémentaire si nécessaire.",
            "4": "Déployer un Elastic Load Balancer (ELB) et configurer le routage vers le centre de données sur site pour un accès sécurisé."
        },
        "Correct Answer": "Mettre en œuvre AWS Direct Connect, offrant un lien réseau privé et dédié entre le centre de données sur site et AWS, avec un support pour le cryptage via une couche VPN supplémentaire si nécessaire.",
        "Explanation": "AWS Direct Connect fournit une connexion dédiée et privée entre le centre de données sur site et AWS, ce qui est idéal pour un accès à faible latence aux applications critiques. Cette solution contourne Internet public, réduisant considérablement les risques de sécurité associés à la transmission de données sensibles sur Internet. De plus, Direct Connect peut être combiné avec un VPN pour un cryptage supplémentaire, garantissant que les données restent sécurisées pendant le transit.",
        "Other Options": [
            "Mettre en place une passerelle Internet (IGW) et utiliser des groupes de sécurité ne fournit pas de connexion dédiée ; au lieu de cela, cela permet d'accéder aux ressources AWS via Internet public, ce qui pose des risques de sécurité pour les données sensibles.",
            "Utiliser AWS VPN établit un tunnel IPsec sécurisé sur Internet, qui crypte les données en transit. Cependant, cela repose toujours sur Internet public, ce qui peut introduire de la latence et des vulnérabilités de sécurité potentielles par rapport à une connexion dédiée.",
            "Bien qu'AWS Direct Connect soit le bon choix, l'option de déployer un Elastic Load Balancer (ELB) n'est pas pertinente pour établir une connexion réseau dédiée. Les ELB sont utilisés pour distribuer le trafic d'application entrant sur plusieurs cibles et ne fournissent pas de lien direct entre les centres de données sur site et AWS."
        ]
    },
    {
        "Question Number": "57",
        "Situation": "Votre équipe doit mettre en œuvre un service de messagerie qui permettra à plusieurs applications de lire, traiter et analyser un flux constant de données à haute fréquence, telles que des analyses en temps réel sur les interactions des utilisateurs avec votre application. Le service doit prendre en charge plusieurs consommateurs simultanément, garantissant que chacun peut lire les données dans une fenêtre glissante définie.",
        "Question": "Quel service correspond le mieux à ces exigences, et pourquoi ?",
        "Options": {
            "1": "Amazon SQS, car il offre un découplage pour une communication asynchrone avec persistance des messages.",
            "2": "Amazon Kinesis, car il est optimisé pour l'ingestion de données à grande échelle et plusieurs consommateurs avec une fenêtre glissante pour des analyses en temps réel.",
            "3": "Amazon SNS, car il prend en charge plusieurs consommateurs et la livraison en temps réel vers divers points de terminaison.",
            "4": "AWS Lambda avec S3, pour ingérer et traiter des données en temps réel à l'aide de déclencheurs basés sur des événements."
        },
        "Correct Answer": "Amazon Kinesis, car il est optimisé pour l'ingestion de données à grande échelle et plusieurs consommateurs avec une fenêtre glissante pour des analyses en temps réel.",
        "Explanation": "Amazon Kinesis est spécifiquement conçu pour gérer des flux de données en temps réel et est optimisé pour l'ingestion de données à haut débit. Il permet à plusieurs consommateurs de lire simultanément à partir du même flux de données, ce qui est essentiel pour l'exigence d'avoir plusieurs applications traitant les données en parallèle. De plus, Kinesis prend en charge le concept de fenêtre glissante, permettant aux applications d'analyser les données sur une période de temps spécifiée, ce qui le rend idéal pour des analyses en temps réel sur les interactions des utilisateurs.",
        "Other Options": [
            "Amazon SQS est principalement conçu pour le découplage des microservices et la communication asynchrone. Bien qu'il offre une persistance des messages, il ne prend pas en charge le streaming de données en temps réel ou le concept de fenêtre glissante pour plusieurs consommateurs, ce qui le rend moins adapté au cas d'utilisation décrit.",
            "Amazon SNS est un service de messagerie pub/sub qui permet aux messages d'être envoyés à plusieurs abonnés. Cependant, il ne fournit pas la capacité pour les consommateurs de lire des données dans une fenêtre glissante définie ou de gérer efficacement des flux de données à haute fréquence, ce qui est crucial pour des analyses en temps réel.",
            "AWS Lambda avec S3 n'est pas un service de messagerie mais plutôt un service de calcul sans serveur qui peut traiter des données en réponse à des événements. Bien qu'il puisse être utilisé pour un traitement en temps réel, il repose sur S3 pour le stockage, qui n'est pas optimisé pour des flux de données à haute fréquence ou pour plusieurs consommateurs accédant simultanément aux mêmes données."
        ]
    },
    {
        "Question Number": "58",
        "Situation": "Une entreprise de médias stocke de gros fichiers vidéo sur site et doit migrer ces fichiers vers Amazon S3 pour un stockage évolutif et un accès mondial. La migration doit être automatisée et minimiser le besoin d'intervention manuelle.",
        "Question": "Quel service AWS le solutions architect devrait-il utiliser pour faciliter ce transfert de données ?",
        "Options": {
            "1": "AWS Snowball",
            "2": "AWS DataSync",
            "3": "Amazon S3 Transfer Acceleration",
            "4": "AWS Direct Connect"
        },
        "Correct Answer": "AWS DataSync",
        "Explanation": "AWS DataSync est spécifiquement conçu pour automatiser le transfert de grandes quantités de données entre le stockage sur site et les services AWS comme Amazon S3. Il simplifie et accélère le processus de migration en gérant efficacement le transfert de données, permettant la planification et le suivi des tâches de transfert. Cela minimise l'intervention manuelle et est idéal pour le scénario décrit, où une entreprise de médias doit migrer de gros fichiers vidéo vers S3.",
        "Other Options": [
            "AWS Snowball est une solution de transport de données physique utilisée pour transférer de grandes quantités de données vers AWS lorsque le transfert réseau n'est pas faisable. Bien qu'il puisse être utilisé pour de grandes migrations de données, il nécessite l'expédition physique des appareils et n'est pas automatisé de la même manière que DataSync.",
            "Amazon S3 Transfer Acceleration est une fonctionnalité qui accélère les téléchargements vers S3 en utilisant les emplacements de périphérie distribués mondialement d'Amazon CloudFront. Cependant, cela n'automatise pas le processus de transfert depuis le stockage sur site ; cela ne fait qu'accélérer le transfert une fois qu'il est initié.",
            "AWS Direct Connect fournit une connexion réseau dédiée entre le site et AWS, ce qui peut améliorer la bande passante et réduire la latence pour les transferts de données. Cependant, cela n'automatise pas le processus de migration et est plus adapté aux besoins de transfert de données continus plutôt qu'aux migrations ponctuelles."
        ]
    },
    {
        "Question Number": "59",
        "Situation": "Une entreprise SaaS a plusieurs applications se connectant à une base de données centrale, ce qui entraîne un nombre élevé de connexions pendant les heures de pointe. Ils souhaitent réduire les coûts associés à l'ouverture et à la maintenance des connexions tout en garantissant des performances fluides de la base de données.",
        "Question": "Quelle solution répondrait le mieux à ces exigences ?",
        "Options": {
            "1": "Ajouter plus d'instances de base de données pour répartir les connexions",
            "2": "Mettre en œuvre un proxy de base de données pour regrouper les connexions",
            "3": "Activer le déploiement multi-AZ pour la bascule",
            "4": "Utiliser une couche de mise en cache pour gérer les connexions"
        },
        "Correct Answer": "Mettre en œuvre un proxy de base de données pour regrouper les connexions",
        "Explanation": "Mettre en œuvre un proxy de base de données pour regrouper les connexions est la meilleure solution pour réduire les coûts associés à l'ouverture et à la maintenance des connexions tout en garantissant des performances fluides de la base de données. Un proxy de base de données peut gérer et réutiliser les connexions existantes, ce qui minimise le surcoût d'établissement de nouvelles connexions et réduit le nombre total de connexions à la base de données. Cela conduit à une meilleure utilisation des ressources et peut améliorer considérablement les performances pendant les heures de pointe en permettant aux applications de partager les connexions efficacement.",
        "Other Options": [
            "Ajouter plus d'instances de base de données pour répartir les connexions peut aider à l'équilibrage de la charge, mais ne traite pas directement le problème du nombre élevé de connexions. Cela pourrait entraîner des coûts accrus sans résoudre le problème sous-jacent de la gestion des connexions.",
            "Activer le déploiement multi-AZ pour la bascule est principalement une stratégie pour améliorer la disponibilité et la récupération après sinistre. Bien qu'elle améliore la résilience, elle ne réduit pas directement le nombre de connexions ou les coûts associés à la gestion de ces connexions.",
            "Utiliser une couche de mise en cache pour gérer les connexions peut améliorer les performances en réduisant la charge sur la base de données, mais cela ne traite pas spécifiquement le problème du regroupement des connexions. La mise en cache concerne davantage le stockage des données fréquemment consultées plutôt que la gestion des connexions à la base de données."
        ]
    },
    {
        "Question Number": "60",
        "Situation": "Une entreprise exige que ses utilisateurs AWS mettent en œuvre l'authentification multi-facteurs (MFA) pour une sécurité renforcée. Chaque utilisateur doit utiliser un appareil unique, tel qu'une application mobile, pour générer un code unique basé sur le temps. Le code change périodiquement et est requis chaque fois qu'ils se connectent, en plus de leur nom d'utilisateur et de leur mot de passe.",
        "Question": "Laquelle des déclarations suivantes décrit le MIEUX le bénéfice de sécurité fourni par ce type de configuration MFA ?",
        "Options": {
            "1": "Cela garantit que seuls les utilisateurs qui connaissent le mot de passe du compte root AWS peuvent se connecter.",
            "2": "Cela exige que les utilisateurs s'authentifient avec quelque chose qu'ils savent et quelque chose qu'ils ont, réduisant ainsi la probabilité d'accès non autorisé.",
            "3": "Cela permet aux utilisateurs de contourner le mot de passe s'ils utilisent le bon code MFA.",
            "4": "Cela ne fonctionne que pour les utilisateurs ayant un accès physique à la console de gestion AWS."
        },
        "Correct Answer": "Cela exige que les utilisateurs s'authentifient avec quelque chose qu'ils savent et quelque chose qu'ils ont, réduisant ainsi la probabilité d'accès non autorisé.",
        "Explanation": "Cette déclaration décrit avec précision le bénéfice de sécurité de l'authentification multi-facteurs (MFA). La MFA renforce la sécurité en exigeant deux formes de vérification : quelque chose que l'utilisateur sait (son mot de passe) et quelque chose que l'utilisateur a (le code unique basé sur le temps généré par son appareil mobile). Cette double exigence réduit considérablement le risque d'accès non autorisé, car un attaquant aurait besoin à la fois du mot de passe et d'un accès à l'appareil de l'utilisateur pour entrer.",
        "Other Options": [
            "Cette déclaration est incorrecte car la MFA ne garantit pas spécifiquement que seuls les utilisateurs qui connaissent le mot de passe du compte root AWS peuvent se connecter. La MFA s'applique à tous les utilisateurs et renforce la sécurité au-delà du seul compte root.",
            "Cette déclaration est incorrecte car c'est la bonne réponse. Elle décrit avec précision le bénéfice de sécurité de la MFA, qui combine quelque chose que l'utilisateur sait (mot de passe) et quelque chose qu'il a (code MFA).",
            "Cette déclaration est incorrecte car la MFA ne permet pas aux utilisateurs de contourner le mot de passe. Le code MFA est une couche de sécurité supplémentaire qui doit être fournie en plus du mot de passe pour une authentification réussie."
        ]
    },
    {
        "Question Number": "61",
        "Situation": "Une organisation utilise AWS CloudFormation pour automatiser le déploiement de son infrastructure, y compris des ressources liées à la sécurité telles que les rôles IAM, les groupes de sécurité et les volumes de stockage chiffrés. Ils souhaitent s'assurer que tous les déploiements restent conformes aux politiques de sécurité et empêchent les modifications non autorisées des ressources critiques.",
        "Question": "Quelles sont les meilleures pratiques qu'ils devraient suivre pour sécuriser leurs ressources gérées par CloudFormation ? (Choisissez deux.)",
        "Options": {
            "1": "Activer StackSets avec la détection de dérive CloudFormation pour surveiller les changements dans les ressources déployées et utiliser des politiques IAM pour limiter qui peut modifier les stacks.",
            "2": "Stocker tous les modèles CloudFormation dans S3 sans aucun contrôle de version pour simplifier les mises à jour et les révisions.",
            "3": "Utiliser CloudFormation pour déployer des ressources uniquement dans des sous-réseaux publics, garantissant un accès facile pour tous les utilisateurs de l'organisation.",
            "4": "Mettre en œuvre des règles AWS Config pour valider les stacks CloudFormation par rapport aux politiques de sécurité lors du déploiement.",
            "5": "Éviter d'utiliser des rôles IAM dans les stacks CloudFormation pour simplifier la sécurité, en s'appuyant plutôt sur des paires de clés EC2 pour le contrôle d'accès."
        },
        "Is_Multiple": true,
        "Correct Answer": [
            "Activer StackSets avec la détection de dérive CloudFormation pour surveiller les changements dans les ressources déployées et utiliser des politiques IAM pour limiter qui peut modifier les stacks.",
            "Mettre en œuvre des règles AWS Config pour valider les stacks CloudFormation par rapport aux politiques de sécurité lors du déploiement."
        ],
        "Explanation": "L'activation de StackSets avec la détection de dérive CloudFormation permet à l'organisation de surveiller les changements dans les ressources déployées. Cela aide à identifier toute modification non autorisée des ressources critiques. L'utilisation de politiques IAM pour limiter qui peut modifier les stacks garantit que seules les personnes autorisées peuvent apporter des modifications à l'infrastructure, renforçant ainsi la sécurité. La mise en œuvre de règles AWS Config pour valider les stacks CloudFormation par rapport aux politiques de sécurité lors du déploiement garantit que tous les déploiements restent conformes aux politiques de sécurité de l'organisation. Cela aide à prévenir toute violation de la sécurité.",
        "Other Options": [
            "Stocker tous les modèles CloudFormation dans S3 sans aucun contrôle de version simplifie les mises à jour et les révisions, mais cela ne fournit pas de moyen de suivre les changements ou de revenir à une version précédente si quelque chose ne va pas. Cela peut entraîner des vulnérabilités de sécurité et n'est donc pas une meilleure pratique.",
            "Utiliser CloudFormation pour déployer des ressources uniquement dans des sous-réseaux publics ne garantit pas la sécurité. Bien que cela offre un accès facile pour tous les utilisateurs de l'organisation, cela expose également les ressources à des menaces externes potentielles. Par conséquent, ce n'est pas une meilleure pratique pour sécuriser les ressources gérées par CloudFormation.",
            "Éviter d'utiliser des rôles IAM dans les stacks CloudFormation et s'appuyer plutôt sur des paires de clés EC2 pour le contrôle d'accès simplifie la sécurité, mais cela ne fournit pas le contrôle granulaire que les rôles IAM offrent. Les rôles IAM offrent plus de flexibilité et de contrôle sur qui peut accéder à quelles ressources, ce qui en fait un meilleur choix pour la sécurité. Par conséquent, ce n'est pas une meilleure pratique."
        ]
    },
    {
        "Question Number": "62",
        "Situation": "Une entreprise met en place un nouvel environnement AWS multi-comptes et souhaite garantir une configuration bien architecturée avec des normes de sécurité et de conformité cohérentes dans tous les comptes. Ils souhaitent également des capacités de surveillance et de notification automatisées.",
        "Question": "Quel service AWS devraient-ils utiliser pour rationaliser ce processus, et quelle fonctionnalité spécifique les aidera à faire respecter les règles et les normes dans tous les comptes de cet environnement ?",
        "Options": {
            "1": "Utiliser AWS Organizations et mettre en œuvre des politiques de contrôle de service (SCP) pour l'application des règles entre les comptes.",
            "2": "Utiliser AWS Control Tower pour automatiser la configuration et la gestion de l'environnement multi-comptes, en utilisant des garde-fous pour faire respecter les règles et surveiller la conformité.",
            "3": "Utiliser AWS Config pour chaque compte et configurer manuellement les règles de conformité pour surveiller les ressources.",
            "4": "Utiliser AWS CloudFormation pour déployer un environnement personnalisé et mettre en œuvre des politiques IAM pour gérer les normes de sécurité entre les comptes."
        },
        "Correct Answer": "Utiliser AWS Control Tower pour automatiser la configuration et la gestion de l'environnement multi-comptes, en utilisant des garde-fous pour faire respecter les règles et surveiller la conformité.",
        "Explanation": "AWS Control Tower est spécifiquement conçu pour aider les organisations à configurer et à gouverner un environnement AWS multi-comptes sécurisé basé sur les meilleures pratiques AWS. Il fournit un moyen rationalisé de créer des comptes, d'appliquer la gouvernance et d'assurer la conformité grâce à des garde-fous préconfigurés, qui sont des règles qui aident à faire respecter les politiques entre les comptes. Ce service automatise le processus de configuration et inclut des capacités de surveillance pour garantir que l'environnement respecte les normes définies, ce qui en fait le meilleur choix pour les besoins de l'entreprise.",
        "Other Options": [
            "Utiliser AWS Organizations avec des politiques de contrôle de service (SCP) est une approche valide pour gérer les autorisations entre les comptes, mais cela ne fournit pas les fonctionnalités d'automatisation et de gouvernance complètes qu'offre AWS Control Tower. Les SCP concernent davantage le contrôle d'accès que l'application de la conformité et la surveillance.",
            "AWS Config est un service qui vous permet d'évaluer, d'auditer et d'évaluer les configurations de vos ressources AWS. Bien qu'il puisse aider à la surveillance de la conformité, il nécessite une configuration manuelle des règles pour chaque compte, ce qui ne correspond pas au désir de l'entreprise d'une configuration automatisée et d'une application cohérente entre plusieurs comptes.",
            "AWS CloudFormation est un service de déploiement d'infrastructure en tant que code, qui peut aider à configurer des environnements de manière cohérente. Cependant, il ne fournit pas intrinsèquement de fonctionnalités de gouvernance ou de surveillance de la conformité entre plusieurs comptes. Les politiques IAM peuvent gérer les normes de sécurité, mais elles n'appliquent pas la conformité ni ne fournissent des capacités de surveillance automatisées comme le fait AWS Control Tower."
        ]
    },
    {
        "Question Number": "63",
        "Situation": "Une plateforme de streaming média, MediaStream, s'appuie fortement sur AWS pour soutenir des millions d'utilisateurs simultanés dans le monde entier. Ils sont préoccupés par le risque d'attaques par déni de service distribué (DDoS), qui pourraient perturber leur service de streaming. MediaStream souhaite une solution qui offre une protection DDoS de base ainsi qu'un niveau avancé pour une protection supplémentaire et une visibilité en temps réel sur les événements DDoS. Ils envisagent AWS Shield Standard et AWS Shield Advanced pour sécuriser leur application contre les attaques potentielles sur divers niveaux, y compris les niveaux réseau, transport et application. MediaStream souhaite également une protection contre les implications de coût potentielles si une attaque augmente considérablement leur utilisation d'AWS.",
        "Question": "Laquelle des affirmations suivantes décrit le mieux la différence entre AWS Shield Standard et AWS Shield Advanced en termes de protection et de fonctionnalités fournies pour l'atténuation DDoS sur l'infrastructure AWS ?",
        "Options": {
            "1": "AWS Shield Standard fournit une protection DDoS de base gratuitement à tous les clients AWS, se concentrant principalement sur la protection à la périphérie des services AWS, mais n'inclut pas d'engagement proactif ni de capacités de détection avancées basées sur la santé.",
            "2": "AWS Shield Advanced est un service gratuit disponible pour tous les clients AWS, offrant une protection DDoS améliorée pour les attaques de couche application (L7) et s'intégrant étroitement avec AWS WAF pour fournir une protection contre les coûts et une visibilité en temps réel sur les événements DDoS.",
            "3": "AWS Shield Standard est un service payant qui fournit une protection automatique contre les attaques DDoS de couche application (L7) sur tous les services AWS, y compris un engagement proactif de l'équipe de réponse AWS Shield.",
            "4": "AWS Shield Advanced est automatiquement activé pour toutes les ressources AWS avec des IP Elastic et fournit des configurations de liste de contrôle d'accès Web gratuites, une protection proactive contre les coûts et une réponse immédiate de l'équipe de réponse AWS Shield pour tous les événements DDoS dans toutes les régions AWS."
        },
        "Correct Answer": "AWS Shield Standard fournit une protection DDoS de base gratuitement à tous les clients AWS, se concentrant principalement sur la protection à la périphérie des services AWS, mais n'inclut pas d'engagement proactif ni de capacités de détection avancées basées sur la santé.",
        "Explanation": "AWS Shield Standard est en effet un service gratuit qui offre une protection DDoS de base à tous les clients AWS. Il protège principalement contre les attaques DDoS courantes et les plus fréquemment rencontrées aux niveaux réseau et transport, en se concentrant sur la périphérie des services AWS. Cependant, il ne fournit pas de fonctionnalités avancées telles que l'engagement proactif de l'équipe de réponse AWS Shield ou des capacités de détection avancées basées sur la santé, qui ne sont disponibles qu'avec AWS Shield Advanced. Cela rend l'affirmation précise en décrivant les limitations d'AWS Shield Standard par rapport à AWS Shield Advanced.",
        "Other Options": [
            "AWS Shield Advanced n'est pas un service gratuit ; c'est un service payant qui fournit une protection DDoS améliorée, y compris pour les attaques de couche application (L7) et s'intègre avec AWS WAF. Cependant, il offre une protection contre les coûts et une visibilité en temps réel, mais il n'est pas disponible gratuitement pour tous les clients AWS.",
            "AWS Shield Standard n'est pas un service payant ; il est gratuit et ne fournit pas de protection automatique contre les attaques DDoS de couche application (L7). L'engagement proactif de l'équipe de réponse AWS Shield est une fonctionnalité d'AWS Shield Advanced, pas de Standard.",
            "AWS Shield Advanced n'est pas automatiquement activé pour toutes les ressources AWS avec des IP Elastic ; il doit être souscrit. De plus, bien qu'il fournisse une protection proactive contre les coûts et une réponse immédiate de l'équipe de réponse AWS Shield, il n'offre pas de configurations de liste de contrôle d'accès Web gratuites dans le cadre de son service."
        ]
    },
    {
        "Question Number": "64",
        "Situation": "Une entreprise construit une application basée sur des microservices en utilisant des conteneurs et souhaite gérer et orchestrer ces conteneurs de manière évolutive sur AWS. L'entreprise envisage Amazon ECS et Amazon EKS pour l'orchestration mais n'est pas sûre de quel service conviendrait le mieux à ses besoins. Elle nécessite un contrôle granulaire sur l'orchestration, un réseau personnalisé et la gestion des conteneurs.",
        "Question": "Laquelle des options suivantes décrit le mieux quand l'entreprise devrait utiliser Amazon EKS au lieu d'Amazon ECS ?",
        "Options": {
            "1": "Utilisez Amazon EKS si l'entreprise nécessite des fonctionnalités natives de Kubernetes, telles que l'orchestration personnalisée et des capacités de mise en réseau complexes.",
            "2": "Utilisez Amazon ECS pour tous les besoins d'orchestration de conteneurs, car il est plus simple et plus rentable pour les applications conteneurisées.",
            "3": "Utilisez Amazon EKS si l'entreprise a besoin d'un service de conteneurs entièrement géré qui gère automatiquement la mise à l'échelle et l'équilibrage de charge pour tous les charges de travail conteneurisées.",
            "4": "Utilisez Amazon ECS uniquement si l'entreprise utilise des conteneurs sans serveur, car Amazon EKS ne prend pas en charge les charges de travail sans serveur."
        },
        "Correct Answer": "Utilisez Amazon EKS si l'entreprise nécessite des fonctionnalités natives de Kubernetes, telles que l'orchestration personnalisée et des capacités de mise en réseau complexes.",
        "Explanation": "Amazon EKS (Elastic Kubernetes Service) est conçu pour les utilisateurs qui ont besoin des fonctionnalités avancées et de la flexibilité offertes par Kubernetes. Cela inclut un contrôle granulaire sur l'orchestration, la capacité de mettre en œuvre des solutions de mise en réseau personnalisées et l'utilisation d'outils et d'API natifs de Kubernetes. Si l'entreprise recherche ces capacités, EKS est le meilleur choix par rapport à ECS (Elastic Container Service), qui est plus simple et plus orienté dans son approche de l'orchestration des conteneurs.",
        "Other Options": [
            "Cette option est incorrecte car bien qu'Amazon EKS fournisse des fonctionnalités natives de Kubernetes, il ne s'agit pas uniquement de simplicité ou de rentabilité. ECS est plus simple et peut être plus rentable pour des besoins d'orchestration de conteneurs simples, mais il manque les fonctionnalités avancées qu'EKS offre.",
            "Cette option est trompeuse car bien qu'Amazon EKS offre un service géré, il ne gère pas automatiquement la mise à l'échelle et l'équilibrage de charge pour toutes les charges de travail de la même manière qu'ECS. EKS nécessite plus de configuration et de compréhension de Kubernetes pour atteindre des résultats similaires.",
            "Cette option est incorrecte car Amazon EKS prend en charge les charges de travail sans serveur via AWS Fargate, tout comme Amazon ECS. Par conséquent, l'affirmation selon laquelle EKS ne prend pas en charge les charges de travail sans serveur est fausse."
        ]
    },
    {
        "Question Number": "65",
        "Situation": "Une entreprise conçoit une architecture de Cloud Privé Virtuel (VPC) dans AWS pour soutenir une application multi-niveaux. L'architecture nécessite trois zones de disponibilité (AZ) avec une zone supplémentaire de secours pour une croissance future. Chaque zone de disponibilité aura des sous-réseaux séparés pour les niveaux web, application et base de données, plus un sous-réseau supplémentaire réservé pour une expansion future. L'entreprise souhaite s'assurer qu'il y a suffisamment d'adresses IP pour faire évoluer l'application dans chaque niveau.",
        "Question": "Laquelle des configurations VPC suivantes répondra le mieux à ces exigences tout en permettant une croissance future ?",
        "Options": {
            "1": "Utilisez un bloc CIDR /28 pour le VPC et divisez chaque zone de disponibilité en sous-réseaux /30 pour maximiser l'utilisation des adresses IP dans chaque sous-réseau.",
            "2": "Configurez un bloc CIDR /16 pour le VPC, fournissant un total de 65 536 adresses IP, et assignez des sous-réseaux /20 pour chaque niveau dans chaque zone de disponibilité afin d'assurer un nombre suffisant d'adresses IP par niveau.",
            "3": "Choisissez un bloc CIDR /24 pour le VPC, fournissant un total de 256 adresses IP, et utilisez des sous-réseaux /26 pour chaque niveau dans chaque zone de disponibilité afin d'optimiser l'espace d'adresses.",
            "4": "Configurez un bloc CIDR /22 pour le VPC afin de prendre en charge 1 024 adresses IP, en divisant chaque zone de disponibilité en sous-réseaux /25 pour chaque niveau afin d'équilibrer l'espace d'adresses et la scalabilité."
        },
        "Correct Answer": "Configurez un bloc CIDR /16 pour le VPC, fournissant un total de 65 536 adresses IP, et assignez des sous-réseaux /20 pour chaque niveau dans chaque zone de disponibilité afin d'assurer un nombre suffisant d'adresses IP par niveau.",
        "Explanation": "Choisir un bloc CIDR /16 pour le VPC permet d'avoir un grand espace d'adresses de 65 536 adresses IP, ce qui est plus que suffisant pour l'application multi-niveaux qui nécessite des sous-réseaux séparés pour les niveaux web, application et base de données à travers trois zones de disponibilité, plus un sous-réseau supplémentaire pour une croissance future. En assignant des sous-réseaux /20, chaque sous-réseau aura 4 096 adresses IP (2^(32-20)), offrant amplement de place pour évoluer au sein de chaque niveau tout en permettant encore une expansion future.",
        "Other Options": [
            "Utiliser un bloc CIDR /28 pour le VPC ne fournit que 16 adresses IP, ce qui est beaucoup trop limité pour une application multi-niveaux qui nécessite plusieurs sous-réseaux dans trois zones de disponibilité. Diviser chaque AZ en sous-réseaux /30 réduirait encore le nombre d'adresses IP utilisables, rendant cette option impraticable.",
            "Un bloc CIDR /24 ne fournit que 256 adresses IP, ce qui est insuffisant pour les exigences de l'application. Utiliser des sous-réseaux /26 ne permettrait que 64 adresses IP par sous-réseau, ce qui n'est pas suffisant pour les niveaux web, application et base de données, surtout en tenant compte de la nécessité d'une croissance future.",
            "Configurer un bloc CIDR /22 permet d'avoir 1 024 adresses IP, ce qui est mieux que les options précédentes mais peut encore ne pas fournir suffisamment de place pour évoluer. Diviser chaque zone de disponibilité en sous-réseaux /25 donnerait 128 adresses IP par sous-réseau, ce qui pourrait être limitant pour les niveaux d'application, surtout si l'entreprise prévoit une expansion future."
        ]
    }
]